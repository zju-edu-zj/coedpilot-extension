{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             self._logs_dir,\n <mask>             stdout_file=stdout_file,\n <mask>             stderr_file=stderr_file,\n <mask>             autoscaling_config=self._ray_params.autoscaling_config,\n <mask>             redis_password=self._ray_params.redis_password,\n <mask>             fate_share=self.kernel_fate_share,\n <mask>             max_bytes=self.max_bytes,\n <mask>             backup_count=self.backup_count,\n <mask>             monitor_ip=self._node_ip_address,\n <mask>         )\n </s> Remove dead redis_address and redis_password (#29788)\n\nSome redis_address and redis_password related code are dead given Ray is redisless now.\r\n\r\nSigned-off-by: Jiajun Yao <jeromeyjj@gmail.com> </s> remove             self.redis_address,\n </s> add  </s> remove     if redis_password:\n        command.append(\"--redis-password=\" + redis_password)\n </s> add  </s> remove from ray._private.worker import global_worker  # type: ignore\n </s> add  </s> remove import ray._private.services as services\n </s> add  </s> remove     if redis_address is not None:\n        command.append(f\"--redis-address={redis_address}\")\n </s> add  </s> remove         redis_password: The password of the redis server.\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7b4b88b4082297d3790b9e542090228970708270", "file_name": "python/ray/_private/node.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         f\"--python_worker_command={subprocess.list2cmdline(start_worker_command)}\",  # noqa\n <mask>         f\"--java_worker_command={subprocess.list2cmdline(java_worker_command)}\",  # noqa\n <mask>         f\"--cpp_worker_command={subprocess.list2cmdline(cpp_worker_command)}\",  # noqa\n <mask>         f\"--native_library_path={DEFAULT_NATIVE_LIBRARY_PATH}\",\n <mask>         f\"--redis_password={redis_password or ''}\",\n <mask>         f\"--temp_dir={temp_dir}\",\n <mask>         f\"--session_dir={session_dir}\",\n <mask>         f\"--log_dir={log_dir}\",\n <mask>         f\"--resource_dir={resource_dir}\",\n <mask>         f\"--metrics-agent-port={metrics_agent_port}\",\n </s> Remove dead redis_address and redis_password (#29788)\n\nSome redis_address and redis_password related code are dead given Ray is redisless now.\r\n\r\nSigned-off-by: Jiajun Yao <jeromeyjj@gmail.com> </s> remove     if redis_address is not None:\n        command.append(f\"--redis-address={redis_address}\")\n </s> add  </s> remove import ray._private.services as services\n </s> add  </s> remove from ray._private.worker import global_worker  # type: ignore\n </s> add  </s> remove     if redis_password:\n        command.append(\"--redis-password=\" + redis_password)\n </s> add  </s> remove         redis_password: The password of the redis server.\n </s> add  </s> remove         redis_address: The address that the Redis server is listening on.\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7b4b88b4082297d3790b9e542090228970708270", "file_name": "python/ray/_private/services.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     return plasma_directory, object_store_memory\n <mask> \n <mask> \n <mask> def start_monitor(\n <mask>     redis_address: str,\n <mask>     gcs_address: str,\n <mask>     logs_dir: str,\n <mask>     stdout_file: Optional[str] = None,\n <mask>     stderr_file: Optional[str] = None,\n <mask>     autoscaling_config: Optional[str] = None,\n </s> Remove dead redis_address and redis_password (#29788)\n\nSome redis_address and redis_password related code are dead given Ray is redisless now.\r\n\r\nSigned-off-by: Jiajun Yao <jeromeyjj@gmail.com> </s> remove     redis_password: Optional[str] = None,\n </s> add  </s> remove         redis_address: The address that the Redis server is listening on.\n </s> add  </s> remove             self.redis_address,\n </s> add  </s> remove     if redis_password:\n        command.append(\"--redis-password=\" + redis_password)\n </s> add  </s> remove         redis_password: The password of the redis server.\n </s> add  </s> remove from ray._private.worker import global_worker  # type: ignore\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7b4b88b4082297d3790b9e542090228970708270", "file_name": "python/ray/_private/services.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     logs_dir: str,\n <mask>     stdout_file: Optional[str] = None,\n <mask>     stderr_file: Optional[str] = None,\n <mask>     autoscaling_config: Optional[str] = None,\n <mask>     redis_password: Optional[str] = None,\n <mask>     fate_share: Optional[bool] = None,\n <mask>     max_bytes: int = 0,\n <mask>     backup_count: int = 0,\n <mask>     monitor_ip: Optional[str] = None,\n <mask> ):\n </s> Remove dead redis_address and redis_password (#29788)\n\nSome redis_address and redis_password related code are dead given Ray is redisless now.\r\n\r\nSigned-off-by: Jiajun Yao <jeromeyjj@gmail.com> </s> remove     redis_address: str,\n </s> add  </s> remove     if redis_password:\n        command.append(\"--redis-password=\" + redis_password)\n </s> add  </s> remove             self.redis_address,\n </s> add  </s> remove         redis_password: The password of the redis server.\n </s> add  </s> remove         redis_address: The address that the Redis server is listening on.\n </s> add  </s> remove from ray._private.worker import global_worker  # type: ignore\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7b4b88b4082297d3790b9e542090228970708270", "file_name": "python/ray/_private/services.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> ):\n <mask>     \"\"\"Run a process to monitor the other processes.\n <mask> \n <mask>     Args:\n <mask>         redis_address: The address that the Redis server is listening on.\n <mask>         gcs_address: The address of GCS server.\n <mask>         logs_dir: The path to the log directory.\n <mask>         stdout_file: A file handle opened for writing to redirect stdout to. If\n <mask>             no redirection should happen, then this should be None.\n <mask>         stderr_file: A file handle opened for writing to redirect stderr to. If\n </s> Remove dead redis_address and redis_password (#29788)\n\nSome redis_address and redis_password related code are dead given Ray is redisless now.\r\n\r\nSigned-off-by: Jiajun Yao <jeromeyjj@gmail.com> </s> remove         redis_password: The password of the redis server.\n </s> add  </s> remove         redis_client: the redis client used to query exports.\n </s> add  </s> remove     if redis_address is not None:\n        command.append(f\"--redis-address={redis_address}\")\n </s> add  </s> remove     redis_address: str,\n </s> add  </s> remove     redis_password: Optional[str] = None,\n </s> add  </s> remove from ray._private.worker import global_worker  # type: ignore\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7b4b88b4082297d3790b9e542090228970708270", "file_name": "python/ray/_private/services.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             no redirection should happen, then this should be None.\n <mask>         stderr_file: A file handle opened for writing to redirect stderr to. If\n <mask>             no redirection should happen, then this should be None.\n <mask>         autoscaling_config: path to autoscaling config file.\n <mask>         redis_password: The password of the redis server.\n <mask>         max_bytes: Log rotation parameter. Corresponding to\n <mask>             RotatingFileHandler's maxBytes.\n <mask>         backup_count: Log rotation parameter. Corresponding to\n <mask>             RotatingFileHandler's backupCount.\n <mask>         monitor_ip: IP address of the machine that the monitor will be\n </s> Remove dead redis_address and redis_password (#29788)\n\nSome redis_address and redis_password related code are dead given Ray is redisless now.\r\n\r\nSigned-off-by: Jiajun Yao <jeromeyjj@gmail.com> </s> remove         redis_address: The address that the Redis server is listening on.\n </s> add  </s> remove         redis_client: the redis client used to query exports.\n </s> add  </s> remove     if redis_address is not None:\n        command.append(f\"--redis-address={redis_address}\")\n </s> add  </s> remove     redis_password: Optional[str] = None,\n </s> add  </s> remove     if redis_password:\n        command.append(\"--redis-password=\" + redis_password)\n </s> add  </s> remove     redis_address: str,\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7b4b88b4082297d3790b9e542090228970708270", "file_name": "python/ray/_private/services.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         f\"--logs-dir={logs_dir}\",\n <mask>         f\"--logging-rotate-bytes={max_bytes}\",\n <mask>         f\"--logging-rotate-backup-count={backup_count}\",\n <mask>     ]\n <mask>     if redis_address is not None:\n <mask>         command.append(f\"--redis-address={redis_address}\")\n <mask>     if gcs_address is not None:\n <mask>         command.append(f\"--gcs-address={gcs_address}\")\n <mask>     if stdout_file is None and stderr_file is None:\n <mask>         # If not redirecting logging to files, unset log filename.\n <mask>         # This will cause log records to go to stderr.\n </s> Remove dead redis_address and redis_password (#29788)\n\nSome redis_address and redis_password related code are dead given Ray is redisless now.\r\n\r\nSigned-off-by: Jiajun Yao <jeromeyjj@gmail.com> </s> remove         redis_client: the redis client used to query exports.\n </s> add  </s> remove         redis_address: The address that the Redis server is listening on.\n </s> add  </s> remove     if redis_password:\n        command.append(\"--redis-password=\" + redis_password)\n </s> add  </s> remove         redis_password: The password of the redis server.\n </s> add  </s> remove         f\"--redis_password={redis_password or ''}\",\n </s> add  </s> remove             self.redis_address,\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7b4b88b4082297d3790b9e542090228970708270", "file_name": "python/ray/_private/services.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         )\n <mask>         command.append(f\"--logging-format={logging_format}\")\n <mask>     if autoscaling_config:\n <mask>         command.append(\"--autoscaling-config=\" + str(autoscaling_config))\n <mask>     if redis_password:\n <mask>         command.append(\"--redis-password=\" + redis_password)\n <mask>     if monitor_ip:\n <mask>         command.append(\"--monitor-ip=\" + monitor_ip)\n <mask>     process_info = start_ray_process(\n <mask>         command,\n <mask>         ray_constants.PROCESS_TYPE_MONITOR,\n </s> Remove dead redis_address and redis_password (#29788)\n\nSome redis_address and redis_password related code are dead given Ray is redisless now.\r\n\r\nSigned-off-by: Jiajun Yao <jeromeyjj@gmail.com> </s> remove     if redis_address is not None:\n        command.append(f\"--redis-address={redis_address}\")\n </s> add  </s> remove     redis_password: Optional[str] = None,\n </s> add  </s> remove             self.redis_address,\n </s> add  </s> remove             redis_password=self._ray_params.redis_password,\n </s> add  </s> remove     redis_address: str,\n </s> add  </s> remove         redis_password: The password of the redis server.\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7b4b88b4082297d3790b9e542090228970708270", "file_name": "python/ray/_private/services.py"}
{"docstring_tokens": "keep replace keep replace", "code_tokens": " <mask> import ray\n <mask> import ray._private.services as services\n <mask> from ray._private.usage import usage_lib\n <mask> from ray._private.worker import global_worker  # type: ignore\n </s> Remove dead redis_address and redis_password (#29788)\n\nSome redis_address and redis_password related code are dead given Ray is redisless now.\r\n\r\nSigned-off-by: Jiajun Yao <jeromeyjj@gmail.com> </s> remove         f\"--redis_password={redis_password or ''}\",\n </s> add  </s> remove     if redis_address is not None:\n        command.append(f\"--redis-address={redis_address}\")\n </s> add  </s> remove     if redis_password:\n        command.append(\"--redis-password=\" + redis_password)\n </s> add  </s> remove         redis_password: The password of the redis server.\n </s> add  </s> remove         redis_address: The address that the Redis server is listening on.\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7b4b88b4082297d3790b9e542090228970708270", "file_name": "python/ray/autoscaler/_private/commands.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> ) -> \"tf.keras.optimizers.Optimizer\":\n <mask>     if policy.config[\"framework\"] in [\"tf2\", \"tfe\"]:\n <mask>         return tf.keras.optimizers.RMSprop(\n <mask>             learning_rate=policy.cur_lr,\n <mask>             epsilon=0.00001,\n <mask>             decay=0.95,\n <mask>             momentum=0.0,\n <mask>             centered=True,\n <mask>         )\n <mask>     else:\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove             epsilon=0.00001,\n </s> add             epsilon=config[\"rmsprop_epsilon\"], </s> remove         slate_q_values = self.get_per_slate_q_values(user, doc)\n\n        if target_slate_q_values is not None:\n            assert self.double_q\n            max_values, best_target_indices = torch.max(target_slate_q_values, dim=1)\n            best_slate_q_value = torch.gather(\n                slate_q_values, 1, best_target_indices.unsqueeze(1)\n            ).squeeze(1)\n            # slates_selected.shape: [batch_size, slate_size]\n            slates_selected = self.slates[best_target_indices]\n        else:\n            # Find the slate that maximizes q value.\n            best_slate_q_value, max_idx = torch.max(slate_q_values, dim=1)\n            # slates_selected.shape: [batch_size, slate_size]\n            slates_selected = self.slates[max_idx]\n\n        return slates_selected, best_slate_q_value, slate_q_values\n\n    def get_per_slate_q_values(self, user, doc):\n        # Compute item scores (proportional to click probabilities)\n        # raw_scores.shape=[batch_size, num_docs+1]\n        raw_scores = self.choice_model(user, doc)\n        # max_raw_scores.shape=[batch_size, 1]\n        max_raw_scores, _ = torch.max(raw_scores, dim=1, keepdim=True)\n        # Deduct scores by max_scores to avoid value explosion.\n        scores = torch.exp(raw_scores - max_raw_scores)\n        scores_doc = scores[:, :-1]  # shape=[batch_size, num_docs]\n        scores_no_click = scores[:, [-1]]  # shape=[batch_size, 1]\n\n        # Calculate the item-wise Q values.\n        # q_values.shape=[batch_size, num_docs+1]\n        q_values = self.q_model(user, doc)\n        q_values_doc = q_values[:, :-1]  # shape=[batch_size, num_docs]\n        q_values_no_click = q_values[:, [-1]]  # shape=[batch_size, 1]\n\n        # Calculate per-slate Q values.\n        batch_size, _ = q_values_doc.shape\n        # Move pre-stored slate combinations to GPU, if necessary.\n        if self.slates.device != q_values_doc.device:\n            self.slates = self.slates.to(q_values.device)\n        # slate_decomp_q_values.shape: [batch_size, num_slates, slate_size]\n        slate_decomp_q_values = torch.gather(\n            # input.shape: [batch_size, num_slates, num_docs]\n            input=q_values_doc.unsqueeze(1).expand(-1, self.num_slates, -1),\n            dim=2,\n            # index.shape: [batch_size, num_slates, slate_size]\n            index=self.slates.unsqueeze(0).expand(batch_size, -1, -1),\n        )\n        # slate_scores.shape: [batch_size, num_slates, slate_size]\n        slate_scores = torch.gather(\n            # input.shape: [batch_size, num_slates, num_docs]\n            input=scores_doc.unsqueeze(1).expand(-1, self.num_slates, -1),\n            dim=2,\n            # index.shape: [batch_size, num_slates, slate_size]\n            index=self.slates.unsqueeze(0).expand(batch_size, -1, -1),\n        )\n\n        # slate_q_values.shape: [batch_size, num_slates]\n        slate_q_values = (\n            (slate_decomp_q_values * slate_scores).sum(dim=2)\n            + (q_values_no_click * scores_no_click)\n        ) / (slate_scores.sum(dim=2) + scores_no_click)\n        return slate_q_values\n\n    def forward(\n        self,\n        input_dict: Dict[str, TensorType],\n        state: List[TensorType],\n        seq_lens: TensorType,\n    ) -> Tuple[TensorType, List[TensorType]]:\n        # user.shape: [batch_size, embedding_size]\n        user = input_dict[SampleBatch.OBS][\"user\"]\n        # doc.shape: [batch_size, num_docs, embedding_size]\n        doc = torch.cat(\n            [val.unsqueeze(1) for val in input_dict[SampleBatch.OBS][\"doc\"].values()], 1\n        )\n\n        slates_selected, _, _ = self.choose_slate(user, doc)\n\n        state_out = []\n        return slates_selected, state_out\n </s> add         return self.q_model(user, docs) </s> remove         # Similar to Google's SlateQ implementation in RecSim, we force the\n        # Q-values to zeros if there are no clicks.\n        # See https://arxiv.org/abs/1905.12767 for details.\n        x_no_click = torch.zeros((batch_size, 1), device=x.device)\n\n        return torch.cat([x.view((batch_size, num_docs)), x_no_click], dim=1)\n </s> add         return torch.cat(q_outs, dim=1) </s> remove         batch_size, num_docs, embedding_size = doc.shape\n        doc_flat = doc.view((batch_size * num_docs, embedding_size))\n\n        # Concat everything.\n        # No user features.\n        if user.shape[-1] == 0:\n            x = doc_flat\n        # User features, repeat user embeddings n times (n=num docs).\n        else:\n            user_repeated = user.repeat(num_docs, 1)\n            x = torch.cat([user_repeated, doc_flat], dim=1)\n\n        x = self.layers(x)\n </s> add         q_outs = []\n        for i in range(self.num_candidates):\n            user_cat_doc = torch.cat([user, docs[i]], dim=1)\n            q_outs.append(self.q_nets[i](user_cat_doc)) </s> remove     def forward(self, user: TensorType, doc: TensorType) -> TensorType:\n        \"\"\"Evaluate the user-doc Q model\n </s> add         self.q_nets = nn.ModuleList()\n        for i in range(self.num_candidates):\n            layers = nn.Sequential()\n            ins = 2 * self.embedding_size\n            for j, h in enumerate(fcnet_hiddens_per_candidate):\n                layers.add_module(\n                    f\"q_layer_{i}_{j}\",\n                    SlimFC(in_size=ins, out_size=h, activation_fn=\"relu\"),\n                )\n                ins = h\n            layers.add_module(f\"q_out_{i}\", SlimFC(ins, 1, activation_fn=None))\n\n            self.q_nets.append(layers)\n\n    def forward(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors. </s> remove         self.q_model = QValueModel(user_embedding_size + doc_embedding_size, q_hiddens)\n        self.slate_size = len(action_space.nvec)\n        self.double_q = double_q\n\n        self.num_docs = num_docs\n        self.indices = torch.arange(\n            self.num_docs, dtype=torch.long\n        )  # , device=doc.device)\n        # slates.shape = [num_slates, slate_size]\n        self.slates = torch.combinations(self.indices, r=self.slate_size)\n        self.num_slates, _ = self.slates.shape\n\n    def choose_slate(\n        self,\n        user: TensorType,\n        doc: TensorType,\n        target_slate_q_values: Optional[TensorType] = None,\n    ) -> Tuple[TensorType, TensorType]:\n        \"\"\"Build a slate by selecting from candidate documents\n </s> add         self.q_model = QValueModel(self.obs_space, fcnet_hiddens_per_candidate)\n\n    def get_q_values(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors.", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_tf_policy.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         )\n <mask>     else:\n <mask>         return tf1.train.RMSPropOptimizer(\n <mask>             learning_rate=policy.cur_lr,\n <mask>             epsilon=0.00001,\n <mask>             decay=0.95,\n <mask>             momentum=0.0,\n <mask>             centered=True,\n <mask>         )\n <mask> \n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove             epsilon=0.00001,\n </s> add             epsilon=config[\"rmsprop_epsilon\"], </s> remove         slate_q_values = self.get_per_slate_q_values(user, doc)\n\n        if target_slate_q_values is not None:\n            assert self.double_q\n            max_values, best_target_indices = torch.max(target_slate_q_values, dim=1)\n            best_slate_q_value = torch.gather(\n                slate_q_values, 1, best_target_indices.unsqueeze(1)\n            ).squeeze(1)\n            # slates_selected.shape: [batch_size, slate_size]\n            slates_selected = self.slates[best_target_indices]\n        else:\n            # Find the slate that maximizes q value.\n            best_slate_q_value, max_idx = torch.max(slate_q_values, dim=1)\n            # slates_selected.shape: [batch_size, slate_size]\n            slates_selected = self.slates[max_idx]\n\n        return slates_selected, best_slate_q_value, slate_q_values\n\n    def get_per_slate_q_values(self, user, doc):\n        # Compute item scores (proportional to click probabilities)\n        # raw_scores.shape=[batch_size, num_docs+1]\n        raw_scores = self.choice_model(user, doc)\n        # max_raw_scores.shape=[batch_size, 1]\n        max_raw_scores, _ = torch.max(raw_scores, dim=1, keepdim=True)\n        # Deduct scores by max_scores to avoid value explosion.\n        scores = torch.exp(raw_scores - max_raw_scores)\n        scores_doc = scores[:, :-1]  # shape=[batch_size, num_docs]\n        scores_no_click = scores[:, [-1]]  # shape=[batch_size, 1]\n\n        # Calculate the item-wise Q values.\n        # q_values.shape=[batch_size, num_docs+1]\n        q_values = self.q_model(user, doc)\n        q_values_doc = q_values[:, :-1]  # shape=[batch_size, num_docs]\n        q_values_no_click = q_values[:, [-1]]  # shape=[batch_size, 1]\n\n        # Calculate per-slate Q values.\n        batch_size, _ = q_values_doc.shape\n        # Move pre-stored slate combinations to GPU, if necessary.\n        if self.slates.device != q_values_doc.device:\n            self.slates = self.slates.to(q_values.device)\n        # slate_decomp_q_values.shape: [batch_size, num_slates, slate_size]\n        slate_decomp_q_values = torch.gather(\n            # input.shape: [batch_size, num_slates, num_docs]\n            input=q_values_doc.unsqueeze(1).expand(-1, self.num_slates, -1),\n            dim=2,\n            # index.shape: [batch_size, num_slates, slate_size]\n            index=self.slates.unsqueeze(0).expand(batch_size, -1, -1),\n        )\n        # slate_scores.shape: [batch_size, num_slates, slate_size]\n        slate_scores = torch.gather(\n            # input.shape: [batch_size, num_slates, num_docs]\n            input=scores_doc.unsqueeze(1).expand(-1, self.num_slates, -1),\n            dim=2,\n            # index.shape: [batch_size, num_slates, slate_size]\n            index=self.slates.unsqueeze(0).expand(batch_size, -1, -1),\n        )\n\n        # slate_q_values.shape: [batch_size, num_slates]\n        slate_q_values = (\n            (slate_decomp_q_values * slate_scores).sum(dim=2)\n            + (q_values_no_click * scores_no_click)\n        ) / (slate_scores.sum(dim=2) + scores_no_click)\n        return slate_q_values\n\n    def forward(\n        self,\n        input_dict: Dict[str, TensorType],\n        state: List[TensorType],\n        seq_lens: TensorType,\n    ) -> Tuple[TensorType, List[TensorType]]:\n        # user.shape: [batch_size, embedding_size]\n        user = input_dict[SampleBatch.OBS][\"user\"]\n        # doc.shape: [batch_size, num_docs, embedding_size]\n        doc = torch.cat(\n            [val.unsqueeze(1) for val in input_dict[SampleBatch.OBS][\"doc\"].values()], 1\n        )\n\n        slates_selected, _, _ = self.choose_slate(user, doc)\n\n        state_out = []\n        return slates_selected, state_out\n </s> add         return self.q_model(user, docs) </s> remove         # Similar to Google's SlateQ implementation in RecSim, we force the\n        # Q-values to zeros if there are no clicks.\n        # See https://arxiv.org/abs/1905.12767 for details.\n        x_no_click = torch.zeros((batch_size, 1), device=x.device)\n\n        return torch.cat([x.view((batch_size, num_docs)), x_no_click], dim=1)\n </s> add         return torch.cat(q_outs, dim=1) </s> add         num_outputs=action_space.nvec[0], </s> add         self.num_outputs = num_outputs\n </s> remove         user_embedding_size=obs_space.original_space[\"user\"].shape[0],\n        doc_embedding_size=obs_space.original_space[\"doc\"][\"0\"].shape[0],\n        num_docs=len(obs_space.original_space[\"doc\"].spaces),\n        q_hiddens=config[\"hiddens\"],\n        double_q=config[\"double_q\"],\n </s> add         fcnet_hiddens_per_candidate=config[\"fcnet_hiddens_per_candidate\"],", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_tf_policy.py"}
{"docstring_tokens": "replace keep keep keep keep keep", "code_tokens": " <mask> from typing import Dict, List, Optional, Sequence, Tuple\n <mask> \n <mask> import gym\n <mask> from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n <mask> from ray.rllib.policy.sample_batch import SampleBatch\n <mask> from ray.rllib.utils.framework import try_import_torch\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove from ray.rllib.policy.sample_batch import SampleBatch\n </s> add  </s> add from ray.rllib.models.torch.misc import SlimFC </s> remove import time\nfrom typing import Dict, List, Tuple, Type\n </s> add from typing import Dict, Tuple, Type </s> remove from ray.rllib.agents.slateq.slateq_torch_model import SlateQModel\nfrom ray.rllib.models.modelv2 import ModelV2, restore_original_dimensions\n </s> add from ray.rllib.agents.slateq.slateq_torch_model import SlateQTorchModel\nfrom ray.rllib.models.modelv2 import ModelV2 </s> remove             q_hiddens: The list of hidden layer sizes for the QValueModel.\n </s> add             fcnet_hiddens_per_candidate: List of layer-sizes for each(!) of the\n                candidate documents. </s> remove             user: User embeddings of shape (batch_size,\n                embedding_size).\n            doc: Doc embeddings of shape (batch_size, num_docs,\n                embedding_size).\n            double_q: Whether to apply double-Q correction on the target\n                term.\n </s> add             user: [B x u] where u=embedding of user features.\n            docs: List[[B x d]] where d=embedding of doc features. Each item in the\n                list represents one document candidate.", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask> \n <mask> import gym\n <mask> from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n <mask> from ray.rllib.utils.framework import try_import_torch\n <mask> from ray.rllib.utils.typing import ModelConfigDict, TensorType\n <mask> \n <mask> torch, nn = try_import_torch()\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove from ray.rllib.policy.sample_batch import SampleBatch\n </s> add  </s> remove from typing import Dict, List, Optional, Sequence, Tuple\n </s> add from typing import List, Sequence </s> remove import time\nfrom typing import Dict, List, Tuple, Type\n </s> add from typing import Dict, Tuple, Type </s> remove from ray.rllib.agents.slateq.slateq_torch_model import SlateQModel\nfrom ray.rllib.models.modelv2 import ModelV2, restore_original_dimensions\n </s> add from ray.rllib.agents.slateq.slateq_torch_model import SlateQTorchModel\nfrom ray.rllib.models.modelv2 import ModelV2 </s> remove         self.q_model = QValueModel(user_embedding_size + doc_embedding_size, q_hiddens)\n        self.slate_size = len(action_space.nvec)\n        self.double_q = double_q\n\n        self.num_docs = num_docs\n        self.indices = torch.arange(\n            self.num_docs, dtype=torch.long\n        )  # , device=doc.device)\n        # slates.shape = [num_slates, slate_size]\n        self.slates = torch.combinations(self.indices, r=self.slate_size)\n        self.num_slates, _ = self.slates.shape\n\n    def choose_slate(\n        self,\n        user: TensorType,\n        doc: TensorType,\n        target_slate_q_values: Optional[TensorType] = None,\n    ) -> Tuple[TensorType, TensorType]:\n        \"\"\"Build a slate by selecting from candidate documents\n </s> add         self.q_model = QValueModel(self.obs_space, fcnet_hiddens_per_candidate)\n\n    def get_q_values(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors. </s> add         num_outputs: int,", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> from typing import Dict, List, Optional, Sequence, Tuple\n <mask> \n <mask> import gym\n <mask> from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n <mask> from ray.rllib.policy.sample_batch import SampleBatch\n <mask> from ray.rllib.utils.framework import try_import_torch\n <mask> from ray.rllib.utils.typing import ModelConfigDict, TensorType\n <mask> \n <mask> torch, nn = try_import_torch()\n <mask> F = None\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove from typing import Dict, List, Optional, Sequence, Tuple\n </s> add from typing import List, Sequence </s> add from ray.rllib.models.torch.misc import SlimFC </s> remove import time\nfrom typing import Dict, List, Tuple, Type\n </s> add from typing import Dict, Tuple, Type </s> remove from ray.rllib.agents.slateq.slateq_torch_model import SlateQModel\nfrom ray.rllib.models.modelv2 import ModelV2, restore_original_dimensions\n </s> add from ray.rllib.agents.slateq.slateq_torch_model import SlateQTorchModel\nfrom ray.rllib.models.modelv2 import ModelV2 </s> remove         self.q_model = QValueModel(user_embedding_size + doc_embedding_size, q_hiddens)\n        self.slate_size = len(action_space.nvec)\n        self.double_q = double_q\n\n        self.num_docs = num_docs\n        self.indices = torch.arange(\n            self.num_docs, dtype=torch.long\n        )  # , device=doc.device)\n        # slates.shape = [num_slates, slate_size]\n        self.slates = torch.combinations(self.indices, r=self.slate_size)\n        self.num_slates, _ = self.slates.shape\n\n    def choose_slate(\n        self,\n        user: TensorType,\n        doc: TensorType,\n        target_slate_q_values: Optional[TensorType] = None,\n    ) -> Tuple[TensorType, TensorType]:\n        \"\"\"Build a slate by selecting from candidate documents\n </s> add         self.q_model = QValueModel(self.obs_space, fcnet_hiddens_per_candidate)\n\n    def get_q_values(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors. </s> remove     model = SlateQModel(\n </s> add     model = SlateQTorchModel(", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     F = nn.functional\n <mask> \n <mask> \n <mask> class QValueModel(nn.Module):\n <mask>     \"\"\"The Q-value model for SlateQ.\n <mask> \n <mask>     A simple MLP with n layers (w/ LeakyReLU activation), whose sizes are\n <mask>     specified via `config.q_hiddens`.\n <mask>     \"\"\"\n <mask> \n <mask>     def __init__(self, embedding_size: int, q_hiddens: Sequence[int]):\n <mask>         \"\"\"Initializes a QValueModel instance.\n <mask> \n <mask>         Args:\n <mask>             embedding_size: The sum of user- and doc embedding sizes. This is the input\n <mask>                 dimension going into the first layer of the MLP.\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove         Args:\n            embedding_size: The sum of user- and doc embedding sizes. This is the input\n                dimension going into the first layer of the MLP.\n            q_hiddens: List of dense layer sizes to build the MLP by.\n </s> add         Each document candidate receives one full Q-value stack, defined by\n        `fcnet_hiddens_per_candidate`. The input to each of these Q-value stacks\n        is always {[user] concat [document[i]] for i in document_candidates}.\n\n        Extra model kwargs:\n            fcnet_hiddens_per_candidate: List of layer-sizes for each(!) of the\n                candidate documents. </s> remove         # Construct hidden layers.\n        layers = []\n        ins = embedding_size\n        for n in q_hiddens:\n            layers.append(nn.Linear(ins, n))\n            layers.append(nn.LeakyReLU())\n            ins = n\n        layers.append(nn.Linear(ins, 1))\n        self.layers = nn.Sequential(*layers)\n </s> add         self.orig_obs_space = obs_space\n        self.embedding_size = self.orig_obs_space[\"doc\"][\"0\"].shape[0]\n        self.num_candidates = len(self.orig_obs_space[\"doc\"])\n        assert self.orig_obs_space[\"user\"].shape[0] == self.embedding_size </s> remove class SlateQModel(TorchModelV2, nn.Module):\n    \"\"\"The SlateQ model class.\n </s> add class SlateQTorchModel(TorchModelV2, nn.Module):\n    \"\"\"Initializes a SlateQTFModel instance.\n\n    Model includes both the user choice model and the Q-value model. </s> remove         user_embedding_size: int,\n        doc_embedding_size: int,\n        num_docs: int,\n        q_hiddens: Sequence[int],\n </s> add         fcnet_hiddens_per_candidate: Sequence[int] = (256, 32), </s> remove     \"\"\"The user choice model for SlateQ\n </s> add     \"\"\"The user choice model for SlateQ. </s> remove     def forward(self, user: TensorType, doc: TensorType) -> TensorType:\n        \"\"\"Evaluate the user-doc Q model\n </s> add         self.q_nets = nn.ModuleList()\n        for i in range(self.num_candidates):\n            layers = nn.Sequential()\n            ins = 2 * self.embedding_size\n            for j, h in enumerate(fcnet_hiddens_per_candidate):\n                layers.add_module(\n                    f\"q_layer_{i}_{j}\",\n                    SlimFC(in_size=ins, out_size=h, activation_fn=\"relu\"),\n                )\n                ins = h\n            layers.add_module(f\"q_out_{i}\", SlimFC(ins, 1, activation_fn=None))\n\n            self.q_nets.append(layers)\n\n    def forward(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors.", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep replace replace replace replace replace replace replace replace replace keep keep keep", "code_tokens": " <mask> \n <mask>     def __init__(self, embedding_size: int, q_hiddens: Sequence[int]):\n <mask>         \"\"\"Initializes a QValueModel instance.\n <mask> \n <mask>         Args:\n <mask>             embedding_size: The sum of user- and doc embedding sizes. This is the input\n <mask>                 dimension going into the first layer of the MLP.\n <mask>             q_hiddens: List of dense layer sizes to build the MLP by.\n <mask>         \"\"\"\n <mask>         super().__init__()\n <mask> \n <mask>         # Construct hidden layers.\n <mask>         layers = []\n <mask>         ins = embedding_size\n <mask>         for n in q_hiddens:\n <mask>             layers.append(nn.Linear(ins, n))\n <mask>             layers.append(nn.LeakyReLU())\n <mask>             ins = n\n <mask>         layers.append(nn.Linear(ins, 1))\n <mask>         self.layers = nn.Sequential(*layers)\n <mask> \n <mask>     def forward(self, user: TensorType, doc: TensorType) -> TensorType:\n <mask>         \"\"\"Evaluate the user-doc Q model\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove     \"\"\"The Q-value model for SlateQ.\n\n    A simple MLP with n layers (w/ LeakyReLU activation), whose sizes are\n    specified via `config.q_hiddens`.\n    \"\"\"\n\n    def __init__(self, embedding_size: int, q_hiddens: Sequence[int]):\n </s> add     def __init__(\n        self,\n        obs_space: gym.spaces.Space,\n        fcnet_hiddens_per_candidate=(256, 32),\n    ): </s> remove     def forward(self, user: TensorType, doc: TensorType) -> TensorType:\n        \"\"\"Evaluate the user-doc Q model\n </s> add         self.q_nets = nn.ModuleList()\n        for i in range(self.num_candidates):\n            layers = nn.Sequential()\n            ins = 2 * self.embedding_size\n            for j, h in enumerate(fcnet_hiddens_per_candidate):\n                layers.add_module(\n                    f\"q_layer_{i}_{j}\",\n                    SlimFC(in_size=ins, out_size=h, activation_fn=\"relu\"),\n                )\n                ins = h\n            layers.add_module(f\"q_out_{i}\", SlimFC(ins, 1, activation_fn=None))\n\n            self.q_nets.append(layers)\n\n    def forward(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors. </s> remove             user: User embedding of shape (batch_size, user embedding size).\n                Note that `self.embedding_size` is the sum of both user- and\n                doc-embedding size.\n            doc: Doc embeddings of shape (batch_size, num_docs, doc embedding size).\n                Note that `self.embedding_size` is the sum of both user- and\n                doc-embedding size.\n </s> add             user: [B x u] where u=embedding of user features.\n            docs: List[[B x d]] where d=embedding of doc features. Each item in the\n                list represents one document candidate. </s> remove         user_embedding_size: int,\n        doc_embedding_size: int,\n        num_docs: int,\n        q_hiddens: Sequence[int],\n </s> add         fcnet_hiddens_per_candidate: Sequence[int] = (256, 32), </s> remove             q_hiddens: The list of hidden layer sizes for the QValueModel.\n </s> add             fcnet_hiddens_per_candidate: List of layer-sizes for each(!) of the\n                candidate documents.", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep keep replace replace keep keep replace replace replace replace replace replace keep keep", "code_tokens": " <mask>         self.layers = nn.Sequential(*layers)\n <mask> \n <mask>     def forward(self, user: TensorType, doc: TensorType) -> TensorType:\n <mask>         \"\"\"Evaluate the user-doc Q model\n <mask> \n <mask>         Args:\n <mask>             user: User embedding of shape (batch_size, user embedding size).\n <mask>                 Note that `self.embedding_size` is the sum of both user- and\n <mask>                 doc-embedding size.\n <mask>             doc: Doc embeddings of shape (batch_size, num_docs, doc embedding size).\n <mask>                 Note that `self.embedding_size` is the sum of both user- and\n <mask>                 doc-embedding size.\n <mask> \n <mask>         Returns:\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove             The q_values per document of shape (batch_size, num_docs + 1). +1 due to\n            also having a Q-value for the non-interaction (no click/no doc).\n </s> add             Tensor ([batch, num candidates) of Q-values.\n            1 Q-value per document candidate. </s> remove         # Construct hidden layers.\n        layers = []\n        ins = embedding_size\n        for n in q_hiddens:\n            layers.append(nn.Linear(ins, n))\n            layers.append(nn.LeakyReLU())\n            ins = n\n        layers.append(nn.Linear(ins, 1))\n        self.layers = nn.Sequential(*layers)\n </s> add         self.orig_obs_space = obs_space\n        self.embedding_size = self.orig_obs_space[\"doc\"][\"0\"].shape[0]\n        self.num_candidates = len(self.orig_obs_space[\"doc\"])\n        assert self.orig_obs_space[\"user\"].shape[0] == self.embedding_size </s> remove         self.q_model = QValueModel(user_embedding_size + doc_embedding_size, q_hiddens)\n        self.slate_size = len(action_space.nvec)\n        self.double_q = double_q\n\n        self.num_docs = num_docs\n        self.indices = torch.arange(\n            self.num_docs, dtype=torch.long\n        )  # , device=doc.device)\n        # slates.shape = [num_slates, slate_size]\n        self.slates = torch.combinations(self.indices, r=self.slate_size)\n        self.num_slates, _ = self.slates.shape\n\n    def choose_slate(\n        self,\n        user: TensorType,\n        doc: TensorType,\n        target_slate_q_values: Optional[TensorType] = None,\n    ) -> Tuple[TensorType, TensorType]:\n        \"\"\"Build a slate by selecting from candidate documents\n </s> add         self.q_model = QValueModel(self.obs_space, fcnet_hiddens_per_candidate)\n\n    def get_q_values(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors. </s> remove             user: User embeddings of shape (batch_size,\n                embedding_size).\n            doc: Doc embeddings of shape (batch_size, num_docs,\n                embedding_size).\n            double_q: Whether to apply double-Q correction on the target\n                term.\n </s> add             user: [B x u] where u=embedding of user features.\n            docs: List[[B x d]] where d=embedding of doc features. Each item in the\n                list represents one document candidate. </s> remove     \"\"\"The Q-value model for SlateQ.\n\n    A simple MLP with n layers (w/ LeakyReLU activation), whose sizes are\n    specified via `config.q_hiddens`.\n    \"\"\"\n\n    def __init__(self, embedding_size: int, q_hiddens: Sequence[int]):\n </s> add     def __init__(\n        self,\n        obs_space: gym.spaces.Space,\n        fcnet_hiddens_per_candidate=(256, 32),\n    ):", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep replace replace keep replace replace replace replace replace replace replace replace replace replace replace replace replace", "code_tokens": " <mask>         Returns:\n <mask>             The q_values per document of shape (batch_size, num_docs + 1). +1 due to\n <mask>             also having a Q-value for the non-interaction (no click/no doc).\n <mask>         \"\"\"\n <mask>         batch_size, num_docs, embedding_size = doc.shape\n <mask>         doc_flat = doc.view((batch_size * num_docs, embedding_size))\n <mask> \n <mask>         # Concat everything.\n <mask>         # No user features.\n <mask>         if user.shape[-1] == 0:\n <mask>             x = doc_flat\n <mask>         # User features, repeat user embeddings n times (n=num docs).\n <mask>         else:\n <mask>             user_repeated = user.repeat(num_docs, 1)\n <mask>             x = torch.cat([user_repeated, doc_flat], dim=1)\n <mask> \n <mask>         x = self.layers(x)\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove             user: User embedding of shape (batch_size, user embedding size).\n                Note that `self.embedding_size` is the sum of both user- and\n                doc-embedding size.\n            doc: Doc embeddings of shape (batch_size, num_docs, doc embedding size).\n                Note that `self.embedding_size` is the sum of both user- and\n                doc-embedding size.\n </s> add             user: [B x u] where u=embedding of user features.\n            docs: List[[B x d]] where d=embedding of doc features. Each item in the\n                list represents one document candidate. </s> remove         # Similar to Google's SlateQ implementation in RecSim, we force the\n        # Q-values to zeros if there are no clicks.\n        # See https://arxiv.org/abs/1905.12767 for details.\n        x_no_click = torch.zeros((batch_size, 1), device=x.device)\n\n        return torch.cat([x.view((batch_size, num_docs)), x_no_click], dim=1)\n </s> add         return torch.cat(q_outs, dim=1) </s> remove         slate_q_values = self.get_per_slate_q_values(user, doc)\n\n        if target_slate_q_values is not None:\n            assert self.double_q\n            max_values, best_target_indices = torch.max(target_slate_q_values, dim=1)\n            best_slate_q_value = torch.gather(\n                slate_q_values, 1, best_target_indices.unsqueeze(1)\n            ).squeeze(1)\n            # slates_selected.shape: [batch_size, slate_size]\n            slates_selected = self.slates[best_target_indices]\n        else:\n            # Find the slate that maximizes q value.\n            best_slate_q_value, max_idx = torch.max(slate_q_values, dim=1)\n            # slates_selected.shape: [batch_size, slate_size]\n            slates_selected = self.slates[max_idx]\n\n        return slates_selected, best_slate_q_value, slate_q_values\n\n    def get_per_slate_q_values(self, user, doc):\n        # Compute item scores (proportional to click probabilities)\n        # raw_scores.shape=[batch_size, num_docs+1]\n        raw_scores = self.choice_model(user, doc)\n        # max_raw_scores.shape=[batch_size, 1]\n        max_raw_scores, _ = torch.max(raw_scores, dim=1, keepdim=True)\n        # Deduct scores by max_scores to avoid value explosion.\n        scores = torch.exp(raw_scores - max_raw_scores)\n        scores_doc = scores[:, :-1]  # shape=[batch_size, num_docs]\n        scores_no_click = scores[:, [-1]]  # shape=[batch_size, 1]\n\n        # Calculate the item-wise Q values.\n        # q_values.shape=[batch_size, num_docs+1]\n        q_values = self.q_model(user, doc)\n        q_values_doc = q_values[:, :-1]  # shape=[batch_size, num_docs]\n        q_values_no_click = q_values[:, [-1]]  # shape=[batch_size, 1]\n\n        # Calculate per-slate Q values.\n        batch_size, _ = q_values_doc.shape\n        # Move pre-stored slate combinations to GPU, if necessary.\n        if self.slates.device != q_values_doc.device:\n            self.slates = self.slates.to(q_values.device)\n        # slate_decomp_q_values.shape: [batch_size, num_slates, slate_size]\n        slate_decomp_q_values = torch.gather(\n            # input.shape: [batch_size, num_slates, num_docs]\n            input=q_values_doc.unsqueeze(1).expand(-1, self.num_slates, -1),\n            dim=2,\n            # index.shape: [batch_size, num_slates, slate_size]\n            index=self.slates.unsqueeze(0).expand(batch_size, -1, -1),\n        )\n        # slate_scores.shape: [batch_size, num_slates, slate_size]\n        slate_scores = torch.gather(\n            # input.shape: [batch_size, num_slates, num_docs]\n            input=scores_doc.unsqueeze(1).expand(-1, self.num_slates, -1),\n            dim=2,\n            # index.shape: [batch_size, num_slates, slate_size]\n            index=self.slates.unsqueeze(0).expand(batch_size, -1, -1),\n        )\n\n        # slate_q_values.shape: [batch_size, num_slates]\n        slate_q_values = (\n            (slate_decomp_q_values * slate_scores).sum(dim=2)\n            + (q_values_no_click * scores_no_click)\n        ) / (slate_scores.sum(dim=2) + scores_no_click)\n        return slate_q_values\n\n    def forward(\n        self,\n        input_dict: Dict[str, TensorType],\n        state: List[TensorType],\n        seq_lens: TensorType,\n    ) -> Tuple[TensorType, List[TensorType]]:\n        # user.shape: [batch_size, embedding_size]\n        user = input_dict[SampleBatch.OBS][\"user\"]\n        # doc.shape: [batch_size, num_docs, embedding_size]\n        doc = torch.cat(\n            [val.unsqueeze(1) for val in input_dict[SampleBatch.OBS][\"doc\"].values()], 1\n        )\n\n        slates_selected, _, _ = self.choose_slate(user, doc)\n\n        state_out = []\n        return slates_selected, state_out\n </s> add         return self.q_model(user, docs) </s> remove             user: User embeddings of shape (batch_size,\n                embedding_size).\n            doc: Doc embeddings of shape (batch_size, num_docs,\n                embedding_size).\n            double_q: Whether to apply double-Q correction on the target\n                term.\n </s> add             user: [B x u] where u=embedding of user features.\n            docs: List[[B x d]] where d=embedding of doc features. Each item in the\n                list represents one document candidate. </s> remove         self.q_model = QValueModel(user_embedding_size + doc_embedding_size, q_hiddens)\n        self.slate_size = len(action_space.nvec)\n        self.double_q = double_q\n\n        self.num_docs = num_docs\n        self.indices = torch.arange(\n            self.num_docs, dtype=torch.long\n        )  # , device=doc.device)\n        # slates.shape = [num_slates, slate_size]\n        self.slates = torch.combinations(self.indices, r=self.slate_size)\n        self.num_slates, _ = self.slates.shape\n\n    def choose_slate(\n        self,\n        user: TensorType,\n        doc: TensorType,\n        target_slate_q_values: Optional[TensorType] = None,\n    ) -> Tuple[TensorType, TensorType]:\n        \"\"\"Build a slate by selecting from candidate documents\n </s> add         self.q_model = QValueModel(self.obs_space, fcnet_hiddens_per_candidate)\n\n    def get_q_values(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors.", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep keep keep replace replace replace replace replace replace keep keep keep replace keep keep", "code_tokens": " <mask> \n <mask>         x = self.layers(x)\n <mask> \n <mask>         # Similar to Google's SlateQ implementation in RecSim, we force the\n <mask>         # Q-values to zeros if there are no clicks.\n <mask>         # See https://arxiv.org/abs/1905.12767 for details.\n <mask>         x_no_click = torch.zeros((batch_size, 1), device=x.device)\n <mask> \n <mask>         return torch.cat([x.view((batch_size, num_docs)), x_no_click], dim=1)\n <mask> \n <mask> \n <mask> class UserChoiceModel(nn.Module):\n <mask>     \"\"\"The user choice model for SlateQ\n <mask> \n <mask>     This class implements a multinomial logit model for predicting user clicks.\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove         batch_size, num_docs, embedding_size = doc.shape\n        doc_flat = doc.view((batch_size * num_docs, embedding_size))\n\n        # Concat everything.\n        # No user features.\n        if user.shape[-1] == 0:\n            x = doc_flat\n        # User features, repeat user embeddings n times (n=num docs).\n        else:\n            user_repeated = user.repeat(num_docs, 1)\n            x = torch.cat([user_repeated, doc_flat], dim=1)\n\n        x = self.layers(x)\n </s> add         q_outs = []\n        for i in range(self.num_candidates):\n            user_cat_doc = torch.cat([user, docs[i]], dim=1)\n            q_outs.append(self.q_nets[i](user_cat_doc)) </s> remove class SlateQModel(TorchModelV2, nn.Module):\n    \"\"\"The SlateQ model class.\n </s> add class SlateQTorchModel(TorchModelV2, nn.Module):\n    \"\"\"Initializes a SlateQTFModel instance.\n\n    Model includes both the user choice model and the Q-value model. </s> remove         slate_q_values = self.get_per_slate_q_values(user, doc)\n\n        if target_slate_q_values is not None:\n            assert self.double_q\n            max_values, best_target_indices = torch.max(target_slate_q_values, dim=1)\n            best_slate_q_value = torch.gather(\n                slate_q_values, 1, best_target_indices.unsqueeze(1)\n            ).squeeze(1)\n            # slates_selected.shape: [batch_size, slate_size]\n            slates_selected = self.slates[best_target_indices]\n        else:\n            # Find the slate that maximizes q value.\n            best_slate_q_value, max_idx = torch.max(slate_q_values, dim=1)\n            # slates_selected.shape: [batch_size, slate_size]\n            slates_selected = self.slates[max_idx]\n\n        return slates_selected, best_slate_q_value, slate_q_values\n\n    def get_per_slate_q_values(self, user, doc):\n        # Compute item scores (proportional to click probabilities)\n        # raw_scores.shape=[batch_size, num_docs+1]\n        raw_scores = self.choice_model(user, doc)\n        # max_raw_scores.shape=[batch_size, 1]\n        max_raw_scores, _ = torch.max(raw_scores, dim=1, keepdim=True)\n        # Deduct scores by max_scores to avoid value explosion.\n        scores = torch.exp(raw_scores - max_raw_scores)\n        scores_doc = scores[:, :-1]  # shape=[batch_size, num_docs]\n        scores_no_click = scores[:, [-1]]  # shape=[batch_size, 1]\n\n        # Calculate the item-wise Q values.\n        # q_values.shape=[batch_size, num_docs+1]\n        q_values = self.q_model(user, doc)\n        q_values_doc = q_values[:, :-1]  # shape=[batch_size, num_docs]\n        q_values_no_click = q_values[:, [-1]]  # shape=[batch_size, 1]\n\n        # Calculate per-slate Q values.\n        batch_size, _ = q_values_doc.shape\n        # Move pre-stored slate combinations to GPU, if necessary.\n        if self.slates.device != q_values_doc.device:\n            self.slates = self.slates.to(q_values.device)\n        # slate_decomp_q_values.shape: [batch_size, num_slates, slate_size]\n        slate_decomp_q_values = torch.gather(\n            # input.shape: [batch_size, num_slates, num_docs]\n            input=q_values_doc.unsqueeze(1).expand(-1, self.num_slates, -1),\n            dim=2,\n            # index.shape: [batch_size, num_slates, slate_size]\n            index=self.slates.unsqueeze(0).expand(batch_size, -1, -1),\n        )\n        # slate_scores.shape: [batch_size, num_slates, slate_size]\n        slate_scores = torch.gather(\n            # input.shape: [batch_size, num_slates, num_docs]\n            input=scores_doc.unsqueeze(1).expand(-1, self.num_slates, -1),\n            dim=2,\n            # index.shape: [batch_size, num_slates, slate_size]\n            index=self.slates.unsqueeze(0).expand(batch_size, -1, -1),\n        )\n\n        # slate_q_values.shape: [batch_size, num_slates]\n        slate_q_values = (\n            (slate_decomp_q_values * slate_scores).sum(dim=2)\n            + (q_values_no_click * scores_no_click)\n        ) / (slate_scores.sum(dim=2) + scores_no_click)\n        return slate_q_values\n\n    def forward(\n        self,\n        input_dict: Dict[str, TensorType],\n        state: List[TensorType],\n        seq_lens: TensorType,\n    ) -> Tuple[TensorType, List[TensorType]]:\n        # user.shape: [batch_size, embedding_size]\n        user = input_dict[SampleBatch.OBS][\"user\"]\n        # doc.shape: [batch_size, num_docs, embedding_size]\n        doc = torch.cat(\n            [val.unsqueeze(1) for val in input_dict[SampleBatch.OBS][\"doc\"].values()], 1\n        )\n\n        slates_selected, _, _ = self.choose_slate(user, doc)\n\n        state_out = []\n        return slates_selected, state_out\n </s> add         return self.q_model(user, docs) </s> remove     It includes both the user choice model and the Q-value model.\n </s> add     For the Q-value model, each document candidate receives one full Q-value\n    stack, defined by `fcnet_hiddens_per_candidate`. The input to each of these\n    Q-value stacks is always {[user] concat [document[i]] for i in document_candidates}.\n\n    Extra model kwargs:\n        fcnet_hiddens_per_candidate: List of layer-sizes for each(!) of the\n            candidate documents. </s> remove             # real imact, and can be set arbitrarily. TODO: fix this.\n </s> add             # real impact, and can be set arbitrarily. TODO: fix this.", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         return s\n <mask> \n <mask> \n <mask> class SlateQModel(TorchModelV2, nn.Module):\n <mask>     \"\"\"The SlateQ model class.\n <mask> \n <mask>     It includes both the user choice model and the Q-value model.\n <mask>     \"\"\"\n <mask> \n <mask>     def __init__(\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove     It includes both the user choice model and the Q-value model.\n </s> add     For the Q-value model, each document candidate receives one full Q-value\n    stack, defined by `fcnet_hiddens_per_candidate`. The input to each of these\n    Q-value stacks is always {[user] concat [document[i]] for i in document_candidates}.\n\n    Extra model kwargs:\n        fcnet_hiddens_per_candidate: List of layer-sizes for each(!) of the\n            candidate documents. </s> remove     \"\"\"The user choice model for SlateQ\n </s> add     \"\"\"The user choice model for SlateQ. </s> remove         # Similar to Google's SlateQ implementation in RecSim, we force the\n        # Q-values to zeros if there are no clicks.\n        # See https://arxiv.org/abs/1905.12767 for details.\n        x_no_click = torch.zeros((batch_size, 1), device=x.device)\n\n        return torch.cat([x.view((batch_size, num_docs)), x_no_click], dim=1)\n </s> add         return torch.cat(q_outs, dim=1) </s> remove     \"\"\"The Q-value model for SlateQ.\n\n    A simple MLP with n layers (w/ LeakyReLU activation), whose sizes are\n    specified via `config.q_hiddens`.\n    \"\"\"\n\n    def __init__(self, embedding_size: int, q_hiddens: Sequence[int]):\n </s> add     def __init__(\n        self,\n        obs_space: gym.spaces.Space,\n        fcnet_hiddens_per_candidate=(256, 32),\n    ): </s> remove     model = SlateQModel(\n </s> add     model = SlateQTorchModel( </s> remove             user: User embedding of shape (batch_size, user embedding size).\n                Note that `self.embedding_size` is the sum of both user- and\n                doc-embedding size.\n            doc: Doc embeddings of shape (batch_size, num_docs, doc embedding size).\n                Note that `self.embedding_size` is the sum of both user- and\n                doc-embedding size.\n </s> add             user: [B x u] where u=embedding of user features.\n            docs: List[[B x d]] where d=embedding of doc features. Each item in the\n                list represents one document candidate.", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> class SlateQModel(TorchModelV2, nn.Module):\n <mask>     \"\"\"The SlateQ model class.\n <mask> \n <mask>     It includes both the user choice model and the Q-value model.\n <mask>     \"\"\"\n <mask> \n <mask>     def __init__(\n <mask>         self,\n <mask>         obs_space: gym.spaces.Space,\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove class SlateQModel(TorchModelV2, nn.Module):\n    \"\"\"The SlateQ model class.\n </s> add class SlateQTorchModel(TorchModelV2, nn.Module):\n    \"\"\"Initializes a SlateQTFModel instance.\n\n    Model includes both the user choice model and the Q-value model. </s> remove     \"\"\"The Q-value model for SlateQ.\n\n    A simple MLP with n layers (w/ LeakyReLU activation), whose sizes are\n    specified via `config.q_hiddens`.\n    \"\"\"\n\n    def __init__(self, embedding_size: int, q_hiddens: Sequence[int]):\n </s> add     def __init__(\n        self,\n        obs_space: gym.spaces.Space,\n        fcnet_hiddens_per_candidate=(256, 32),\n    ): </s> remove     \"\"\"The user choice model for SlateQ\n </s> add     \"\"\"The user choice model for SlateQ. </s> add         num_outputs: int, </s> remove         # Similar to Google's SlateQ implementation in RecSim, we force the\n        # Q-values to zeros if there are no clicks.\n        # See https://arxiv.org/abs/1905.12767 for details.\n        x_no_click = torch.zeros((batch_size, 1), device=x.device)\n\n        return torch.cat([x.view((batch_size, num_docs)), x_no_click], dim=1)\n </s> add         return torch.cat(q_outs, dim=1) </s> remove     model = SlateQModel(\n </s> add     model = SlateQTorchModel(", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask>     def __init__(\n <mask>         self,\n <mask>         obs_space: gym.spaces.Space,\n <mask>         action_space: gym.spaces.Space,\n <mask>         model_config: ModelConfigDict,\n <mask>         name: str,\n <mask>         *,\n <mask>         fcnet_hiddens_per_candidate: Sequence[int] = (256, 32),\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove         user_embedding_size: int,\n        doc_embedding_size: int,\n        num_docs: int,\n        q_hiddens: Sequence[int],\n </s> add         fcnet_hiddens_per_candidate: Sequence[int] = (256, 32), </s> remove     \"\"\"The Q-value model for SlateQ.\n\n    A simple MLP with n layers (w/ LeakyReLU activation), whose sizes are\n    specified via `config.q_hiddens`.\n    \"\"\"\n\n    def __init__(self, embedding_size: int, q_hiddens: Sequence[int]):\n </s> add     def __init__(\n        self,\n        obs_space: gym.spaces.Space,\n        fcnet_hiddens_per_candidate=(256, 32),\n    ): </s> remove     It includes both the user choice model and the Q-value model.\n </s> add     For the Q-value model, each document candidate receives one full Q-value\n    stack, defined by `fcnet_hiddens_per_candidate`. The input to each of these\n    Q-value stacks is always {[user] concat [document[i]] for i in document_candidates}.\n\n    Extra model kwargs:\n        fcnet_hiddens_per_candidate: List of layer-sizes for each(!) of the\n            candidate documents. </s> add from ray.rllib.models.torch.misc import SlimFC </s> remove from ray.rllib.policy.sample_batch import SampleBatch\n </s> add  </s> remove         self.q_model = QValueModel(user_embedding_size + doc_embedding_size, q_hiddens)\n        self.slate_size = len(action_space.nvec)\n        self.double_q = double_q\n\n        self.num_docs = num_docs\n        self.indices = torch.arange(\n            self.num_docs, dtype=torch.long\n        )  # , device=doc.device)\n        # slates.shape = [num_slates, slate_size]\n        self.slates = torch.combinations(self.indices, r=self.slate_size)\n        self.num_slates, _ = self.slates.shape\n\n    def choose_slate(\n        self,\n        user: TensorType,\n        doc: TensorType,\n        target_slate_q_values: Optional[TensorType] = None,\n    ) -> Tuple[TensorType, TensorType]:\n        \"\"\"Build a slate by selecting from candidate documents\n </s> add         self.q_model = QValueModel(self.obs_space, fcnet_hiddens_per_candidate)\n\n    def get_q_values(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors.", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         action_space: gym.spaces.Space,\n <mask>         model_config: ModelConfigDict,\n <mask>         name: str,\n <mask>         *,\n <mask>         user_embedding_size: int,\n <mask>         doc_embedding_size: int,\n <mask>         num_docs: int,\n <mask>         q_hiddens: Sequence[int],\n <mask>         double_q: bool = True,\n <mask>     ):\n <mask>         \"\"\"Initializes a SlateQModel instance.\n <mask> \n <mask>         Args:\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> add         num_outputs: int, </s> remove     \"\"\"The Q-value model for SlateQ.\n\n    A simple MLP with n layers (w/ LeakyReLU activation), whose sizes are\n    specified via `config.q_hiddens`.\n    \"\"\"\n\n    def __init__(self, embedding_size: int, q_hiddens: Sequence[int]):\n </s> add     def __init__(\n        self,\n        obs_space: gym.spaces.Space,\n        fcnet_hiddens_per_candidate=(256, 32),\n    ): </s> remove         Args:\n            embedding_size: The sum of user- and doc embedding sizes. This is the input\n                dimension going into the first layer of the MLP.\n            q_hiddens: List of dense layer sizes to build the MLP by.\n </s> add         Each document candidate receives one full Q-value stack, defined by\n        `fcnet_hiddens_per_candidate`. The input to each of these Q-value stacks\n        is always {[user] concat [document[i]] for i in document_candidates}.\n\n        Extra model kwargs:\n            fcnet_hiddens_per_candidate: List of layer-sizes for each(!) of the\n                candidate documents. </s> remove             q_hiddens: The list of hidden layer sizes for the QValueModel.\n </s> add             fcnet_hiddens_per_candidate: List of layer-sizes for each(!) of the\n                candidate documents. </s> remove class SlateQModel(TorchModelV2, nn.Module):\n    \"\"\"The SlateQ model class.\n </s> add class SlateQTorchModel(TorchModelV2, nn.Module):\n    \"\"\"Initializes a SlateQTFModel instance.\n\n    Model includes both the user choice model and the Q-value model. </s> remove         # Construct hidden layers.\n        layers = []\n        ins = embedding_size\n        for n in q_hiddens:\n            layers.append(nn.Linear(ins, n))\n            layers.append(nn.LeakyReLU())\n            ins = n\n        layers.append(nn.Linear(ins, 1))\n        self.layers = nn.Sequential(*layers)\n </s> add         self.orig_obs_space = obs_space\n        self.embedding_size = self.orig_obs_space[\"doc\"][\"0\"].shape[0]\n        self.num_candidates = len(self.orig_obs_space[\"doc\"])\n        assert self.orig_obs_space[\"user\"].shape[0] == self.embedding_size", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             doc_embedding_size: The size of the doc embedding (number of doc\n <mask>                 specific features).\n <mask>             num_docs: The number of docs to select a slate from. Note that the slate\n <mask>                 size is inferred from the action space.\n <mask>             q_hiddens: The list of hidden layer sizes for the QValueModel.\n <mask>             double_q: Whether \"double Q-learning\" is applied in the loss function.\n <mask>         \"\"\"\n <mask>         nn.Module.__init__(self)\n <mask>         TorchModelV2.__init__(\n <mask>             self,\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove         Args:\n            embedding_size: The sum of user- and doc embedding sizes. This is the input\n                dimension going into the first layer of the MLP.\n            q_hiddens: List of dense layer sizes to build the MLP by.\n </s> add         Each document candidate receives one full Q-value stack, defined by\n        `fcnet_hiddens_per_candidate`. The input to each of these Q-value stacks\n        is always {[user] concat [document[i]] for i in document_candidates}.\n\n        Extra model kwargs:\n            fcnet_hiddens_per_candidate: List of layer-sizes for each(!) of the\n                candidate documents. </s> remove             user: User embedding of shape (batch_size, user embedding size).\n                Note that `self.embedding_size` is the sum of both user- and\n                doc-embedding size.\n            doc: Doc embeddings of shape (batch_size, num_docs, doc embedding size).\n                Note that `self.embedding_size` is the sum of both user- and\n                doc-embedding size.\n </s> add             user: [B x u] where u=embedding of user features.\n            docs: List[[B x d]] where d=embedding of doc features. Each item in the\n                list represents one document candidate. </s> remove     \"\"\"The Q-value model for SlateQ.\n\n    A simple MLP with n layers (w/ LeakyReLU activation), whose sizes are\n    specified via `config.q_hiddens`.\n    \"\"\"\n\n    def __init__(self, embedding_size: int, q_hiddens: Sequence[int]):\n </s> add     def __init__(\n        self,\n        obs_space: gym.spaces.Space,\n        fcnet_hiddens_per_candidate=(256, 32),\n    ): </s> remove             user: User embeddings of shape (batch_size,\n                embedding_size).\n            doc: Doc embeddings of shape (batch_size, num_docs,\n                embedding_size).\n            double_q: Whether to apply double-Q correction on the target\n                term.\n </s> add             user: [B x u] where u=embedding of user features.\n            docs: List[[B x d]] where d=embedding of doc features. Each item in the\n                list represents one document candidate. </s> remove         user_embedding_size: int,\n        doc_embedding_size: int,\n        num_docs: int,\n        q_hiddens: Sequence[int],\n </s> add         fcnet_hiddens_per_candidate: Sequence[int] = (256, 32), </s> remove         # Construct hidden layers.\n        layers = []\n        ins = embedding_size\n        for n in q_hiddens:\n            layers.append(nn.Linear(ins, n))\n            layers.append(nn.LeakyReLU())\n            ins = n\n        layers.append(nn.Linear(ins, 1))\n        self.layers = nn.Sequential(*layers)\n </s> add         self.orig_obs_space = obs_space\n        self.embedding_size = self.orig_obs_space[\"doc\"][\"0\"].shape[0]\n        self.num_candidates = len(self.orig_obs_space[\"doc\"])\n        assert self.orig_obs_space[\"user\"].shape[0] == self.embedding_size", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             self,\n <mask>             obs_space,\n <mask>             action_space,\n <mask>             # This required parameter (num_outputs) seems redundant: it has no\n <mask>             # real imact, and can be set arbitrarily. TODO: fix this.\n <mask>             num_outputs=0,\n <mask>             model_config=model_config,\n <mask>             name=name,\n <mask>         )\n <mask>         self.choice_model = UserChoiceModel()\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove         self.q_model = QValueModel(user_embedding_size + doc_embedding_size, q_hiddens)\n        self.slate_size = len(action_space.nvec)\n        self.double_q = double_q\n\n        self.num_docs = num_docs\n        self.indices = torch.arange(\n            self.num_docs, dtype=torch.long\n        )  # , device=doc.device)\n        # slates.shape = [num_slates, slate_size]\n        self.slates = torch.combinations(self.indices, r=self.slate_size)\n        self.num_slates, _ = self.slates.shape\n\n    def choose_slate(\n        self,\n        user: TensorType,\n        doc: TensorType,\n        target_slate_q_values: Optional[TensorType] = None,\n    ) -> Tuple[TensorType, TensorType]:\n        \"\"\"Build a slate by selecting from candidate documents\n </s> add         self.q_model = QValueModel(self.obs_space, fcnet_hiddens_per_candidate)\n\n    def get_q_values(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors. </s> add         self.num_outputs = num_outputs\n </s> remove         user_embedding_size=obs_space.original_space[\"user\"].shape[0],\n        doc_embedding_size=obs_space.original_space[\"doc\"][\"0\"].shape[0],\n        num_docs=len(obs_space.original_space[\"doc\"].spaces),\n        q_hiddens=config[\"hiddens\"],\n        double_q=config[\"double_q\"],\n </s> add         fcnet_hiddens_per_candidate=config[\"fcnet_hiddens_per_candidate\"], </s> add         num_outputs=action_space.nvec[0], </s> remove     model = SlateQModel(\n </s> add     model = SlateQTorchModel( </s> remove         # Similar to Google's SlateQ implementation in RecSim, we force the\n        # Q-values to zeros if there are no clicks.\n        # See https://arxiv.org/abs/1905.12767 for details.\n        x_no_click = torch.zeros((batch_size, 1), device=x.device)\n\n        return torch.cat([x.view((batch_size, num_docs)), x_no_click], dim=1)\n </s> add         return torch.cat(q_outs, dim=1)", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask>             name=name,\n <mask>         )\n <mask>         self.choice_model = UserChoiceModel()\n <mask> \n <mask>         self.q_model = QValueModel(self.obs_space, fcnet_hiddens_per_candidate)\n <mask> \n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove         self.q_model = QValueModel(user_embedding_size + doc_embedding_size, q_hiddens)\n        self.slate_size = len(action_space.nvec)\n        self.double_q = double_q\n\n        self.num_docs = num_docs\n        self.indices = torch.arange(\n            self.num_docs, dtype=torch.long\n        )  # , device=doc.device)\n        # slates.shape = [num_slates, slate_size]\n        self.slates = torch.combinations(self.indices, r=self.slate_size)\n        self.num_slates, _ = self.slates.shape\n\n    def choose_slate(\n        self,\n        user: TensorType,\n        doc: TensorType,\n        target_slate_q_values: Optional[TensorType] = None,\n    ) -> Tuple[TensorType, TensorType]:\n        \"\"\"Build a slate by selecting from candidate documents\n </s> add         self.q_model = QValueModel(self.obs_space, fcnet_hiddens_per_candidate)\n\n    def get_q_values(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors. </s> remove             # real imact, and can be set arbitrarily. TODO: fix this.\n </s> add             # real impact, and can be set arbitrarily. TODO: fix this. </s> remove         slate_q_values = self.get_per_slate_q_values(user, doc)\n\n        if target_slate_q_values is not None:\n            assert self.double_q\n            max_values, best_target_indices = torch.max(target_slate_q_values, dim=1)\n            best_slate_q_value = torch.gather(\n                slate_q_values, 1, best_target_indices.unsqueeze(1)\n            ).squeeze(1)\n            # slates_selected.shape: [batch_size, slate_size]\n            slates_selected = self.slates[best_target_indices]\n        else:\n            # Find the slate that maximizes q value.\n            best_slate_q_value, max_idx = torch.max(slate_q_values, dim=1)\n            # slates_selected.shape: [batch_size, slate_size]\n            slates_selected = self.slates[max_idx]\n\n        return slates_selected, best_slate_q_value, slate_q_values\n\n    def get_per_slate_q_values(self, user, doc):\n        # Compute item scores (proportional to click probabilities)\n        # raw_scores.shape=[batch_size, num_docs+1]\n        raw_scores = self.choice_model(user, doc)\n        # max_raw_scores.shape=[batch_size, 1]\n        max_raw_scores, _ = torch.max(raw_scores, dim=1, keepdim=True)\n        # Deduct scores by max_scores to avoid value explosion.\n        scores = torch.exp(raw_scores - max_raw_scores)\n        scores_doc = scores[:, :-1]  # shape=[batch_size, num_docs]\n        scores_no_click = scores[:, [-1]]  # shape=[batch_size, 1]\n\n        # Calculate the item-wise Q values.\n        # q_values.shape=[batch_size, num_docs+1]\n        q_values = self.q_model(user, doc)\n        q_values_doc = q_values[:, :-1]  # shape=[batch_size, num_docs]\n        q_values_no_click = q_values[:, [-1]]  # shape=[batch_size, 1]\n\n        # Calculate per-slate Q values.\n        batch_size, _ = q_values_doc.shape\n        # Move pre-stored slate combinations to GPU, if necessary.\n        if self.slates.device != q_values_doc.device:\n            self.slates = self.slates.to(q_values.device)\n        # slate_decomp_q_values.shape: [batch_size, num_slates, slate_size]\n        slate_decomp_q_values = torch.gather(\n            # input.shape: [batch_size, num_slates, num_docs]\n            input=q_values_doc.unsqueeze(1).expand(-1, self.num_slates, -1),\n            dim=2,\n            # index.shape: [batch_size, num_slates, slate_size]\n            index=self.slates.unsqueeze(0).expand(batch_size, -1, -1),\n        )\n        # slate_scores.shape: [batch_size, num_slates, slate_size]\n        slate_scores = torch.gather(\n            # input.shape: [batch_size, num_slates, num_docs]\n            input=scores_doc.unsqueeze(1).expand(-1, self.num_slates, -1),\n            dim=2,\n            # index.shape: [batch_size, num_slates, slate_size]\n            index=self.slates.unsqueeze(0).expand(batch_size, -1, -1),\n        )\n\n        # slate_q_values.shape: [batch_size, num_slates]\n        slate_q_values = (\n            (slate_decomp_q_values * slate_scores).sum(dim=2)\n            + (q_values_no_click * scores_no_click)\n        ) / (slate_scores.sum(dim=2) + scores_no_click)\n        return slate_q_values\n\n    def forward(\n        self,\n        input_dict: Dict[str, TensorType],\n        state: List[TensorType],\n        seq_lens: TensorType,\n    ) -> Tuple[TensorType, List[TensorType]]:\n        # user.shape: [batch_size, embedding_size]\n        user = input_dict[SampleBatch.OBS][\"user\"]\n        # doc.shape: [batch_size, num_docs, embedding_size]\n        doc = torch.cat(\n            [val.unsqueeze(1) for val in input_dict[SampleBatch.OBS][\"doc\"].values()], 1\n        )\n\n        slates_selected, _, _ = self.choose_slate(user, doc)\n\n        state_out = []\n        return slates_selected, state_out\n </s> add         return self.q_model(user, docs) </s> add         num_outputs=action_space.nvec[0], </s> remove     def forward(self, user: TensorType, doc: TensorType) -> TensorType:\n        \"\"\"Evaluate the user-doc Q model\n </s> add         self.q_nets = nn.ModuleList()\n        for i in range(self.num_candidates):\n            layers = nn.Sequential()\n            ins = 2 * self.embedding_size\n            for j, h in enumerate(fcnet_hiddens_per_candidate):\n                layers.add_module(\n                    f\"q_layer_{i}_{j}\",\n                    SlimFC(in_size=ins, out_size=h, activation_fn=\"relu\"),\n                )\n                ins = h\n            layers.add_module(f\"q_out_{i}\", SlimFC(ins, 1, activation_fn=None))\n\n            self.q_nets.append(layers)\n\n    def forward(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors. </s> remove         user_embedding_size=obs_space.original_space[\"user\"].shape[0],\n        doc_embedding_size=obs_space.original_space[\"doc\"][\"0\"].shape[0],\n        num_docs=len(obs_space.original_space[\"doc\"].spaces),\n        q_hiddens=config[\"hiddens\"],\n        double_q=config[\"double_q\"],\n </s> add         fcnet_hiddens_per_candidate=config[\"fcnet_hiddens_per_candidate\"],", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep keep replace replace replace replace replace replace keep", "code_tokens": " <mask>             model_config=model_config,\n <mask>             name=name,\n <mask>         )\n <mask>         self.choice_model = UserChoiceModel()\n <mask>         self.q_model = QValueModel(user_embedding_size + doc_embedding_size, q_hiddens)\n <mask>         self.slate_size = len(action_space.nvec)\n <mask>         self.double_q = double_q\n <mask> \n <mask>         self.num_docs = num_docs\n <mask>         self.indices = torch.arange(\n <mask>             self.num_docs, dtype=torch.long\n <mask>         )  # , device=doc.device)\n <mask>         # slates.shape = [num_slates, slate_size]\n <mask>         self.slates = torch.combinations(self.indices, r=self.slate_size)\n <mask>         self.num_slates, _ = self.slates.shape\n <mask> \n <mask>     def choose_slate(\n <mask>         self,\n <mask>         user: TensorType,\n <mask>         doc: TensorType,\n <mask>         target_slate_q_values: Optional[TensorType] = None,\n <mask>     ) -> Tuple[TensorType, TensorType]:\n <mask>         \"\"\"Build a slate by selecting from candidate documents\n <mask> \n <mask>         Args:\n <mask>             user: User embeddings of shape (batch_size,\n <mask>                 embedding_size).\n <mask>             doc: Doc embeddings of shape (batch_size, num_docs,\n <mask>                 embedding_size).\n <mask>             double_q: Whether to apply double-Q correction on the target\n <mask>                 term.\n <mask> \n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove             slate_selected (TensorType): Indices of documents selected for\n                the slate, with shape (batch_size, slate_size).\n            best_slate_q_value (TensorType): The Q-value of the selected slate,\n                with shape (batch_size).\n </s> add             Tensor ([batch, num candidates) of Q-values.\n            1 Q-value per document candidate. </s> remove         slate_q_values = self.get_per_slate_q_values(user, doc)\n\n        if target_slate_q_values is not None:\n            assert self.double_q\n            max_values, best_target_indices = torch.max(target_slate_q_values, dim=1)\n            best_slate_q_value = torch.gather(\n                slate_q_values, 1, best_target_indices.unsqueeze(1)\n            ).squeeze(1)\n            # slates_selected.shape: [batch_size, slate_size]\n            slates_selected = self.slates[best_target_indices]\n        else:\n            # Find the slate that maximizes q value.\n            best_slate_q_value, max_idx = torch.max(slate_q_values, dim=1)\n            # slates_selected.shape: [batch_size, slate_size]\n            slates_selected = self.slates[max_idx]\n\n        return slates_selected, best_slate_q_value, slate_q_values\n\n    def get_per_slate_q_values(self, user, doc):\n        # Compute item scores (proportional to click probabilities)\n        # raw_scores.shape=[batch_size, num_docs+1]\n        raw_scores = self.choice_model(user, doc)\n        # max_raw_scores.shape=[batch_size, 1]\n        max_raw_scores, _ = torch.max(raw_scores, dim=1, keepdim=True)\n        # Deduct scores by max_scores to avoid value explosion.\n        scores = torch.exp(raw_scores - max_raw_scores)\n        scores_doc = scores[:, :-1]  # shape=[batch_size, num_docs]\n        scores_no_click = scores[:, [-1]]  # shape=[batch_size, 1]\n\n        # Calculate the item-wise Q values.\n        # q_values.shape=[batch_size, num_docs+1]\n        q_values = self.q_model(user, doc)\n        q_values_doc = q_values[:, :-1]  # shape=[batch_size, num_docs]\n        q_values_no_click = q_values[:, [-1]]  # shape=[batch_size, 1]\n\n        # Calculate per-slate Q values.\n        batch_size, _ = q_values_doc.shape\n        # Move pre-stored slate combinations to GPU, if necessary.\n        if self.slates.device != q_values_doc.device:\n            self.slates = self.slates.to(q_values.device)\n        # slate_decomp_q_values.shape: [batch_size, num_slates, slate_size]\n        slate_decomp_q_values = torch.gather(\n            # input.shape: [batch_size, num_slates, num_docs]\n            input=q_values_doc.unsqueeze(1).expand(-1, self.num_slates, -1),\n            dim=2,\n            # index.shape: [batch_size, num_slates, slate_size]\n            index=self.slates.unsqueeze(0).expand(batch_size, -1, -1),\n        )\n        # slate_scores.shape: [batch_size, num_slates, slate_size]\n        slate_scores = torch.gather(\n            # input.shape: [batch_size, num_slates, num_docs]\n            input=scores_doc.unsqueeze(1).expand(-1, self.num_slates, -1),\n            dim=2,\n            # index.shape: [batch_size, num_slates, slate_size]\n            index=self.slates.unsqueeze(0).expand(batch_size, -1, -1),\n        )\n\n        # slate_q_values.shape: [batch_size, num_slates]\n        slate_q_values = (\n            (slate_decomp_q_values * slate_scores).sum(dim=2)\n            + (q_values_no_click * scores_no_click)\n        ) / (slate_scores.sum(dim=2) + scores_no_click)\n        return slate_q_values\n\n    def forward(\n        self,\n        input_dict: Dict[str, TensorType],\n        state: List[TensorType],\n        seq_lens: TensorType,\n    ) -> Tuple[TensorType, List[TensorType]]:\n        # user.shape: [batch_size, embedding_size]\n        user = input_dict[SampleBatch.OBS][\"user\"]\n        # doc.shape: [batch_size, num_docs, embedding_size]\n        doc = torch.cat(\n            [val.unsqueeze(1) for val in input_dict[SampleBatch.OBS][\"doc\"].values()], 1\n        )\n\n        slates_selected, _, _ = self.choose_slate(user, doc)\n\n        state_out = []\n        return slates_selected, state_out\n </s> add         return self.q_model(user, docs) </s> remove     def forward(self, user: TensorType, doc: TensorType) -> TensorType:\n        \"\"\"Evaluate the user-doc Q model\n </s> add         self.q_nets = nn.ModuleList()\n        for i in range(self.num_candidates):\n            layers = nn.Sequential()\n            ins = 2 * self.embedding_size\n            for j, h in enumerate(fcnet_hiddens_per_candidate):\n                layers.add_module(\n                    f\"q_layer_{i}_{j}\",\n                    SlimFC(in_size=ins, out_size=h, activation_fn=\"relu\"),\n                )\n                ins = h\n            layers.add_module(f\"q_out_{i}\", SlimFC(ins, 1, activation_fn=None))\n\n            self.q_nets.append(layers)\n\n    def forward(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors. </s> add         self.num_outputs = num_outputs\n </s> remove         batch_size, num_docs, embedding_size = doc.shape\n        doc_flat = doc.view((batch_size * num_docs, embedding_size))\n\n        # Concat everything.\n        # No user features.\n        if user.shape[-1] == 0:\n            x = doc_flat\n        # User features, repeat user embeddings n times (n=num docs).\n        else:\n            user_repeated = user.repeat(num_docs, 1)\n            x = torch.cat([user_repeated, doc_flat], dim=1)\n\n        x = self.layers(x)\n </s> add         q_outs = []\n        for i in range(self.num_candidates):\n            user_cat_doc = torch.cat([user, docs[i]], dim=1)\n            q_outs.append(self.q_nets[i](user_cat_doc))", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace", "code_tokens": " <mask>             double_q: Whether to apply double-Q correction on the target\n <mask>                 term.\n <mask> \n <mask>         Returns:\n <mask>             slate_selected (TensorType): Indices of documents selected for\n <mask>                 the slate, with shape (batch_size, slate_size).\n <mask>             best_slate_q_value (TensorType): The Q-value of the selected slate,\n <mask>                 with shape (batch_size).\n <mask>         \"\"\"\n <mask>         slate_q_values = self.get_per_slate_q_values(user, doc)\n <mask> \n <mask>         if target_slate_q_values is not None:\n <mask>             assert self.double_q\n <mask>             max_values, best_target_indices = torch.max(target_slate_q_values, dim=1)\n <mask>             best_slate_q_value = torch.gather(\n <mask>                 slate_q_values, 1, best_target_indices.unsqueeze(1)\n <mask>             ).squeeze(1)\n <mask>             # slates_selected.shape: [batch_size, slate_size]\n <mask>             slates_selected = self.slates[best_target_indices]\n <mask>         else:\n <mask>             # Find the slate that maximizes q value.\n <mask>             best_slate_q_value, max_idx = torch.max(slate_q_values, dim=1)\n <mask>             # slates_selected.shape: [batch_size, slate_size]\n <mask>             slates_selected = self.slates[max_idx]\n <mask> \n <mask>         return slates_selected, best_slate_q_value, slate_q_values\n <mask> \n <mask>     def get_per_slate_q_values(self, user, doc):\n <mask>         # Compute item scores (proportional to click probabilities)\n <mask>         # raw_scores.shape=[batch_size, num_docs+1]\n <mask>         raw_scores = self.choice_model(user, doc)\n <mask>         # max_raw_scores.shape=[batch_size, 1]\n <mask>         max_raw_scores, _ = torch.max(raw_scores, dim=1, keepdim=True)\n <mask>         # Deduct scores by max_scores to avoid value explosion.\n <mask>         scores = torch.exp(raw_scores - max_raw_scores)\n <mask>         scores_doc = scores[:, :-1]  # shape=[batch_size, num_docs]\n <mask>         scores_no_click = scores[:, [-1]]  # shape=[batch_size, 1]\n <mask> \n <mask>         # Calculate the item-wise Q values.\n <mask>         # q_values.shape=[batch_size, num_docs+1]\n <mask>         q_values = self.q_model(user, doc)\n <mask>         q_values_doc = q_values[:, :-1]  # shape=[batch_size, num_docs]\n <mask>         q_values_no_click = q_values[:, [-1]]  # shape=[batch_size, 1]\n <mask> \n <mask>         # Calculate per-slate Q values.\n <mask>         batch_size, _ = q_values_doc.shape\n <mask>         # Move pre-stored slate combinations to GPU, if necessary.\n <mask>         if self.slates.device != q_values_doc.device:\n <mask>             self.slates = self.slates.to(q_values.device)\n <mask>         # slate_decomp_q_values.shape: [batch_size, num_slates, slate_size]\n <mask>         slate_decomp_q_values = torch.gather(\n <mask>             # input.shape: [batch_size, num_slates, num_docs]\n <mask>             input=q_values_doc.unsqueeze(1).expand(-1, self.num_slates, -1),\n <mask>             dim=2,\n <mask>             # index.shape: [batch_size, num_slates, slate_size]\n <mask>             index=self.slates.unsqueeze(0).expand(batch_size, -1, -1),\n <mask>         )\n <mask>         # slate_scores.shape: [batch_size, num_slates, slate_size]\n <mask>         slate_scores = torch.gather(\n <mask>             # input.shape: [batch_size, num_slates, num_docs]\n <mask>             input=scores_doc.unsqueeze(1).expand(-1, self.num_slates, -1),\n <mask>             dim=2,\n <mask>             # index.shape: [batch_size, num_slates, slate_size]\n <mask>             index=self.slates.unsqueeze(0).expand(batch_size, -1, -1),\n <mask>         )\n <mask> \n <mask>         # slate_q_values.shape: [batch_size, num_slates]\n <mask>         slate_q_values = (\n <mask>             (slate_decomp_q_values * slate_scores).sum(dim=2)\n <mask>             + (q_values_no_click * scores_no_click)\n <mask>         ) / (slate_scores.sum(dim=2) + scores_no_click)\n <mask>         return slate_q_values\n <mask> \n <mask>     def forward(\n <mask>         self,\n <mask>         input_dict: Dict[str, TensorType],\n <mask>         state: List[TensorType],\n <mask>         seq_lens: TensorType,\n <mask>     ) -> Tuple[TensorType, List[TensorType]]:\n <mask>         # user.shape: [batch_size, embedding_size]\n <mask>         user = input_dict[SampleBatch.OBS][\"user\"]\n <mask>         # doc.shape: [batch_size, num_docs, embedding_size]\n <mask>         doc = torch.cat(\n <mask>             [val.unsqueeze(1) for val in input_dict[SampleBatch.OBS][\"doc\"].values()], 1\n <mask>         )\n <mask> \n <mask>         slates_selected, _, _ = self.choose_slate(user, doc)\n <mask> \n <mask>         state_out = []\n <mask>         return slates_selected, state_out\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove         self.q_model = QValueModel(user_embedding_size + doc_embedding_size, q_hiddens)\n        self.slate_size = len(action_space.nvec)\n        self.double_q = double_q\n\n        self.num_docs = num_docs\n        self.indices = torch.arange(\n            self.num_docs, dtype=torch.long\n        )  # , device=doc.device)\n        # slates.shape = [num_slates, slate_size]\n        self.slates = torch.combinations(self.indices, r=self.slate_size)\n        self.num_slates, _ = self.slates.shape\n\n    def choose_slate(\n        self,\n        user: TensorType,\n        doc: TensorType,\n        target_slate_q_values: Optional[TensorType] = None,\n    ) -> Tuple[TensorType, TensorType]:\n        \"\"\"Build a slate by selecting from candidate documents\n </s> add         self.q_model = QValueModel(self.obs_space, fcnet_hiddens_per_candidate)\n\n    def get_q_values(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors. </s> remove         batch_size, num_docs, embedding_size = doc.shape\n        doc_flat = doc.view((batch_size * num_docs, embedding_size))\n\n        # Concat everything.\n        # No user features.\n        if user.shape[-1] == 0:\n            x = doc_flat\n        # User features, repeat user embeddings n times (n=num docs).\n        else:\n            user_repeated = user.repeat(num_docs, 1)\n            x = torch.cat([user_repeated, doc_flat], dim=1)\n\n        x = self.layers(x)\n </s> add         q_outs = []\n        for i in range(self.num_candidates):\n            user_cat_doc = torch.cat([user, docs[i]], dim=1)\n            q_outs.append(self.q_nets[i](user_cat_doc)) </s> remove         # Similar to Google's SlateQ implementation in RecSim, we force the\n        # Q-values to zeros if there are no clicks.\n        # See https://arxiv.org/abs/1905.12767 for details.\n        x_no_click = torch.zeros((batch_size, 1), device=x.device)\n\n        return torch.cat([x.view((batch_size, num_docs)), x_no_click], dim=1)\n </s> add         return torch.cat(q_outs, dim=1) </s> remove             user: User embeddings of shape (batch_size,\n                embedding_size).\n            doc: Doc embeddings of shape (batch_size, num_docs,\n                embedding_size).\n            double_q: Whether to apply double-Q correction on the target\n                term.\n </s> add             user: [B x u] where u=embedding of user features.\n            docs: List[[B x d]] where d=embedding of doc features. Each item in the\n                list represents one document candidate. </s> remove             The q_values per document of shape (batch_size, num_docs + 1). +1 due to\n            also having a Q-value for the non-interaction (no click/no doc).\n </s> add             Tensor ([batch, num candidates) of Q-values.\n            1 Q-value per document candidate.", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_model.py"}
{"docstring_tokens": "keep replace replace keep keep keep replace replace keep keep", "code_tokens": " <mask> import numpy as np\n <mask> import time\n <mask> from typing import Dict, List, Tuple, Type\n <mask> \n <mask> import ray\n <mask> from ray.rllib.agents.sac.sac_torch_policy import TargetNetworkMixin\n <mask> from ray.rllib.agents.slateq.slateq_torch_model import SlateQModel\n <mask> from ray.rllib.models.modelv2 import ModelV2, restore_original_dimensions\n <mask> from ray.rllib.models.torch.torch_action_dist import (\n <mask>     TorchCategorical,\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove from typing import Dict, List, Optional, Sequence, Tuple\n </s> add from typing import List, Sequence </s> remove from ray.rllib.policy.sample_batch import SampleBatch\n </s> add  </s> add from ray.rllib.models.torch.misc import SlimFC </s> remove             q_hiddens: The list of hidden layer sizes for the QValueModel.\n </s> add             fcnet_hiddens_per_candidate: List of layer-sizes for each(!) of the\n                candidate documents. </s> remove             user: User embeddings of shape (batch_size,\n                embedding_size).\n            doc: Doc embeddings of shape (batch_size, num_docs,\n                embedding_size).\n            double_q: Whether to apply double-Q correction on the target\n                term.\n </s> add             user: [B x u] where u=embedding of user features.\n            docs: List[[B x d]] where d=embedding of doc features. Each item in the\n                list represents one document candidate.", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_policy.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     Returns:\n <mask>         Tuple consisting of 1) Q-model and 2) an action distribution class.\n <mask>     \"\"\"\n <mask>     model = SlateQModel(\n <mask>         obs_space,\n <mask>         action_space,\n <mask>         model_config=config[\"model\"],\n <mask>         name=\"slateq_model\",\n <mask>         user_embedding_size=obs_space.original_space[\"user\"].shape[0],\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove         user_embedding_size=obs_space.original_space[\"user\"].shape[0],\n        doc_embedding_size=obs_space.original_space[\"doc\"][\"0\"].shape[0],\n        num_docs=len(obs_space.original_space[\"doc\"].spaces),\n        q_hiddens=config[\"hiddens\"],\n        double_q=config[\"double_q\"],\n </s> add         fcnet_hiddens_per_candidate=config[\"fcnet_hiddens_per_candidate\"], </s> add         num_outputs=action_space.nvec[0], </s> remove             # real imact, and can be set arbitrarily. TODO: fix this.\n </s> add             # real impact, and can be set arbitrarily. TODO: fix this. </s> remove class SlateQModel(TorchModelV2, nn.Module):\n    \"\"\"The SlateQ model class.\n </s> add class SlateQTorchModel(TorchModelV2, nn.Module):\n    \"\"\"Initializes a SlateQTFModel instance.\n\n    Model includes both the user choice model and the Q-value model. </s> remove         batch_size, num_docs, embedding_size = doc.shape\n        doc_flat = doc.view((batch_size * num_docs, embedding_size))\n\n        # Concat everything.\n        # No user features.\n        if user.shape[-1] == 0:\n            x = doc_flat\n        # User features, repeat user embeddings n times (n=num docs).\n        else:\n            user_repeated = user.repeat(num_docs, 1)\n            x = torch.cat([user_repeated, doc_flat], dim=1)\n\n        x = self.layers(x)\n </s> add         q_outs = []\n        for i in range(self.num_candidates):\n            user_cat_doc = torch.cat([user, docs[i]], dim=1)\n            q_outs.append(self.q_nets[i](user_cat_doc)) </s> remove     It includes both the user choice model and the Q-value model.\n </s> add     For the Q-value model, each document candidate receives one full Q-value\n    stack, defined by `fcnet_hiddens_per_candidate`. The input to each of these\n    Q-value stacks is always {[user] concat [document[i]] for i in document_candidates}.\n\n    Extra model kwargs:\n        fcnet_hiddens_per_candidate: List of layer-sizes for each(!) of the\n            candidate documents.", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_policy.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>     model = SlateQTorchModel(\n <mask>         obs_space,\n <mask>         action_space,\n <mask>         model_config=config[\"model\"],\n <mask>         name=\"slateq_model\",\n <mask>         fcnet_hiddens_per_candidate=config[\"fcnet_hiddens_per_candidate\"],\n <mask>     )\n <mask> \n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove         user_embedding_size=obs_space.original_space[\"user\"].shape[0],\n        doc_embedding_size=obs_space.original_space[\"doc\"][\"0\"].shape[0],\n        num_docs=len(obs_space.original_space[\"doc\"].spaces),\n        q_hiddens=config[\"hiddens\"],\n        double_q=config[\"double_q\"],\n </s> add         fcnet_hiddens_per_candidate=config[\"fcnet_hiddens_per_candidate\"], </s> remove     model = SlateQModel(\n </s> add     model = SlateQTorchModel( </s> remove             # real imact, and can be set arbitrarily. TODO: fix this.\n </s> add             # real impact, and can be set arbitrarily. TODO: fix this. </s> add         self.num_outputs = num_outputs\n </s> remove         self.q_model = QValueModel(user_embedding_size + doc_embedding_size, q_hiddens)\n        self.slate_size = len(action_space.nvec)\n        self.double_q = double_q\n\n        self.num_docs = num_docs\n        self.indices = torch.arange(\n            self.num_docs, dtype=torch.long\n        )  # , device=doc.device)\n        # slates.shape = [num_slates, slate_size]\n        self.slates = torch.combinations(self.indices, r=self.slate_size)\n        self.num_slates, _ = self.slates.shape\n\n    def choose_slate(\n        self,\n        user: TensorType,\n        doc: TensorType,\n        target_slate_q_values: Optional[TensorType] = None,\n    ) -> Tuple[TensorType, TensorType]:\n        \"\"\"Build a slate by selecting from candidate documents\n </s> add         self.q_model = QValueModel(self.obs_space, fcnet_hiddens_per_candidate)\n\n    def get_q_values(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors. </s> remove     def forward(self, user: TensorType, doc: TensorType) -> TensorType:\n        \"\"\"Evaluate the user-doc Q model\n </s> add         self.q_nets = nn.ModuleList()\n        for i in range(self.num_candidates):\n            layers = nn.Sequential()\n            ins = 2 * self.embedding_size\n            for j, h in enumerate(fcnet_hiddens_per_candidate):\n                layers.add_module(\n                    f\"q_layer_{i}_{j}\",\n                    SlimFC(in_size=ins, out_size=h, activation_fn=\"relu\"),\n                )\n                ins = h\n            layers.add_module(f\"q_out_{i}\", SlimFC(ins, 1, activation_fn=None))\n\n            self.q_nets.append(layers)\n\n    def forward(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors.", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_policy.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         obs_space,\n <mask>         action_space,\n <mask>         model_config=config[\"model\"],\n <mask>         name=\"slateq_model\",\n <mask>         user_embedding_size=obs_space.original_space[\"user\"].shape[0],\n <mask>         doc_embedding_size=obs_space.original_space[\"doc\"][\"0\"].shape[0],\n <mask>         num_docs=len(obs_space.original_space[\"doc\"].spaces),\n <mask>         q_hiddens=config[\"hiddens\"],\n <mask>         double_q=config[\"double_q\"],\n <mask>     )\n <mask> \n <mask>     policy.target_model = SlateQModel(\n <mask>         obs_space,\n <mask>         action_space,\n </s> [RLlib] SlateQ: Add a hard-task learning test to weekly regression suite. (#22544) </s> remove     model = SlateQModel(\n </s> add     model = SlateQTorchModel( </s> add         num_outputs=action_space.nvec[0], </s> remove             # real imact, and can be set arbitrarily. TODO: fix this.\n </s> add             # real impact, and can be set arbitrarily. TODO: fix this. </s> add         self.num_outputs = num_outputs\n </s> remove         self.q_model = QValueModel(user_embedding_size + doc_embedding_size, q_hiddens)\n        self.slate_size = len(action_space.nvec)\n        self.double_q = double_q\n\n        self.num_docs = num_docs\n        self.indices = torch.arange(\n            self.num_docs, dtype=torch.long\n        )  # , device=doc.device)\n        # slates.shape = [num_slates, slate_size]\n        self.slates = torch.combinations(self.indices, r=self.slate_size)\n        self.num_slates, _ = self.slates.shape\n\n    def choose_slate(\n        self,\n        user: TensorType,\n        doc: TensorType,\n        target_slate_q_values: Optional[TensorType] = None,\n    ) -> Tuple[TensorType, TensorType]:\n        \"\"\"Build a slate by selecting from candidate documents\n </s> add         self.q_model = QValueModel(self.obs_space, fcnet_hiddens_per_candidate)\n\n    def get_q_values(self, user: TensorType, docs: List[TensorType]) -> TensorType:\n        \"\"\"Returns Q-values, 1 for each candidate document, given user and doc tensors. </s> remove         slate_q_values = self.get_per_slate_q_values(user, doc)\n\n        if target_slate_q_values is not None:\n            assert self.double_q\n            max_values, best_target_indices = torch.max(target_slate_q_values, dim=1)\n            best_slate_q_value = torch.gather(\n                slate_q_values, 1, best_target_indices.unsqueeze(1)\n            ).squeeze(1)\n            # slates_selected.shape: [batch_size, slate_size]\n            slates_selected = self.slates[best_target_indices]\n        else:\n            # Find the slate that maximizes q value.\n            best_slate_q_value, max_idx = torch.max(slate_q_values, dim=1)\n            # slates_selected.shape: [batch_size, slate_size]\n            slates_selected = self.slates[max_idx]\n\n        return slates_selected, best_slate_q_value, slate_q_values\n\n    def get_per_slate_q_values(self, user, doc):\n        # Compute item scores (proportional to click probabilities)\n        # raw_scores.shape=[batch_size, num_docs+1]\n        raw_scores = self.choice_model(user, doc)\n        # max_raw_scores.shape=[batch_size, 1]\n        max_raw_scores, _ = torch.max(raw_scores, dim=1, keepdim=True)\n        # Deduct scores by max_scores to avoid value explosion.\n        scores = torch.exp(raw_scores - max_raw_scores)\n        scores_doc = scores[:, :-1]  # shape=[batch_size, num_docs]\n        scores_no_click = scores[:, [-1]]  # shape=[batch_size, 1]\n\n        # Calculate the item-wise Q values.\n        # q_values.shape=[batch_size, num_docs+1]\n        q_values = self.q_model(user, doc)\n        q_values_doc = q_values[:, :-1]  # shape=[batch_size, num_docs]\n        q_values_no_click = q_values[:, [-1]]  # shape=[batch_size, 1]\n\n        # Calculate per-slate Q values.\n        batch_size, _ = q_values_doc.shape\n        # Move pre-stored slate combinations to GPU, if necessary.\n        if self.slates.device != q_values_doc.device:\n            self.slates = self.slates.to(q_values.device)\n        # slate_decomp_q_values.shape: [batch_size, num_slates, slate_size]\n        slate_decomp_q_values = torch.gather(\n            # input.shape: [batch_size, num_slates, num_docs]\n            input=q_values_doc.unsqueeze(1).expand(-1, self.num_slates, -1),\n            dim=2,\n            # index.shape: [batch_size, num_slates, slate_size]\n            index=self.slates.unsqueeze(0).expand(batch_size, -1, -1),\n        )\n        # slate_scores.shape: [batch_size, num_slates, slate_size]\n        slate_scores = torch.gather(\n            # input.shape: [batch_size, num_slates, num_docs]\n            input=scores_doc.unsqueeze(1).expand(-1, self.num_slates, -1),\n            dim=2,\n            # index.shape: [batch_size, num_slates, slate_size]\n            index=self.slates.unsqueeze(0).expand(batch_size, -1, -1),\n        )\n\n        # slate_q_values.shape: [batch_size, num_slates]\n        slate_q_values = (\n            (slate_decomp_q_values * slate_scores).sum(dim=2)\n            + (q_values_no_click * scores_no_click)\n        ) / (slate_scores.sum(dim=2) + scores_no_click)\n        return slate_q_values\n\n    def forward(\n        self,\n        input_dict: Dict[str, TensorType],\n        state: List[TensorType],\n        seq_lens: TensorType,\n    ) -> Tuple[TensorType, List[TensorType]]:\n        # user.shape: [batch_size, embedding_size]\n        user = input_dict[SampleBatch.OBS][\"user\"]\n        # doc.shape: [batch_size, num_docs, embedding_size]\n        doc = torch.cat(\n            [val.unsqueeze(1) for val in input_dict[SampleBatch.OBS][\"doc\"].values()], 1\n        )\n\n        slates_selected, _, _ = self.choose_slate(user, doc)\n\n        state_out = []\n        return slates_selected, state_out\n </s> add         return self.q_model(user, docs)", "html_url": "https://github.com/ray-project/ray/commit/7b687e6cd849046f62e2631df9926c7986da9c29", "file_name": "rllib/agents/slateq/slateq_torch_policy.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>                 RAY_CI_STREAMING_JAVA_AFFECTED = 1\n <mask>             elif changed_file.startswith(\"streaming/python\"):\n <mask>                 RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n <mask>             else:\n <mask>                 RAY_CI_TUNE_AFFECTED = 1\n <mask>                 RAY_CI_RLLIB_AFFECTED = 1\n <mask>                 RAY_CI_SERVE_AFFECTED = 1\n <mask>                 RAY_CI_JAVA_AFFECTED = 1\n <mask>                 RAY_CI_PYTHON_AFFECTED = 1\n </s> [Streaming] Streaming data transfer java (#6474) </s> add                 RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n                RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add         RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n        RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> remove java_binary(\n    name = \"streaming_tests\",\n    main_class = \"org.testng.TestNG\",\n    data = [\"streaming/testng.xml\"],\n    args = [\"java/streaming/testng.xml\"],\n    runtime_deps = [\n        \":org_ray_ray_streaming_test\",\n    ],\n)\n\n </s> add  </s> remove define_java_module(\n    name = \"streaming\",\n    deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n    ],\n    define_test_lib = True,\n    test_deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \":org_ray_ray_streaming\",\n        \"@maven//:com_beust_jcommander\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n        \"@maven//:org_testng_testng\",\n    ],\n)\n\n </s> add  </s> add     visibility = [\"//visibility:public\"] </s> add     visibility = [\"//visibility:public\"]", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "ci/travis/determine_tests_to_run.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask>                 RAY_CI_PYTHON_AFFECTED = 1\n <mask>                 RAY_CI_LINUX_WHEELS_AFFECTED = 1\n <mask>                 RAY_CI_MACOS_WHEELS_AFFECTED = 1\n <mask>                 RAY_CI_STREAMING_CPP_AFFECTED = 1\n <mask>     else:\n <mask>         RAY_CI_TUNE_AFFECTED = 1\n <mask>         RAY_CI_RLLIB_AFFECTED = 1\n <mask>         RAY_CI_SERVE_AFFECTED = 1\n </s> [Streaming] Streaming data transfer java (#6474) </s> add             elif changed_file.startswith(\"streaming/java\"):\n                RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add         RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n        RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> remove java_binary(\n    name = \"streaming_tests\",\n    main_class = \"org.testng.TestNG\",\n    data = [\"streaming/testng.xml\"],\n    args = [\"java/streaming/testng.xml\"],\n    runtime_deps = [\n        \":org_ray_ray_streaming_test\",\n    ],\n)\n\n </s> add  </s> remove define_java_module(\n    name = \"streaming\",\n    deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n    ],\n    define_test_lib = True,\n    test_deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \":org_ray_ray_streaming\",\n        \"@maven//:com_beust_jcommander\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n        \"@maven//:org_testng_testng\",\n    ],\n)\n\n </s> add  </s> add     visibility = [\"//visibility:public\"] </s> add     visibility = [\"//visibility:public\"]", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "ci/travis/determine_tests_to_run.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>         RAY_CI_LINUX_WHEELS_AFFECTED = 1\n <mask>         RAY_CI_MACOS_WHEELS_AFFECTED = 1\n <mask>         RAY_CI_STREAMING_CPP_AFFECTED = 1\n <mask> \n <mask>     # Log the modified environment variables visible in console.\n <mask>     for output_stream in [sys.stdout, sys.stderr]:\n <mask>         _print = partial(print, file=output_stream)\n </s> [Streaming] Streaming data transfer java (#6474) </s> add                 RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n                RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add             elif changed_file.startswith(\"streaming/java\"):\n                RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> remove         \":streaming_tests\",\n </s> add  </s> add     \"ray/streaming/_streaming.so\", </s> remove         \"streaming_tests_deploy.jar\",\n        \"streaming_tests_deploy-src.jar\",\n </s> add  </s> remove java_binary(\n    name = \"streaming_tests\",\n    main_class = \"org.testng.TestNG\",\n    data = [\"streaming/testng.xml\"],\n    args = [\"java/streaming/testng.xml\"],\n    runtime_deps = [\n        \":org_ray_ray_streaming_test\",\n    ],\n)\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "ci/travis/determine_tests_to_run.py"}
{"docstring_tokens": "keep keep keep add", "code_tokens": " <mask>         _print(\"export RAY_CI_STREAMING_CPP_AFFECTED={}\"\n <mask>                .format(RAY_CI_STREAMING_CPP_AFFECTED))\n <mask>         _print(\"export RAY_CI_STREAMING_PYTHON_AFFECTED={}\"\n <mask>                .format(RAY_CI_STREAMING_PYTHON_AFFECTED))\n </s> [Streaming] Streaming data transfer java (#6474) </s> add         touch $$GENERATED_DIR/__init__.py </s> remove         cp -f $(location //java:org_ray_ray_streaming_pom) $$WORK_DIR/java/streaming/pom.xml\n </s> add  </s> add                 RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n                RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add         RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n        RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> remove     \"streaming/testng.xml\",\n </s> add  </s> remove     \"streaming\",\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "ci/travis/determine_tests_to_run.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> exports_files([\n <mask>     \"testng.xml\",\n <mask>     \"checkstyle.xml\",\n <mask>     \"checkstyle-suppressions.xml\",\n <mask>     \"streaming/testng.xml\",\n <mask> ])\n <mask> \n <mask> all_modules = [\n <mask>     \"api\",\n <mask>     \"runtime\",\n </s> [Streaming] Streaming data transfer java (#6474) </s> remove     \"streaming\",\n </s> add  </s> remove         \":streaming_tests\",\n </s> add  </s> add     visibility = [\"//visibility:public\"] </s> remove         \"streaming_tests_deploy.jar\",\n        \"streaming_tests_deploy-src.jar\",\n </s> add  </s> add     \"ray/streaming/generated\", </s> add     visibility = [\"//visibility:public\"]", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/BUILD.bazel"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     \"api\",\n <mask>     \"runtime\",\n <mask>     \"test\",\n <mask>     \"tutorial\",\n <mask>     \"streaming\",\n <mask> ]\n <mask> \n <mask> java_import(\n <mask>     name = \"all_modules\",\n <mask>     jars = [\n </s> [Streaming] Streaming data transfer java (#6474) </s> remove         \":streaming_tests\",\n </s> add  </s> remove     \"streaming/testng.xml\",\n </s> add  </s> add     visibility = [\"//visibility:public\"] </s> add     visibility = [\"//visibility:public\"] </s> remove define_java_module(\n    name = \"streaming\",\n    deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n    ],\n    define_test_lib = True,\n    test_deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \":org_ray_ray_streaming\",\n        \"@maven//:com_beust_jcommander\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n        \"@maven//:org_testng_testng\",\n    ],\n)\n\n </s> add  </s> add     \"ray/streaming/generated\",", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/BUILD.bazel"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         \"liborg_ray_ray_\" + module + \"-src.jar\" for module in all_modules\n <mask>     ] + [\n <mask>         \"all_tests_deploy.jar\",\n <mask>         \"all_tests_deploy-src.jar\",\n <mask>         \"streaming_tests_deploy.jar\",\n <mask>         \"streaming_tests_deploy-src.jar\",\n <mask>     ],\n <mask>     deps = [\n <mask>         \":org_ray_ray_\" + module for module in all_modules\n <mask>     ] + [\n <mask>         \":all_tests\",\n </s> [Streaming] Streaming data transfer java (#6474) </s> remove         \":streaming_tests\",\n </s> add  </s> remove     \"streaming/testng.xml\",\n </s> add  </s> add         RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n        RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add     \"ray/streaming/generated\", </s> remove     \"streaming\",\n </s> add  </s> add         touch $$GENERATED_DIR/__init__.py", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/BUILD.bazel"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     deps = [\n <mask>         \":org_ray_ray_\" + module for module in all_modules\n <mask>     ] + [\n <mask>         \":all_tests\",\n <mask>         \":streaming_tests\",\n <mask>     ],\n <mask> )\n <mask> \n <mask> define_java_module(\n <mask>     name = \"api\",\n </s> [Streaming] Streaming data transfer java (#6474) </s> remove         \"streaming_tests_deploy.jar\",\n        \"streaming_tests_deploy-src.jar\",\n </s> add  </s> add     visibility = [\"//visibility:public\"] </s> remove     \"streaming\",\n </s> add  </s> remove define_java_module(\n    name = \"streaming\",\n    deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n    ],\n    define_test_lib = True,\n    test_deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \":org_ray_ray_streaming\",\n        \"@maven//:com_beust_jcommander\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n        \"@maven//:org_testng_testng\",\n    ],\n)\n\n </s> add  </s> add     visibility = [\"//visibility:public\"] </s> remove java_binary(\n    name = \"streaming_tests\",\n    main_class = \"org.testng.TestNG\",\n    data = [\"streaming/testng.xml\"],\n    args = [\"java/streaming/testng.xml\"],\n    runtime_deps = [\n        \":org_ray_ray_streaming_test\",\n    ],\n)\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/BUILD.bazel"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>         \"@maven//:com_sun_xml_bind_jaxb_impl\",\n <mask>     ],\n <mask> )\n <mask> \n <mask> define_java_module(\n <mask>     name = \"runtime\",\n <mask>     additional_srcs = [\n <mask>         \":all_java_proto\",\n </s> [Streaming] Streaming data transfer java (#6474) </s> add     visibility = [\"//visibility:public\"] </s> remove define_java_module(\n    name = \"streaming\",\n    deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n    ],\n    define_test_lib = True,\n    test_deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \":org_ray_ray_streaming\",\n        \"@maven//:com_beust_jcommander\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n        \"@maven//:org_testng_testng\",\n    ],\n)\n\n </s> add  </s> remove         \":streaming_tests\",\n </s> add  </s> remove java_binary(\n    name = \"streaming_tests\",\n    main_class = \"org.testng.TestNG\",\n    data = [\"streaming/testng.xml\"],\n    args = [\"java/streaming/testng.xml\"],\n    runtime_deps = [\n        \":org_ray_ray_streaming_test\",\n    ],\n)\n\n </s> add  </s> remove     \"streaming\",\n </s> add  </s> add         \"@maven//:net_java_dev_jna_jna\",", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/BUILD.bazel"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask>         \"@maven//:org_slf4j_slf4j_log4j12\",\n <mask>         \"@maven//:redis_clients_jedis\",\n <mask>     ],\n <mask>     visibility = [\"//visibility:public\"]\n <mask> )\n <mask> \n <mask> define_java_module(\n </s> [Streaming] Streaming data transfer java (#6474) </s> add     visibility = [\"//visibility:public\"] </s> add     visibility = [\"//visibility:public\"] </s> remove define_java_module(\n    name = \"streaming\",\n    deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n    ],\n    define_test_lib = True,\n    test_deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \":org_ray_ray_streaming\",\n        \"@maven//:com_beust_jcommander\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n        \"@maven//:org_testng_testng\",\n    ],\n)\n\n </s> add  </s> remove         \":streaming_tests\",\n </s> add  </s> remove java_binary(\n    name = \"streaming_tests\",\n    main_class = \"org.testng.TestNG\",\n    data = [\"streaming/testng.xml\"],\n    args = [\"java/streaming/testng.xml\"],\n    runtime_deps = [\n        \":org_ray_ray_streaming_test\",\n    ],\n)\n\n </s> add  </s> add             \"net.java.dev.jna:jna:5.5.0\"", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/BUILD.bazel"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>         \"@maven//:net_java_dev_jna_jna\",\n <mask>     ],\n <mask> )\n <mask> \n <mask> define_java_module(\n <mask>     name = \"tutorial\",\n <mask>     deps = [\n <mask>         \":org_ray_ray_api\",\n </s> [Streaming] Streaming data transfer java (#6474) </s> remove define_java_module(\n    name = \"streaming\",\n    deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n    ],\n    define_test_lib = True,\n    test_deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \":org_ray_ray_streaming\",\n        \"@maven//:com_beust_jcommander\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n        \"@maven//:org_testng_testng\",\n    ],\n)\n\n </s> add  </s> add         \"@maven//:net_java_dev_jna_jna\", </s> remove         \":streaming_tests\",\n </s> add  </s> remove java_binary(\n    name = \"streaming_tests\",\n    main_class = \"org.testng.TestNG\",\n    data = [\"streaming/testng.xml\"],\n    args = [\"java/streaming/testng.xml\"],\n    runtime_deps = [\n        \":org_ray_ray_streaming_test\",\n    ],\n)\n\n </s> add  </s> add     visibility = [\"//visibility:public\"] </s> remove     \"streaming\",\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/BUILD.bazel"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         \"@maven//:org_testng_testng\",\n <mask>     ],\n <mask> )\n <mask> \n <mask> define_java_module(\n <mask>     name = \"streaming\",\n <mask>     deps = [\n <mask>         \":org_ray_ray_api\",\n <mask>         \":org_ray_ray_runtime\",\n <mask>         \"@maven//:com_google_guava_guava\",\n <mask>         \"@maven//:org_slf4j_slf4j_api\",\n <mask>         \"@maven//:org_slf4j_slf4j_log4j12\",\n <mask>     ],\n <mask>     define_test_lib = True,\n <mask>     test_deps = [\n <mask>         \":org_ray_ray_api\",\n <mask>         \":org_ray_ray_runtime\",\n <mask>         \":org_ray_ray_streaming\",\n <mask>         \"@maven//:com_beust_jcommander\",\n <mask>         \"@maven//:com_google_guava_guava\",\n <mask>         \"@maven//:org_slf4j_slf4j_api\",\n <mask>         \"@maven//:org_slf4j_slf4j_log4j12\",\n <mask>         \"@maven//:org_testng_testng\",\n <mask>     ],\n <mask> )\n <mask> \n <mask> java_binary(\n <mask>     name = \"all_tests\",\n <mask>     main_class = \"org.testng.TestNG\",\n <mask>     data = [\"testng.xml\"],\n <mask>     args = [\"java/testng.xml\"],\n </s> [Streaming] Streaming data transfer java (#6474) </s> remove java_binary(\n    name = \"streaming_tests\",\n    main_class = \"org.testng.TestNG\",\n    data = [\"streaming/testng.xml\"],\n    args = [\"java/streaming/testng.xml\"],\n    runtime_deps = [\n        \":org_ray_ray_streaming_test\",\n    ],\n)\n\n </s> add  </s> add     visibility = [\"//visibility:public\"] </s> add         \"@maven//:net_java_dev_jna_jna\", </s> add     visibility = [\"//visibility:public\"] </s> remove         \":streaming_tests\",\n </s> add  </s> remove     \"streaming\",\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/BUILD.bazel"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         \":org_ray_ray_runtime_test\",\n <mask>     ],\n <mask> )\n <mask> \n <mask> java_binary(\n <mask>     name = \"streaming_tests\",\n <mask>     main_class = \"org.testng.TestNG\",\n <mask>     data = [\"streaming/testng.xml\"],\n <mask>     args = [\"java/streaming/testng.xml\"],\n <mask>     runtime_deps = [\n <mask>         \":org_ray_ray_streaming_test\",\n <mask>     ],\n <mask> )\n <mask> \n <mask> java_proto_compile(\n <mask>     name = \"common_java_proto\",\n <mask>     deps = [\"@//:common_proto\"],\n <mask> )\n <mask> \n </s> [Streaming] Streaming data transfer java (#6474) </s> remove define_java_module(\n    name = \"streaming\",\n    deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n    ],\n    define_test_lib = True,\n    test_deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \":org_ray_ray_streaming\",\n        \"@maven//:com_beust_jcommander\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n        \"@maven//:org_testng_testng\",\n    ],\n)\n\n </s> add  </s> add     visibility = [\"//visibility:public\"] </s> add     visibility = [\"//visibility:public\"] </s> remove         \":streaming_tests\",\n </s> add  </s> add             \"net.java.dev.jna:jna:5.5.0\" </s> remove             \"https://repo1.maven.org/maven2\",\n </s> add             \"https://repo1.maven.org/maven2/\",", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/BUILD.bazel"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         cp -f $(location //java:org_ray_ray_api_pom) $$WORK_DIR/java/api/pom.xml\n <mask>         cp -f $(location //java:org_ray_ray_runtime_pom) $$WORK_DIR/java/runtime/pom.xml\n <mask>         cp -f $(location //java:org_ray_ray_tutorial_pom) $$WORK_DIR/java/tutorial/pom.xml\n <mask>         cp -f $(location //java:org_ray_ray_test_pom) $$WORK_DIR/java/test/pom.xml\n <mask>         cp -f $(location //java:org_ray_ray_streaming_pom) $$WORK_DIR/java/streaming/pom.xml\n <mask>         echo $$(date) > $@\n <mask>     \"\"\",\n <mask>     local = 1,\n <mask>     tags = [\"no-cache\"],\n <mask> )\n </s> [Streaming] Streaming data transfer java (#6474) </s> add         touch $$GENERATED_DIR/__init__.py </s> remove java_binary(\n    name = \"streaming_tests\",\n    main_class = \"org.testng.TestNG\",\n    data = [\"streaming/testng.xml\"],\n    args = [\"java/streaming/testng.xml\"],\n    runtime_deps = [\n        \":org_ray_ray_streaming_test\",\n    ],\n)\n\n </s> add  </s> add     visibility = [\"//visibility:public\"] </s> add     visibility = [\"//visibility:public\"] </s> remove define_java_module(\n    name = \"streaming\",\n    deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n    ],\n    define_test_lib = True,\n    test_deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \":org_ray_ray_streaming\",\n        \"@maven//:com_beust_jcommander\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n        \"@maven//:org_testng_testng\",\n    ],\n)\n\n </s> add  </s> add         \"@maven//:net_java_dev_jna_jna\",", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/BUILD.bazel"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask>             \"org.testng:testng:6.9.10\",\n <mask>             \"redis.clients:jedis:2.8.0\",\n <mask>         ],\n <mask>         repositories = [\n <mask>             \"https://repo1.maven.org/maven2/\",\n <mask>         ],\n <mask>     )\n </s> [Streaming] Streaming data transfer java (#6474) </s> remove             \"https://repo1.maven.org/maven2\",\n </s> add             \"https://repo1.maven.org/maven2/\", </s> remove java_binary(\n    name = \"streaming_tests\",\n    main_class = \"org.testng.TestNG\",\n    data = [\"streaming/testng.xml\"],\n    args = [\"java/streaming/testng.xml\"],\n    runtime_deps = [\n        \":org_ray_ray_streaming_test\",\n    ],\n)\n\n </s> add  </s> remove define_java_module(\n    name = \"streaming\",\n    deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n    ],\n    define_test_lib = True,\n    test_deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \":org_ray_ray_streaming\",\n        \"@maven//:com_beust_jcommander\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n        \"@maven//:org_testng_testng\",\n    ],\n)\n\n </s> add  </s> add     visibility = [\"//visibility:public\"] </s> add     visibility = [\"//visibility:public\"] </s> add         \"@maven//:net_java_dev_jna_jna\",", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/dependencies.bzl"}
{"docstring_tokens": "keep keep keep keep replace keep keep", "code_tokens": " <mask>             \"org.testng:testng:6.9.10\",\n <mask>             \"redis.clients:jedis:2.8.0\",\n <mask>         ],\n <mask>         repositories = [\n <mask>             \"https://repo1.maven.org/maven2\",\n <mask>         ],\n <mask>     )\n </s> [Streaming] Streaming data transfer java (#6474) </s> add             \"net.java.dev.jna:jna:5.5.0\" </s> remove java_binary(\n    name = \"streaming_tests\",\n    main_class = \"org.testng.TestNG\",\n    data = [\"streaming/testng.xml\"],\n    args = [\"java/streaming/testng.xml\"],\n    runtime_deps = [\n        \":org_ray_ray_streaming_test\",\n    ],\n)\n\n </s> add  </s> remove define_java_module(\n    name = \"streaming\",\n    deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n    ],\n    define_test_lib = True,\n    test_deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \":org_ray_ray_streaming\",\n        \"@maven//:com_beust_jcommander\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n        \"@maven//:org_testng_testng\",\n    ],\n)\n\n </s> add  </s> add     visibility = [\"//visibility:public\"] </s> add     visibility = [\"//visibility:public\"] </s> add         \"@maven//:net_java_dev_jna_jna\",", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/dependencies.bzl"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   <modules>\n <mask>     <module>api</module>\n <mask>     <module>runtime</module>\n <mask>     <module>test</module>\n <mask>     <module>streaming</module>\n <mask>     <module>tutorial</module>\n <mask>   </modules>\n <mask> \n <mask>   <properties>\n <mask>     <java.version>1.8</java.version>\n </s> [Streaming] Streaming data transfer java (#6474) </s> add         touch $$GENERATED_DIR/__init__.py </s> remove         cp -f $(location //java:org_ray_ray_streaming_pom) $$WORK_DIR/java/streaming/pom.xml\n </s> add  </s> add                 RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n                RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add         RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n        RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add         _print(\"export RAY_CI_STREAMING_JAVA_AFFECTED={}\"\n               .format(RAY_CI_STREAMING_JAVA_AFFECTED)) </s> remove     \"streaming/testng.xml\",\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/pom.xml"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>   <groupId>de.ruedigermoeller</groupId>\n <mask>   <artifactId>fst</artifactId>\n <mask>   <version>2.57</version>\n <mask> </dependency>\n <mask> <dependency>\n <mask>   <groupId>org.apache.commons</groupId>\n <mask>   <artifactId>commons-lang3</artifactId>\n <mask>   <version>3.4</version>\n <mask> </dependency>\n </s> [Streaming] Streaming data transfer java (#6474) </s> add         touch $$GENERATED_DIR/__init__.py </s> remove         cp -f $(location //java:org_ray_ray_streaming_pom) $$WORK_DIR/java/streaming/pom.xml\n </s> add  </s> add                 RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n                RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add         RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n        RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add         _print(\"export RAY_CI_STREAMING_JAVA_AFFECTED={}\"\n               .format(RAY_CI_STREAMING_JAVA_AFFECTED)) </s> remove     \"streaming/testng.xml\",\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/runtime/pom.xml"}
{"docstring_tokens": "keep keep replace keep keep replace keep keep keep", "code_tokens": " <mask> \n <mask> import com.google.common.base.Preconditions;\n <mask> import com.google.common.base.Strings;\n <mask> import java.io.File;\n <mask> import java.io.IOException;\n <mask> import java.lang.reflect.Field;\n <mask> import java.util.HashMap;\n <mask> import java.util.Map;\n <mask> import org.apache.commons.io.FileUtils;\n </s> [Streaming] Streaming data transfer java (#6474) </s> remove import org.ray.runtime.util.FileUtil;\n </s> add import org.ray.runtime.util.JniUtils; </s> add         touch $$GENERATED_DIR/__init__.py </s> remove java_binary(\n    name = \"streaming_tests\",\n    main_class = \"org.testng.TestNG\",\n    data = [\"streaming/testng.xml\"],\n    args = [\"java/streaming/testng.xml\"],\n    runtime_deps = [\n        \":org_ray_ray_streaming_test\",\n    ],\n)\n\n </s> add  </s> add                 RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n                RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add         RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n        RAY_CI_STREAMING_JAVA_AFFECTED = 1", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/runtime/src/main/java/org/ray/runtime/RayNativeRuntime.java"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> import org.ray.runtime.runner.RunManager;\n <mask> import org.ray.runtime.task.NativeTaskExecutor;\n <mask> import org.ray.runtime.task.NativeTaskSubmitter;\n <mask> import org.ray.runtime.task.TaskExecutor;\n <mask> import org.ray.runtime.util.FileUtil;\n <mask> import org.slf4j.Logger;\n <mask> import org.slf4j.LoggerFactory;\n <mask> \n <mask> /**\n <mask>  * Native runtime for cluster mode.\n </s> [Streaming] Streaming data transfer java (#6474) </s> remove import java.lang.reflect.Field;\n </s> add  </s> remove import com.google.common.base.Strings;\n </s> add  </s> remove         \"streaming_tests_deploy.jar\",\n        \"streaming_tests_deploy-src.jar\",\n </s> add  </s> remove         \":streaming_tests\",\n </s> add  </s> add         touch $$GENERATED_DIR/__init__.py </s> add         RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n        RAY_CI_STREAMING_JAVA_AFFECTED = 1", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/runtime/src/main/java/org/ray/runtime/RayNativeRuntime.java"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>   private long nativeCoreWorkerPointer;\n <mask> \n <mask>   static {\n <mask>     LOGGER.debug(\"Loading native libraries.\");\n <mask>     // Load native libraries.\n <mask>     String[] libraries = new String[]{\"core_worker_library_java\"};\n <mask>     for (String library : libraries) {\n <mask>       String fileName = System.mapLibraryName(library);\n <mask>       try (FileUtil.TempFile libFile = FileUtil.getTempFileFromResource(fileName)) {\n <mask>         System.load(libFile.getFile().getAbsolutePath());\n <mask>       }\n <mask>       LOGGER.debug(\"Native libraries loaded.\");\n <mask>     }\n <mask> \n <mask>     RayConfig globalRayConfig = RayConfig.create();\n <mask>     resetLibraryPath(globalRayConfig);\n <mask> \n <mask>     try {\n <mask>       FileUtils.forceMkdir(new File(globalRayConfig.logDir));\n </s> [Streaming] Streaming data transfer java (#6474) </s> remove     if (rayConfig.libraryPath.isEmpty()) {\n      return;\n    }\n\n    String path = System.getProperty(\"java.library.path\");\n    if (Strings.isNullOrEmpty(path)) {\n      path = \"\";\n    } else {\n      path += \":\";\n    }\n    path += String.join(\":\", rayConfig.libraryPath);\n\n    // This is a hack to reset library path at runtime,\n    // see https://stackoverflow.com/questions/15409223/.\n    System.setProperty(\"java.library.path\", path);\n    // Set sys_paths to null so that java.library.path will be re-evaluated next time it is needed.\n    final Field sysPathsField;\n    try {\n      sysPathsField = ClassLoader.class.getDeclaredField(\"sys_paths\");\n      sysPathsField.setAccessible(true);\n      sysPathsField.set(null, null);\n    } catch (NoSuchFieldException | IllegalAccessException e) {\n      LOGGER.error(\"Failed to set library path.\", e);\n    }\n </s> add     String separator = System.getProperty(\"path.separator\");\n    String libraryPath = String.join(separator, rayConfig.libraryPath);\n    JniUtils.resetLibraryPath(libraryPath); </s> add       // Set run-mode to `CLUSTER` explicitly, to prevent the DefaultWorker to receive\n      // a wrong run-mode parameter through jvm options.\n      System.setProperty(\"ray.run-mode\", \"CLUSTER\"); </s> add         RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n        RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add                 RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n                RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add             elif changed_file.startswith(\"streaming/java\"):\n                RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> remove java_binary(\n    name = \"streaming_tests\",\n    main_class = \"org.testng.TestNG\",\n    data = [\"streaming/testng.xml\"],\n    args = [\"java/streaming/testng.xml\"],\n    runtime_deps = [\n        \":org_ray_ray_streaming_test\",\n    ],\n)\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/runtime/src/main/java/org/ray/runtime/RayNativeRuntime.java"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     Runtime.getRuntime().addShutdownHook(new Thread(RayNativeRuntime::nativeShutdownHook));\n <mask>   }\n <mask> \n <mask>   private static void resetLibraryPath(RayConfig rayConfig) {\n <mask>     if (rayConfig.libraryPath.isEmpty()) {\n <mask>       return;\n <mask>     }\n <mask> \n <mask>     String path = System.getProperty(\"java.library.path\");\n <mask>     if (Strings.isNullOrEmpty(path)) {\n <mask>       path = \"\";\n <mask>     } else {\n <mask>       path += \":\";\n <mask>     }\n <mask>     path += String.join(\":\", rayConfig.libraryPath);\n <mask> \n <mask>     // This is a hack to reset library path at runtime,\n <mask>     // see https://stackoverflow.com/questions/15409223/.\n <mask>     System.setProperty(\"java.library.path\", path);\n <mask>     // Set sys_paths to null so that java.library.path will be re-evaluated next time it is needed.\n <mask>     final Field sysPathsField;\n <mask>     try {\n <mask>       sysPathsField = ClassLoader.class.getDeclaredField(\"sys_paths\");\n <mask>       sysPathsField.setAccessible(true);\n <mask>       sysPathsField.set(null, null);\n <mask>     } catch (NoSuchFieldException | IllegalAccessException e) {\n <mask>       LOGGER.error(\"Failed to set library path.\", e);\n <mask>     }\n <mask>   }\n <mask> \n <mask>   public RayNativeRuntime(RayConfig rayConfig, FunctionManager functionManager) {\n <mask>     super(rayConfig, functionManager);\n <mask> \n </s> [Streaming] Streaming data transfer java (#6474) </s> remove     // Load native libraries.\n    String[] libraries = new String[]{\"core_worker_library_java\"};\n    for (String library : libraries) {\n      String fileName = System.mapLibraryName(library);\n      try (FileUtil.TempFile libFile = FileUtil.getTempFileFromResource(fileName)) {\n        System.load(libFile.getFile().getAbsolutePath());\n      }\n      LOGGER.debug(\"Native libraries loaded.\");\n    }\n\n </s> add     // Expose ray ABI symbols which may be depended by other shared\n    // libraries such as libstreaming_java.so.\n    // See BUILD.bazel:libcore_worker_library_java.so\n    JniUtils.loadLibrary(\"core_worker_library_java\", true);\n    LOGGER.debug(\"Native libraries loaded.\"); </s> add       // Set run-mode to `CLUSTER` explicitly, to prevent the DefaultWorker to receive\n      // a wrong run-mode parameter through jvm options.\n      System.setProperty(\"ray.run-mode\", \"CLUSTER\"); </s> add                 RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n                RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add             elif changed_file.startswith(\"streaming/java\"):\n                RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> remove java_binary(\n    name = \"streaming_tests\",\n    main_class = \"org.testng.TestNG\",\n    data = [\"streaming/testng.xml\"],\n    args = [\"java/streaming/testng.xml\"],\n    runtime_deps = [\n        \":org_ray_ray_streaming_test\",\n    ],\n)\n\n </s> add  </s> remove define_java_module(\n    name = \"streaming\",\n    deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n    ],\n    define_test_lib = True,\n    test_deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \":org_ray_ray_streaming\",\n        \"@maven//:com_beust_jcommander\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n        \"@maven//:org_testng_testng\",\n    ],\n)\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/runtime/src/main/java/org/ray/runtime/RayNativeRuntime.java"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask> \n <mask>   public static void main(String[] args) {\n <mask>     try {\n <mask>       System.setProperty(\"ray.worker.mode\", \"WORKER\");\n <mask>       Thread.setDefaultUncaughtExceptionHandler((Thread t, Throwable e) -> {\n <mask>         LOGGER.error(\"Uncaught worker exception in thread {}: {}\", t, e);\n <mask>       });\n <mask>       Ray.init();\n <mask>       LOGGER.info(\"Worker started.\");\n </s> [Streaming] Streaming data transfer java (#6474) </s> remove     if (rayConfig.libraryPath.isEmpty()) {\n      return;\n    }\n\n    String path = System.getProperty(\"java.library.path\");\n    if (Strings.isNullOrEmpty(path)) {\n      path = \"\";\n    } else {\n      path += \":\";\n    }\n    path += String.join(\":\", rayConfig.libraryPath);\n\n    // This is a hack to reset library path at runtime,\n    // see https://stackoverflow.com/questions/15409223/.\n    System.setProperty(\"java.library.path\", path);\n    // Set sys_paths to null so that java.library.path will be re-evaluated next time it is needed.\n    final Field sysPathsField;\n    try {\n      sysPathsField = ClassLoader.class.getDeclaredField(\"sys_paths\");\n      sysPathsField.setAccessible(true);\n      sysPathsField.set(null, null);\n    } catch (NoSuchFieldException | IllegalAccessException e) {\n      LOGGER.error(\"Failed to set library path.\", e);\n    }\n </s> add     String separator = System.getProperty(\"path.separator\");\n    String libraryPath = String.join(separator, rayConfig.libraryPath);\n    JniUtils.resetLibraryPath(libraryPath); </s> remove     // Load native libraries.\n    String[] libraries = new String[]{\"core_worker_library_java\"};\n    for (String library : libraries) {\n      String fileName = System.mapLibraryName(library);\n      try (FileUtil.TempFile libFile = FileUtil.getTempFileFromResource(fileName)) {\n        System.load(libFile.getFile().getAbsolutePath());\n      }\n      LOGGER.debug(\"Native libraries loaded.\");\n    }\n\n </s> add     // Expose ray ABI symbols which may be depended by other shared\n    // libraries such as libstreaming_java.so.\n    // See BUILD.bazel:libcore_worker_library_java.so\n    JniUtils.loadLibrary(\"core_worker_library_java\", true);\n    LOGGER.debug(\"Native libraries loaded.\"); </s> remove         \"streaming_tests_deploy.jar\",\n        \"streaming_tests_deploy-src.jar\",\n </s> add  </s> add         RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n        RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> remove         \":streaming_tests\",\n </s> add  </s> add         touch $$GENERATED_DIR/__init__.py", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "java/runtime/src/main/java/org/ray/runtime/runner/worker/DefaultWorker.java"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>     \"ray/core/src/ray/raylet/raylet_monitor\",\n <mask>     \"ray/core/src/ray/raylet/raylet\",\n <mask>     \"ray/dashboard/dashboard.py\",\n <mask> ]\n <mask> \n <mask> # These are the directories where automatically generated Python protobuf\n <mask> # bindings are created.\n <mask> generated_python_directories = [\n </s> [Streaming] Streaming data transfer java (#6474) </s> add         RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n        RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add     \"ray/streaming/generated\", </s> remove     \"streaming\",\n </s> add  </s> remove         \":streaming_tests\",\n </s> add  </s> remove         \"streaming_tests_deploy.jar\",\n        \"streaming_tests_deploy-src.jar\",\n </s> add  </s> add     visibility = [\"//visibility:public\"]", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "python/setup.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> generated_python_directories = [\n <mask>     \"ray/core/generated\",\n <mask> ]\n <mask> \n <mask> optional_ray_files = []\n <mask> \n </s> [Streaming] Streaming data transfer java (#6474) </s> add     \"ray/streaming/_streaming.so\", </s> remove     \"streaming\",\n </s> add  </s> remove         \":streaming_tests\",\n </s> add  </s> remove         \"streaming_tests_deploy.jar\",\n        \"streaming_tests_deploy-src.jar\",\n </s> add  </s> remove java_binary(\n    name = \"streaming_tests\",\n    main_class = \"org.testng.TestNG\",\n    data = [\"streaming/testng.xml\"],\n    args = [\"java/streaming/testng.xml\"],\n    runtime_deps = [\n        \":org_ray_ray_streaming_test\",\n    ],\n)\n\n </s> add  </s> remove define_java_module(\n    name = \"streaming\",\n    deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n    ],\n    define_test_lib = True,\n    test_deps = [\n        \":org_ray_ray_api\",\n        \":org_ray_ray_runtime\",\n        \":org_ray_ray_streaming\",\n        \"@maven//:com_beust_jcommander\",\n        \"@maven//:com_google_guava_guava\",\n        \"@maven//:org_slf4j_slf4j_api\",\n        \"@maven//:org_slf4j_slf4j_log4j12\",\n        \"@maven//:org_testng_testng\",\n    ],\n)\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "python/setup.py"}
{"docstring_tokens": "keep keep keep add", "code_tokens": " <mask> *ray*CoreWorker*\n <mask> *PyInit*\n <mask> *init_raylet*\n <mask> *Java*\n </s> [Streaming] Streaming data transfer java (#6474) </s> add         touch $$GENERATED_DIR/__init__.py </s> remove         cp -f $(location //java:org_ray_ray_streaming_pom) $$WORK_DIR/java/streaming/pom.xml\n </s> add  </s> add                 RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n                RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add         RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n        RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add         _print(\"export RAY_CI_STREAMING_JAVA_AFFECTED={}\"\n               .format(RAY_CI_STREAMING_JAVA_AFFECTED)) </s> remove     \"streaming/testng.xml\",\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "src/ray/ray_exported_symbols.lds"}
{"docstring_tokens": "keep add keep keep", "code_tokens": " <mask>         *init_raylet*;\n <mask>         *Java*;\n <mask>     local: *;\n <mask> };\n </s> [Streaming] Streaming data transfer java (#6474) </s> add         touch $$GENERATED_DIR/__init__.py </s> remove         cp -f $(location //java:org_ray_ray_streaming_pom) $$WORK_DIR/java/streaming/pom.xml\n </s> add  </s> add                 RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n                RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add         RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n        RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> add         _print(\"export RAY_CI_STREAMING_JAVA_AFFECTED={}\"\n               .format(RAY_CI_STREAMING_JAVA_AFFECTED)) </s> remove     \"streaming/testng.xml\",\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "src/ray/ray_version_script.lds"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>         GENERATED_DIR=$$WORK_DIR/streaming/python/generated\n <mask>         rm -rf $$GENERATED_DIR\n <mask>         mkdir -p $$GENERATED_DIR\n <mask>         for f in $(locations //streaming:streaming_py_proto); do\n <mask>             cp $$f $$GENERATED_DIR\n <mask>         done\n <mask>         echo $$(date) > $@\n <mask>     \"\"\",\n </s> [Streaming] Streaming data transfer java (#6474) </s> remove         cp -f $(location //java:org_ray_ray_streaming_pom) $$WORK_DIR/java/streaming/pom.xml\n </s> add  </s> remove         \"streaming_tests_deploy.jar\",\n        \"streaming_tests_deploy-src.jar\",\n </s> add  </s> add         RAY_CI_STREAMING_PYTHON_AFFECTED = 1\n        RAY_CI_STREAMING_JAVA_AFFECTED = 1 </s> remove         \":streaming_tests\",\n </s> add  </s> remove import org.ray.runtime.util.FileUtil;\n </s> add import org.ray.runtime.util.JniUtils; </s> add       // Set run-mode to `CLUSTER` explicitly, to prevent the DefaultWorker to receive\n      // a wrong run-mode parameter through jvm options.\n      System.setProperty(\"ray.run-mode\", \"CLUSTER\");", "html_url": "https://github.com/ray-project/ray/commit/7bbfa85c6681c65b5da4e01e3c2cea65f694f4b3", "file_name": "streaming/BUILD.bazel"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>           io_context_.stop();\n <mask>           thread_.join();\n <mask>         }\n <mask>        private:\n <mask>         instrumented_io_context io_context_;\n <mask>         boost::asio::io_service::work work_;\n <mask>         std::thread thread_;\n <mask>       };\n <mask>       std::shared_ptr<ray::gcs::GcsClient> make_gcs(\n </s> Do not connect in constructor to avoid potential risk. (#17916)\n\n* Do not connect in ctor.\r\n\r\n* Fix lint.\r\n\r\nCo-authored-by: Qing Wang <jovany.wq@antgroup.com> </s> remove         return std::make_shared<RayletGcsClient>(\n </s> add         std::shared_ptr<RayletGcsClient> raylet_gcs_client =\n          std::make_shared<RayletGcsClient>( </s> add         raylet_gcs_client->DoConnect();\n        return raylet_gcs_client; </s> remove   RAY_LOG(DEBUG) << \"Getting actor info, name = \" << name;\n </s> add   RAY_LOG(DEBUG) << \"Getting actor info, name = \" << name\n                 << \" , namespace = \" << ray_namespace;", "html_url": "https://github.com/ray-project/ray/commit/7c1f14ddd8adb8e6eaf1209f3126e55eb30346bd", "file_name": "python/ray/includes/gcs_client.pxd"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>       std::shared_ptr<ray::gcs::GcsClient> make_gcs(\n <mask>           const std::string& ip,\n <mask>           int port,\n <mask>           const std::string& password) {\n <mask>         return std::make_shared<RayletGcsClient>(\n <mask>             ray::gcs::GcsClientOptions(ip, port, password));\n <mask>       }\n <mask>     }\n <mask>     \"\"\"\n <mask>     shared_ptr[CGcsClient] make_gcs(\n </s> Do not connect in constructor to avoid potential risk. (#17916)\n\n* Do not connect in ctor.\r\n\r\n* Fix lint.\r\n\r\nCo-authored-by: Qing Wang <jovany.wq@antgroup.com> </s> add         raylet_gcs_client->DoConnect();\n        return raylet_gcs_client; </s> add         void DoConnect() {\n          RAY_CHECK_OK(Connect(io_context_));\n        }\n </s> remove   RAY_LOG(DEBUG) << \"Getting actor info, name = \" << name;\n </s> add   RAY_LOG(DEBUG) << \"Getting actor info, name = \" << name\n                 << \" , namespace = \" << ray_namespace;", "html_url": "https://github.com/ray-project/ray/commit/7c1f14ddd8adb8e6eaf1209f3126e55eb30346bd", "file_name": "python/ray/includes/gcs_client.pxd"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>           const std::string& password) {\n <mask>         std::shared_ptr<RayletGcsClient> raylet_gcs_client =\n <mask>           std::make_shared<RayletGcsClient>(\n <mask>             ray::gcs::GcsClientOptions(ip, port, password));\n <mask>       }\n <mask>     }\n <mask>     \"\"\"\n <mask>     shared_ptr[CGcsClient] make_gcs(\n <mask>         const c_string &ip, int port, const c_string &password)\n </s> Do not connect in constructor to avoid potential risk. (#17916)\n\n* Do not connect in ctor.\r\n\r\n* Fix lint.\r\n\r\nCo-authored-by: Qing Wang <jovany.wq@antgroup.com> </s> remove         return std::make_shared<RayletGcsClient>(\n </s> add         std::shared_ptr<RayletGcsClient> raylet_gcs_client =\n          std::make_shared<RayletGcsClient>( </s> remove   RAY_LOG(DEBUG) << \"Getting actor info, name = \" << name;\n </s> add   RAY_LOG(DEBUG) << \"Getting actor info, name = \" << name\n                 << \" , namespace = \" << ray_namespace; </s> add         void DoConnect() {\n          RAY_CHECK_OK(Connect(io_context_));\n        }\n", "html_url": "https://github.com/ray-project/ray/commit/7c1f14ddd8adb8e6eaf1209f3126e55eb30346bd", "file_name": "python/ray/includes/gcs_client.pxd"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     const rpc::GetNamedActorInfoRequest &request, rpc::GetNamedActorInfoReply *reply,\n <mask>     rpc::SendReplyCallback send_reply_callback) {\n <mask>   const std::string &name = request.name();\n <mask>   const std::string &ray_namespace = request.ray_namespace();\n <mask>   RAY_LOG(DEBUG) << \"Getting actor info, name = \" << name;\n <mask> \n <mask>   // Try to look up the actor ID for the named actor.\n <mask>   ActorID actor_id = GetActorIDByName(name, ray_namespace);\n <mask> \n <mask>   Status status = Status::OK();\n </s> Do not connect in constructor to avoid potential risk. (#17916)\n\n* Do not connect in ctor.\r\n\r\n* Fix lint.\r\n\r\nCo-authored-by: Qing Wang <jovany.wq@antgroup.com> </s> add         raylet_gcs_client->DoConnect();\n        return raylet_gcs_client; </s> remove         return std::make_shared<RayletGcsClient>(\n </s> add         std::shared_ptr<RayletGcsClient> raylet_gcs_client =\n          std::make_shared<RayletGcsClient>( </s> add         void DoConnect() {\n          RAY_CHECK_OK(Connect(io_context_));\n        }\n", "html_url": "https://github.com/ray-project/ray/commit/7c1f14ddd8adb8e6eaf1209f3126e55eb30346bd", "file_name": "src/ray/gcs/gcs_server/gcs_actor_manager.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>  private:\n <mask>   static std::once_flag is_inited_;\n <mask> \n <mask>   template <typename T>\n <mask>   static std::vector<std::shared_ptr<T>> Get(const std::vector<ObjectID> &ids);\n <mask> \n <mask>   template <typename FuncType>\n <mask>   static TaskCaller<FuncType> TaskInternal(FuncType &func);\n <mask> \n <mask>   template <typename FuncType>\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   ObjectID id_;\n </s> add   std::string id_; </s> remove inline static std::vector<ObjectID> ObjectRefsToObjectIDs(\n </s> add inline static std::vector<std::string> ObjectRefsToObjectIDs( </s> remove   std::vector<ObjectID> object_ids;\n </s> add   std::vector<std::string> object_ids; </s> remove inline std::vector<std::shared_ptr<T>> Ray::Get(const std::vector<ObjectID> &ids) {\n </s> add inline std::vector<std::shared_ptr<T>> Ray::Get(const std::vector<std::string> &ids) { </s> remove ObjectRef<T>::ObjectRef(const ObjectID &id) {\n  CopyAndAddRefrence(id_, id);\n </s> add ObjectRef<T>::ObjectRef(const std::string &id) {\n  CopyAndAddReference(id_, id); </s> remove   SubRefrence(id_);\n </s> add   SubReference(id_);", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api.h"}
{"docstring_tokens": "keep replace keep replace keep keep keep", "code_tokens": " <mask> template <typename T>\n <mask> inline static std::vector<ObjectID> ObjectRefsToObjectIDs(\n <mask>     const std::vector<ObjectRef<T>> &object_refs) {\n <mask>   std::vector<ObjectID> object_ids;\n <mask>   for (auto it = object_refs.begin(); it != object_refs.end(); it++) {\n <mask>     object_ids.push_back(it->ID());\n <mask>   }\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove inline std::vector<std::shared_ptr<T>> Ray::Get(const std::vector<ObjectID> &ids) {\n </s> add inline std::vector<std::shared_ptr<T>> Ray::Get(const std::vector<std::string> &ids) { </s> remove std::shared_ptr<msgpack::sbuffer> AbstractRayRuntime::Get(const ObjectID &object_id) {\n  return object_store_->Get(object_id, -1);\n </s> add std::shared_ptr<msgpack::sbuffer> AbstractRayRuntime::Get(const std::string &object_id) {\n  return object_store_->Get(ObjectID::FromBinary(object_id), -1);\n}\n\ninline static std::vector<ObjectID> StringIDsToObjectIDs(\n    const std::vector<std::string> &ids) {\n  std::vector<ObjectID> object_ids;\n  for (std::string id : ids) {\n    object_ids.push_back(ObjectID::FromBinary(id));\n  }\n  return object_ids; </s> remove const ObjectID &ObjectRef<T>::ID() const {\n </s> add const std::string &ObjectRef<T>::ID() const { </s> remove   static std::vector<std::shared_ptr<T>> Get(const std::vector<ObjectID> &ids);\n </s> add   static std::vector<std::shared_ptr<T>> Get(const std::vector<std::string> &ids); </s> remove   SubRefrence(id_);\n </s> add   SubReference(id_);", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   return GetFromRuntime(object);\n <mask> }\n <mask> \n <mask> template <typename T>\n <mask> inline std::vector<std::shared_ptr<T>> Ray::Get(const std::vector<ObjectID> &ids) {\n <mask>   auto result = ray::internal::RayRuntime()->Get(ids);\n <mask>   std::vector<std::shared_ptr<T>> return_objects;\n <mask>   return_objects.reserve(result.size());\n <mask>   for (auto it = result.begin(); it != result.end(); it++) {\n <mask>     auto obj = Serializer::Deserialize<std::shared_ptr<T>>((*it)->data(), (*it)->size());\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   std::vector<ObjectID> object_ids;\n </s> add   std::vector<std::string> object_ids; </s> remove inline static std::vector<ObjectID> ObjectRefsToObjectIDs(\n </s> add inline static std::vector<std::string> ObjectRefsToObjectIDs( </s> remove   static std::vector<std::shared_ptr<T>> Get(const std::vector<ObjectID> &ids);\n </s> add   static std::vector<std::shared_ptr<T>> Get(const std::vector<std::string> &ids); </s> remove   SubRefrence(id_);\n </s> add   SubReference(id_); </s> remove std::shared_ptr<msgpack::sbuffer> AbstractRayRuntime::Get(const ObjectID &object_id) {\n  return object_store_->Get(object_id, -1);\n </s> add std::shared_ptr<msgpack::sbuffer> AbstractRayRuntime::Get(const std::string &object_id) {\n  return object_store_->Get(ObjectID::FromBinary(object_id), -1);\n}\n\ninline static std::vector<ObjectID> StringIDsToObjectIDs(\n    const std::vector<std::string> &ids) {\n  std::vector<ObjectID> object_ids;\n  for (std::string id : ids) {\n    object_ids.push_back(ObjectID::FromBinary(id));\n  }\n  return object_ids; </s> remove const ObjectID &ObjectRef<T>::ID() const {\n </s> add const std::string &ObjectRef<T>::ID() const {", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> class ActorHandle {\n <mask>  public:\n <mask>   ActorHandle();\n <mask> \n <mask>   ActorHandle(const ActorID &id);\n <mask> \n <mask>   /// Get a untyped ID of the actor\n <mask>   const ActorID &ID() const;\n <mask> \n <mask>   /// Include the `Call` methods for calling remote functions.\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   const ActorID &ID() const;\n </s> add   const std::string &ID() const; </s> remove   const ObjectID &ID() const;\n </s> add   const std::string &ID() const; </s> remove   ObjectRef(const ObjectID &id);\n </s> add   ObjectRef(const std::string &id); </s> remove   const ObjectID &ID() const { return id_; }\n </s> add   const std::string &ID() const { return id_; } </s> remove   ObjectRef(const ObjectID &id) { CopyAndAddRefrence(id_, id); }\n </s> add   ObjectRef(const std::string &id) { CopyAndAddReference(id_, id); } </s> remove inline void SubRefrence(const ObjectID &id) { RemoveLocalReference(id); }\n </s> add inline void SubReference(const std::string &id) {\n  ray::internal::RayRuntime()->RemoveLocalReference(id);\n}", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/actor_handle.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>   ActorHandle(const ActorID &id);\n <mask> \n <mask>   /// Get a untyped ID of the actor\n <mask>   const ActorID &ID() const;\n <mask> \n <mask>   /// Include the `Call` methods for calling remote functions.\n <mask>   template <typename F>\n <mask>   ActorTaskCaller<F> Task(F actor_func);\n <mask> \n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   ActorHandle(const ActorID &id);\n </s> add   ActorHandle(const std::string &id); </s> remove   const ObjectID &ID() const;\n </s> add   const std::string &ID() const; </s> remove   ObjectRef(const ObjectID &id);\n </s> add   ObjectRef(const std::string &id); </s> remove   const ObjectID &ID() const { return id_; }\n </s> add   const std::string &ID() const { return id_; } </s> remove   ObjectRef(const ObjectID &id) { CopyAndAddRefrence(id_, id); }\n </s> add   ObjectRef(const std::string &id) { CopyAndAddReference(id_, id); } </s> remove const ActorID &ActorHandle<ActorType>::ID() const {\n </s> add const std::string &ActorHandle<ActorType>::ID() const {", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/actor_handle.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   /// Make ActorHandle serializable\n <mask>   MSGPACK_DEFINE(id_);\n <mask> \n <mask>  private:\n <mask>   ActorID id_;\n <mask> };\n <mask> \n <mask> // ---------- implementation ----------\n <mask> template <typename ActorType>\n <mask> ActorHandle<ActorType>::ActorHandle() {}\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   ObjectID id_;\n </s> add   std::string id_; </s> remove   ActorID id_;\n </s> add   std::string id_; </s> remove   ObjectID id_;\n </s> add   std::string id_; </s> remove ActorHandle<ActorType>::ActorHandle(const ActorID &id) {\n </s> add ActorHandle<ActorType>::ActorHandle(const std::string &id) { </s> remove const ActorID &ActorHandle<ActorType>::ID() const {\n </s> add const std::string &ActorHandle<ActorType>::ID() const { </s> remove   ActorHandle(const ActorID &id);\n </s> add   ActorHandle(const std::string &id);", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/actor_handle.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep replace", "code_tokens": " <mask> template <typename ActorType>\n <mask> ActorHandle<ActorType>::ActorHandle() {}\n <mask> \n <mask> template <typename ActorType>\n <mask> ActorHandle<ActorType>::ActorHandle(const ActorID &id) {\n <mask>   id_ = id;\n <mask> }\n <mask> \n <mask> template <typename ActorType>\n <mask> const ActorID &ActorHandle<ActorType>::ID() const {\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   ActorID id_;\n </s> add   std::string id_; </s> remove ObjectRef<T>::ObjectRef(const ObjectID &id) {\n  CopyAndAddRefrence(id_, id);\n </s> add ObjectRef<T>::ObjectRef(const std::string &id) {\n  CopyAndAddReference(id_, id); </s> remove const ObjectID &ObjectRef<T>::ID() const {\n </s> add const std::string &ObjectRef<T>::ID() const { </s> remove   SubRefrence(id_);\n </s> add   SubReference(id_); </s> remove inline void SubRefrence(const ObjectID &id) { RemoveLocalReference(id); }\n </s> add inline void SubReference(const std::string &id) {\n  ray::internal::RayRuntime()->RemoveLocalReference(id);\n}", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/actor_handle.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> class ActorTaskCaller {\n <mask>  public:\n <mask>   ActorTaskCaller() = default;\n <mask> \n <mask>   ActorTaskCaller(RayRuntime *runtime, ActorID id,\n <mask>                   RemoteFunctionHolder remote_function_holder)\n <mask>       : runtime_(runtime), id_(id), remote_function_holder_(remote_function_holder) {}\n <mask> \n <mask>   template <typename... Args>\n <mask>   ObjectRef<boost::callable_traits::return_type_t<F>> Remote(Args... args);\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   ActorID id_;\n </s> add   std::string id_; </s> remove   ~ObjectRef() { SubRefrence(id_); }\n </s> add   ~ObjectRef() { SubReference(id_); } </s> remove ActorHandle<ActorType>::ActorHandle(const ActorID &id) {\n </s> add ActorHandle<ActorType>::ActorHandle(const std::string &id) { </s> remove   ActorHandle(const ActorID &id);\n </s> add   ActorHandle(const std::string &id); </s> remove   ObjectRef(const ObjectRef &rhs) { CopyAndAddRefrence(id_, rhs.id_); }\n </s> add   ObjectRef(const ObjectRef &rhs) { CopyAndAddReference(id_, rhs.id_); } </s> remove ObjectRef<T>::ObjectRef(const ObjectID &id) {\n  CopyAndAddRefrence(id_, id);\n </s> add ObjectRef<T>::ObjectRef(const std::string &id) {\n  CopyAndAddReference(id_, id);", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/actor_task_caller.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   ObjectRef<boost::callable_traits::return_type_t<F>> Remote(Args... args);\n <mask> \n <mask>  private:\n <mask>   RayRuntime *runtime_;\n <mask>   ActorID id_;\n <mask>   RemoteFunctionHolder remote_function_holder_;\n <mask>   std::vector<ray::api::TaskArg> args_;\n <mask> };\n <mask> \n <mask> // ---------- implementation ----------\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   ActorID id_;\n </s> add   std::string id_; </s> remove   ObjectID id_;\n </s> add   std::string id_; </s> remove   ActorTaskCaller(RayRuntime *runtime, ActorID id,\n </s> add   ActorTaskCaller(RayRuntime *runtime, std::string id, </s> remove   ObjectID id_;\n </s> add   std::string id_; </s> remove \nvoid AddLocalReference(const ObjectID &id);\n\nvoid RemoveLocalReference(const ObjectID &id);\n\n </s> add  </s> remove   virtual ObjectID Call(const RemoteFunctionHolder &remote_function_holder,\n                        std::vector<ray::api::TaskArg> &args) = 0;\n  virtual ActorID CreateActor(const RemoteFunctionHolder &remote_function_holder,\n                              std::vector<ray::api::TaskArg> &args) = 0;\n  virtual ObjectID CallActor(const RemoteFunctionHolder &remote_function_holder,\n                             const ActorID &actor,\n                             std::vector<ray::api::TaskArg> &args) = 0;\n </s> add   virtual std::string Call(const RemoteFunctionHolder &remote_function_holder,\n                           std::vector<ray::api::TaskArg> &args) = 0;\n  virtual std::string CreateActor(const RemoteFunctionHolder &remote_function_holder,\n                                  std::vector<ray::api::TaskArg> &args) = 0;\n  virtual std::string CallActor(const RemoteFunctionHolder &remote_function_holder,\n                                const std::string &actor,\n                                std::vector<ray::api::TaskArg> &args) = 0;\n  virtual void AddLocalReference(const std::string &id) = 0;\n  virtual void RemoveLocalReference(const std::string &id) = 0;", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/actor_task_caller.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep", "code_tokens": " <mask> \n <mask>   /// If the buf is initialized shows it is a value argument.\n <mask>   boost::optional<msgpack::sbuffer> buf;\n <mask>   /// If the id is initialized shows it is a reference argument.\n <mask>   boost::optional<ObjectID> id;\n <mask> };\n <mask> \n <mask> }  // namespace api\n <mask> }  // namespace ray </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   ObjectID id_;\n </s> add   std::string id_; </s> remove \nvoid AddLocalReference(const ObjectID &id);\n\nvoid RemoveLocalReference(const ObjectID &id);\n\n </s> add  </s> remove   const ObjectID &ID() const;\n </s> add   const std::string &ID() const; </s> remove   const ObjectID &ID() const { return id_; }\n </s> add   const std::string &ID() const { return id_; } </s> remove inline static std::vector<ObjectID> ObjectRefsToObjectIDs(\n </s> add inline static std::vector<std::string> ObjectRefsToObjectIDs( </s> remove   ActorHandle(const ActorID &id);\n </s> add   ActorHandle(const std::string &id);", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/common_types.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     throw RayException(err_msg);\n <mask>   }\n <mask> }\n <mask> \n <mask> inline void CopyAndAddRefrence(ObjectID &dest_id, const ObjectID &id) {\n <mask>   dest_id = id;\n <mask>   AddLocalReference(id);\n <mask> }\n <mask> \n <mask> inline void SubRefrence(const ObjectID &id) { RemoveLocalReference(id); }\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   AddLocalReference(id);\n </s> add   ray::internal::RayRuntime()->AddLocalReference(id); </s> remove inline void SubRefrence(const ObjectID &id) { RemoveLocalReference(id); }\n </s> add inline void SubReference(const std::string &id) {\n  ray::internal::RayRuntime()->RemoveLocalReference(id);\n} </s> remove ActorHandle<ActorType>::ActorHandle(const ActorID &id) {\n </s> add ActorHandle<ActorType>::ActorHandle(const std::string &id) { </s> remove   ObjectRef(const ObjectID &id) { CopyAndAddRefrence(id_, id); }\n </s> add   ObjectRef(const std::string &id) { CopyAndAddReference(id_, id); } </s> remove \nvoid AddLocalReference(const ObjectID &id);\n\nvoid RemoveLocalReference(const ObjectID &id);\n\n </s> add  </s> remove const ObjectID &ObjectRef<T>::ID() const {\n </s> add const std::string &ObjectRef<T>::ID() const {", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/object_ref.h"}
{"docstring_tokens": "keep keep keep replace keep keep replace", "code_tokens": " <mask> \n <mask> inline void CopyAndAddRefrence(ObjectID &dest_id, const ObjectID &id) {\n <mask>   dest_id = id;\n <mask>   AddLocalReference(id);\n <mask> }\n <mask> \n <mask> inline void SubRefrence(const ObjectID &id) { RemoveLocalReference(id); }\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove inline void CopyAndAddRefrence(ObjectID &dest_id, const ObjectID &id) {\n </s> add inline void CopyAndAddReference(std::string &dest_id, const std::string &id) { </s> remove ActorHandle<ActorType>::ActorHandle(const ActorID &id) {\n </s> add ActorHandle<ActorType>::ActorHandle(const std::string &id) { </s> remove   ObjectRef(const ObjectID &id) { CopyAndAddRefrence(id_, id); }\n </s> add   ObjectRef(const std::string &id) { CopyAndAddReference(id_, id); } </s> remove \nvoid AddLocalReference(const ObjectID &id);\n\nvoid RemoveLocalReference(const ObjectID &id);\n\n </s> add  </s> remove const ObjectID &ObjectRef<T>::ID() const {\n </s> add const std::string &ObjectRef<T>::ID() const {", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/object_ref.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>  public:\n <mask>   ObjectRef();\n <mask>   ~ObjectRef();\n <mask> \n <mask>   ObjectRef(const ObjectRef &rhs) { CopyAndAddRefrence(id_, rhs.id_); }\n <mask> \n <mask>   ObjectRef &operator=(const ObjectRef &rhs) {\n <mask>     CopyAndAddRefrence(id_, rhs.id_);\n <mask>     return *this;\n <mask>   }\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   ObjectRef(const ObjectRef &rhs) { CopyAndAddRefrence(id_, rhs.id_); }\n </s> add   ObjectRef(const ObjectRef &rhs) { CopyAndAddReference(id_, rhs.id_); } </s> remove     CopyAndAddRefrence(id_, rhs.id_);\n </s> add     CopyAndAddReference(id_, rhs.id_); </s> remove     CopyAndAddRefrence(id_, rhs.id_);\n </s> add     CopyAndAddReference(id_, rhs.id_); </s> remove   ~ObjectRef() { SubRefrence(id_); }\n </s> add   ~ObjectRef() { SubReference(id_); } </s> remove   ObjectRef(const ObjectID &id) { CopyAndAddRefrence(id_, id); }\n </s> add   ObjectRef(const std::string &id) { CopyAndAddReference(id_, id); } </s> remove   ObjectRef(const ObjectID &id);\n </s> add   ObjectRef(const std::string &id);", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/object_ref.h"}
{"docstring_tokens": "keep keep replace keep keep keep replace keep keep", "code_tokens": " <mask> \n <mask>   ObjectRef &operator=(const ObjectRef &rhs) {\n <mask>     CopyAndAddRefrence(id_, rhs.id_);\n <mask>     return *this;\n <mask>   }\n <mask> \n <mask>   ObjectRef(const ObjectID &id);\n <mask> \n <mask>   bool operator==(const ObjectRef<T> &object) const;\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   ObjectRef(const ObjectRef &rhs) { CopyAndAddRefrence(id_, rhs.id_); }\n </s> add   ObjectRef(const ObjectRef &rhs) { CopyAndAddReference(id_, rhs.id_); } </s> remove     CopyAndAddRefrence(id_, rhs.id_);\n </s> add     CopyAndAddReference(id_, rhs.id_); </s> remove   ObjectRef(const ObjectRef &rhs) { CopyAndAddRefrence(id_, rhs.id_); }\n </s> add   ObjectRef(const ObjectRef &rhs) { CopyAndAddReference(id_, rhs.id_); } </s> remove   ~ObjectRef() { SubRefrence(id_); }\n </s> add   ~ObjectRef() { SubReference(id_); } </s> remove   ObjectRef(const ObjectID &id) { CopyAndAddRefrence(id_, id); }\n </s> add   ObjectRef(const std::string &id) { CopyAndAddReference(id_, id); }", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/object_ref.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>   bool operator==(const ObjectRef<T> &object) const;\n <mask> \n <mask>   /// Get a untyped ID of the object\n <mask>   const ObjectID &ID() const;\n <mask> \n <mask>   /// Get the object from the object store.\n <mask>   /// This method will be blocked until the object is ready.\n <mask>   ///\n <mask>   /// \\return shared pointer of the result.\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   const ObjectID &ID() const { return id_; }\n </s> add   const std::string &ID() const { return id_; } </s> remove   ObjectRef(const ObjectID &id);\n </s> add   ObjectRef(const std::string &id); </s> remove   const ActorID &ID() const;\n </s> add   const std::string &ID() const; </s> remove   ActorHandle(const ActorID &id);\n </s> add   ActorHandle(const std::string &id); </s> remove   ObjectRef(const ObjectID &id) { CopyAndAddRefrence(id_, id); }\n </s> add   ObjectRef(const std::string &id) { CopyAndAddReference(id_, id); } </s> remove inline void SubRefrence(const ObjectID &id) { RemoveLocalReference(id); }\n </s> add inline void SubReference(const std::string &id) {\n  ray::internal::RayRuntime()->RemoveLocalReference(id);\n}", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/object_ref.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   /// Make ObjectRef serializable\n <mask>   MSGPACK_DEFINE(id_);\n <mask> \n <mask>  private:\n <mask>   ObjectID id_;\n <mask> };\n <mask> \n <mask> // ---------- implementation ----------\n <mask> template <typename T>\n <mask> inline static std::shared_ptr<T> GetFromRuntime(const ObjectRef<T> &object) {\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   ActorID id_;\n </s> add   std::string id_; </s> remove   ObjectID id_;\n </s> add   std::string id_; </s> remove   ActorID id_;\n </s> add   std::string id_; </s> remove   SubRefrence(id_);\n </s> add   SubReference(id_); </s> remove const ObjectID &ObjectRef<T>::ID() const {\n </s> add const std::string &ObjectRef<T>::ID() const { </s> remove   static std::vector<std::shared_ptr<T>> Get(const std::vector<ObjectID> &ids);\n </s> add   static std::vector<std::shared_ptr<T>> Get(const std::vector<std::string> &ids);", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/object_ref.h"}
{"docstring_tokens": "keep keep replace replace keep keep keep keep replace keep keep keep keep", "code_tokens": " <mask> \n <mask> template <typename T>\n <mask> ObjectRef<T>::ObjectRef(const ObjectID &id) {\n <mask>   CopyAndAddRefrence(id_, id);\n <mask> }\n <mask> \n <mask> template <typename T>\n <mask> ObjectRef<T>::~ObjectRef() {\n <mask>   SubRefrence(id_);\n <mask> }\n <mask> \n <mask> template <typename T>\n <mask> inline bool ObjectRef<T>::operator==(const ObjectRef<T> &object) const {\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove const ObjectID &ObjectRef<T>::ID() const {\n </s> add const std::string &ObjectRef<T>::ID() const { </s> remove   ObjectID id_;\n </s> add   std::string id_; </s> remove   ObjectRef(const ObjectID &id) { CopyAndAddRefrence(id_, id); }\n </s> add   ObjectRef(const std::string &id) { CopyAndAddReference(id_, id); } </s> remove inline void SubRefrence(const ObjectID &id) { RemoveLocalReference(id); }\n </s> add inline void SubReference(const std::string &id) {\n  ray::internal::RayRuntime()->RemoveLocalReference(id);\n} </s> remove   std::vector<ObjectID> object_ids;\n </s> add   std::vector<std::string> object_ids;", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/object_ref.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   return id_ == object.id_;\n <mask> }\n <mask> \n <mask> template <typename T>\n <mask> const ObjectID &ObjectRef<T>::ID() const {\n <mask>   return id_;\n <mask> }\n <mask> \n <mask> template <typename T>\n <mask> inline std::shared_ptr<T> ObjectRef<T>::Get() const {\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   SubRefrence(id_);\n </s> add   SubReference(id_); </s> remove const ActorID &ActorHandle<ActorType>::ID() const {\n </s> add const std::string &ActorHandle<ActorType>::ID() const { </s> remove   ObjectID id_;\n </s> add   std::string id_; </s> remove   ObjectRef(const ObjectID &id) { CopyAndAddRefrence(id_, id); }\n </s> add   ObjectRef(const std::string &id) { CopyAndAddReference(id_, id); } </s> remove ActorHandle<ActorType>::ActorHandle(const ActorID &id) {\n </s> add ActorHandle<ActorType>::ActorHandle(const std::string &id) { </s> remove ObjectRef<T>::ObjectRef(const ObjectID &id) {\n  CopyAndAddRefrence(id_, id);\n </s> add ObjectRef<T>::ObjectRef(const std::string &id) {\n  CopyAndAddReference(id_, id);", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/object_ref.h"}
{"docstring_tokens": "keep keep keep replace keep replace keep keep keep keep", "code_tokens": " <mask> class ObjectRef<void> {\n <mask>  public:\n <mask>   ObjectRef() = default;\n <mask>   ~ObjectRef() { SubRefrence(id_); }\n <mask> \n <mask>   ObjectRef(const ObjectRef &rhs) { CopyAndAddRefrence(id_, rhs.id_); }\n <mask> \n <mask>   ObjectRef &operator=(const ObjectRef &rhs) {\n <mask>     CopyAndAddRefrence(id_, rhs.id_);\n <mask>     return *this;\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   ObjectRef(const ObjectRef &rhs) { CopyAndAddRefrence(id_, rhs.id_); }\n </s> add   ObjectRef(const ObjectRef &rhs) { CopyAndAddReference(id_, rhs.id_); } </s> remove     CopyAndAddRefrence(id_, rhs.id_);\n </s> add     CopyAndAddReference(id_, rhs.id_); </s> remove     CopyAndAddRefrence(id_, rhs.id_);\n </s> add     CopyAndAddReference(id_, rhs.id_); </s> remove   ObjectRef(const ObjectID &id) { CopyAndAddRefrence(id_, id); }\n </s> add   ObjectRef(const std::string &id) { CopyAndAddReference(id_, id); } </s> remove   ObjectRef(const ObjectID &id);\n </s> add   ObjectRef(const std::string &id);", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/object_ref.h"}
{"docstring_tokens": "keep replace keep keep keep replace", "code_tokens": " <mask>   ObjectRef &operator=(const ObjectRef &rhs) {\n <mask>     CopyAndAddRefrence(id_, rhs.id_);\n <mask>     return *this;\n <mask>   }\n <mask> \n <mask>   ObjectRef(const ObjectID &id) { CopyAndAddRefrence(id_, id); }\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   ObjectRef(const ObjectRef &rhs) { CopyAndAddRefrence(id_, rhs.id_); }\n </s> add   ObjectRef(const ObjectRef &rhs) { CopyAndAddReference(id_, rhs.id_); } </s> remove     CopyAndAddRefrence(id_, rhs.id_);\n </s> add     CopyAndAddReference(id_, rhs.id_); </s> remove   ObjectRef(const ObjectRef &rhs) { CopyAndAddRefrence(id_, rhs.id_); }\n </s> add   ObjectRef(const ObjectRef &rhs) { CopyAndAddReference(id_, rhs.id_); } </s> remove   ~ObjectRef() { SubRefrence(id_); }\n </s> add   ~ObjectRef() { SubReference(id_); } </s> remove ObjectRef<T>::ObjectRef(const ObjectID &id) {\n  CopyAndAddRefrence(id_, id);\n </s> add ObjectRef<T>::ObjectRef(const std::string &id) {\n  CopyAndAddReference(id_, id);", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/object_ref.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>   bool operator==(const ObjectRef<void> &object) const { return id_ == object.id_; }\n <mask> \n <mask>   /// Get a untyped ID of the object\n <mask>   const ObjectID &ID() const { return id_; }\n <mask> \n <mask>   /// Get the object from the object store.\n <mask>   /// This method will be blocked until the object is ready.\n <mask>   ///\n <mask>   /// \\return shared pointer of the result.\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   const ObjectID &ID() const;\n </s> add   const std::string &ID() const; </s> remove   ObjectRef(const ObjectID &id) { CopyAndAddRefrence(id_, id); }\n </s> add   ObjectRef(const std::string &id) { CopyAndAddReference(id_, id); } </s> remove   ObjectRef(const ObjectID &id);\n </s> add   ObjectRef(const std::string &id); </s> remove   ActorHandle(const ActorID &id);\n </s> add   ActorHandle(const std::string &id); </s> remove   const ActorID &ID() const;\n </s> add   const std::string &ID() const; </s> remove inline void SubRefrence(const ObjectID &id) { RemoveLocalReference(id); }\n </s> add inline void SubReference(const std::string &id) {\n  ray::internal::RayRuntime()->RemoveLocalReference(id);\n}", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/object_ref.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep", "code_tokens": " <mask>   /// Make ObjectRef serializable\n <mask>   MSGPACK_DEFINE(id_);\n <mask> \n <mask>  private:\n <mask>   ObjectID id_;\n <mask> };\n <mask> }  // namespace api\n <mask> }  // namespace ray </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   ObjectID id_;\n </s> add   std::string id_; </s> remove   ActorID id_;\n </s> add   std::string id_; </s> remove \nvoid AddLocalReference(const ObjectID &id);\n\nvoid RemoveLocalReference(const ObjectID &id);\n\n </s> add  </s> remove   boost::optional<ObjectID> id;\n </s> add   boost::optional<std::string> id; </s> remove inline static std::vector<ObjectID> ObjectRefsToObjectIDs(\n </s> add inline static std::vector<std::string> ObjectRefsToObjectIDs( </s> remove   ActorID id_;\n </s> add   std::string id_;", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/object_ref.h"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep replace keep keep keep", "code_tokens": " <mask> };\n <mask> \n <mask> class RayRuntime {\n <mask>  public:\n <mask>   virtual ObjectID Put(std::shared_ptr<msgpack::sbuffer> data) = 0;\n <mask>   virtual std::shared_ptr<msgpack::sbuffer> Get(const ObjectID &id) = 0;\n <mask> \n <mask>   virtual std::vector<std::shared_ptr<msgpack::sbuffer>> Get(\n <mask>       const std::vector<ObjectID> &ids) = 0;\n <mask> \n <mask>   virtual std::vector<bool> Wait(const std::vector<ObjectID> &ids, int num_objects,\n <mask>                                  int timeout_ms) = 0;\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   virtual std::vector<bool> Wait(const std::vector<ObjectID> &ids, int num_objects,\n </s> add   virtual std::vector<bool> Wait(const std::vector<std::string> &ids, int num_objects, </s> remove   virtual ObjectID Call(const RemoteFunctionHolder &remote_function_holder,\n                        std::vector<ray::api::TaskArg> &args) = 0;\n  virtual ActorID CreateActor(const RemoteFunctionHolder &remote_function_holder,\n                              std::vector<ray::api::TaskArg> &args) = 0;\n  virtual ObjectID CallActor(const RemoteFunctionHolder &remote_function_holder,\n                             const ActorID &actor,\n                             std::vector<ray::api::TaskArg> &args) = 0;\n </s> add   virtual std::string Call(const RemoteFunctionHolder &remote_function_holder,\n                           std::vector<ray::api::TaskArg> &args) = 0;\n  virtual std::string CreateActor(const RemoteFunctionHolder &remote_function_holder,\n                                  std::vector<ray::api::TaskArg> &args) = 0;\n  virtual std::string CallActor(const RemoteFunctionHolder &remote_function_holder,\n                                const std::string &actor,\n                                std::vector<ray::api::TaskArg> &args) = 0;\n  virtual void AddLocalReference(const std::string &id) = 0;\n  virtual void RemoveLocalReference(const std::string &id) = 0; </s> remove     const std::vector<ObjectID> &ids) {\n  return object_store_->Get(ids, -1);\n </s> add     const std::vector<std::string> &ids) {\n  return object_store_->Get(StringIDsToObjectIDs(ids), -1); </s> remove std::vector<bool> AbstractRayRuntime::Wait(const std::vector<ObjectID> &ids,\n </s> add std::vector<bool> AbstractRayRuntime::Wait(const std::vector<std::string> &ids, </s> remove \nvoid AddLocalReference(const ObjectID &id);\n\nvoid RemoveLocalReference(const ObjectID &id);\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/ray_runtime.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>   virtual std::vector<std::shared_ptr<msgpack::sbuffer>> Get(\n <mask>       const std::vector<ObjectID> &ids) = 0;\n <mask> \n <mask>   virtual std::vector<bool> Wait(const std::vector<ObjectID> &ids, int num_objects,\n <mask>                                  int timeout_ms) = 0;\n <mask> \n <mask>   virtual ObjectID Call(const RemoteFunctionHolder &remote_function_holder,\n <mask>                         std::vector<ray::api::TaskArg> &args) = 0;\n <mask>   virtual ActorID CreateActor(const RemoteFunctionHolder &remote_function_holder,\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove       const std::vector<ObjectID> &ids) = 0;\n </s> add       const std::vector<std::string> &ids) = 0; </s> remove   virtual ObjectID Call(const RemoteFunctionHolder &remote_function_holder,\n                        std::vector<ray::api::TaskArg> &args) = 0;\n  virtual ActorID CreateActor(const RemoteFunctionHolder &remote_function_holder,\n                              std::vector<ray::api::TaskArg> &args) = 0;\n  virtual ObjectID CallActor(const RemoteFunctionHolder &remote_function_holder,\n                             const ActorID &actor,\n                             std::vector<ray::api::TaskArg> &args) = 0;\n </s> add   virtual std::string Call(const RemoteFunctionHolder &remote_function_holder,\n                           std::vector<ray::api::TaskArg> &args) = 0;\n  virtual std::string CreateActor(const RemoteFunctionHolder &remote_function_holder,\n                                  std::vector<ray::api::TaskArg> &args) = 0;\n  virtual std::string CallActor(const RemoteFunctionHolder &remote_function_holder,\n                                const std::string &actor,\n                                std::vector<ray::api::TaskArg> &args) = 0;\n  virtual void AddLocalReference(const std::string &id) = 0;\n  virtual void RemoveLocalReference(const std::string &id) = 0; </s> remove   virtual ObjectID Put(std::shared_ptr<msgpack::sbuffer> data) = 0;\n  virtual std::shared_ptr<msgpack::sbuffer> Get(const ObjectID &id) = 0;\n </s> add   virtual std::string Put(std::shared_ptr<msgpack::sbuffer> data) = 0;\n  virtual std::shared_ptr<msgpack::sbuffer> Get(const std::string &id) = 0; </s> remove \nvoid AddLocalReference(const ObjectID &id);\n\nvoid RemoveLocalReference(const ObjectID &id);\n\n </s> add  </s> remove     const std::vector<ObjectID> &ids) {\n  return object_store_->Get(ids, -1);\n </s> add     const std::vector<std::string> &ids) {\n  return object_store_->Get(StringIDsToObjectIDs(ids), -1); </s> remove std::vector<bool> AbstractRayRuntime::Wait(const std::vector<ObjectID> &ids,\n </s> add std::vector<bool> AbstractRayRuntime::Wait(const std::vector<std::string> &ids,", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/ray_runtime.h"}
{"docstring_tokens": "keep keep replace replace replace replace replace replace replace keep replace replace replace replace replace", "code_tokens": " <mask>                                  int timeout_ms) = 0;\n <mask> \n <mask>   virtual ObjectID Call(const RemoteFunctionHolder &remote_function_holder,\n <mask>                         std::vector<ray::api::TaskArg> &args) = 0;\n <mask>   virtual ActorID CreateActor(const RemoteFunctionHolder &remote_function_holder,\n <mask>                               std::vector<ray::api::TaskArg> &args) = 0;\n <mask>   virtual ObjectID CallActor(const RemoteFunctionHolder &remote_function_holder,\n <mask>                              const ActorID &actor,\n <mask>                              std::vector<ray::api::TaskArg> &args) = 0;\n <mask> };\n <mask> \n <mask> void AddLocalReference(const ObjectID &id);\n <mask> \n <mask> void RemoveLocalReference(const ObjectID &id);\n <mask> \n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   virtual std::vector<bool> Wait(const std::vector<ObjectID> &ids, int num_objects,\n </s> add   virtual std::vector<bool> Wait(const std::vector<std::string> &ids, int num_objects, </s> remove       const std::vector<ObjectID> &ids) = 0;\n </s> add       const std::vector<std::string> &ids) = 0; </s> remove   virtual ObjectID Put(std::shared_ptr<msgpack::sbuffer> data) = 0;\n  virtual std::shared_ptr<msgpack::sbuffer> Get(const ObjectID &id) = 0;\n </s> add   virtual std::string Put(std::shared_ptr<msgpack::sbuffer> data) = 0;\n  virtual std::shared_ptr<msgpack::sbuffer> Get(const std::string &id) = 0; </s> remove   return task_submitter_->CreateActor(invocation_spec);\n </s> add   return task_submitter_->CreateActor(invocation_spec).Binary(); </s> remove   return task_submitter_->SubmitTask(invocation_spec);\n </s> add   return task_submitter_->SubmitTask(invocation_spec).Binary();", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/api/ray_runtime.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> #pragma once\n <mask> \n <mask> #include \"ray/common/buffer.h\"\n <mask> #include \"ray/common/function_descriptor.h\"\n <mask> #include \"ray/common/id.h\"\n <mask> #include \"ray/common/status.h\"\n <mask> #include \"ray/common/task/task_common.h\"\n <mask> #include \"ray/common/task/task_spec.h\"\n <mask> #include \"ray/common/task/task_util.h\"\n <mask> #include \"ray/common/test_util.h\"\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   ActorID id_;\n </s> add   std::string id_; </s> remove   ObjectID id_;\n </s> add   std::string id_; </s> remove   const ObjectID &ID() const;\n </s> add   const std::string &ID() const; </s> remove   ObjectRef(const ObjectID &id);\n </s> add   ObjectRef(const std::string &id); </s> remove     CopyAndAddRefrence(id_, rhs.id_);\n </s> add     CopyAndAddReference(id_, rhs.id_); </s> remove   ObjectRef(const ObjectRef &rhs) { CopyAndAddRefrence(id_, rhs.id_); }\n </s> add   ObjectRef(const ObjectRef &rhs) { CopyAndAddReference(id_, rhs.id_); }", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/include/ray/core.h"}
{"docstring_tokens": "keep keep replace keep keep keep replace keep", "code_tokens": " <mask> }\n <mask> \n <mask> ObjectID AbstractRayRuntime::Put(std::shared_ptr<msgpack::sbuffer> data) {\n <mask>   ObjectID object_id =\n <mask>       ObjectID::FromIndex(worker_->GetCurrentTaskID(), worker_->GetNextPutIndex());\n <mask>   Put(data, &object_id);\n <mask>   return object_id;\n <mask> }\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove std::shared_ptr<msgpack::sbuffer> AbstractRayRuntime::Get(const ObjectID &object_id) {\n  return object_store_->Get(object_id, -1);\n </s> add std::shared_ptr<msgpack::sbuffer> AbstractRayRuntime::Get(const std::string &object_id) {\n  return object_store_->Get(ObjectID::FromBinary(object_id), -1);\n}\n\ninline static std::vector<ObjectID> StringIDsToObjectIDs(\n    const std::vector<std::string> &ids) {\n  std::vector<ObjectID> object_ids;\n  for (std::string id : ids) {\n    object_ids.push_back(ObjectID::FromBinary(id));\n  }\n  return object_ids; </s> remove   ObjectRef(const ObjectRef &rhs) { CopyAndAddRefrence(id_, rhs.id_); }\n </s> add   ObjectRef(const ObjectRef &rhs) { CopyAndAddReference(id_, rhs.id_); } </s> remove   ~ObjectRef() { SubRefrence(id_); }\n </s> add   ~ObjectRef() { SubReference(id_); } </s> remove inline void CopyAndAddRefrence(ObjectID &dest_id, const ObjectID &id) {\n </s> add inline void CopyAndAddReference(std::string &dest_id, const std::string &id) { </s> remove   ObjectRef(const ObjectID &id) { CopyAndAddRefrence(id_, id); }\n </s> add   ObjectRef(const std::string &id) { CopyAndAddReference(id_, id); }", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/src/ray/runtime/abstract_ray_runtime.cc"}
{"docstring_tokens": "keep keep replace replace keep keep keep replace replace keep", "code_tokens": " <mask> }\n <mask> \n <mask> std::shared_ptr<msgpack::sbuffer> AbstractRayRuntime::Get(const ObjectID &object_id) {\n <mask>   return object_store_->Get(object_id, -1);\n <mask> }\n <mask> \n <mask> std::vector<std::shared_ptr<msgpack::sbuffer>> AbstractRayRuntime::Get(\n <mask>     const std::vector<ObjectID> &ids) {\n <mask>   return object_store_->Get(ids, -1);\n <mask> }\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   return object_id;\n </s> add   return object_id.Binary(); </s> remove std::vector<bool> AbstractRayRuntime::Wait(const std::vector<ObjectID> &ids,\n </s> add std::vector<bool> AbstractRayRuntime::Wait(const std::vector<std::string> &ids, </s> remove ObjectID AbstractRayRuntime::Put(std::shared_ptr<msgpack::sbuffer> data) {\n </s> add std::string AbstractRayRuntime::Put(std::shared_ptr<msgpack::sbuffer> data) { </s> remove   virtual ObjectID Put(std::shared_ptr<msgpack::sbuffer> data) = 0;\n  virtual std::shared_ptr<msgpack::sbuffer> Get(const ObjectID &id) = 0;\n </s> add   virtual std::string Put(std::shared_ptr<msgpack::sbuffer> data) = 0;\n  virtual std::shared_ptr<msgpack::sbuffer> Get(const std::string &id) = 0; </s> remove       const std::vector<ObjectID> &ids) = 0;\n </s> add       const std::vector<std::string> &ids) = 0;", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/src/ray/runtime/abstract_ray_runtime.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     const std::vector<ObjectID> &ids) {\n <mask>   return object_store_->Get(ids, -1);\n <mask> }\n <mask> \n <mask> std::vector<bool> AbstractRayRuntime::Wait(const std::vector<ObjectID> &ids,\n <mask>                                            int num_objects, int timeout_ms) {\n <mask>   return object_store_->Wait(ids, num_objects, timeout_ms);\n <mask> }\n <mask> \n <mask> std::vector<std::unique_ptr<::ray::TaskArg>> TransformArgs(\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   return object_store_->Wait(ids, num_objects, timeout_ms);\n </s> add   return object_store_->Wait(StringIDsToObjectIDs(ids), num_objects, timeout_ms); </s> remove     const std::vector<ObjectID> &ids) {\n  return object_store_->Get(ids, -1);\n </s> add     const std::vector<std::string> &ids) {\n  return object_store_->Get(StringIDsToObjectIDs(ids), -1); </s> remove   virtual std::vector<bool> Wait(const std::vector<ObjectID> &ids, int num_objects,\n </s> add   virtual std::vector<bool> Wait(const std::vector<std::string> &ids, int num_objects, </s> remove       const std::vector<ObjectID> &ids) = 0;\n </s> add       const std::vector<std::string> &ids) = 0; </s> remove std::shared_ptr<msgpack::sbuffer> AbstractRayRuntime::Get(const ObjectID &object_id) {\n  return object_store_->Get(object_id, -1);\n </s> add std::shared_ptr<msgpack::sbuffer> AbstractRayRuntime::Get(const std::string &object_id) {\n  return object_store_->Get(ObjectID::FromBinary(object_id), -1);\n}\n\ninline static std::vector<ObjectID> StringIDsToObjectIDs(\n    const std::vector<std::string> &ids) {\n  std::vector<ObjectID> object_ids;\n  for (std::string id : ids) {\n    object_ids.push_back(ObjectID::FromBinary(id));\n  }\n  return object_ids; </s> remove   virtual ObjectID Put(std::shared_ptr<msgpack::sbuffer> data) = 0;\n  virtual std::shared_ptr<msgpack::sbuffer> Get(const ObjectID &id) = 0;\n </s> add   virtual std::string Put(std::shared_ptr<msgpack::sbuffer> data) = 0;\n  virtual std::shared_ptr<msgpack::sbuffer> Get(const std::string &id) = 0;", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/src/ray/runtime/abstract_ray_runtime.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> }\n <mask> \n <mask> std::vector<bool> AbstractRayRuntime::Wait(const std::vector<ObjectID> &ids,\n <mask>                                            int num_objects, int timeout_ms) {\n <mask>   return object_store_->Wait(ids, num_objects, timeout_ms);\n <mask> }\n <mask> \n <mask> std::vector<std::unique_ptr<::ray::TaskArg>> TransformArgs(\n <mask>     std::vector<ray::api::TaskArg> &args) {\n <mask>   std::vector<std::unique_ptr<::ray::TaskArg>> ray_args;\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove std::vector<bool> AbstractRayRuntime::Wait(const std::vector<ObjectID> &ids,\n </s> add std::vector<bool> AbstractRayRuntime::Wait(const std::vector<std::string> &ids, </s> remove     const std::vector<ObjectID> &ids) {\n  return object_store_->Get(ids, -1);\n </s> add     const std::vector<std::string> &ids) {\n  return object_store_->Get(StringIDsToObjectIDs(ids), -1); </s> remove   virtual std::vector<bool> Wait(const std::vector<ObjectID> &ids, int num_objects,\n </s> add   virtual std::vector<bool> Wait(const std::vector<std::string> &ids, int num_objects, </s> remove       const std::vector<ObjectID> &ids) = 0;\n </s> add       const std::vector<std::string> &ids) = 0; </s> remove   virtual ObjectID Call(const RemoteFunctionHolder &remote_function_holder,\n                        std::vector<ray::api::TaskArg> &args) = 0;\n  virtual ActorID CreateActor(const RemoteFunctionHolder &remote_function_holder,\n                              std::vector<ray::api::TaskArg> &args) = 0;\n  virtual ObjectID CallActor(const RemoteFunctionHolder &remote_function_holder,\n                             const ActorID &actor,\n                             std::vector<ray::api::TaskArg> &args) = 0;\n </s> add   virtual std::string Call(const RemoteFunctionHolder &remote_function_holder,\n                           std::vector<ray::api::TaskArg> &args) = 0;\n  virtual std::string CreateActor(const RemoteFunctionHolder &remote_function_holder,\n                                  std::vector<ray::api::TaskArg> &args) = 0;\n  virtual std::string CallActor(const RemoteFunctionHolder &remote_function_holder,\n                                const std::string &actor,\n                                std::vector<ray::api::TaskArg> &args) = 0;\n  virtual void AddLocalReference(const std::string &id) = 0;\n  virtual void RemoveLocalReference(const std::string &id) = 0; </s> remove   virtual ObjectID Put(std::shared_ptr<msgpack::sbuffer> data) = 0;\n  virtual std::shared_ptr<msgpack::sbuffer> Get(const ObjectID &id) = 0;\n </s> add   virtual std::string Put(std::shared_ptr<msgpack::sbuffer> data) = 0;\n  virtual std::shared_ptr<msgpack::sbuffer> Get(const std::string &id) = 0;", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/src/ray/runtime/abstract_ray_runtime.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>       ray_arg = absl::make_unique<ray::TaskArgByValue>(std::make_shared<ray::RayObject>(\n <mask>           memory_buffer, nullptr, std::vector<ObjectID>()));\n <mask>     } else {\n <mask>       RAY_CHECK(arg.id);\n <mask>       ray_arg = absl::make_unique<ray::TaskArgByReference>(*arg.id, ray::rpc::Address{});\n <mask>     }\n <mask>     ray_args.push_back(std::move(ray_arg));\n <mask>   }\n <mask> \n <mask>   return ray_args;\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   return object_store_->Wait(ids, num_objects, timeout_ms);\n </s> add   return object_store_->Wait(StringIDsToObjectIDs(ids), num_objects, timeout_ms); </s> remove   ObjectRef(const ObjectRef &rhs) { CopyAndAddRefrence(id_, rhs.id_); }\n </s> add   ObjectRef(const ObjectRef &rhs) { CopyAndAddReference(id_, rhs.id_); } </s> remove inline void CopyAndAddRefrence(ObjectID &dest_id, const ObjectID &id) {\n </s> add inline void CopyAndAddReference(std::string &dest_id, const std::string &id) { </s> remove   ObjectRef(const ObjectID &id) { CopyAndAddRefrence(id_, id); }\n </s> add   ObjectRef(const std::string &id) { CopyAndAddReference(id_, id); } </s> remove     CopyAndAddRefrence(id_, rhs.id_);\n </s> add     CopyAndAddReference(id_, rhs.id_); </s> remove   ~ObjectRef() { SubRefrence(id_); }\n </s> add   ~ObjectRef() { SubReference(id_); }", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/src/ray/runtime/abstract_ray_runtime.cc"}
{"docstring_tokens": "keep keep keep replace replace keep keep keep replace keep keep keep", "code_tokens": " <mask>   return invocation_spec;\n <mask> }\n <mask> \n <mask> ObjectID AbstractRayRuntime::Call(const RemoteFunctionHolder &remote_function_holder,\n <mask>                                   std::vector<ray::api::TaskArg> &args) {\n <mask>   auto invocation_spec =\n <mask>       BuildInvocationSpec1(TaskType::NORMAL_TASK, this->config_->lib_name,\n <mask>                            remote_function_holder, args, ActorID::Nil());\n <mask>   return task_submitter_->SubmitTask(invocation_spec);\n <mask> }\n <mask> \n <mask> ActorID AbstractRayRuntime::CreateActor(\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove ActorID AbstractRayRuntime::CreateActor(\n </s> add std::string AbstractRayRuntime::CreateActor( </s> remove   return task_submitter_->CreateActor(invocation_spec);\n </s> add   return task_submitter_->CreateActor(invocation_spec).Binary(); </s> remove   virtual ObjectID Call(const RemoteFunctionHolder &remote_function_holder,\n                        std::vector<ray::api::TaskArg> &args) = 0;\n  virtual ActorID CreateActor(const RemoteFunctionHolder &remote_function_holder,\n                              std::vector<ray::api::TaskArg> &args) = 0;\n  virtual ObjectID CallActor(const RemoteFunctionHolder &remote_function_holder,\n                             const ActorID &actor,\n                             std::vector<ray::api::TaskArg> &args) = 0;\n </s> add   virtual std::string Call(const RemoteFunctionHolder &remote_function_holder,\n                           std::vector<ray::api::TaskArg> &args) = 0;\n  virtual std::string CreateActor(const RemoteFunctionHolder &remote_function_holder,\n                                  std::vector<ray::api::TaskArg> &args) = 0;\n  virtual std::string CallActor(const RemoteFunctionHolder &remote_function_holder,\n                                const std::string &actor,\n                                std::vector<ray::api::TaskArg> &args) = 0;\n  virtual void AddLocalReference(const std::string &id) = 0;\n  virtual void RemoveLocalReference(const std::string &id) = 0; </s> remove \nvoid AddLocalReference(const ObjectID &id);\n\nvoid RemoveLocalReference(const ObjectID &id);\n\n </s> add  </s> remove   virtual std::vector<bool> Wait(const std::vector<ObjectID> &ids, int num_objects,\n </s> add   virtual std::vector<bool> Wait(const std::vector<std::string> &ids, int num_objects,", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/src/ray/runtime/abstract_ray_runtime.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                            remote_function_holder, args, ActorID::Nil());\n <mask>   return task_submitter_->SubmitTask(invocation_spec);\n <mask> }\n <mask> \n <mask> ActorID AbstractRayRuntime::CreateActor(\n <mask>     const RemoteFunctionHolder &remote_function_holder,\n <mask>     std::vector<ray::api::TaskArg> &args) {\n <mask>   auto invocation_spec =\n <mask>       BuildInvocationSpec1(TaskType::ACTOR_CREATION_TASK, this->config_->lib_name,\n <mask>                            remote_function_holder, args, ActorID::Nil());\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove   return task_submitter_->SubmitTask(invocation_spec);\n </s> add   return task_submitter_->SubmitTask(invocation_spec).Binary(); </s> remove   return task_submitter_->CreateActor(invocation_spec);\n </s> add   return task_submitter_->CreateActor(invocation_spec).Binary(); </s> remove ObjectID AbstractRayRuntime::Call(const RemoteFunctionHolder &remote_function_holder,\n                                  std::vector<ray::api::TaskArg> &args) {\n </s> add std::string AbstractRayRuntime::Call(const RemoteFunctionHolder &remote_function_holder,\n                                     std::vector<ray::api::TaskArg> &args) { </s> remove   virtual ObjectID Call(const RemoteFunctionHolder &remote_function_holder,\n                        std::vector<ray::api::TaskArg> &args) = 0;\n  virtual ActorID CreateActor(const RemoteFunctionHolder &remote_function_holder,\n                              std::vector<ray::api::TaskArg> &args) = 0;\n  virtual ObjectID CallActor(const RemoteFunctionHolder &remote_function_holder,\n                             const ActorID &actor,\n                             std::vector<ray::api::TaskArg> &args) = 0;\n </s> add   virtual std::string Call(const RemoteFunctionHolder &remote_function_holder,\n                           std::vector<ray::api::TaskArg> &args) = 0;\n  virtual std::string CreateActor(const RemoteFunctionHolder &remote_function_holder,\n                                  std::vector<ray::api::TaskArg> &args) = 0;\n  virtual std::string CallActor(const RemoteFunctionHolder &remote_function_holder,\n                                const std::string &actor,\n                                std::vector<ray::api::TaskArg> &args) = 0;\n  virtual void AddLocalReference(const std::string &id) = 0;\n  virtual void RemoveLocalReference(const std::string &id) = 0; </s> remove \nvoid AddLocalReference(const ObjectID &id);\n\nvoid RemoveLocalReference(const ObjectID &id);\n\n </s> add  </s> remove   virtual std::vector<bool> Wait(const std::vector<ObjectID> &ids, int num_objects,\n </s> add   virtual std::vector<bool> Wait(const std::vector<std::string> &ids, int num_objects,", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/src/ray/runtime/abstract_ray_runtime.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     std::vector<ray::api::TaskArg> &args) {\n <mask>   auto invocation_spec =\n <mask>       BuildInvocationSpec1(TaskType::ACTOR_CREATION_TASK, this->config_->lib_name,\n <mask>                            remote_function_holder, args, ActorID::Nil());\n <mask>   return task_submitter_->CreateActor(invocation_spec);\n <mask> }\n <mask> \n <mask> ObjectID AbstractRayRuntime::CallActor(const RemoteFunctionHolder &remote_function_holder,\n <mask>                                        const ActorID &actor,\n <mask>                                        std::vector<ray::api::TaskArg> &args) {\n </s> remove id.h dependence for c++ worker headers (#16055) </s> remove ActorID AbstractRayRuntime::CreateActor(\n </s> add std::string AbstractRayRuntime::CreateActor( </s> remove   return task_submitter_->SubmitTask(invocation_spec);\n </s> add   return task_submitter_->SubmitTask(invocation_spec).Binary(); </s> remove ObjectID AbstractRayRuntime::Call(const RemoteFunctionHolder &remote_function_holder,\n                                  std::vector<ray::api::TaskArg> &args) {\n </s> add std::string AbstractRayRuntime::Call(const RemoteFunctionHolder &remote_function_holder,\n                                     std::vector<ray::api::TaskArg> &args) { </s> remove   virtual ObjectID Call(const RemoteFunctionHolder &remote_function_holder,\n                        std::vector<ray::api::TaskArg> &args) = 0;\n  virtual ActorID CreateActor(const RemoteFunctionHolder &remote_function_holder,\n                              std::vector<ray::api::TaskArg> &args) = 0;\n  virtual ObjectID CallActor(const RemoteFunctionHolder &remote_function_holder,\n                             const ActorID &actor,\n                             std::vector<ray::api::TaskArg> &args) = 0;\n </s> add   virtual std::string Call(const RemoteFunctionHolder &remote_function_holder,\n                           std::vector<ray::api::TaskArg> &args) = 0;\n  virtual std::string CreateActor(const RemoteFunctionHolder &remote_function_holder,\n                                  std::vector<ray::api::TaskArg> &args) = 0;\n  virtual std::string CallActor(const RemoteFunctionHolder &remote_function_holder,\n                                const std::string &actor,\n                                std::vector<ray::api::TaskArg> &args) = 0;\n  virtual void AddLocalReference(const std::string &id) = 0;\n  virtual void RemoveLocalReference(const std::string &id) = 0; </s> remove \nvoid AddLocalReference(const ObjectID &id);\n\nvoid RemoveLocalReference(const ObjectID &id);\n\n </s> add  </s> remove   virtual std::vector<bool> Wait(const std::vector<ObjectID> &ids, int num_objects,\n </s> add   virtual std::vector<bool> Wait(const std::vector<std::string> &ids, int num_objects,", "html_url": "https://github.com/ray-project/ray/commit/7c3874b38e5e5c24ddc9e0e0e9db1a3c593928e3", "file_name": "cpp/src/ray/runtime/abstract_ray_runtime.cc"}
{"docstring_tokens": "keep keep replace replace replace replace replace replace replace replace replace keep keep replace", "code_tokens": " <mask>             results = []\n <mask>             for batch in batches:\n <mask>                 result = self.learner_group.update(\n <mask>                     batch,\n <mask>                     reduce_fn=_reduce_impala_results,\n <mask>                     block=blocking,\n <mask>                     num_iters=self.config.num_sgd_iter,\n <mask>                     minibatch_size=self.config.minibatch_size,\n <mask>                 )\n <mask>                 if result:\n <mask>                     self._counters[NUM_ENV_STEPS_TRAINED] += result[ALL_MODULES].pop(\n <mask>                         NUM_ENV_STEPS_TRAINED\n <mask>                     )\n <mask>                     self._counters[NUM_AGENT_STEPS_TRAINED] += result[ALL_MODULES].pop(\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove                     results.append(result)\n </s> add  </s> remove                         ]\n </s> add                         ],\n                        tag=update_tag, </s> remove                 if not results:\n </s> add                 if not async_results: </s> remove \n                for res1, res2 in zip(results, results[1:]):\n                    self.assertEqual(\n                        res1[DEFAULT_POLICY_ID][\"mean_weight\"],\n                        res2[DEFAULT_POLICY_ID][\"mean_weight\"],\n                    )\n </s> add                 for results in async_results:\n                    for res1, res2 in zip(results, results[1:]):\n                        self.assertEqual(\n                            res1[DEFAULT_POLICY_ID][\"mean_weight\"],\n                            res2[DEFAULT_POLICY_ID][\"mean_weight\"],\n                        )\n                iter_i += 1 </s> remove                 results = learner_group.update(\n                    batch.as_multi_agent(), block=False, reduce_fn=None\n </s> add                 async_results = learner_group.async_update(\n                    batch.as_multi_agent(), reduce_fn=None", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/algorithms/impala/impala.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                     )\n <mask>                     self._counters[NUM_AGENT_STEPS_TRAINED] += result[ALL_MODULES].pop(\n <mask>                         NUM_AGENT_STEPS_TRAINED\n <mask>                     )\n <mask>                     results.append(result)\n <mask>             self._counters.update(self.learner_group.get_in_queue_stats())\n <mask>             # If there are results, reduce-mean over each individual value and return.\n <mask>             if results:\n <mask>                 return tree.map_structure(lambda *x: np.mean(x), *results)\n <mask> \n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove                     self._counters[NUM_AGENT_STEPS_TRAINED] += result[ALL_MODULES].pop(\n </s> add                     self._counters[NUM_AGENT_STEPS_TRAINED] += r[ALL_MODULES].pop( </s> remove                 result = self.learner_group.update(\n                    batch,\n                    reduce_fn=_reduce_impala_results,\n                    block=blocking,\n                    num_iters=self.config.num_sgd_iter,\n                    minibatch_size=self.config.minibatch_size,\n                )\n                if result:\n                    self._counters[NUM_ENV_STEPS_TRAINED] += result[ALL_MODULES].pop(\n </s> add                 if blocking:\n                    result = self.learner_group.update(\n                        batch,\n                        reduce_fn=_reduce_impala_results,\n                        num_iters=self.config.num_sgd_iter,\n                        minibatch_size=self.config.minibatch_size,\n                    )\n                    results = [result]\n                else:\n                    results = self.learner_group.async_update(\n                        batch,\n                        reduce_fn=_reduce_impala_results,\n                        num_iters=self.config.num_sgd_iter,\n                        minibatch_size=self.config.minibatch_size,\n                    )\n\n                for r in results:\n                    self._counters[NUM_ENV_STEPS_TRAINED] += r[ALL_MODULES].pop( </s> remove             A list of dictionaries of results from the updates from the individual\n            Learner(s)\n </s> add             A list of list of dictionaries of results, where the outer list\n            corresponds to separate calls to `async_update`, and the inner\n            list corresponds to the results from each Learner(s). Or if the results\n            are reduced, a list of dictionaries of the reduced results from each\n            call to async_update that is ready. </s> remove         # No reduce function -> Return results as is: (possibly empty) list of mappings.\n </s> add         # TODO(sven): Move reduce_fn to the training_step </s> remove             results = self._worker_manager.fetch_ready_async_reqs()\n </s> add             results = self._worker_manager.fetch_ready_async_reqs(\n                tags=list(self._inflight_request_tags)\n            ) </s> remove             results = self._get_results(results)\n </s> add             # NOTE: There is a strong assumption here that the requests launched to\n            # learner workers will return at the same time, since they are have a\n            # barrier inside of themselves for gradient aggregation. Therefore results\n            # should be a list of lists where each inner list should be the length of\n            # the number of learner workers, if results from an  non-blocking update are\n            # ready.\n            results = self._get_async_results(results)", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/algorithms/impala/impala.py"}
{"docstring_tokens": "replace keep keep keep keep keep", "code_tokens": " <mask> from collections import deque\n <mask> from functools import partial\n <mask> import pathlib\n <mask> from typing import (\n <mask>     Any,\n <mask>     Callable,\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> add import uuid </s> remove             A list of dictionaries of results from the updates from the Learner(s)\n </s> add             A dictionary with the reduced results of the updates from the Learner(s) or\n            a list of dictionaries of results from the updates from the Learner(s). </s> remove             A list of dictionaries of results from the updates from the individual\n            Learner(s)\n </s> add             A list of list of dictionaries of results, where the outer list\n            corresponds to separate calls to `async_update`, and the inner\n            list corresponds to the results from each Learner(s). Or if the results\n            are reduced, a list of dictionaries of the reduced results from each\n            call to async_update that is ready. </s> remove             block: Whether to block until the update is complete.\n </s> add  </s> remove         # Make sure minibatch size is reduced to the correct number of shards as well\n        # (just like we split each batch into the number of learner workers).\n        if minibatch_size is not None:\n            minibatch_size //= len(self._workers)\n\n        def _learner_update(learner, minibatch):\n            return learner.update(\n                minibatch,\n                minibatch_size=minibatch_size,\n                num_iters=num_iters,\n                reduce_fn=reduce_fn,\n </s> add         if self.is_local:\n            raise ValueError(\n                \"Cannot call `async_update` when running in local mode with \"\n                \"num_workers=0.\" </s> add                     update_tag = str(uuid.uuid4())\n                    self._inflight_request_tags.add(update_tag)", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask>     Union,\n <mask> )\n <mask> \n <mask> import ray\n <mask> from ray.rllib.core.learner.reduce_result_dict_fn import _reduce_mean_results\n <mask> from ray.rllib.core.rl_module.rl_module import (\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove from collections import deque\n </s> add from collections import defaultdict, deque </s> remove         reduce_fn: Callable[[List[Mapping[str, Any]]], ResultDict] = (\n </s> add         reduce_fn: Optional[Callable[[List[Mapping[str, Any]]], ResultDict]] = ( </s> remove         block: bool = True,\n </s> add  </s> remove         block: bool = True,\n    ) -> List[Mapping[str, Any]]:\n        \"\"\"Do a gradient based update to the Learners using DDP training.\n\n        Note: this function is used if the num_gpus this LearnerGroup is configured\n            with is > 0. If _fake_gpus is True then this function will still be used\n            for distributed training, but the workers will be configured to use a\n            different backend than the cuda backend.\n </s> add     ) -> Union[List[Mapping[str, Any]], List[List[Mapping[str, Any]]]]:\n        \"\"\"Asnychronously do gradient based updates to the Learner(s) with `batch`. </s> remove             A list of dictionaries of results from the updates from the Learner(s)\n </s> add             A dictionary with the reduced results of the updates from the Learner(s) or\n            a list of dictionaries of results from the updates from the Learner(s). </s> remove         # Make sure minibatch size is reduced to the correct number of shards as well\n        # (just like we split each batch into the number of learner workers).\n        if minibatch_size is not None:\n            minibatch_size //= len(self._workers)\n\n        def _learner_update(learner, minibatch):\n            return learner.update(\n                minibatch,\n                minibatch_size=minibatch_size,\n                num_iters=num_iters,\n                reduce_fn=reduce_fn,\n </s> add         if self.is_local:\n            raise ValueError(\n                \"Cannot call `async_update` when running in local mode with \"\n                \"num_workers=0.\"", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         .remove_module() -> remove an RLModule from the MultiAgentRLModule being trained\n <mask>                             by this LearnerGroup.\n <mask>     Args:\n <mask>         learner_spec: The specification for constructing Learners.\n <mask>         max_queue_len: The maximum number of batches to queue up if doing non-blocking\n <mask>             updates (e.g. `self.update(batch, block=False)`). If the queue is full it\n <mask>             will evict the oldest batch first.\n <mask>     \"\"\"\n <mask> \n <mask>     def __init__(\n <mask>         self,\n <mask>         learner_spec: LearnerSpec,\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove             See `.update()` docstring.\n </s> add             batch: The data batch to use for the update.\n            minibatch_size: The minibatch size to use for the update.\n            num_iters: The number of complete passes over all the sub-batches in the\n                input multi-agent batch.\n            reduce_fn: An optional callable to reduce the results from a list of the\n                Learner actors into a single result. This can be any arbitrary function\n                that takes a list of dictionaries and returns a single dictionary. For\n                example you can either take an average (default) or concatenate the\n                results (for example for metrics) or be more selective about you want to\n                report back to the algorithm's training_step. If None is passed, the\n                results will not get reduced. </s> add             # This is a list of the tags for asynchronous update requests that are\n            # inflight, and is used for grouping together the results of requests\n            # that were sent to the workers at the same time.\n            self._inflight_request_tags: Set[str] = set() </s> remove         block: bool = True,\n    ) -> List[Mapping[str, Any]]:\n        \"\"\"Do a gradient based update to the Learners using DDP training.\n\n        Note: this function is used if the num_gpus this LearnerGroup is configured\n            with is > 0. If _fake_gpus is True then this function will still be used\n            for distributed training, but the workers will be configured to use a\n            different backend than the cuda backend.\n </s> add     ) -> Union[List[Mapping[str, Any]], List[List[Mapping[str, Any]]]]:\n        \"\"\"Asnychronously do gradient based updates to the Learner(s) with `batch`. </s> remove             block: Whether to block until the update is complete.\n </s> add  </s> remove             A list of dictionaries of results from the updates from the individual\n            Learner(s)\n </s> add             A list of list of dictionaries of results, where the outer list\n            corresponds to separate calls to `async_update`, and the inner\n            list corresponds to the results from each Learner(s). Or if the results\n            are reduced, a list of dictionaries of the reduced results from each\n            call to async_update that is ready. </s> remove             )\n        else:\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>                 #  an async algo, remove this restriction entirely.\n <mask>                 max_remote_requests_in_flight_per_actor=3,\n <mask>             )\n <mask>             self._in_queue = deque(maxlen=max_queue_len)\n <mask> \n <mask>     def get_in_queue_stats(self) -> Mapping[str, Any]:\n <mask>         \"\"\"Returns the current stats for the input queue for this learner group.\"\"\"\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove         return results\n </s> add             # TODO(sven): Move reduce_fn to the training_step\n            if reduce_fn is None:\n                return results\n            else:\n                return [reduce_fn(r) for r in results] </s> remove         max_queue_len: The maximum number of batches to queue up if doing non-blocking\n            updates (e.g. `self.update(batch, block=False)`). If the queue is full it\n            will evict the oldest batch first.\n </s> add         max_queue_len: The maximum number of batches to queue up if doing async_update\n            If the queue is full itwill evict the oldest batch first. </s> remove         block: bool = True,\n    ) -> List[Mapping[str, Any]]:\n        \"\"\"Do a gradient based update to the Learners using DDP training.\n\n        Note: this function is used if the num_gpus this LearnerGroup is configured\n            with is > 0. If _fake_gpus is True then this function will still be used\n            for distributed training, but the workers will be configured to use a\n            different backend than the cuda backend.\n </s> add     ) -> Union[List[Mapping[str, Any]], List[List[Mapping[str, Any]]]]:\n        \"\"\"Asnychronously do gradient based updates to the Learner(s) with `batch`. </s> remove             results = self._get_results(results)\n </s> add             # NOTE: There is a strong assumption here that the requests launched to\n            # learner workers will return at the same time, since they are have a\n            # barrier inside of themselves for gradient aggregation. Therefore results\n            # should be a list of lists where each inner list should be the length of\n            # the number of learner workers, if results from an  non-blocking update are\n            # ready.\n            results = self._get_async_results(results) </s> remove             results = self._worker_manager.fetch_ready_async_reqs()\n </s> add             results = self._worker_manager.fetch_ready_async_reqs(\n                tags=list(self._inflight_request_tags)\n            ) </s> remove                 result_async = learner_group.update(\n                    batch.as_multi_agent(), block=False, reduce_fn=None\n </s> add                 result_async = learner_group.async_update(\n                    batch.as_multi_agent(), reduce_fn=None", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         num_iters: int = 1,\n <mask>         reduce_fn: Optional[Callable[[List[Mapping[str, Any]]], ResultDict]] = (\n <mask>             _reduce_mean_results\n <mask>         ),\n <mask>         block: bool = True,\n <mask>     ) -> Union[Mapping[str, Any], List[Mapping[str, Any]]]:\n <mask>         \"\"\"Do one or more gradient based updates to the Learner(s) based on given data.\n <mask> \n <mask>         Args:\n <mask>             batch: The data batch to use for the update.\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove         reduce_fn: Callable[[List[Mapping[str, Any]]], ResultDict] = (\n </s> add         reduce_fn: Optional[Callable[[List[Mapping[str, Any]]], ResultDict]] = ( </s> remove         block: bool = True,\n    ) -> List[Mapping[str, Any]]:\n        \"\"\"Do a gradient based update to the Learners using DDP training.\n\n        Note: this function is used if the num_gpus this LearnerGroup is configured\n            with is > 0. If _fake_gpus is True then this function will still be used\n            for distributed training, but the workers will be configured to use a\n            different backend than the cuda backend.\n </s> add     ) -> Union[List[Mapping[str, Any]], List[List[Mapping[str, Any]]]]:\n        \"\"\"Asnychronously do gradient based updates to the Learner(s) with `batch`. </s> remove             See `.update()` docstring.\n </s> add             batch: The data batch to use for the update.\n            minibatch_size: The minibatch size to use for the update.\n            num_iters: The number of complete passes over all the sub-batches in the\n                input multi-agent batch.\n            reduce_fn: An optional callable to reduce the results from a list of the\n                Learner actors into a single result. This can be any arbitrary function\n                that takes a list of dictionaries and returns a single dictionary. For\n                example you can either take an average (default) or concatenate the\n                results (for example for metrics) or be more selective about you want to\n                report back to the algorithm's training_step. If None is passed, the\n                results will not get reduced. </s> remove             block: Whether to block until the update is complete.\n </s> add  </s> remove             results = self._worker_manager.fetch_ready_async_reqs()\n </s> add             results = self._worker_manager.fetch_ready_async_reqs(\n                tags=list(self._inflight_request_tags)\n            ) </s> remove             A list of dictionaries of results from the updates from the Learner(s)\n </s> add             A dictionary with the reduced results of the updates from the Learner(s) or\n            a list of dictionaries of results from the updates from the Learner(s).", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep replace keep keep keep keep", "code_tokens": " <mask>                 example you can either take an average (default) or concatenate the\n <mask>                 results (for example for metrics) or be more selective about you want to\n <mask>                 report back to the algorithm's training_step. If None is passed, the\n <mask>                 results will not get reduced.\n <mask>             block: Whether to block until the update is complete.\n <mask> \n <mask>         Returns:\n <mask>             A list of dictionaries of results from the updates from the Learner(s)\n <mask>         \"\"\"\n <mask> \n <mask>         # Construct a multi-agent batch with only the trainable modules.\n <mask>         train_batch = {}\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove             See `.update()` docstring.\n </s> add             batch: The data batch to use for the update.\n            minibatch_size: The minibatch size to use for the update.\n            num_iters: The number of complete passes over all the sub-batches in the\n                input multi-agent batch.\n            reduce_fn: An optional callable to reduce the results from a list of the\n                Learner actors into a single result. This can be any arbitrary function\n                that takes a list of dictionaries and returns a single dictionary. For\n                example you can either take an average (default) or concatenate the\n                results (for example for metrics) or be more selective about you want to\n                report back to the algorithm's training_step. If None is passed, the\n                results will not get reduced. </s> remove             A list of dictionaries of results from the updates from the individual\n            Learner(s)\n </s> add             A list of list of dictionaries of results, where the outer list\n            corresponds to separate calls to `async_update`, and the inner\n            list corresponds to the results from each Learner(s). Or if the results\n            are reduced, a list of dictionaries of the reduced results from each\n            call to async_update that is ready. </s> remove         # Make sure minibatch size is reduced to the correct number of shards as well\n        # (just like we split each batch into the number of learner workers).\n        if minibatch_size is not None:\n            minibatch_size //= len(self._workers)\n\n        def _learner_update(learner, minibatch):\n            return learner.update(\n                minibatch,\n                minibatch_size=minibatch_size,\n                num_iters=num_iters,\n                reduce_fn=reduce_fn,\n </s> add         if self.is_local:\n            raise ValueError(\n                \"Cannot call `async_update` when running in local mode with \"\n                \"num_workers=0.\" </s> remove         block: bool = True,\n    ) -> List[Mapping[str, Any]]:\n        \"\"\"Do a gradient based update to the Learners using DDP training.\n\n        Note: this function is used if the num_gpus this LearnerGroup is configured\n            with is > 0. If _fake_gpus is True then this function will still be used\n            for distributed training, but the workers will be configured to use a\n            different backend than the cuda backend.\n </s> add     ) -> Union[List[Mapping[str, Any]], List[List[Mapping[str, Any]]]]:\n        \"\"\"Asnychronously do gradient based updates to the Learner(s) with `batch`. </s> remove             results = self._get_results(results)\n </s> add             # NOTE: There is a strong assumption here that the requests launched to\n            # learner workers will return at the same time, since they are have a\n            # barrier inside of themselves for gradient aggregation. Therefore results\n            # should be a list of lists where each inner list should be the length of\n            # the number of learner workers, if results from an  non-blocking update are\n            # ready.\n            results = self._get_async_results(results)", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>                 train_batch[module_id] = batch.policy_batches[module_id]\n <mask>         train_batch = MultiAgentBatch(train_batch, batch.count)\n <mask> \n <mask>         if self.is_local:\n <mask>             if not block:\n <mask>                 raise ValueError(\n <mask>                     \"Cannot run update in non-blocking mode when running in local \"\n <mask>                     \"mode with num_workers=0.\"\n <mask>                 )\n <mask>             results = [\n <mask>                 self._learner.update(\n <mask>                     train_batch,\n <mask>                     minibatch_size=minibatch_size,\n <mask>                     num_iters=num_iters,\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove         # Make sure minibatch size is reduced to the correct number of shards as well\n        # (just like we split each batch into the number of learner workers).\n        if minibatch_size is not None:\n            minibatch_size //= len(self._workers)\n\n        def _learner_update(learner, minibatch):\n            return learner.update(\n                minibatch,\n                minibatch_size=minibatch_size,\n                num_iters=num_iters,\n                reduce_fn=reduce_fn,\n </s> add         if self.is_local:\n            raise ValueError(\n                \"Cannot call `async_update` when running in local mode with \"\n                \"num_workers=0.\" </s> remove             results = self._distributed_update(\n                train_batch,\n                minibatch_size=minibatch_size,\n                num_iters=num_iters,\n                reduce_fn=reduce_fn,\n                block=block,\n </s> add             def _learner_update(learner, minibatch):\n                return learner.update(\n                    minibatch,\n                    minibatch_size=minibatch_size,\n                    num_iters=num_iters,\n                    reduce_fn=reduce_fn,\n                )\n\n            results = self._get_results(\n                self._worker_manager.foreach_actor(\n                    [\n                        partial(_learner_update, minibatch=minibatch)\n                        for minibatch in ShardBatchIterator(batch, len(self._workers))\n                    ]\n                ) </s> remove         if block:\n            results = self._get_results(\n                self._worker_manager.foreach_actor(\n                    [\n                        partial(_learner_update, minibatch=minibatch)\n                        for minibatch in ShardBatchIterator(batch, len(self._workers))\n                    ]\n </s> add             def _learner_update(learner, minibatch):\n                return learner.update(\n                    minibatch,\n                    minibatch_size=minibatch_size,\n                    num_iters=num_iters,\n                    reduce_fn=reduce_fn, </s> remove                 loss = np.mean(\n                    [res[ALL_MODULES][Learner.TOTAL_LOSS_KEY] for res in results]\n </s> add                 losses = [\n                    np.mean(\n                        [res[ALL_MODULES][Learner.TOTAL_LOSS_KEY] for res in results]\n                    )\n                    for results in async_results\n                ]\n                min_loss_this_iter = min(losses)\n                min_loss = min(min_loss_this_iter, min_loss)\n                print(\n                    f\"[iter = {iter_i}] Loss: {min_loss_this_iter:.3f}, Min Loss: \"\n                    f\"{min_loss:.3f}\" </s> remove             A list of dictionaries of results from the updates from the Learner(s)\n </s> add             A dictionary with the reduced results of the updates from the Learner(s) or\n            a list of dictionaries of results from the updates from the Learner(s). </s> remove                 if not results:\n </s> add                 if not async_results:", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep keep keep replace replace replace replace replace replace keep keep replace keep keep keep", "code_tokens": " <mask>                 )\n <mask>             ]\n <mask>         else:\n <mask>             results = self._distributed_update(\n <mask>                 train_batch,\n <mask>                 minibatch_size=minibatch_size,\n <mask>                 num_iters=num_iters,\n <mask>                 reduce_fn=reduce_fn,\n <mask>                 block=block,\n <mask>             )\n <mask> \n <mask>         # No reduce function -> Return results as is: (possibly empty) list of mappings.\n <mask>         if reduce_fn is None:\n <mask>             return results\n <mask>         # If results are empty, don't run them through reduce_fn, but return empty dict.\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove         # If results are empty, don't run them through reduce_fn, but return empty dict.\n        elif not results:\n            return {}\n        # Run results (list of result dicts from our n learner actors) through\n        # reduction function and return single mapping.\n        # TODO (Kourosh): Maybe we should use LearnerInfoBuilder() here?\n        return reduce_fn(results)\n\n    def _distributed_update(\n </s> add         else:\n            return reduce_fn(results)\n\n    def async_update( </s> remove         if block:\n            results = self._get_results(\n                self._worker_manager.foreach_actor(\n                    [\n                        partial(_learner_update, minibatch=minibatch)\n                        for minibatch in ShardBatchIterator(batch, len(self._workers))\n                    ]\n </s> add             def _learner_update(learner, minibatch):\n                return learner.update(\n                    minibatch,\n                    minibatch_size=minibatch_size,\n                    num_iters=num_iters,\n                    reduce_fn=reduce_fn, </s> remove         # Make sure minibatch size is reduced to the correct number of shards as well\n        # (just like we split each batch into the number of learner workers).\n        if minibatch_size is not None:\n            minibatch_size //= len(self._workers)\n\n        def _learner_update(learner, minibatch):\n            return learner.update(\n                minibatch,\n                minibatch_size=minibatch_size,\n                num_iters=num_iters,\n                reduce_fn=reduce_fn,\n </s> add         if self.is_local:\n            raise ValueError(\n                \"Cannot call `async_update` when running in local mode with \"\n                \"num_workers=0.\" </s> remove             if not block:\n                raise ValueError(\n                    \"Cannot run update in non-blocking mode when running in local \"\n                    \"mode with num_workers=0.\"\n                )\n </s> add  </s> add             # TODO(sven): Move reduce_fn to the training_step", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         # No reduce function -> Return results as is: (possibly empty) list of mappings.\n <mask>         if reduce_fn is None:\n <mask>             return results\n <mask>         # If results are empty, don't run them through reduce_fn, but return empty dict.\n <mask>         elif not results:\n <mask>             return {}\n <mask>         # Run results (list of result dicts from our n learner actors) through\n <mask>         # reduction function and return single mapping.\n <mask>         # TODO (Kourosh): Maybe we should use LearnerInfoBuilder() here?\n <mask>         return reduce_fn(results)\n <mask> \n <mask>     def _distributed_update(\n <mask>         self,\n <mask>         batch: MultiAgentBatch,\n <mask>         *,\n <mask>         minibatch_size: Optional[int] = None,\n <mask>         num_iters: int = 1,\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove         # No reduce function -> Return results as is: (possibly empty) list of mappings.\n </s> add         # TODO(sven): Move reduce_fn to the training_step </s> remove             results = self._distributed_update(\n                train_batch,\n                minibatch_size=minibatch_size,\n                num_iters=num_iters,\n                reduce_fn=reduce_fn,\n                block=block,\n </s> add             def _learner_update(learner, minibatch):\n                return learner.update(\n                    minibatch,\n                    minibatch_size=minibatch_size,\n                    num_iters=num_iters,\n                    reduce_fn=reduce_fn,\n                )\n\n            results = self._get_results(\n                self._worker_manager.foreach_actor(\n                    [\n                        partial(_learner_update, minibatch=minibatch)\n                        for minibatch in ShardBatchIterator(batch, len(self._workers))\n                    ]\n                ) </s> add             # TODO(sven): Move reduce_fn to the training_step </s> remove             results = self._get_results(results)\n </s> add             # NOTE: There is a strong assumption here that the requests launched to\n            # learner workers will return at the same time, since they are have a\n            # barrier inside of themselves for gradient aggregation. Therefore results\n            # should be a list of lists where each inner list should be the length of\n            # the number of learner workers, if results from an  non-blocking update are\n            # ready.\n            results = self._get_async_results(results) </s> remove         reduce_fn: Callable[[List[Mapping[str, Any]]], ResultDict] = (\n </s> add         reduce_fn: Optional[Callable[[List[Mapping[str, Any]]], ResultDict]] = ( </s> remove         return results\n </s> add             # TODO(sven): Move reduce_fn to the training_step\n            if reduce_fn is None:\n                return results\n            else:\n                return [reduce_fn(r) for r in results]", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep replace keep keep replace replace replace replace replace replace replace replace keep keep keep", "code_tokens": " <mask>         num_iters: int = 1,\n <mask>         reduce_fn: Callable[[List[Mapping[str, Any]]], ResultDict] = (\n <mask>             _reduce_mean_results\n <mask>         ),\n <mask>         block: bool = True,\n <mask>     ) -> List[Mapping[str, Any]]:\n <mask>         \"\"\"Do a gradient based update to the Learners using DDP training.\n <mask> \n <mask>         Note: this function is used if the num_gpus this LearnerGroup is configured\n <mask>             with is > 0. If _fake_gpus is True then this function will still be used\n <mask>             for distributed training, but the workers will be configured to use a\n <mask>             different backend than the cuda backend.\n <mask> \n <mask>         Args:\n <mask>             See `.update()` docstring.\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove         block: bool = True,\n </s> add  </s> remove             See `.update()` docstring.\n </s> add             batch: The data batch to use for the update.\n            minibatch_size: The minibatch size to use for the update.\n            num_iters: The number of complete passes over all the sub-batches in the\n                input multi-agent batch.\n            reduce_fn: An optional callable to reduce the results from a list of the\n                Learner actors into a single result. This can be any arbitrary function\n                that takes a list of dictionaries and returns a single dictionary. For\n                example you can either take an average (default) or concatenate the\n                results (for example for metrics) or be more selective about you want to\n                report back to the algorithm's training_step. If None is passed, the\n                results will not get reduced. </s> add             # This is a list of the tags for asynchronous update requests that are\n            # inflight, and is used for grouping together the results of requests\n            # that were sent to the workers at the same time.\n            self._inflight_request_tags: Set[str] = set() </s> remove                 result_async = learner_group.update(\n                    batch.as_multi_agent(), block=False, reduce_fn=None\n </s> add                 result_async = learner_group.async_update(\n                    batch.as_multi_agent(), reduce_fn=None </s> remove             results = self._get_results(results)\n </s> add             # NOTE: There is a strong assumption here that the requests launched to\n            # learner workers will return at the same time, since they are have a\n            # barrier inside of themselves for gradient aggregation. Therefore results\n            # should be a list of lists where each inner list should be the length of\n            # the number of learner workers, if results from an  non-blocking update are\n            # ready.\n            results = self._get_async_results(results)", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep keep keep replace keep keep replace replace", "code_tokens": " <mask>             different backend than the cuda backend.\n <mask> \n <mask>         Args:\n <mask>             See `.update()` docstring.\n <mask> \n <mask>         Returns:\n <mask>             A list of dictionaries of results from the updates from the individual\n <mask>             Learner(s)\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove         block: bool = True,\n    ) -> List[Mapping[str, Any]]:\n        \"\"\"Do a gradient based update to the Learners using DDP training.\n\n        Note: this function is used if the num_gpus this LearnerGroup is configured\n            with is > 0. If _fake_gpus is True then this function will still be used\n            for distributed training, but the workers will be configured to use a\n            different backend than the cuda backend.\n </s> add     ) -> Union[List[Mapping[str, Any]], List[List[Mapping[str, Any]]]]:\n        \"\"\"Asnychronously do gradient based updates to the Learner(s) with `batch`. </s> remove             A list of dictionaries of results from the updates from the Learner(s)\n </s> add             A dictionary with the reduced results of the updates from the Learner(s) or\n            a list of dictionaries of results from the updates from the Learner(s). </s> remove             block: Whether to block until the update is complete.\n </s> add  </s> remove         # Make sure minibatch size is reduced to the correct number of shards as well\n        # (just like we split each batch into the number of learner workers).\n        if minibatch_size is not None:\n            minibatch_size //= len(self._workers)\n\n        def _learner_update(learner, minibatch):\n            return learner.update(\n                minibatch,\n                minibatch_size=minibatch_size,\n                num_iters=num_iters,\n                reduce_fn=reduce_fn,\n </s> add         if self.is_local:\n            raise ValueError(\n                \"Cannot call `async_update` when running in local mode with \"\n                \"num_workers=0.\" </s> remove         max_queue_len: The maximum number of batches to queue up if doing non-blocking\n            updates (e.g. `self.update(batch, block=False)`). If the queue is full it\n            will evict the oldest batch first.\n </s> add         max_queue_len: The maximum number of batches to queue up if doing async_update\n            If the queue is full itwill evict the oldest batch first.", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         Returns:\n <mask>             A list of dictionaries of results from the updates from the individual\n <mask>             Learner(s)\n <mask>         \"\"\"\n <mask>         # Make sure minibatch size is reduced to the correct number of shards as well\n <mask>         # (just like we split each batch into the number of learner workers).\n <mask>         if minibatch_size is not None:\n <mask>             minibatch_size //= len(self._workers)\n <mask> \n <mask>         def _learner_update(learner, minibatch):\n <mask>             return learner.update(\n <mask>                 minibatch,\n <mask>                 minibatch_size=minibatch_size,\n <mask>                 num_iters=num_iters,\n <mask>                 reduce_fn=reduce_fn,\n <mask>             )\n <mask> \n <mask>         if block:\n <mask>             results = self._get_results(\n <mask>                 self._worker_manager.foreach_actor(\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove             A list of dictionaries of results from the updates from the individual\n            Learner(s)\n </s> add             A list of list of dictionaries of results, where the outer list\n            corresponds to separate calls to `async_update`, and the inner\n            list corresponds to the results from each Learner(s). Or if the results\n            are reduced, a list of dictionaries of the reduced results from each\n            call to async_update that is ready. </s> add         else:\n            if minibatch_size is not None:\n                minibatch_size //= len(self._workers) </s> remove             results = self._distributed_update(\n                train_batch,\n                minibatch_size=minibatch_size,\n                num_iters=num_iters,\n                reduce_fn=reduce_fn,\n                block=block,\n </s> add             def _learner_update(learner, minibatch):\n                return learner.update(\n                    minibatch,\n                    minibatch_size=minibatch_size,\n                    num_iters=num_iters,\n                    reduce_fn=reduce_fn,\n                )\n\n            results = self._get_results(\n                self._worker_manager.foreach_actor(\n                    [\n                        partial(_learner_update, minibatch=minibatch)\n                        for minibatch in ShardBatchIterator(batch, len(self._workers))\n                    ]\n                ) </s> remove         if block:\n            results = self._get_results(\n                self._worker_manager.foreach_actor(\n                    [\n                        partial(_learner_update, minibatch=minibatch)\n                        for minibatch in ShardBatchIterator(batch, len(self._workers))\n                    ]\n </s> add             def _learner_update(learner, minibatch):\n                return learner.update(\n                    minibatch,\n                    minibatch_size=minibatch_size,\n                    num_iters=num_iters,\n                    reduce_fn=reduce_fn, </s> remove             A list of dictionaries of results from the updates from the Learner(s)\n </s> add             A dictionary with the reduced results of the updates from the Learner(s) or\n            a list of dictionaries of results from the updates from the Learner(s). </s> remove             See `.update()` docstring.\n </s> add             batch: The data batch to use for the update.\n            minibatch_size: The minibatch size to use for the update.\n            num_iters: The number of complete passes over all the sub-batches in the\n                input multi-agent batch.\n            reduce_fn: An optional callable to reduce the results from a list of the\n                Learner actors into a single result. This can be any arbitrary function\n                that takes a list of dictionaries and returns a single dictionary. For\n                example you can either take an average (default) or concatenate the\n                results (for example for metrics) or be more selective about you want to\n                report back to the algorithm's training_step. If None is passed, the\n                results will not get reduced.", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>                 \"num_workers=0.\"\n <mask>             )\n <mask> \n <mask>             def _learner_update(learner, minibatch):\n <mask>                 return learner.update(\n <mask>                     minibatch,\n <mask>                     minibatch_size=minibatch_size,\n <mask>                     num_iters=num_iters,\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove             results = self._distributed_update(\n                train_batch,\n                minibatch_size=minibatch_size,\n                num_iters=num_iters,\n                reduce_fn=reduce_fn,\n                block=block,\n </s> add             def _learner_update(learner, minibatch):\n                return learner.update(\n                    minibatch,\n                    minibatch_size=minibatch_size,\n                    num_iters=num_iters,\n                    reduce_fn=reduce_fn,\n                )\n\n            results = self._get_results(\n                self._worker_manager.foreach_actor(\n                    [\n                        partial(_learner_update, minibatch=minibatch)\n                        for minibatch in ShardBatchIterator(batch, len(self._workers))\n                    ]\n                ) </s> remove         if block:\n            results = self._get_results(\n                self._worker_manager.foreach_actor(\n                    [\n                        partial(_learner_update, minibatch=minibatch)\n                        for minibatch in ShardBatchIterator(batch, len(self._workers))\n                    ]\n </s> add             def _learner_update(learner, minibatch):\n                return learner.update(\n                    minibatch,\n                    minibatch_size=minibatch_size,\n                    num_iters=num_iters,\n                    reduce_fn=reduce_fn, </s> remove         # Make sure minibatch size is reduced to the correct number of shards as well\n        # (just like we split each batch into the number of learner workers).\n        if minibatch_size is not None:\n            minibatch_size //= len(self._workers)\n\n        def _learner_update(learner, minibatch):\n            return learner.update(\n                minibatch,\n                minibatch_size=minibatch_size,\n                num_iters=num_iters,\n                reduce_fn=reduce_fn,\n </s> add         if self.is_local:\n            raise ValueError(\n                \"Cannot call `async_update` when running in local mode with \"\n                \"num_workers=0.\" </s> remove             if not block:\n                raise ValueError(\n                    \"Cannot run update in non-blocking mode when running in local \"\n                    \"mode with num_workers=0.\"\n                )\n </s> add  </s> add             # TODO(sven): Move reduce_fn to the training_step </s> remove \n                for res1, res2 in zip(results, results[1:]):\n                    self.assertEqual(\n                        res1[DEFAULT_POLICY_ID][\"mean_weight\"],\n                        res2[DEFAULT_POLICY_ID][\"mean_weight\"],\n                    )\n </s> add                 for results in async_results:\n                    for res1, res2 in zip(results, results[1:]):\n                        self.assertEqual(\n                            res1[DEFAULT_POLICY_ID][\"mean_weight\"],\n                            res2[DEFAULT_POLICY_ID][\"mean_weight\"],\n                        )\n                iter_i += 1", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace keep replace replace", "code_tokens": " <mask>                 num_iters=num_iters,\n <mask>                 reduce_fn=reduce_fn,\n <mask>             )\n <mask> \n <mask>         if block:\n <mask>             results = self._get_results(\n <mask>                 self._worker_manager.foreach_actor(\n <mask>                     [\n <mask>                         partial(_learner_update, minibatch=minibatch)\n <mask>                         for minibatch in ShardBatchIterator(batch, len(self._workers))\n <mask>                     ]\n <mask>                 )\n <mask>             )\n <mask>         else:\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove             results = self._distributed_update(\n                train_batch,\n                minibatch_size=minibatch_size,\n                num_iters=num_iters,\n                reduce_fn=reduce_fn,\n                block=block,\n </s> add             def _learner_update(learner, minibatch):\n                return learner.update(\n                    minibatch,\n                    minibatch_size=minibatch_size,\n                    num_iters=num_iters,\n                    reduce_fn=reduce_fn,\n                )\n\n            results = self._get_results(\n                self._worker_manager.foreach_actor(\n                    [\n                        partial(_learner_update, minibatch=minibatch)\n                        for minibatch in ShardBatchIterator(batch, len(self._workers))\n                    ]\n                ) </s> remove                         ]\n </s> add                         ],\n                        tag=update_tag, </s> remove         # Make sure minibatch size is reduced to the correct number of shards as well\n        # (just like we split each batch into the number of learner workers).\n        if minibatch_size is not None:\n            minibatch_size //= len(self._workers)\n\n        def _learner_update(learner, minibatch):\n            return learner.update(\n                minibatch,\n                minibatch_size=minibatch_size,\n                num_iters=num_iters,\n                reduce_fn=reduce_fn,\n </s> add         if self.is_local:\n            raise ValueError(\n                \"Cannot call `async_update` when running in local mode with \"\n                \"num_workers=0.\" </s> remove             if not block:\n                raise ValueError(\n                    \"Cannot run update in non-blocking mode when running in local \"\n                    \"mode with num_workers=0.\"\n                )\n </s> add  </s> add                     update_tag = str(uuid.uuid4())\n                    self._inflight_request_tags.add(update_tag)", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>             self._in_queue.append(batch)\n <mask> \n <mask>             # Retrieve all ready results (kicked off by prior calls to this method).\n <mask>             results = self._worker_manager.fetch_ready_async_reqs()\n <mask>             # Only if there are no more requests in-flight on any of the learners,\n <mask>             # we can send in one new batch for sharding and parallel learning.\n <mask>             if self._worker_manager_ready():\n <mask>                 count = 0\n <mask>                 # TODO (sven): This probably works even without any restriction\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove         return results\n </s> add             # TODO(sven): Move reduce_fn to the training_step\n            if reduce_fn is None:\n                return results\n            else:\n                return [reduce_fn(r) for r in results] </s> remove             results = self._get_results(results)\n </s> add             # NOTE: There is a strong assumption here that the requests launched to\n            # learner workers will return at the same time, since they are have a\n            # barrier inside of themselves for gradient aggregation. Therefore results\n            # should be a list of lists where each inner list should be the length of\n            # the number of learner workers, if results from an  non-blocking update are\n            # ready.\n            results = self._get_async_results(results) </s> add             # This is a list of the tags for asynchronous update requests that are\n            # inflight, and is used for grouping together the results of requests\n            # that were sent to the workers at the same time.\n            self._inflight_request_tags: Set[str] = set() </s> remove                 result = self.learner_group.update(\n                    batch,\n                    reduce_fn=_reduce_impala_results,\n                    block=blocking,\n                    num_iters=self.config.num_sgd_iter,\n                    minibatch_size=self.config.minibatch_size,\n                )\n                if result:\n                    self._counters[NUM_ENV_STEPS_TRAINED] += result[ALL_MODULES].pop(\n </s> add                 if blocking:\n                    result = self.learner_group.update(\n                        batch,\n                        reduce_fn=_reduce_impala_results,\n                        num_iters=self.config.num_sgd_iter,\n                        minibatch_size=self.config.minibatch_size,\n                    )\n                    results = [result]\n                else:\n                    results = self.learner_group.async_update(\n                        batch,\n                        reduce_fn=_reduce_impala_results,\n                        num_iters=self.config.num_sgd_iter,\n                        minibatch_size=self.config.minibatch_size,\n                    )\n\n                for r in results:\n                    self._counters[NUM_ENV_STEPS_TRAINED] += r[ALL_MODULES].pop( </s> remove             See `.update()` docstring.\n </s> add             batch: The data batch to use for the update.\n            minibatch_size: The minibatch size to use for the update.\n            num_iters: The number of complete passes over all the sub-batches in the\n                input multi-agent batch.\n            reduce_fn: An optional callable to reduce the results from a list of the\n                Learner actors into a single result. This can be any arbitrary function\n                that takes a list of dictionaries and returns a single dictionary. For\n                example you can either take an average (default) or concatenate the\n                results (for example for metrics) or be more selective about you want to\n                report back to the algorithm's training_step. If None is passed, the\n                results will not get reduced. </s> remove             for iter_i in range(1000):\n </s> add             iter_i = 0\n            while True:", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>                     # Pull a single batch from the queue (from the left side, meaning:\n <mask>                     # use the oldest one first).\n <mask>                     batch = self._in_queue.popleft()\n <mask>                     self._worker_manager.foreach_actor_async(\n <mask>                         [\n <mask>                             partial(_learner_update, minibatch=minibatch)\n <mask>                             for minibatch in ShardBatchIterator(\n <mask>                                 batch, len(self._workers)\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove                         ]\n </s> add                         ],\n                        tag=update_tag, </s> remove         if block:\n            results = self._get_results(\n                self._worker_manager.foreach_actor(\n                    [\n                        partial(_learner_update, minibatch=minibatch)\n                        for minibatch in ShardBatchIterator(batch, len(self._workers))\n                    ]\n </s> add             def _learner_update(learner, minibatch):\n                return learner.update(\n                    minibatch,\n                    minibatch_size=minibatch_size,\n                    num_iters=num_iters,\n                    reduce_fn=reduce_fn, </s> remove             )\n        else:\n </s> add  </s> remove             See `.update()` docstring.\n </s> add             batch: The data batch to use for the update.\n            minibatch_size: The minibatch size to use for the update.\n            num_iters: The number of complete passes over all the sub-batches in the\n                input multi-agent batch.\n            reduce_fn: An optional callable to reduce the results from a list of the\n                Learner actors into a single result. This can be any arbitrary function\n                that takes a list of dictionaries and returns a single dictionary. For\n                example you can either take an average (default) or concatenate the\n                results (for example for metrics) or be more selective about you want to\n                report back to the algorithm's training_step. If None is passed, the\n                results will not get reduced. </s> remove         max_queue_len: The maximum number of batches to queue up if doing non-blocking\n            updates (e.g. `self.update(batch, block=False)`). If the queue is full it\n            will evict the oldest batch first.\n </s> add         max_queue_len: The maximum number of batches to queue up if doing async_update\n            If the queue is full itwill evict the oldest batch first. </s> remove             A list of dictionaries of results from the updates from the Learner(s)\n </s> add             A dictionary with the reduced results of the updates from the Learner(s) or\n            a list of dictionaries of results from the updates from the Learner(s).", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep keep keep replace keep keep keep replace keep keep keep keep", "code_tokens": " <mask>                             for minibatch in ShardBatchIterator(\n <mask>                                 batch, len(self._workers)\n <mask>                             )\n <mask>                         ]\n <mask>                     )\n <mask>                     count += 1\n <mask> \n <mask>             results = self._get_results(results)\n <mask> \n <mask>         return results\n <mask> \n <mask>     def _worker_manager_ready(self):\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove         return results\n </s> add             # TODO(sven): Move reduce_fn to the training_step\n            if reduce_fn is None:\n                return results\n            else:\n                return [reduce_fn(r) for r in results] </s> remove                 result = self.learner_group.update(\n                    batch,\n                    reduce_fn=_reduce_impala_results,\n                    block=blocking,\n                    num_iters=self.config.num_sgd_iter,\n                    minibatch_size=self.config.minibatch_size,\n                )\n                if result:\n                    self._counters[NUM_ENV_STEPS_TRAINED] += result[ALL_MODULES].pop(\n </s> add                 if blocking:\n                    result = self.learner_group.update(\n                        batch,\n                        reduce_fn=_reduce_impala_results,\n                        num_iters=self.config.num_sgd_iter,\n                        minibatch_size=self.config.minibatch_size,\n                    )\n                    results = [result]\n                else:\n                    results = self.learner_group.async_update(\n                        batch,\n                        reduce_fn=_reduce_impala_results,\n                        num_iters=self.config.num_sgd_iter,\n                        minibatch_size=self.config.minibatch_size,\n                    )\n\n                for r in results:\n                    self._counters[NUM_ENV_STEPS_TRAINED] += r[ALL_MODULES].pop( </s> add                     update_tag = str(uuid.uuid4())\n                    self._inflight_request_tags.add(update_tag) </s> remove             results = self._distributed_update(\n                train_batch,\n                minibatch_size=minibatch_size,\n                num_iters=num_iters,\n                reduce_fn=reduce_fn,\n                block=block,\n </s> add             def _learner_update(learner, minibatch):\n                return learner.update(\n                    minibatch,\n                    minibatch_size=minibatch_size,\n                    num_iters=num_iters,\n                    reduce_fn=reduce_fn,\n                )\n\n            results = self._get_results(\n                self._worker_manager.foreach_actor(\n                    [\n                        partial(_learner_update, minibatch=minibatch)\n                        for minibatch in ShardBatchIterator(batch, len(self._workers))\n                    ]\n                ) </s> remove         if block:\n            results = self._get_results(\n                self._worker_manager.foreach_actor(\n                    [\n                        partial(_learner_update, minibatch=minibatch)\n                        for minibatch in ShardBatchIterator(batch, len(self._workers))\n                    ]\n </s> add             def _learner_update(learner, minibatch):\n                return learner.update(\n                    minibatch,\n                    minibatch_size=minibatch_size,\n                    num_iters=num_iters,\n                    reduce_fn=reduce_fn,", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                     count += 1\n <mask> \n <mask>             results = self._get_results(results)\n <mask> \n <mask>         return results\n <mask> \n <mask>     def _worker_manager_ready(self):\n <mask>         # TODO (sven): This probably works even without any restriction (allowing for\n <mask>         #  any arbitrary number of requests in-flight). Test with 3 first, then with\n <mask>         #  unlimited, and if both show the same behavior on an async algo, remove\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove             results = self._get_results(results)\n </s> add             # NOTE: There is a strong assumption here that the requests launched to\n            # learner workers will return at the same time, since they are have a\n            # barrier inside of themselves for gradient aggregation. Therefore results\n            # should be a list of lists where each inner list should be the length of\n            # the number of learner workers, if results from an  non-blocking update are\n            # ready.\n            results = self._get_async_results(results) </s> remove             results = self._worker_manager.fetch_ready_async_reqs()\n </s> add             results = self._worker_manager.fetch_ready_async_reqs(\n                tags=list(self._inflight_request_tags)\n            ) </s> add             # This is a list of the tags for asynchronous update requests that are\n            # inflight, and is used for grouping together the results of requests\n            # that were sent to the workers at the same time.\n            self._inflight_request_tags: Set[str] = set() </s> remove                 result_async = learner_group.update(\n                    batch.as_multi_agent(), block=False, reduce_fn=None\n </s> add                 result_async = learner_group.async_update(\n                    batch.as_multi_agent(), reduce_fn=None </s> remove                         ]\n </s> add                         ],\n                        tag=update_tag, </s> remove                 learner_group.update(batch.as_multi_agent(), block=True, reduce_fn=None)\n </s> add                 learner_group.update(batch.as_multi_agent(), reduce_fn=None)", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>             )\n <mask>             results = self._get_results(results)\n <mask>             if reduce_fn is None:\n <mask>                 return results\n <mask>             return reduce_fn(results)\n <mask> \n <mask>     def add_module(\n <mask>         self,\n <mask>         *,\n <mask>         module_id: ModuleID,\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove         # If results are empty, don't run them through reduce_fn, but return empty dict.\n        elif not results:\n            return {}\n        # Run results (list of result dicts from our n learner actors) through\n        # reduction function and return single mapping.\n        # TODO (Kourosh): Maybe we should use LearnerInfoBuilder() here?\n        return reduce_fn(results)\n\n    def _distributed_update(\n </s> add         else:\n            return reduce_fn(results)\n\n    def async_update( </s> remove         return results\n </s> add             # TODO(sven): Move reduce_fn to the training_step\n            if reduce_fn is None:\n                return results\n            else:\n                return [reduce_fn(r) for r in results] </s> remove             results = self._distributed_update(\n                train_batch,\n                minibatch_size=minibatch_size,\n                num_iters=num_iters,\n                reduce_fn=reduce_fn,\n                block=block,\n </s> add             def _learner_update(learner, minibatch):\n                return learner.update(\n                    minibatch,\n                    minibatch_size=minibatch_size,\n                    num_iters=num_iters,\n                    reduce_fn=reduce_fn,\n                )\n\n            results = self._get_results(\n                self._worker_manager.foreach_actor(\n                    [\n                        partial(_learner_update, minibatch=minibatch)\n                        for minibatch in ShardBatchIterator(batch, len(self._workers))\n                    ]\n                ) </s> remove         # No reduce function -> Return results as is: (possibly empty) list of mappings.\n </s> add         # TODO(sven): Move reduce_fn to the training_step </s> remove             results = self._get_results(results)\n </s> add             # NOTE: There is a strong assumption here that the requests launched to\n            # learner workers will return at the same time, since they are have a\n            # barrier inside of themselves for gradient aggregation. Therefore results\n            # should be a list of lists where each inner list should be the length of\n            # the number of learner workers, if results from an  non-blocking update are\n            # ready.\n            results = self._get_async_results(results) </s> remove         # Make sure minibatch size is reduced to the correct number of shards as well\n        # (just like we split each batch into the number of learner workers).\n        if minibatch_size is not None:\n            minibatch_size //= len(self._workers)\n\n        def _learner_update(learner, minibatch):\n            return learner.update(\n                minibatch,\n                minibatch_size=minibatch_size,\n                num_iters=num_iters,\n                reduce_fn=reduce_fn,\n </s> add         if self.is_local:\n            raise ValueError(\n                \"Cannot call `async_update` when running in local mode with \"\n                \"num_workers=0.\"", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/learner_group.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     def test_async_update(self):\n <mask>         \"\"\"Test that async style updates converge to the same result as sync.\"\"\"\n <mask>         fws = [\"torch\", \"tf2\"]\n <mask>         # block=True only needs to be tested for the most complex case.\n <mask>         # so we'll only test it for multi-gpu-ddp.\n <mask>         scaling_modes = [\"multi-gpu-ddp\"]\n <mask>         test_iterator = itertools.product(fws, scaling_modes)\n <mask> \n <mask>         for fw, scaling_mode in test_iterator:\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove \n                for res1, res2 in zip(results, results[1:]):\n                    self.assertEqual(\n                        res1[DEFAULT_POLICY_ID][\"mean_weight\"],\n                        res2[DEFAULT_POLICY_ID][\"mean_weight\"],\n                    )\n </s> add                 for results in async_results:\n                    for res1, res2 in zip(results, results[1:]):\n                        self.assertEqual(\n                            res1[DEFAULT_POLICY_ID][\"mean_weight\"],\n                            res2[DEFAULT_POLICY_ID][\"mean_weight\"],\n                        )\n                iter_i += 1 </s> add             # This is a list of the tags for asynchronous update requests that are\n            # inflight, and is used for grouping together the results of requests\n            # that were sent to the workers at the same time.\n            self._inflight_request_tags: Set[str] = set() </s> remove             learner_group = get_learner_group(fw, env, scaling_config)\n </s> add             learner_group = get_learner_group(\n                fw, env, scaling_config, eager_tracing=True\n            ) </s> remove                 result = self.learner_group.update(\n                    batch,\n                    reduce_fn=_reduce_impala_results,\n                    block=blocking,\n                    num_iters=self.config.num_sgd_iter,\n                    minibatch_size=self.config.minibatch_size,\n                )\n                if result:\n                    self._counters[NUM_ENV_STEPS_TRAINED] += result[ALL_MODULES].pop(\n </s> add                 if blocking:\n                    result = self.learner_group.update(\n                        batch,\n                        reduce_fn=_reduce_impala_results,\n                        num_iters=self.config.num_sgd_iter,\n                        minibatch_size=self.config.minibatch_size,\n                    )\n                    results = [result]\n                else:\n                    results = self.learner_group.async_update(\n                        batch,\n                        reduce_fn=_reduce_impala_results,\n                        num_iters=self.config.num_sgd_iter,\n                        minibatch_size=self.config.minibatch_size,\n                    )\n\n                for r in results:\n                    self._counters[NUM_ENV_STEPS_TRAINED] += r[ALL_MODULES].pop( </s> remove             A list of dictionaries of results from the updates from the Learner(s)\n </s> add             A dictionary with the reduced results of the updates from the Learner(s) or\n            a list of dictionaries of results from the updates from the Learner(s). </s> remove             results = self._get_results(results)\n </s> add             # NOTE: There is a strong assumption here that the requests launched to\n            # learner workers will return at the same time, since they are have a\n            # barrier inside of themselves for gradient aggregation. Therefore results\n            # should be a list of lists where each inner list should be the length of\n            # the number of learner workers, if results from an  non-blocking update are\n            # ready.\n            results = self._get_async_results(results)", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/tests/test_learner_group.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         for fw, scaling_mode in test_iterator:\n <mask>             print(f\"Testing framework: {fw}, scaling mode: {scaling_mode}.\")\n <mask>             env = gym.make(\"CartPole-v1\")\n <mask>             scaling_config = REMOTE_SCALING_CONFIGS[scaling_mode]\n <mask>             learner_group = get_learner_group(fw, env, scaling_config)\n <mask>             reader = get_cartpole_dataset_reader(batch_size=512)\n <mask>             min_loss = float(\"inf\")\n <mask>             batch = reader.next()\n <mask>             timer_sync = _Timer()\n <mask>             timer_async = _Timer()\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove                 learner_group.update(batch.as_multi_agent(), block=True, reduce_fn=None)\n </s> add                 learner_group.update(batch.as_multi_agent(), reduce_fn=None) </s> remove         # block=True only needs to be tested for the most complex case.\n </s> add         # async_update only needs to be tested for the most complex case. </s> remove                 if not results:\n </s> add                 if not async_results: </s> remove                 result_async = learner_group.update(\n                    batch.as_multi_agent(), block=False, reduce_fn=None\n </s> add                 result_async = learner_group.async_update(\n                    batch.as_multi_agent(), reduce_fn=None </s> remove                 results = learner_group.update(\n                    batch.as_multi_agent(), block=False, reduce_fn=None\n </s> add                 async_results = learner_group.async_update(\n                    batch.as_multi_agent(), reduce_fn=None </s> remove                 loss = np.mean(\n                    [res[ALL_MODULES][Learner.TOTAL_LOSS_KEY] for res in results]\n </s> add                 losses = [\n                    np.mean(\n                        [res[ALL_MODULES][Learner.TOTAL_LOSS_KEY] for res in results]\n                    )\n                    for results in async_results\n                ]\n                min_loss_this_iter = min(losses)\n                min_loss = min(min_loss_this_iter, min_loss)\n                print(\n                    f\"[iter = {iter_i}] Loss: {min_loss_this_iter:.3f}, Min Loss: \"\n                    f\"{min_loss:.3f}\"", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/tests/test_learner_group.py"}
{"docstring_tokens": "keep keep keep keep replace keep replace replace keep", "code_tokens": " <mask>             batch = reader.next()\n <mask>             timer_sync = _Timer()\n <mask>             timer_async = _Timer()\n <mask>             with timer_sync:\n <mask>                 learner_group.update(batch.as_multi_agent(), block=True, reduce_fn=None)\n <mask>             with timer_async:\n <mask>                 result_async = learner_group.update(\n <mask>                     batch.as_multi_agent(), block=False, reduce_fn=None\n <mask>                 )\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove             learner_group = get_learner_group(fw, env, scaling_config)\n </s> add             learner_group = get_learner_group(\n                fw, env, scaling_config, eager_tracing=True\n            ) </s> remove                 results = learner_group.update(\n                    batch.as_multi_agent(), block=False, reduce_fn=None\n </s> add                 async_results = learner_group.async_update(\n                    batch.as_multi_agent(), reduce_fn=None </s> remove                 if not results:\n </s> add                 if not async_results: </s> remove             for iter_i in range(1000):\n </s> add             iter_i = 0\n            while True: </s> remove                 loss = np.mean(\n                    [res[ALL_MODULES][Learner.TOTAL_LOSS_KEY] for res in results]\n </s> add                 losses = [\n                    np.mean(\n                        [res[ALL_MODULES][Learner.TOTAL_LOSS_KEY] for res in results]\n                    )\n                    for results in async_results\n                ]\n                min_loss_this_iter = min(losses)\n                min_loss = min(min_loss_this_iter, min_loss)\n                print(\n                    f\"[iter = {iter_i}] Loss: {min_loss_this_iter:.3f}, Min Loss: \"\n                    f\"{min_loss:.3f}\"", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/tests/test_learner_group.py"}
{"docstring_tokens": "keep replace keep replace replace keep keep keep", "code_tokens": " <mask>             self.assertEqual(len(result_async), 0)\n <mask>             for iter_i in range(1000):\n <mask>                 batch = reader.next()\n <mask>                 results = learner_group.update(\n <mask>                     batch.as_multi_agent(), block=False, reduce_fn=None\n <mask>                 )\n <mask>                 if not results:\n <mask>                     continue\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove                 if not results:\n </s> add                 if not async_results: </s> remove                 learner_group.update(batch.as_multi_agent(), block=True, reduce_fn=None)\n </s> add                 learner_group.update(batch.as_multi_agent(), reduce_fn=None) </s> remove                 loss = np.mean(\n                    [res[ALL_MODULES][Learner.TOTAL_LOSS_KEY] for res in results]\n </s> add                 losses = [\n                    np.mean(\n                        [res[ALL_MODULES][Learner.TOTAL_LOSS_KEY] for res in results]\n                    )\n                    for results in async_results\n                ]\n                min_loss_this_iter = min(losses)\n                min_loss = min(min_loss_this_iter, min_loss)\n                print(\n                    f\"[iter = {iter_i}] Loss: {min_loss_this_iter:.3f}, Min Loss: \"\n                    f\"{min_loss:.3f}\" </s> remove                 result_async = learner_group.update(\n                    batch.as_multi_agent(), block=False, reduce_fn=None\n </s> add                 result_async = learner_group.async_update(\n                    batch.as_multi_agent(), reduce_fn=None </s> remove                 result = self.learner_group.update(\n                    batch,\n                    reduce_fn=_reduce_impala_results,\n                    block=blocking,\n                    num_iters=self.config.num_sgd_iter,\n                    minibatch_size=self.config.minibatch_size,\n                )\n                if result:\n                    self._counters[NUM_ENV_STEPS_TRAINED] += result[ALL_MODULES].pop(\n </s> add                 if blocking:\n                    result = self.learner_group.update(\n                        batch,\n                        reduce_fn=_reduce_impala_results,\n                        num_iters=self.config.num_sgd_iter,\n                        minibatch_size=self.config.minibatch_size,\n                    )\n                    results = [result]\n                else:\n                    results = self.learner_group.async_update(\n                        batch,\n                        reduce_fn=_reduce_impala_results,\n                        num_iters=self.config.num_sgd_iter,\n                        minibatch_size=self.config.minibatch_size,\n                    )\n\n                for r in results:\n                    self._counters[NUM_ENV_STEPS_TRAINED] += r[ALL_MODULES].pop(", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/tests/test_learner_group.py"}
{"docstring_tokens": "keep keep keep keep replace keep replace replace keep keep keep", "code_tokens": " <mask>                 batch = reader.next()\n <mask>                 results = learner_group.update(\n <mask>                     batch.as_multi_agent(), block=False, reduce_fn=None\n <mask>                 )\n <mask>                 if not results:\n <mask>                     continue\n <mask>                 loss = np.mean(\n <mask>                     [res[ALL_MODULES][Learner.TOTAL_LOSS_KEY] for res in results]\n <mask>                 )\n <mask>                 min_loss = min(loss, min_loss)\n <mask>                 print(f\"[iter = {iter_i}] Loss: {loss:.3f}, Min Loss: {min_loss:.3f}\")\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove                 min_loss = min(loss, min_loss)\n                print(f\"[iter = {iter_i}] Loss: {loss:.3f}, Min Loss: {min_loss:.3f}\")\n </s> add  </s> remove                 results = learner_group.update(\n                    batch.as_multi_agent(), block=False, reduce_fn=None\n </s> add                 async_results = learner_group.async_update(\n                    batch.as_multi_agent(), reduce_fn=None </s> remove             for iter_i in range(1000):\n </s> add             iter_i = 0\n            while True: </s> remove                 learner_group.update(batch.as_multi_agent(), block=True, reduce_fn=None)\n </s> add                 learner_group.update(batch.as_multi_agent(), reduce_fn=None) </s> remove                 result_async = learner_group.update(\n                    batch.as_multi_agent(), block=False, reduce_fn=None\n </s> add                 result_async = learner_group.async_update(\n                    batch.as_multi_agent(), reduce_fn=None", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/tests/test_learner_group.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep replace replace replace replace replace replace", "code_tokens": " <mask>                     continue\n <mask>                 loss = np.mean(\n <mask>                     [res[ALL_MODULES][Learner.TOTAL_LOSS_KEY] for res in results]\n <mask>                 )\n <mask>                 min_loss = min(loss, min_loss)\n <mask>                 print(f\"[iter = {iter_i}] Loss: {loss:.3f}, Min Loss: {min_loss:.3f}\")\n <mask>                 # The loss is initially around 0.69 (ln2). When it gets to around\n <mask>                 # 0.57 the return of the policy gets to around 100.\n <mask>                 if min_loss < 0.57:\n <mask>                     break\n <mask> \n <mask>                 for res1, res2 in zip(results, results[1:]):\n <mask>                     self.assertEqual(\n <mask>                         res1[DEFAULT_POLICY_ID][\"mean_weight\"],\n <mask>                         res2[DEFAULT_POLICY_ID][\"mean_weight\"],\n <mask>                     )\n </s> [RLlib] Make learner group with non blocking update return multiple results (#35858)\n\nSigned-off-by: Avnish <avnishnarayan@gmail.com> </s> remove                 loss = np.mean(\n                    [res[ALL_MODULES][Learner.TOTAL_LOSS_KEY] for res in results]\n </s> add                 losses = [\n                    np.mean(\n                        [res[ALL_MODULES][Learner.TOTAL_LOSS_KEY] for res in results]\n                    )\n                    for results in async_results\n                ]\n                min_loss_this_iter = min(losses)\n                min_loss = min(min_loss_this_iter, min_loss)\n                print(\n                    f\"[iter = {iter_i}] Loss: {min_loss_this_iter:.3f}, Min Loss: \"\n                    f\"{min_loss:.3f}\" </s> remove                 if not results:\n </s> add                 if not async_results: </s> remove                 results = learner_group.update(\n                    batch.as_multi_agent(), block=False, reduce_fn=None\n </s> add                 async_results = learner_group.async_update(\n                    batch.as_multi_agent(), reduce_fn=None </s> remove         # block=True only needs to be tested for the most complex case.\n </s> add         # async_update only needs to be tested for the most complex case. </s> remove         return results\n </s> add             # TODO(sven): Move reduce_fn to the training_step\n            if reduce_fn is None:\n                return results\n            else:\n                return [reduce_fn(r) for r in results]", "html_url": "https://github.com/ray-project/ray/commit/7d52c2fa69d66ce9e05281e4f402f03964bcb7c4", "file_name": "rllib/core/learner/tests/test_learner_group.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>     from dm_env import specs\n <mask> except ImportError:\n <mask>     specs = None\n <mask> try:\n <mask>     from dm_control import suite\n <mask> except (ImportError, OSError):\n <mask>     suite = None\n <mask> import numpy as np\n <mask> \n <mask> \n </s> [RLlib] Remove all remaining tf- and MuJoCo warnings from RLlib. (#15454) </s> add         tf1_module.logging.set_verbosity(tf1_module.logging.ERROR) </s> add         tf1_module.logging.set_verbosity(tf1_module.logging.WARN)", "html_url": "https://github.com/ray-project/ray/commit/7e1a191f17388769b7e1461b1f65c6592d73bfce", "file_name": "rllib/env/wrappers/dm_control_wrapper.py"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask>     try:\n <mask>         tf1_module = tf_module.compat.v1\n <mask>         if not was_imported:\n <mask>             tf1_module.disable_v2_behavior()\n <mask>             tf1_module.enable_resource_variables()\n <mask>         tf1_module.logging.set_verbosity(tf1_module.logging.WARN)\n <mask>     # No compat.v1 -> return tf as is.\n </s> [RLlib] Remove all remaining tf- and MuJoCo warnings from RLlib. (#15454) </s> add     # Suppress MuJoCo warning (dm_control uses absl logging).\n    import absl.logging\n    absl.logging.set_verbosity(\"error\") </s> add         tf1_module.logging.set_verbosity(tf1_module.logging.WARN)", "html_url": "https://github.com/ray-project/ray/commit/7e1a191f17388769b7e1461b1f65c6592d73bfce", "file_name": "rllib/utils/framework.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask>         tf1_module.logging.set_verbosity(tf1_module.logging.ERROR)\n <mask>         if not was_imported:\n <mask>             tf1_module.disable_v2_behavior()\n <mask>             tf1_module.enable_resource_variables()\n <mask>     # No compat.v1 -> return tf as is.\n <mask>     except AttributeError:\n <mask>         tf1_module = tf_module\n <mask> \n </s> [RLlib] Remove all remaining tf- and MuJoCo warnings from RLlib. (#15454) </s> add     # Suppress MuJoCo warning (dm_control uses absl logging).\n    import absl.logging\n    absl.logging.set_verbosity(\"error\") </s> add         tf1_module.logging.set_verbosity(tf1_module.logging.ERROR)", "html_url": "https://github.com/ray-project/ray/commit/7e1a191f17388769b7e1461b1f65c6592d73bfce", "file_name": "rllib/utils/framework.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     Example_0:  TERMINATED [pid=68248], 179 s, 2 iter, 60000 ts, 94 rew\n <mask> \n <mask> \n <mask> Visualizing Results\n <mask> -------------------\n <mask> \n <mask> To visualize learning in tensorboard, install TensorFlow:\n <mask> \n <mask> .. code-block:: bash\n <mask> \n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove To use rllab's VisKit (you may have to install some dependencies), run:\n </s> add If using TF2, Tune also automatically generates TensorBoard HParams output, as shown below:\n\n.. code-block:: python\n\n    tune.run(\n        ...,\n        config={\n            \"lr\": tune.grid_search([1e-5, 1e-4]),\n            \"momentum\": tune.grid_search([0, 0.9])\n        }\n    )\n\n.. image:: images/tune-hparams.png\n\n\nThe nonrelevant metrics (like timing stats) can be disabled on the left to show only the relevant ones (like accuracy, loss, etc.).\n\n\nViskit\n------\n\nTo use VisKit (you may have to install some dependencies), run: </s> add If using TF2 and TensorBoard, Tune will also automatically generate TensorBoard HParams output:\n\n.. image:: images/tune-hparams-coord.png\n </s> remove .. important:: Take the 3 minute `2019 Ray Tune User Survey <https://forms.gle/7u5eH1avbTfpZ3dE6>`_!\n </s> add .. important:: Take the `2019 Ray Tune User Survey <https://forms.gle/7u5eH1avbTfpZ3dE6>`_ and show us your Tune project! </s> add                 \"baz\": \"asd\", </s> add     \"\"\"Formats the resolved variable dict into a single string.\"\"\" </s> remove                     experiment_tag += \"_{}\".format(resolved_vars)\n </s> add                     experiment_tag += \"_{}\".format(format_vars(resolved_vars))", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "doc/source/tune-usage.rst"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     $ export TMPDIR=/tmp/$USER; mkdir -p $TMPDIR; tensorboard --logdir=~/ray_results\n <mask> \n <mask> .. image:: ray-tune-tensorboard.png\n <mask> \n <mask> To use rllab's VisKit (you may have to install some dependencies), run:\n <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     $ git clone https://github.com/rll/rllab.git\n <mask>     $ python rllab/rllab/viskit/frontend.py ~/ray_results/my_experiment\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove Visualizing Results\n-------------------\n </s> add TensorBoard\n----------- </s> add If using TF2 and TensorBoard, Tune will also automatically generate TensorBoard HParams output:\n\n.. image:: images/tune-hparams-coord.png\n </s> remove .. important:: Take the 3 minute `2019 Ray Tune User Survey <https://forms.gle/7u5eH1avbTfpZ3dE6>`_!\n </s> add .. important:: Take the `2019 Ray Tune User Survey <https://forms.gle/7u5eH1avbTfpZ3dE6>`_ and show us your Tune project! </s> remove     def __init__(self, config, logdir):\n </s> add     def __init__(self, config, logdir, trial=None): </s> add     Use `format_vars` to format the returned dict of hyperparameters.\n\n    Yields:\n        (Dict of resolved variables, Spec object) </s> remove     def __init__(self, config, logdir, loggers=None, sync_function=None):\n </s> add     def __init__(self,\n                 config,\n                 logdir,\n                 trial=None,\n                 loggers=None,\n                 sync_function=None):", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "doc/source/tune-usage.rst"}
{"docstring_tokens": "keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> Tune: A Scalable Hyperparameter Tuning Library\n <mask> ==============================================\n <mask> \n <mask> .. important:: Take the 3 minute `2019 Ray Tune User Survey <https://forms.gle/7u5eH1avbTfpZ3dE6>`_!\n <mask> \n <mask> .. image:: images/tune.png\n <mask>     :scale: 30%\n <mask>     :align: center\n <mask> \n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> add If using TF2 and TensorBoard, Tune will also automatically generate TensorBoard HParams output:\n\n.. image:: images/tune-hparams-coord.png\n </s> remove To use rllab's VisKit (you may have to install some dependencies), run:\n </s> add If using TF2, Tune also automatically generates TensorBoard HParams output, as shown below:\n\n.. code-block:: python\n\n    tune.run(\n        ...,\n        config={\n            \"lr\": tune.grid_search([1e-5, 1e-4]),\n            \"momentum\": tune.grid_search([0, 0.9])\n        }\n    )\n\n.. image:: images/tune-hparams.png\n\n\nThe nonrelevant metrics (like timing stats) can be disabled on the left to show only the relevant ones (like accuracy, loss, etc.).\n\n\nViskit\n------\n\nTo use VisKit (you may have to install some dependencies), run: </s> remove Visualizing Results\n-------------------\n </s> add TensorBoard\n----------- </s> add     \"\"\"Formats the resolved variable dict into a single string.\"\"\" </s> add     Use `format_vars` to format the returned dict of hyperparameters.\n\n    Yields:\n        (Dict of resolved variables, Spec object) </s> add def flatten_resolved_vars(resolved_vars):\n    \"\"\"Formats the resolved variable dict into a mapping of (str -> value).\"\"\"\n    flattened_resolved_vars_dict = {}\n    for pieces, value in resolved_vars.items():\n        if pieces[0] == \"config\":\n            pieces = pieces[1:]\n        pieces = [str(piece) for piece in pieces]\n        flattened_resolved_vars_dict[\"/\".join(pieces)] = value\n    return flattened_resolved_vars_dict\n\n", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "doc/source/tune.rst"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask> \n <mask> .. image:: images/tune-start-tb.png\n <mask> \n <mask> Distributed Quick Start\n <mask> -----------------------\n <mask> \n <mask> 1. Import and initialize Ray by appending the following to your example script.\n <mask> \n <mask> .. code-block:: python\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove .. important:: Take the 3 minute `2019 Ray Tune User Survey <https://forms.gle/7u5eH1avbTfpZ3dE6>`_!\n </s> add .. important:: Take the `2019 Ray Tune User Survey <https://forms.gle/7u5eH1avbTfpZ3dE6>`_ and show us your Tune project! </s> remove To use rllab's VisKit (you may have to install some dependencies), run:\n </s> add If using TF2, Tune also automatically generates TensorBoard HParams output, as shown below:\n\n.. code-block:: python\n\n    tune.run(\n        ...,\n        config={\n            \"lr\": tune.grid_search([1e-5, 1e-4]),\n            \"momentum\": tune.grid_search([0, 0.9])\n        }\n    )\n\n.. image:: images/tune-hparams.png\n\n\nThe nonrelevant metrics (like timing stats) can be disabled on the left to show only the relevant ones (like accuracy, loss, etc.).\n\n\nViskit\n------\n\nTo use VisKit (you may have to install some dependencies), run: </s> remove Visualizing Results\n-------------------\n </s> add TensorBoard\n----------- </s> add     Use `format_vars` to format the returned dict of hyperparameters.\n\n    Yields:\n        (Dict of resolved variables, Spec object) </s> remove     def __init__(self, config, logdir):\n </s> add     def __init__(self, config, logdir, trial=None): </s> add                 if not self._hp_logged:\n                    if self.trial and self.trial.evaluated_params:\n                        try:\n                            hp.hparams(\n                                self.trial.evaluated_params,\n                                trial_id=self.trial.trial_id)\n                        except Exception as exc:\n                            logger.error(\"HParams failed with %s\", exc)\n                    self._hp_logged = True\n", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "doc/source/tune.rst"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         config: Configuration passed to all logger creators.\n <mask>         logdir: Directory for all logger creators to log to.\n <mask>     \"\"\"\n <mask> \n <mask>     def __init__(self, config, logdir):\n <mask>         self.config = config\n <mask>         self.logdir = logdir\n <mask>         self._init()\n <mask> \n <mask>     def _init(self):\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> add         self.trial = trial </s> remove     def __init__(self, config, logdir, loggers=None, sync_function=None):\n </s> add     def __init__(self,\n                 config,\n                 logdir,\n                 trial=None,\n                 loggers=None,\n                 sync_function=None): </s> remove def tf2_compat_logger(config, logdir):\n </s> add def tf2_compat_logger(config, logdir, trial=None): </s> add         self._hp_logged = False </s> remove         super(UnifiedLogger, self).__init__(config, logdir)\n </s> add         super(UnifiedLogger, self).__init__(config, logdir, trial) </s> remove                 self._loggers.append(cls(self.config, self.logdir))\n </s> add                 self._loggers.append(cls(self.config, self.logdir, self.trial))", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/logger.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>     def __init__(self, config, logdir, trial=None):\n <mask>         self.config = config\n <mask>         self.logdir = logdir\n <mask>         self._init()\n <mask> \n <mask>     def _init(self):\n <mask>         pass\n <mask> \n <mask>     def on_result(self, result):\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove     def __init__(self, config, logdir):\n </s> add     def __init__(self, config, logdir, trial=None): </s> remove     def __init__(self, config, logdir, loggers=None, sync_function=None):\n </s> add     def __init__(self,\n                 config,\n                 logdir,\n                 trial=None,\n                 loggers=None,\n                 sync_function=None): </s> add         self._hp_logged = False </s> add             from tensorboard.plugins.hparams import api as hp </s> remove         super(UnifiedLogger, self).__init__(config, logdir)\n </s> add         super(UnifiedLogger, self).__init__(config, logdir, trial) </s> remove def tf2_compat_logger(config, logdir):\n </s> add def tf2_compat_logger(config, logdir, trial=None):", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/logger.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         with open(config_pkl, \"wb\") as f:\n <mask>             cloudpickle.dump(self.config, f)\n <mask> \n <mask> \n <mask> def tf2_compat_logger(config, logdir):\n <mask>     \"\"\"Chooses TensorBoard logger depending on imported TF version.\"\"\"\n <mask>     global tf\n <mask>     if \"RLLIB_TEST_NO_TF_IMPORT\" in os.environ:\n <mask>         logger.warning(\"Not importing TensorFlow for test purposes\")\n <mask>         tf = None\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove             return TFLogger(config, logdir)\n </s> add             return TFLogger(config, logdir, trial) </s> remove     def __init__(self, config, logdir):\n </s> add     def __init__(self, config, logdir, trial=None): </s> add             from tensorboard.plugins.hparams import api as hp </s> remove             return TF2Logger(config, logdir)\n </s> add             return TF2Logger(config, logdir, trial) </s> add                 if not self._hp_logged:\n                    if self.trial and self.trial.evaluated_params:\n                        try:\n                            hp.hparams(\n                                self.trial.evaluated_params,\n                                trial_id=self.trial.trial_id)\n                        except Exception as exc:\n                            logger.error(\"HParams failed with %s\", exc)\n                    self._hp_logged = True\n </s> add         self._hp_logged = False", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/logger.py"}
{"docstring_tokens": "keep keep keep replace keep replace keep keep keep keep", "code_tokens": " <mask>                        distutils.version.LooseVersion(\"2.0.0\"))\n <mask>         if use_tf2_api:\n <mask>             tf = tf.compat.v2  # setting this for TF2.0\n <mask>             return TF2Logger(config, logdir)\n <mask>         else:\n <mask>             return TFLogger(config, logdir)\n <mask> \n <mask> \n <mask> class TF2Logger(Logger):\n <mask>     \"\"\"TensorBoard Logger for TF version >= 1.14.\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> add def flatten_resolved_vars(resolved_vars):\n    \"\"\"Formats the resolved variable dict into a mapping of (str -> value).\"\"\"\n    flattened_resolved_vars_dict = {}\n    for pieces, value in resolved_vars.items():\n        if pieces[0] == \"config\":\n            pieces = pieces[1:]\n        pieces = [str(piece) for piece in pieces]\n        flattened_resolved_vars_dict[\"/\".join(pieces)] = value\n    return flattened_resolved_vars_dict\n\n </s> remove def tf2_compat_logger(config, logdir):\n </s> add def tf2_compat_logger(config, logdir, trial=None): </s> remove         super(UnifiedLogger, self).__init__(config, logdir)\n </s> add         super(UnifiedLogger, self).__init__(config, logdir, trial) </s> remove                     evaluated_params=resolved_vars,\n </s> add                     evaluated_params=flatten_resolved_vars(resolved_vars), </s> remove                 evaluated_params=list(suggested_config),\n </s> add                 evaluated_params=flatten_dict(suggested_config),", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/logger.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>     \"\"\"\n <mask> \n <mask>     def _init(self):\n <mask>         self._file_writer = None\n <mask> \n <mask>     def on_result(self, result):\n <mask>         if self._file_writer is None:\n <mask>             from tensorflow.python.eager import context\n <mask>             from tensorboard.plugins.hparams import api as hp\n <mask>             self._context = context\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> add             from tensorboard.plugins.hparams import api as hp </s> remove from ray.tune.util import merge_dicts\n </s> add from ray.tune.util import merge_dicts, flatten_dict </s> remove from ray.tune.suggest.variant_generator import generate_variants\n </s> add from ray.tune.suggest.variant_generator import (generate_variants, format_vars,\n                                                flatten_resolved_vars) </s> add         self.trial = trial </s> remove     def __init__(self, config, logdir, loggers=None, sync_function=None):\n </s> add     def __init__(self,\n                 config,\n                 logdir,\n                 trial=None,\n                 loggers=None,\n                 sync_function=None): </s> remove         super(UnifiedLogger, self).__init__(config, logdir)\n </s> add         super(UnifiedLogger, self).__init__(config, logdir, trial)", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/logger.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     def on_result(self, result):\n <mask>         if self._file_writer is None:\n <mask>             from tensorflow.python.eager import context\n <mask>             self._context = context\n <mask>             self._file_writer = tf.summary.create_file_writer(self.logdir)\n <mask>         with tf.device(\"/CPU:0\"), self._context.eager_mode():\n <mask>             with tf.summary.record_if(True), self._file_writer.as_default():\n <mask>                 step = result.get(\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> add         self._hp_logged = False </s> add         self.trial = trial </s> remove     def __init__(self, config, logdir, loggers=None, sync_function=None):\n </s> add     def __init__(self,\n                 config,\n                 logdir,\n                 trial=None,\n                 loggers=None,\n                 sync_function=None): </s> add                 if not self._hp_logged:\n                    if self.trial and self.trial.evaluated_params:\n                        try:\n                            hp.hparams(\n                                self.trial.evaluated_params,\n                                trial_id=self.trial.trial_id)\n                        except Exception as exc:\n                            logger.error(\"HParams failed with %s\", exc)\n                    self._hp_logged = True\n </s> remove def tf2_compat_logger(config, logdir):\n </s> add def tf2_compat_logger(config, logdir, trial=None): </s> remove from ray.tune.util import merge_dicts\n </s> add from ray.tune.util import merge_dicts, flatten_dict", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/logger.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> \n <mask>                 tmp = result.copy()\n <mask>                 for k in [\n <mask>                         \"config\", \"pid\", \"timestamp\", TIME_TOTAL_S,\n <mask>                         TRAINING_ITERATION\n <mask>                 ]:\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove         super(UnifiedLogger, self).__init__(config, logdir)\n </s> add         super(UnifiedLogger, self).__init__(config, logdir, trial) </s> add def flatten_resolved_vars(resolved_vars):\n    \"\"\"Formats the resolved variable dict into a mapping of (str -> value).\"\"\"\n    flattened_resolved_vars_dict = {}\n    for pieces, value in resolved_vars.items():\n        if pieces[0] == \"config\":\n            pieces = pieces[1:]\n        pieces = [str(piece) for piece in pieces]\n        flattened_resolved_vars_dict[\"/\".join(pieces)] = value\n    return flattened_resolved_vars_dict\n\n </s> remove                     experiment_tag += \"_{}\".format(resolved_vars)\n </s> add                     experiment_tag += \"_{}\".format(format_vars(resolved_vars)) </s> add     \"\"\"Formats the resolved variable dict into a single string.\"\"\" </s> remove                 self._loggers.append(cls(self.config, self.logdir))\n </s> add                 self._loggers.append(cls(self.config, self.logdir, self.trial)) </s> remove def tf2_compat_logger(config, logdir):\n </s> add def tf2_compat_logger(config, logdir, trial=None):", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/logger.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         sync_function (func|str): Optional function for syncer to run.\n <mask>             See ray/python/ray/tune/log_sync.py\n <mask>     \"\"\"\n <mask> \n <mask>     def __init__(self, config, logdir, loggers=None, sync_function=None):\n <mask>         if loggers is None:\n <mask>             self._logger_cls_list = DEFAULT_LOGGERS\n <mask>         else:\n <mask>             self._logger_cls_list = loggers\n <mask>         self._sync_function = sync_function\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove         super(UnifiedLogger, self).__init__(config, logdir)\n </s> add         super(UnifiedLogger, self).__init__(config, logdir, trial) </s> remove     def __init__(self, config, logdir):\n </s> add     def __init__(self, config, logdir, trial=None): </s> add         self._hp_logged = False </s> add         self.trial = trial </s> add             from tensorboard.plugins.hparams import api as hp </s> remove             return TF2Logger(config, logdir)\n </s> add             return TF2Logger(config, logdir, trial)", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/logger.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             self._logger_cls_list = loggers\n <mask>         self._sync_function = sync_function\n <mask>         self._log_syncer = None\n <mask> \n <mask>         super(UnifiedLogger, self).__init__(config, logdir)\n <mask> \n <mask>     def _init(self):\n <mask>         self._loggers = []\n <mask>         for cls in self._logger_cls_list:\n <mask>             try:\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove                 self._loggers.append(cls(self.config, self.logdir))\n </s> add                 self._loggers.append(cls(self.config, self.logdir, self.trial)) </s> remove     def __init__(self, config, logdir, loggers=None, sync_function=None):\n </s> add     def __init__(self,\n                 config,\n                 logdir,\n                 trial=None,\n                 loggers=None,\n                 sync_function=None): </s> add         self._hp_logged = False </s> add         self.trial = trial </s> remove             return TF2Logger(config, logdir)\n </s> add             return TF2Logger(config, logdir, trial) </s> remove             spec[\"config\"] = merge_dicts(spec[\"config\"], suggested_config)\n </s> add             spec[\"config\"] = merge_dicts(spec[\"config\"],\n                                         copy.deepcopy(suggested_config))", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/logger.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     def _init(self):\n <mask>         self._loggers = []\n <mask>         for cls in self._logger_cls_list:\n <mask>             try:\n <mask>                 self._loggers.append(cls(self.config, self.logdir))\n <mask>             except Exception as exc:\n <mask>                 logger.warning(\"Could not instantiate {}: {}.\".format(\n <mask>                     cls.__name__, str(exc)))\n <mask>         self._log_syncer = get_log_syncer(\n <mask>             self.logdir,\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove         super(UnifiedLogger, self).__init__(config, logdir)\n </s> add         super(UnifiedLogger, self).__init__(config, logdir, trial) </s> add                 if not self._hp_logged:\n                    if self.trial and self.trial.evaluated_params:\n                        try:\n                            hp.hparams(\n                                self.trial.evaluated_params,\n                                trial_id=self.trial.trial_id)\n                        except Exception as exc:\n                            logger.error(\"HParams failed with %s\", exc)\n                    self._hp_logged = True\n </s> add         self._hp_logged = False </s> add     \"\"\"Formats the resolved variable dict into a single string.\"\"\" </s> add         self.trial = trial </s> remove     def __init__(self, config, logdir):\n </s> add     def __init__(self, config, logdir, trial=None):", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/logger.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> from ray.tune.error import TuneError\n <mask> from ray.tune.experiment import convert_to_experiment_list\n <mask> from ray.tune.config_parser import make_parser, create_trial_from_spec\n <mask> from ray.tune.suggest.variant_generator import generate_variants\n <mask> from ray.tune.suggest.search import SearchAlgorithm\n <mask> \n <mask> \n <mask> class BasicVariantGenerator(SearchAlgorithm):\n <mask>     \"\"\"Uses Tune's variant generation for resolving variables.\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove from ray.tune.util import merge_dicts\n </s> add from ray.tune.util import merge_dicts, flatten_dict </s> add         self._hp_logged = False </s> add             from tensorboard.plugins.hparams import api as hp </s> remove             return TFLogger(config, logdir)\n </s> add             return TFLogger(config, logdir, trial) </s> remove             return TF2Logger(config, logdir)\n </s> add             return TF2Logger(config, logdir, trial) </s> remove                     experiment_tag += \"_{}\".format(resolved_vars)\n </s> add                     experiment_tag += \"_{}\".format(format_vars(resolved_vars))", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/suggest/basic_variant.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         for _ in range(unresolved_spec.get(\"num_samples\", 1)):\n <mask>             for resolved_vars, spec in generate_variants(unresolved_spec):\n <mask>                 experiment_tag = str(self._counter)\n <mask>                 if resolved_vars:\n <mask>                     experiment_tag += \"_{}\".format(resolved_vars)\n <mask>                 self._counter += 1\n <mask>                 yield create_trial_from_spec(\n <mask>                     spec,\n <mask>                     output_path,\n <mask>                     self._parser,\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove             spec[\"config\"] = merge_dicts(spec[\"config\"], suggested_config)\n </s> add             spec[\"config\"] = merge_dicts(spec[\"config\"],\n                                         copy.deepcopy(suggested_config)) </s> remove                     evaluated_params=resolved_vars,\n </s> add                     evaluated_params=flatten_resolved_vars(resolved_vars), </s> remove                 evaluated_params=list(suggested_config),\n </s> add                 evaluated_params=flatten_dict(suggested_config), </s> remove         yield format_vars(resolved_vars), spec\n </s> add         yield resolved_vars, spec </s> add     \"\"\"Formats the resolved variable dict into a single string.\"\"\" </s> add     Use `format_vars` to format the returned dict of hyperparameters.\n\n    Yields:\n        (Dict of resolved variables, Spec object)", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/suggest/basic_variant.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep", "code_tokens": " <mask>                 yield create_trial_from_spec(\n <mask>                     spec,\n <mask>                     output_path,\n <mask>                     self._parser,\n <mask>                     evaluated_params=resolved_vars,\n <mask>                     experiment_tag=experiment_tag)\n <mask> \n <mask>     def is_finished(self):\n <mask>         return self._finished\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove                 evaluated_params=list(suggested_config),\n </s> add                 evaluated_params=flatten_dict(suggested_config), </s> remove                     experiment_tag += \"_{}\".format(resolved_vars)\n </s> add                     experiment_tag += \"_{}\".format(format_vars(resolved_vars)) </s> remove             spec[\"config\"] = merge_dicts(spec[\"config\"], suggested_config)\n </s> add             spec[\"config\"] = merge_dicts(spec[\"config\"],\n                                         copy.deepcopy(suggested_config)) </s> add def flatten_resolved_vars(resolved_vars):\n    \"\"\"Formats the resolved variable dict into a mapping of (str -> value).\"\"\"\n    flattened_resolved_vars_dict = {}\n    for pieces, value in resolved_vars.items():\n        if pieces[0] == \"config\":\n            pieces = pieces[1:]\n        pieces = [str(piece) for piece in pieces]\n        flattened_resolved_vars_dict[\"/\".join(pieces)] = value\n    return flattened_resolved_vars_dict\n\n </s> remove         yield format_vars(resolved_vars), spec\n </s> add         yield resolved_vars, spec </s> remove             return TF2Logger(config, logdir)\n </s> add             return TF2Logger(config, logdir, trial)", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/suggest/basic_variant.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> import copy\n <mask> \n <mask> from ray.tune.error import TuneError\n <mask> from ray.tune.trial import Trial\n <mask> from ray.tune.util import merge_dicts\n <mask> from ray.tune.experiment import convert_to_experiment_list\n <mask> from ray.tune.config_parser import make_parser, create_trial_from_spec\n <mask> from ray.tune.suggest.search import SearchAlgorithm\n <mask> from ray.tune.suggest.variant_generator import format_vars, resolve_nested_dict\n <mask> \n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove from ray.tune.suggest.variant_generator import generate_variants\n </s> add from ray.tune.suggest.variant_generator import (generate_variants, format_vars,\n                                                flatten_resolved_vars) </s> add         self._hp_logged = False </s> add             from tensorboard.plugins.hparams import api as hp </s> remove         self.assertEqual(trials[0].config, {\"bar\": True, \"foo\": 1, \"qux\": 4})\n </s> add         self.assertEqual(trials[0].config, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"qux\": 4,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[0].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"qux\": 4,\n        }) </s> remove             return TFLogger(config, logdir)\n </s> add             return TFLogger(config, logdir, trial) </s> remove         super(UnifiedLogger, self).__init__(config, logdir)\n </s> add         super(UnifiedLogger, self).__init__(config, logdir, trial)", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/suggest/suggestion.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                     yield None\n <mask>                 else:\n <mask>                     break\n <mask>             spec = copy.deepcopy(experiment_spec)\n <mask>             spec[\"config\"] = merge_dicts(spec[\"config\"], suggested_config)\n <mask>             flattened_config = resolve_nested_dict(spec[\"config\"])\n <mask>             self._counter += 1\n <mask>             tag = \"{0}_{1}\".format(\n <mask>                 str(self._counter), format_vars(flattened_config))\n <mask>             yield create_trial_from_spec(\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove                     experiment_tag += \"_{}\".format(resolved_vars)\n </s> add                     experiment_tag += \"_{}\".format(format_vars(resolved_vars)) </s> remove         yield format_vars(resolved_vars), spec\n </s> add         yield resolved_vars, spec </s> remove         super(UnifiedLogger, self).__init__(config, logdir)\n </s> add         super(UnifiedLogger, self).__init__(config, logdir, trial) </s> add         self._hp_logged = False </s> remove                     evaluated_params=resolved_vars,\n </s> add                     evaluated_params=flatten_resolved_vars(resolved_vars), </s> remove                 evaluated_params=list(suggested_config),\n </s> add                 evaluated_params=flatten_dict(suggested_config),", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/suggest/suggestion.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             yield create_trial_from_spec(\n <mask>                 spec,\n <mask>                 output_path,\n <mask>                 self._parser,\n <mask>                 evaluated_params=list(suggested_config),\n <mask>                 experiment_tag=tag,\n <mask>                 trial_id=trial_id)\n <mask> \n <mask>     def is_finished(self):\n <mask>         return self._finished\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove                     evaluated_params=resolved_vars,\n </s> add                     evaluated_params=flatten_resolved_vars(resolved_vars), </s> remove                     experiment_tag += \"_{}\".format(resolved_vars)\n </s> add                     experiment_tag += \"_{}\".format(format_vars(resolved_vars)) </s> remove             spec[\"config\"] = merge_dicts(spec[\"config\"], suggested_config)\n </s> add             spec[\"config\"] = merge_dicts(spec[\"config\"],\n                                         copy.deepcopy(suggested_config)) </s> add def flatten_resolved_vars(resolved_vars):\n    \"\"\"Formats the resolved variable dict into a mapping of (str -> value).\"\"\"\n    flattened_resolved_vars_dict = {}\n    for pieces, value in resolved_vars.items():\n        if pieces[0] == \"config\":\n            pieces = pieces[1:]\n        pieces = [str(piece) for piece in pieces]\n        flattened_resolved_vars_dict[\"/\".join(pieces)] = value\n    return flattened_resolved_vars_dict\n\n </s> remove         yield format_vars(resolved_vars), spec\n </s> add         yield resolved_vars, spec </s> remove             return TF2Logger(config, logdir)\n </s> add             return TF2Logger(config, logdir, trial)", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/suggest/suggestion.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         \"activation\": {\"grid_search\": [\"relu\", \"tanh\"]}\n <mask>         \"cpu\": {\"eval\": \"spec.config.num_workers\"}\n <mask>     \"\"\"\n <mask>     for resolved_vars, spec in _generate_variants(unresolved_spec):\n <mask>         assert not _unresolved_values(spec)\n <mask>         yield resolved_vars, spec\n <mask> \n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove         yield format_vars(resolved_vars), spec\n </s> add         yield resolved_vars, spec </s> remove                     experiment_tag += \"_{}\".format(resolved_vars)\n </s> add                     experiment_tag += \"_{}\".format(format_vars(resolved_vars)) </s> remove             spec[\"config\"] = merge_dicts(spec[\"config\"], suggested_config)\n </s> add             spec[\"config\"] = merge_dicts(spec[\"config\"],\n                                         copy.deepcopy(suggested_config)) </s> remove                 self._loggers.append(cls(self.config, self.logdir))\n </s> add                 self._loggers.append(cls(self.config, self.logdir, self.trial)) </s> add                 if not self._hp_logged:\n                    if self.trial and self.trial.evaluated_params:\n                        try:\n                            hp.hparams(\n                                self.trial.evaluated_params,\n                                trial_id=self.trial.trial_id)\n                        except Exception as exc:\n                            logger.error(\"HParams failed with %s\", exc)\n                    self._hp_logged = True\n </s> remove                     evaluated_params=resolved_vars,\n </s> add                     evaluated_params=flatten_resolved_vars(resolved_vars),", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/suggest/variant_generator.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         \"cpu\": {\"eval\": \"spec.config.num_workers\"}\n <mask>     \"\"\"\n <mask>     for resolved_vars, spec in _generate_variants(unresolved_spec):\n <mask>         assert not _unresolved_values(spec)\n <mask>         yield format_vars(resolved_vars), spec\n <mask> \n <mask> \n <mask> def grid_search(values):\n <mask>     \"\"\"Convenience method for specifying grid search over a value.\n <mask> \n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> add     Use `format_vars` to format the returned dict of hyperparameters.\n\n    Yields:\n        (Dict of resolved variables, Spec object) </s> remove                     experiment_tag += \"_{}\".format(resolved_vars)\n </s> add                     experiment_tag += \"_{}\".format(format_vars(resolved_vars)) </s> remove             spec[\"config\"] = merge_dicts(spec[\"config\"], suggested_config)\n </s> add             spec[\"config\"] = merge_dicts(spec[\"config\"],\n                                         copy.deepcopy(suggested_config)) </s> add     \"\"\"Formats the resolved variable dict into a single string.\"\"\" </s> add def flatten_resolved_vars(resolved_vars):\n    \"\"\"Formats the resolved variable dict into a mapping of (str -> value).\"\"\"\n    flattened_resolved_vars_dict = {}\n    for pieces, value in resolved_vars.items():\n        if pieces[0] == \"config\":\n            pieces = pieces[1:]\n        pieces = [str(piece) for piece in pieces]\n        flattened_resolved_vars_dict[\"/\".join(pieces)] = value\n    return flattened_resolved_vars_dict\n\n </s> remove                 self._loggers.append(cls(self.config, self.logdir))\n </s> add                 self._loggers.append(cls(self.config, self.logdir, self.trial))", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/suggest/variant_generator.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> \n <mask> def format_vars(resolved_vars):\n <mask>     out = []\n <mask>     for path, value in sorted(resolved_vars.items()):\n <mask>         if path[0] in [\"run\", \"env\", \"resources_per_trial\"]:\n <mask>             continue  # TrialRunner already has these in the experiment_tag\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> add def flatten_resolved_vars(resolved_vars):\n    \"\"\"Formats the resolved variable dict into a mapping of (str -> value).\"\"\"\n    flattened_resolved_vars_dict = {}\n    for pieces, value in resolved_vars.items():\n        if pieces[0] == \"config\":\n            pieces = pieces[1:]\n        pieces = [str(piece) for piece in pieces]\n        flattened_resolved_vars_dict[\"/\".join(pieces)] = value\n    return flattened_resolved_vars_dict\n\n </s> remove                     experiment_tag += \"_{}\".format(resolved_vars)\n </s> add                     experiment_tag += \"_{}\".format(format_vars(resolved_vars)) </s> remove         super(UnifiedLogger, self).__init__(config, logdir)\n </s> add         super(UnifiedLogger, self).__init__(config, logdir, trial) </s> remove                 self._loggers.append(cls(self.config, self.logdir))\n </s> add                 self._loggers.append(cls(self.config, self.logdir, self.trial)) </s> remove def tf2_compat_logger(config, logdir):\n </s> add def tf2_compat_logger(config, logdir, trial=None): </s> add                 if not self._hp_logged:\n                    if self.trial and self.trial.evaluated_params:\n                        try:\n                            hp.hparams(\n                                self.trial.evaluated_params,\n                                trial_id=self.trial.trial_id)\n                        except Exception as exc:\n                            logger.error(\"HParams failed with %s\", exc)\n                    self._hp_logged = True\n", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/suggest/variant_generator.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>     return \",\".join(out)\n <mask> \n <mask> \n <mask> def _clean_value(value):\n <mask>     if isinstance(value, float):\n <mask>         return \"{:.5}\".format(value)\n <mask>     else:\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove             return TF2Logger(config, logdir)\n </s> add             return TF2Logger(config, logdir, trial) </s> remove             return TFLogger(config, logdir)\n </s> add             return TFLogger(config, logdir, trial) </s> remove                     evaluated_params=resolved_vars,\n </s> add                     evaluated_params=flatten_resolved_vars(resolved_vars), </s> remove                 evaluated_params=list(suggested_config),\n </s> add                 evaluated_params=flatten_dict(suggested_config), </s> remove     def __init__(self, config, logdir, loggers=None, sync_function=None):\n </s> add     def __init__(self,\n                 config,\n                 logdir,\n                 trial=None,\n                 loggers=None,\n                 sync_function=None): </s> add         self._hp_logged = False", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/suggest/variant_generator.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>         self.assertEqual(trials[0].config, {\"foo\": \"bar\", \"env\": \"Pong-v0\"})\n <mask>         self.assertEqual(trials[0].trainable_name, \"PPO\")\n <mask>         self.assertEqual(trials[0].experiment_tag, \"0\")\n <mask>         self.assertEqual(trials[0].max_failures, 5)\n <mask>         self.assertEqual(trials[0].local_dir,\n <mask>                          os.path.join(DEFAULT_RESULTS_DIR, \"tune-pong\"))\n <mask>         self.assertEqual(trials[1].experiment_tag, \"1\")\n <mask> \n <mask>     def testEval(self):\n <mask>         trials = self.generate_trials({\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> add         self.assertEqual(trials[0].evaluated_params, {\"foo\": 4}) </s> remove         self.assertEqual(trials[0].config, {\"bar\": True, \"foo\": 1, \"qux\": 4})\n </s> add         self.assertEqual(trials[0].config, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"qux\": 4,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[0].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"qux\": 4,\n        }) </s> remove         self.assertEqual(trials[0].config, {\"bar\": True, \"foo\": 1})\n </s> add         self.assertEqual(trials[0].config, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[0].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 1,\n        }) </s> remove         self.assertEqual(trials[1].config, {\"bar\": False, \"foo\": 1})\n </s> add         self.assertEqual(trials[1].config, {\n            \"bar\": False,\n            \"foo\": 1,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[1].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 1,\n        }) </s> remove         self.assertEqual(trials[2].config, {\"bar\": True, \"foo\": 2})\n        self.assertEqual(trials[3].config, {\"bar\": False, \"foo\": 2})\n        self.assertEqual(trials[4].config, {\"bar\": True, \"foo\": 3})\n        self.assertEqual(trials[5].config, {\"bar\": False, \"foo\": 3})\n </s> add         self.assertEqual(trials[2].config, {\n            \"bar\": True,\n            \"foo\": 2,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[2].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 2,\n        })\n\n        self.assertEqual(trials[3].config, {\n            \"bar\": False,\n            \"foo\": 2,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[3].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 2,\n        })\n\n        self.assertEqual(trials[4].config, {\n            \"bar\": True,\n            \"foo\": 3,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[4].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 3,\n        })\n\n        self.assertEqual(trials[5].config, {\n            \"bar\": False,\n            \"foo\": 3,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[5].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 3,\n        }) </s> add                 \"baz\": \"asd\",", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/tests/test_trial_runner.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>         }, \"eval\")\n <mask>         trials = list(trials)\n <mask>         self.assertEqual(len(trials), 1)\n <mask>         self.assertEqual(trials[0].config, {\"foo\": 4})\n <mask>         self.assertEqual(trials[0].experiment_tag, \"0_foo=4\")\n <mask> \n <mask>     def testGridSearch(self):\n <mask>         trials = self.generate_trials({\n <mask>             \"run\": \"PPO\",\n <mask>             \"config\": {\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove         self.assertEqual(trials[0].config, {\"bar\": True, \"foo\": 1, \"qux\": 4})\n </s> add         self.assertEqual(trials[0].config, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"qux\": 4,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[0].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"qux\": 4,\n        }) </s> add                 \"baz\": \"asd\", </s> add         self.assertEqual(trials[0].evaluated_params, {}) </s> add                 \"baz\": \"asd\", </s> remove         self.assertEqual(trials[0].config, {\"bar\": True, \"foo\": 1})\n </s> add         self.assertEqual(trials[0].config, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[0].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 1,\n        }) </s> remove         self.assertEqual(trials[2].config, {\"bar\": True, \"foo\": 2})\n        self.assertEqual(trials[3].config, {\"bar\": False, \"foo\": 2})\n        self.assertEqual(trials[4].config, {\"bar\": True, \"foo\": 3})\n        self.assertEqual(trials[5].config, {\"bar\": False, \"foo\": 3})\n </s> add         self.assertEqual(trials[2].config, {\n            \"bar\": True,\n            \"foo\": 2,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[2].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 2,\n        })\n\n        self.assertEqual(trials[3].config, {\n            \"bar\": False,\n            \"foo\": 2,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[3].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 2,\n        })\n\n        self.assertEqual(trials[4].config, {\n            \"bar\": True,\n            \"foo\": 3,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[4].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 3,\n        })\n\n        self.assertEqual(trials[5].config, {\n            \"bar\": False,\n            \"foo\": 3,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[5].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 3,\n        })", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/tests/test_trial_runner.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>                 },\n <mask>                 \"foo\": {\n <mask>                     \"grid_search\": [1, 2, 3]\n <mask>                 },\n <mask>             },\n <mask>         }, \"grid_search\")\n <mask>         trials = list(trials)\n <mask>         self.assertEqual(len(trials), 6)\n <mask>         self.assertEqual(trials[0].config, {\n <mask>             \"bar\": True,\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove         self.assertEqual(trials[0].config, {\"bar\": True, \"foo\": 1})\n </s> add         self.assertEqual(trials[0].config, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[0].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 1,\n        }) </s> add                 \"baz\": \"asd\", </s> remove         self.assertEqual(trials[0].config, {\"bar\": True, \"foo\": 1, \"qux\": 4})\n </s> add         self.assertEqual(trials[0].config, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"qux\": 4,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[0].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"qux\": 4,\n        }) </s> add         self.assertEqual(trials[0].evaluated_params, {\"foo\": 4}) </s> remove         self.assertEqual(trials[2].config, {\"bar\": True, \"foo\": 2})\n        self.assertEqual(trials[3].config, {\"bar\": False, \"foo\": 2})\n        self.assertEqual(trials[4].config, {\"bar\": True, \"foo\": 3})\n        self.assertEqual(trials[5].config, {\"bar\": False, \"foo\": 3})\n </s> add         self.assertEqual(trials[2].config, {\n            \"bar\": True,\n            \"foo\": 2,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[2].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 2,\n        })\n\n        self.assertEqual(trials[3].config, {\n            \"bar\": False,\n            \"foo\": 2,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[3].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 2,\n        })\n\n        self.assertEqual(trials[4].config, {\n            \"bar\": True,\n            \"foo\": 3,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[4].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 3,\n        })\n\n        self.assertEqual(trials[5].config, {\n            \"bar\": False,\n            \"foo\": 3,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[5].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 3,\n        }) </s> remove         self.assertEqual(trials[1].config, {\"bar\": False, \"foo\": 1})\n </s> add         self.assertEqual(trials[1].config, {\n            \"bar\": False,\n            \"foo\": 1,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[1].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 1,\n        })", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/tests/test_trial_runner.py"}
{"docstring_tokens": "keep replace keep replace keep keep", "code_tokens": " <mask>         self.assertEqual(len(trials), 6)\n <mask>         self.assertEqual(trials[0].config, {\"bar\": True, \"foo\": 1})\n <mask>         self.assertEqual(trials[0].experiment_tag, \"0_bar=True,foo=1\")\n <mask>         self.assertEqual(trials[1].config, {\"bar\": False, \"foo\": 1})\n <mask>         self.assertEqual(trials[1].experiment_tag, \"1_bar=False,foo=1\")\n <mask>         self.assertEqual(trials[2].config, {\"bar\": True, \"foo\": 2})\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove         self.assertEqual(trials[2].config, {\"bar\": True, \"foo\": 2})\n        self.assertEqual(trials[3].config, {\"bar\": False, \"foo\": 2})\n        self.assertEqual(trials[4].config, {\"bar\": True, \"foo\": 3})\n        self.assertEqual(trials[5].config, {\"bar\": False, \"foo\": 3})\n </s> add         self.assertEqual(trials[2].config, {\n            \"bar\": True,\n            \"foo\": 2,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[2].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 2,\n        })\n\n        self.assertEqual(trials[3].config, {\n            \"bar\": False,\n            \"foo\": 2,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[3].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 2,\n        })\n\n        self.assertEqual(trials[4].config, {\n            \"bar\": True,\n            \"foo\": 3,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[4].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 3,\n        })\n\n        self.assertEqual(trials[5].config, {\n            \"bar\": False,\n            \"foo\": 3,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[5].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 3,\n        }) </s> remove         self.assertEqual(trials[0].config, {\"bar\": True, \"foo\": 1, \"qux\": 4})\n </s> add         self.assertEqual(trials[0].config, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"qux\": 4,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[0].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"qux\": 4,\n        }) </s> add                 \"baz\": \"asd\", </s> add                 \"baz\": \"asd\", </s> add         self.assertEqual(trials[0].evaluated_params, {})", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/tests/test_trial_runner.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         self.assertEqual(trials[0].config, {\"bar\": True, \"foo\": 1})\n <mask>         self.assertEqual(trials[0].experiment_tag, \"0_bar=True,foo=1\")\n <mask>         self.assertEqual(trials[1].config, {\"bar\": False, \"foo\": 1})\n <mask>         self.assertEqual(trials[1].experiment_tag, \"1_bar=False,foo=1\")\n <mask>         self.assertEqual(trials[2].config, {\"bar\": True, \"foo\": 2})\n <mask>         self.assertEqual(trials[3].config, {\"bar\": False, \"foo\": 2})\n <mask>         self.assertEqual(trials[4].config, {\"bar\": True, \"foo\": 3})\n <mask>         self.assertEqual(trials[5].config, {\"bar\": False, \"foo\": 3})\n <mask> \n <mask>     def testGridSearchAndEval(self):\n <mask>         trials = self.generate_trials({\n <mask>             \"run\": \"PPO\",\n <mask>             \"config\": {\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove         self.assertEqual(trials[1].config, {\"bar\": False, \"foo\": 1})\n </s> add         self.assertEqual(trials[1].config, {\n            \"bar\": False,\n            \"foo\": 1,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[1].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 1,\n        }) </s> remove         self.assertEqual(trials[0].config, {\"bar\": True, \"foo\": 1})\n </s> add         self.assertEqual(trials[0].config, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[0].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 1,\n        }) </s> remove         self.assertEqual(trials[0].config, {\"bar\": True, \"foo\": 1, \"qux\": 4})\n </s> add         self.assertEqual(trials[0].config, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"qux\": 4,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[0].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"qux\": 4,\n        }) </s> add                 \"baz\": \"asd\", </s> add         self.assertEqual(trials[0].evaluated_params, {\"foo\": 4}) </s> add                 \"baz\": \"asd\",", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/tests/test_trial_runner.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>                 \"qux\": tune.sample_from(lambda spec: 2 + 2),\n <mask>                 \"bar\": grid_search([True, False]),\n <mask>                 \"foo\": grid_search([1, 2, 3]),\n <mask>             },\n <mask>         }, \"grid_eval\")\n <mask>         trials = list(trials)\n <mask>         self.assertEqual(len(trials), 6)\n <mask>         self.assertEqual(trials[0].config, {\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> remove         self.assertEqual(trials[0].config, {\"bar\": True, \"foo\": 1, \"qux\": 4})\n </s> add         self.assertEqual(trials[0].config, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"qux\": 4,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[0].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"qux\": 4,\n        }) </s> add                 \"baz\": \"asd\", </s> remove         self.assertEqual(trials[0].config, {\"bar\": True, \"foo\": 1})\n </s> add         self.assertEqual(trials[0].config, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[0].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 1,\n        }) </s> add         self.assertEqual(trials[0].evaluated_params, {\"foo\": 4}) </s> remove         self.assertEqual(trials[1].config, {\"bar\": False, \"foo\": 1})\n </s> add         self.assertEqual(trials[1].config, {\n            \"bar\": False,\n            \"foo\": 1,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[1].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 1,\n        }) </s> remove         self.assertEqual(trials[2].config, {\"bar\": True, \"foo\": 2})\n        self.assertEqual(trials[3].config, {\"bar\": False, \"foo\": 2})\n        self.assertEqual(trials[4].config, {\"bar\": True, \"foo\": 3})\n        self.assertEqual(trials[5].config, {\"bar\": False, \"foo\": 3})\n </s> add         self.assertEqual(trials[2].config, {\n            \"bar\": True,\n            \"foo\": 2,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[2].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 2,\n        })\n\n        self.assertEqual(trials[3].config, {\n            \"bar\": False,\n            \"foo\": 2,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[3].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 2,\n        })\n\n        self.assertEqual(trials[4].config, {\n            \"bar\": True,\n            \"foo\": 3,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[4].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 3,\n        })\n\n        self.assertEqual(trials[5].config, {\n            \"bar\": False,\n            \"foo\": 3,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[5].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 3,\n        })", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/tests/test_trial_runner.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             },\n <mask>         }, \"grid_eval\")\n <mask>         trials = list(trials)\n <mask>         self.assertEqual(len(trials), 6)\n <mask>         self.assertEqual(trials[0].config, {\"bar\": True, \"foo\": 1, \"qux\": 4})\n <mask>         self.assertEqual(trials[0].experiment_tag, \"0_bar=True,foo=1,qux=4\")\n <mask> \n <mask>     def testConditionResolution(self):\n <mask>         trials = self.generate_trials({\n <mask>             \"run\": \"PPO\",\n </s> [tune] TensorBoard HParams for TF2.0 (#5678) </s> add         self.assertEqual(trials[0].evaluated_params, {\"foo\": 4}) </s> remove         self.assertEqual(trials[0].config, {\"bar\": True, \"foo\": 1})\n </s> add         self.assertEqual(trials[0].config, {\n            \"bar\": True,\n            \"foo\": 1,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[0].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 1,\n        }) </s> add                 \"baz\": \"asd\", </s> add                 \"baz\": \"asd\", </s> remove         self.assertEqual(trials[1].config, {\"bar\": False, \"foo\": 1})\n </s> add         self.assertEqual(trials[1].config, {\n            \"bar\": False,\n            \"foo\": 1,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[1].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 1,\n        }) </s> remove         self.assertEqual(trials[2].config, {\"bar\": True, \"foo\": 2})\n        self.assertEqual(trials[3].config, {\"bar\": False, \"foo\": 2})\n        self.assertEqual(trials[4].config, {\"bar\": True, \"foo\": 3})\n        self.assertEqual(trials[5].config, {\"bar\": False, \"foo\": 3})\n </s> add         self.assertEqual(trials[2].config, {\n            \"bar\": True,\n            \"foo\": 2,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[2].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 2,\n        })\n\n        self.assertEqual(trials[3].config, {\n            \"bar\": False,\n            \"foo\": 2,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[3].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 2,\n        })\n\n        self.assertEqual(trials[4].config, {\n            \"bar\": True,\n            \"foo\": 3,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[4].evaluated_params, {\n            \"bar\": True,\n            \"foo\": 3,\n        })\n\n        self.assertEqual(trials[5].config, {\n            \"bar\": False,\n            \"foo\": 3,\n            \"baz\": \"asd\",\n        })\n        self.assertEqual(trials[5].evaluated_params, {\n            \"bar\": False,\n            \"foo\": 3,\n        })", "html_url": "https://github.com/ray-project/ray/commit/7e214fd95e3195406f1d18d7b7f75c9949e22b28", "file_name": "python/ray/tune/tests/test_trial_runner.py"}
{"docstring_tokens": "keep keep replace replace keep keep replace keep keep keep keep", "code_tokens": " <mask> import asyncio\n <mask> import atexit\n <mask> import time\n <mask> from functools import wraps\n <mask> import inspect\n <mask> import os\n <mask> from uuid import UUID\n <mask> import threading\n <mask> from typing import (Any, Callable, Coroutine, Dict, List, Optional,\n <mask>                     TYPE_CHECKING, Type, Union)\n <mask> from dataclasses import dataclass\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove from typing import (Any, Callable, Coroutine, Dict, List, Optional,\n                    TYPE_CHECKING, Type, Union)\n </s> add import time </s> add from functools import wraps\nfrom typing import (TYPE_CHECKING, Any, Callable, Coroutine, Dict, List,\n                    Optional, Type, Union)\nfrom uuid import UUID </s> remove from typing import Any, ChainMap, Dict, Iterable, List, Optional\n </s> add from typing import Any, Dict, Iterable, List, Optional </s> remove from enum import Enum\n </s> add  </s> remove import ray\n </s> add from ray.actor import ActorHandle\nfrom ray.serve.common import EndpointTag\nfrom ray.serve.config import (BackendConfig, BackendMetadata, HTTPOptions,\n                              ReplicaConfig)", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/api.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> import inspect\n <mask> import os\n <mask> from uuid import UUID\n <mask> import threading\n <mask> from typing import (Any, Callable, Coroutine, Dict, List, Optional,\n <mask>                     TYPE_CHECKING, Type, Union)\n <mask> from dataclasses import dataclass\n <mask> from warnings import warn\n <mask> \n <mask> import ray\n <mask> from ray.serve.constants import (DEFAULT_HTTP_HOST, DEFAULT_HTTP_PORT,\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove from uuid import UUID\n </s> add  </s> remove import time\nfrom functools import wraps\n </s> add  </s> add from functools import wraps\nfrom typing import (TYPE_CHECKING, Any, Callable, Coroutine, Dict, List,\n                    Optional, Type, Union)\nfrom uuid import UUID </s> remove import ray\n </s> add from ray.actor import ActorHandle\nfrom ray.serve.common import EndpointTag\nfrom ray.serve.config import (BackendConfig, BackendMetadata, HTTPOptions,\n                              ReplicaConfig) </s> remove from typing import Any, ChainMap, Dict, Iterable, List, Optional\n </s> add from typing import Any, Dict, Iterable, List, Optional </s> remove                                  SERVE_CONTROLLER_NAME, HTTP_PROXY_TIMEOUT)\nfrom ray.serve.controller import ServeController, BackendTag, ReplicaTag\nfrom ray.serve.handle import RayServeHandle, RayServeSyncHandle\nfrom ray.serve.utils import (\n    block_until_http_ready, format_actor_name, get_random_letters, logger,\n    get_current_node_resource_key, register_custom_serializers)\n </s> add                                  HTTP_PROXY_TIMEOUT, SERVE_CONTROLLER_NAME)\nfrom ray.serve.controller import BackendTag, ReplicaTag, ServeController", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/api.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> import time\n <mask> from dataclasses import dataclass\n <mask> from warnings import warn\n <mask> \n <mask> from ray.actor import ActorHandle\n <mask> from ray.serve.common import EndpointTag\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove import ray\n </s> add from ray.actor import ActorHandle\nfrom ray.serve.common import EndpointTag\nfrom ray.serve.config import (BackendConfig, BackendMetadata, HTTPOptions,\n                              ReplicaConfig) </s> remove from uuid import UUID\n </s> add  </s> remove from typing import (Any, Callable, Coroutine, Dict, List, Optional,\n                    TYPE_CHECKING, Type, Union)\n </s> add import time </s> remove import ray\n </s> add  </s> remove                                  SERVE_CONTROLLER_NAME, HTTP_PROXY_TIMEOUT)\nfrom ray.serve.controller import ServeController, BackendTag, ReplicaTag\nfrom ray.serve.handle import RayServeHandle, RayServeSyncHandle\nfrom ray.serve.utils import (\n    block_until_http_ready, format_actor_name, get_random_letters, logger,\n    get_current_node_resource_key, register_custom_serializers)\n </s> add                                  HTTP_PROXY_TIMEOUT, SERVE_CONTROLLER_NAME)\nfrom ray.serve.controller import BackendTag, ReplicaTag, ServeController </s> remove from ray.serve.common import (\n    BackendInfo,\n    BackendTag,\n    Duration,\n    GoalId,\n    ReplicaTag,\n)\n </s> add from ray.serve.common import (BackendInfo, BackendTag, Duration, GoalId,\n                              ReplicaTag)", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/api.py"}
{"docstring_tokens": "keep replace keep replace replace replace replace replace replace keep keep keep keep", "code_tokens": " <mask> \n <mask> import ray\n <mask> from ray.serve.constants import (DEFAULT_HTTP_HOST, DEFAULT_HTTP_PORT,\n <mask>                                  SERVE_CONTROLLER_NAME, HTTP_PROXY_TIMEOUT)\n <mask> from ray.serve.controller import ServeController, BackendTag, ReplicaTag\n <mask> from ray.serve.handle import RayServeHandle, RayServeSyncHandle\n <mask> from ray.serve.utils import (\n <mask>     block_until_http_ready, format_actor_name, get_random_letters, logger,\n <mask>     get_current_node_resource_key, register_custom_serializers)\n <mask> from ray.serve.exceptions import RayServeException\n <mask> from ray.serve.config import (BackendConfig, ReplicaConfig, BackendMetadata,\n <mask>                               HTTPOptions)\n <mask> from ray.serve.router import RequestMetadata, Router\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove from ray.serve.config import (BackendConfig, ReplicaConfig, BackendMetadata,\n                              HTTPOptions)\n </s> add from ray.serve.handle import RayServeHandle, RayServeSyncHandle </s> remove from ray.actor import ActorHandle\n </s> add from ray.serve.utils import (block_until_http_ready, format_actor_name,\n                             get_current_node_resource_key, get_random_letters,\n                             logger, register_custom_serializers)\n\nimport ray </s> remove from ray.serve.constants import LongPollKey\n </s> add from ray.serve.common import BackendTag, EndpointTag, TrafficPolicy\nfrom ray.serve.config import BackendConfig </s> remove from ray.serve.common import (\n    BackendInfo,\n    BackendTag,\n    Duration,\n    GoalId,\n    ReplicaTag,\n)\n </s> add from ray.serve.common import (BackendInfo, BackendTag, Duration, GoalId,\n                              ReplicaTag) </s> remove from ray.serve.long_poll import LongPollAsyncClient\n </s> add from ray.serve.long_poll import LongPollClient, LongPollNamespace", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/api.py"}
{"docstring_tokens": "keep keep keep replace replace keep replace", "code_tokens": " <mask>     block_until_http_ready, format_actor_name, get_random_letters, logger,\n <mask>     get_current_node_resource_key, register_custom_serializers)\n <mask> from ray.serve.exceptions import RayServeException\n <mask> from ray.serve.config import (BackendConfig, ReplicaConfig, BackendMetadata,\n <mask>                               HTTPOptions)\n <mask> from ray.serve.router import RequestMetadata, Router\n <mask> from ray.actor import ActorHandle\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove                                  SERVE_CONTROLLER_NAME, HTTP_PROXY_TIMEOUT)\nfrom ray.serve.controller import ServeController, BackendTag, ReplicaTag\nfrom ray.serve.handle import RayServeHandle, RayServeSyncHandle\nfrom ray.serve.utils import (\n    block_until_http_ready, format_actor_name, get_random_letters, logger,\n    get_current_node_resource_key, register_custom_serializers)\n </s> add                                  HTTP_PROXY_TIMEOUT, SERVE_CONTROLLER_NAME)\nfrom ray.serve.controller import BackendTag, ReplicaTag, ServeController </s> remove from ray.serve.constants import LongPollKey\n </s> add from ray.serve.common import BackendTag, EndpointTag, TrafficPolicy\nfrom ray.serve.config import BackendConfig </s> remove     from fastapi import FastAPI, APIRouter  # noqa: F401\n </s> add     from fastapi import APIRouter, FastAPI  # noqa: F401 </s> remove import ray\n </s> add from ray.actor import ActorHandle\nfrom ray.serve.common import EndpointTag\nfrom ray.serve.config import (BackendConfig, BackendMetadata, HTTPOptions,\n                              ReplicaConfig) </s> remove from ray.serve.exceptions import RayServeException\n\nimport ray\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> from ray.serve.router import RequestMetadata, Router\n <mask> from ray.actor import ActorHandle\n <mask> \n <mask> if TYPE_CHECKING:\n <mask>     from fastapi import FastAPI, APIRouter  # noqa: F401\n <mask> \n <mask> _INTERNAL_REPLICA_CONTEXT = None\n <mask> _global_async_loop = None\n <mask> _global_client = None\n <mask> \n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove from ray.actor import ActorHandle\n </s> add from ray.serve.utils import (block_until_http_ready, format_actor_name,\n                             get_current_node_resource_key, get_random_letters,\n                             logger, register_custom_serializers)\n\nimport ray </s> remove from ray.serve.config import (BackendConfig, ReplicaConfig, BackendMetadata,\n                              HTTPOptions)\n </s> add from ray.serve.handle import RayServeHandle, RayServeSyncHandle </s> remove                                  SERVE_CONTROLLER_NAME, HTTP_PROXY_TIMEOUT)\nfrom ray.serve.controller import ServeController, BackendTag, ReplicaTag\nfrom ray.serve.handle import RayServeHandle, RayServeSyncHandle\nfrom ray.serve.utils import (\n    block_until_http_ready, format_actor_name, get_random_letters, logger,\n    get_current_node_resource_key, register_custom_serializers)\n </s> add                                  HTTP_PROXY_TIMEOUT, SERVE_CONTROLLER_NAME)\nfrom ray.serve.controller import BackendTag, ReplicaTag, ServeController </s> remove         self._sync_proxied_router = None\n        self._async_proxied_router = None\n </s> add         # TODO(simon): remove this when dropping router object and making\n        # ServeHandle sync only.\n        self._cached_routers = dict() </s> remove     LongPollKey,\n </s> add  </s> remove from ray.serve.long_poll import LongPollAsyncClient\nfrom ray.serve.utils import logger, compute_dict_delta, compute_iterable_delta\n </s> add from ray.serve.long_poll import LongPollClient, LongPollNamespace\nfrom ray.serve.utils import compute_iterable_delta, logger\n\nimport ray", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/api.py"}
{"docstring_tokens": "keep keep keep replace keep keep replace replace", "code_tokens": " <mask> \n <mask> \n <mask> class ThreadProxiedRouter:\n <mask>     def __init__(self, controller_handle, sync: bool):\n <mask>         self.controller_handle = controller_handle\n <mask>         self.sync = sync\n <mask>         self.router = Router(controller_handle)\n <mask> \n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove             self.async_loop = create_or_get_async_loop_in_thread()\n            asyncio.run_coroutine_threadsafe(\n                self.router.setup_in_async_loop(),\n                self.async_loop,\n            )\n </s> add             self._async_loop = create_or_get_async_loop_in_thread() </s> remove         serialized_data = (self.controller_handle, self.sync)\n </s> add         serialized_data = (\n            self.controller_handle,\n            self.sync,\n            self.endpoint_tag,\n        ) </s> remove     def __init__(self, backend_tag):\n </s> add     def __init__(\n            self,\n            controller_handle,\n            backend_tag,\n            event_loop: asyncio.AbstractEventLoop,\n    ): </s> remove             handle = RayServeSyncHandle(\n                self._get_proxied_router(sync=sync), endpoint_name)\n </s> add             handle = RayServeSyncHandle(router, endpoint_name) </s> remove class _PendingEndpointFound(Enum):\n    \"\"\"Enum for the status of pending endpoint registration.\"\"\"\n    ADDED = 1\n    REMOVED = 2\n\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/api.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace keep replace replace keep keep keep keep", "code_tokens": " <mask>         self.sync = sync\n <mask>         self.router = Router(controller_handle)\n <mask> \n <mask>         if sync:\n <mask>             self.async_loop = create_or_get_async_loop_in_thread()\n <mask>             asyncio.run_coroutine_threadsafe(\n <mask>                 self.router.setup_in_async_loop(),\n <mask>                 self.async_loop,\n <mask>             )\n <mask>         else:\n <mask>             self.async_loop = asyncio.get_event_loop()\n <mask>             self.async_loop.create_task(self.router.setup_in_async_loop())\n <mask> \n <mask>     def _remote(self, endpoint_name, handle_options, request_data,\n <mask>                 kwargs) -> Coroutine:\n <mask>         request_metadata = RequestMetadata(\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         self.router = Router(controller_handle)\n\n </s> add         self.endpoint_tag = endpoint_tag </s> remove     def __init__(self, controller_handle, sync: bool):\n </s> add     def __init__(self, controller_handle, sync: bool,\n                 endpoint_tag: EndpointTag): </s> remove             handle = RayServeSyncHandle(\n                self._get_proxied_router(sync=sync), endpoint_name)\n </s> add             handle = RayServeSyncHandle(router, endpoint_name) </s> remove             handle = RayServeHandle(\n                self._get_proxied_router(sync=sync), endpoint_name)\n </s> add             handle = RayServeHandle(router, endpoint_name) </s> remove     def _get_proxied_router(self, sync: bool):\n        if sync:\n            if self._sync_proxied_router is None:\n                self._sync_proxied_router = ThreadProxiedRouter(\n                    self._controller, sync=True)\n            return self._sync_proxied_router\n        else:\n            if self._async_proxied_router is None:\n                self._async_proxied_router = ThreadProxiedRouter(\n                    self._controller, sync=False)\n            return self._async_proxied_router\n </s> add     def _get_proxied_router(self, sync: bool, endpoint: EndpointTag):\n        key = (sync, endpoint)\n        if key not in self._cached_routers:\n            self._cached_routers[key] = ThreadProxiedRouter(\n                self._controller,\n                sync,\n                endpoint,\n            )\n        return self._cached_routers[key]", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         return coro\n <mask> \n <mask>     def __reduce__(self):\n <mask>         deserializer = ThreadProxiedRouter\n <mask>         serialized_data = (self.controller_handle, self.sync)\n <mask>         return deserializer, serialized_data\n <mask> \n <mask> \n <mask> class Client:\n <mask>     def __init__(self,\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove     def __init__(self, controller_handle, sync: bool):\n </s> add     def __init__(self, controller_handle, sync: bool,\n                 endpoint_tag: EndpointTag): </s> remove class _PendingEndpointFound(Enum):\n    \"\"\"Enum for the status of pending endpoint registration.\"\"\"\n    ADDED = 1\n    REMOVED = 2\n\n\n </s> add  </s> remove         self.router = Router(controller_handle)\n\n </s> add         self.endpoint_tag = endpoint_tag </s> remove     def __init__(self, backend_tag):\n </s> add     def __init__(\n            self,\n            controller_handle,\n            backend_tag,\n            event_loop: asyncio.AbstractEventLoop,\n    ): </s> remove             handle = RayServeHandle(\n                self._get_proxied_router(sync=sync), endpoint_name)\n </s> add             handle = RayServeHandle(router, endpoint_name) </s> remove     def _get_proxied_router(self, sync: bool):\n        if sync:\n            if self._sync_proxied_router is None:\n                self._sync_proxied_router = ThreadProxiedRouter(\n                    self._controller, sync=True)\n            return self._sync_proxied_router\n        else:\n            if self._async_proxied_router is None:\n                self._async_proxied_router = ThreadProxiedRouter(\n                    self._controller, sync=False)\n            return self._async_proxied_router\n </s> add     def _get_proxied_router(self, sync: bool, endpoint: EndpointTag):\n        key = (sync, endpoint)\n        if key not in self._cached_routers:\n            self._cached_routers[key] = ThreadProxiedRouter(\n                self._controller,\n                sync,\n                endpoint,\n            )\n        return self._cached_routers[key]", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/api.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         self._detached = detached\n <mask>         self._shutdown = False\n <mask>         self._http_config = ray.get(controller.get_http_config.remote())\n <mask> \n <mask>         self._sync_proxied_router = None\n <mask>         self._async_proxied_router = None\n <mask> \n <mask>         # NOTE(edoakes): Need this because the shutdown order isn't guaranteed\n <mask>         # when the interpreter is exiting so we can't rely on __del__ (it\n <mask>         # throws a nasty stacktrace).\n <mask>         if not self._detached:\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove     def _get_proxied_router(self, sync: bool):\n        if sync:\n            if self._sync_proxied_router is None:\n                self._sync_proxied_router = ThreadProxiedRouter(\n                    self._controller, sync=True)\n            return self._sync_proxied_router\n        else:\n            if self._async_proxied_router is None:\n                self._async_proxied_router = ThreadProxiedRouter(\n                    self._controller, sync=False)\n            return self._async_proxied_router\n </s> add     def _get_proxied_router(self, sync: bool, endpoint: EndpointTag):\n        key = (sync, endpoint)\n        if key not in self._cached_routers:\n            self._cached_routers[key] = ThreadProxiedRouter(\n                self._controller,\n                sync,\n                endpoint,\n            )\n        return self._cached_routers[key] </s> remove         return object_ref\n\n    async def _do_long_poll(self):\n        while True:\n            try:\n                updates: Dict[str, UpdatedObject] = await self._poll_once()\n                logger.debug(\"LongPollClient received updates for keys: \"\n                             f\"{list(updates.keys())}.\")\n                for key, update in updates.items():\n                    self.object_snapshots[key] = update.object_snapshot\n                    self.snapshot_ids[key] = update.snapshot_id\n                    # NOTE(simon):\n                    # This blocks the loop from doing another poll. Consider\n                    # use loop.create_task here or poll first then call the\n                    # callbacks.\n                    callback = self.key_listeners[key]\n                    await callback(update.object_snapshot)\n            except ray.exceptions.RayActorError:\n                # This can happen during shutdown where the controller is\n                # intentionally killed, the client should just gracefully\n                # exit.\n                logger.debug(\"LongPollClient failed to connect to host. \"\n                             \"Shutting down.\")\n                break\n </s> add         self._current_ref._on_completed(\n            lambda update: self._process_update(update))\n\n    def _process_update(self, updates: Dict[str, UpdatedObject]):\n        if isinstance(updates, (ray.exceptions.RayActorError)):\n            # This can happen during shutdown where the controller is\n            # intentionally killed, the client should just gracefully\n            # exit.\n            logger.debug(\"LongPollClient failed to connect to host. \"\n                         \"Shutting down.\")\n            return\n\n        if isinstance(updates, (ray.exceptions.RayTaskError)):\n            # This can happen during shutdown where the controller doesn't\n            # contain this key, we will just repull.\n            # NOTE(simon): should we repull or just wait in the long poll\n            # host?\n            if not isinstance(updates.as_instanceof_cause(), ValueError):\n                logger.error(\"LongPollHost errored\\n\" + updates.traceback_str)\n            self._poll_next()\n            return\n\n        # Before we process the updates and calling callbacks, kick off\n        # another poll so we can pipeline the polling and processing.\n        self._poll_next()\n        logger.debug(\"LongPollClient received updates for keys: \"\n                     f\"{list(updates.keys())}.\")\n        for key, update in updates.items():\n            self.object_snapshots[key] = update.object_snapshot\n            self.snapshot_ids[key] = update.snapshot_id\n            callback = self.key_listeners[key]\n            if self.event_loop is None:\n                callback(update.object_snapshot)\n            else:\n                self.event_loop.call_soon_threadsafe(\n                    lambda: callback(update.object_snapshot)) </s> remove         self._notify_backend_configs_changed()\n </s> add         self._notify_backend_configs_changed(backend_tag) </s> remove         self.config_updated_event = asyncio.Event()\n </s> add         self.config_updated_event = asyncio.Event(loop=event_loop) </s> remove     from fastapi import FastAPI, APIRouter  # noqa: F401\n </s> add     from fastapi import APIRouter, FastAPI  # noqa: F401 </s> add         # NOTE(simon): this extra layer of router seems unnecessary\n        # BUT it's needed still because of the shared asyncio thread.\n        router = self._get_proxied_router(sync=sync, endpoint=endpoint_name)", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/api.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>                 self.shutdown()\n <mask> \n <mask>             atexit.register(shutdown_serve_client)\n <mask> \n <mask>     def _get_proxied_router(self, sync: bool):\n <mask>         if sync:\n <mask>             if self._sync_proxied_router is None:\n <mask>                 self._sync_proxied_router = ThreadProxiedRouter(\n <mask>                     self._controller, sync=True)\n <mask>             return self._sync_proxied_router\n <mask>         else:\n <mask>             if self._async_proxied_router is None:\n <mask>                 self._async_proxied_router = ThreadProxiedRouter(\n <mask>                     self._controller, sync=False)\n <mask>             return self._async_proxied_router\n <mask> \n <mask>     def __del__(self):\n <mask>         if not self._detached:\n <mask>             logger.debug(\"Shutting down Ray Serve because client went out of \"\n <mask>                          \"scope. To prevent this, either keep a reference to \"\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         self._sync_proxied_router = None\n        self._async_proxied_router = None\n </s> add         # TODO(simon): remove this when dropping router object and making\n        # ServeHandle sync only.\n        self._cached_routers = dict() </s> add     @property\n    def backend_tags(self):\n        return set(self.traffic_dict.keys()).union(\n            set(self.shadow_dict.keys()))\n </s> remove     def __init__(self, controller_handle, sync: bool):\n </s> add     def __init__(self, controller_handle, sync: bool,\n                 endpoint_tag: EndpointTag): </s> remove         self._notify_backend_configs_changed()\n </s> add         self._notify_backend_configs_changed(backend_tag) </s> add         # NOTE(simon): this extra layer of router seems unnecessary\n        # BUT it's needed still because of the shared asyncio thread.\n        router = self._get_proxied_router(sync=sync, endpoint=endpoint_name) </s> remove         return object_ref\n\n    async def _do_long_poll(self):\n        while True:\n            try:\n                updates: Dict[str, UpdatedObject] = await self._poll_once()\n                logger.debug(\"LongPollClient received updates for keys: \"\n                             f\"{list(updates.keys())}.\")\n                for key, update in updates.items():\n                    self.object_snapshots[key] = update.object_snapshot\n                    self.snapshot_ids[key] = update.snapshot_id\n                    # NOTE(simon):\n                    # This blocks the loop from doing another poll. Consider\n                    # use loop.create_task here or poll first then call the\n                    # callbacks.\n                    callback = self.key_listeners[key]\n                    await callback(update.object_snapshot)\n            except ray.exceptions.RayActorError:\n                # This can happen during shutdown where the controller is\n                # intentionally killed, the client should just gracefully\n                # exit.\n                logger.debug(\"LongPollClient failed to connect to host. \"\n                             \"Shutting down.\")\n                break\n </s> add         self._current_ref._on_completed(\n            lambda update: self._process_update(update))\n\n    def _process_update(self, updates: Dict[str, UpdatedObject]):\n        if isinstance(updates, (ray.exceptions.RayActorError)):\n            # This can happen during shutdown where the controller is\n            # intentionally killed, the client should just gracefully\n            # exit.\n            logger.debug(\"LongPollClient failed to connect to host. \"\n                         \"Shutting down.\")\n            return\n\n        if isinstance(updates, (ray.exceptions.RayTaskError)):\n            # This can happen during shutdown where the controller doesn't\n            # contain this key, we will just repull.\n            # NOTE(simon): should we repull or just wait in the long poll\n            # host?\n            if not isinstance(updates.as_instanceof_cause(), ValueError):\n                logger.error(\"LongPollHost errored\\n\" + updates.traceback_str)\n            self._poll_next()\n            return\n\n        # Before we process the updates and calling callbacks, kick off\n        # another poll so we can pipeline the polling and processing.\n        self._poll_next()\n        logger.debug(\"LongPollClient received updates for keys: \"\n                     f\"{list(updates.keys())}.\")\n        for key, update in updates.items():\n            self.object_snapshots[key] = update.object_snapshot\n            self.snapshot_ids[key] = update.snapshot_id\n            callback = self.key_listeners[key]\n            if self.event_loop is None:\n                callback(update.object_snapshot)\n            else:\n                self.event_loop.call_soon_threadsafe(\n                    lambda: callback(update.object_snapshot))", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/api.py"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask>                 \"master/serve/advanced.html#sync-and-async-handles\")\n <mask> \n <mask>         if sync:\n <mask>             handle = RayServeSyncHandle(router, endpoint_name)\n <mask>         else:\n <mask>             handle = RayServeHandle(router, endpoint_name)\n <mask>         return handle\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove             handle = RayServeSyncHandle(\n                self._get_proxied_router(sync=sync), endpoint_name)\n </s> add             handle = RayServeSyncHandle(router, endpoint_name) </s> remove             handle = RayServeHandle(\n                self._get_proxied_router(sync=sync), endpoint_name)\n </s> add             handle = RayServeHandle(router, endpoint_name) </s> remove     def _get_proxied_router(self, sync: bool):\n        if sync:\n            if self._sync_proxied_router is None:\n                self._sync_proxied_router = ThreadProxiedRouter(\n                    self._controller, sync=True)\n            return self._sync_proxied_router\n        else:\n            if self._async_proxied_router is None:\n                self._async_proxied_router = ThreadProxiedRouter(\n                    self._controller, sync=False)\n            return self._async_proxied_router\n </s> add     def _get_proxied_router(self, sync: bool, endpoint: EndpointTag):\n        key = (sync, endpoint)\n        if key not in self._cached_routers:\n            self._cached_routers[key] = ThreadProxiedRouter(\n                self._controller,\n                sync,\n                endpoint,\n            )\n        return self._cached_routers[key] </s> remove     def __init__(self, controller_handle, sync: bool):\n </s> add     def __init__(self, controller_handle, sync: bool,\n                 endpoint_tag: EndpointTag): </s> remove             self.async_loop = create_or_get_async_loop_in_thread()\n            asyncio.run_coroutine_threadsafe(\n                self.router.setup_in_async_loop(),\n                self.async_loop,\n            )\n </s> add             self._async_loop = create_or_get_async_loop_in_thread() </s> remove         self.router = Router(controller_handle)\n\n </s> add         self.endpoint_tag = endpoint_tag", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/api.py"}
{"docstring_tokens": "keep keep replace replace keep replace replace keep keep", "code_tokens": " <mask> \n <mask>         if sync:\n <mask>             handle = RayServeSyncHandle(\n <mask>                 self._get_proxied_router(sync=sync), endpoint_name)\n <mask>         else:\n <mask>             handle = RayServeHandle(\n <mask>                 self._get_proxied_router(sync=sync), endpoint_name)\n <mask>         return handle\n <mask> \n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> add         # NOTE(simon): this extra layer of router seems unnecessary\n        # BUT it's needed still because of the shared asyncio thread.\n        router = self._get_proxied_router(sync=sync, endpoint=endpoint_name) </s> remove     def _get_proxied_router(self, sync: bool):\n        if sync:\n            if self._sync_proxied_router is None:\n                self._sync_proxied_router = ThreadProxiedRouter(\n                    self._controller, sync=True)\n            return self._sync_proxied_router\n        else:\n            if self._async_proxied_router is None:\n                self._async_proxied_router = ThreadProxiedRouter(\n                    self._controller, sync=False)\n            return self._async_proxied_router\n </s> add     def _get_proxied_router(self, sync: bool, endpoint: EndpointTag):\n        key = (sync, endpoint)\n        if key not in self._cached_routers:\n            self._cached_routers[key] = ThreadProxiedRouter(\n                self._controller,\n                sync,\n                endpoint,\n            )\n        return self._cached_routers[key] </s> remove     def __init__(self, controller_handle, sync: bool):\n </s> add     def __init__(self, controller_handle, sync: bool,\n                 endpoint_tag: EndpointTag): </s> remove             self.async_loop = create_or_get_async_loop_in_thread()\n            asyncio.run_coroutine_threadsafe(\n                self.router.setup_in_async_loop(),\n                self.async_loop,\n            )\n </s> add             self._async_loop = create_or_get_async_loop_in_thread() </s> remove         self.router = Router(controller_handle)\n\n </s> add         self.endpoint_tag = endpoint_tag", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/api.py"}
{"docstring_tokens": "keep keep keep replace replace keep keep replace", "code_tokens": " <mask> from abc import ABC\n <mask> from collections import defaultdict\n <mask> from enum import Enum\n <mask> import math\n <mask> import time\n <mask> from typing import Any, Dict, List, Optional, Tuple\n <mask> \n <mask> import ray\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove from typing import Any, ChainMap, Dict, Iterable, List, Optional\n </s> add from typing import Any, Dict, Iterable, List, Optional </s> remove from enum import Enum\n </s> add  </s> add from functools import wraps\nfrom typing import (TYPE_CHECKING, Any, Callable, Coroutine, Dict, List,\n                    Optional, Type, Union)\nfrom uuid import UUID </s> remove from typing import (Any, Callable, Coroutine, Dict, List, Optional,\n                    TYPE_CHECKING, Type, Union)\n </s> add import time </s> remove import time\nfrom functools import wraps\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep replace replace replace replace replace replace replace keep replace keep keep", "code_tokens": " <mask> from ray.serve.backend_worker import create_backend_replica\n <mask> from ray.serve.common import (\n <mask>     BackendInfo,\n <mask>     BackendTag,\n <mask>     Duration,\n <mask>     GoalId,\n <mask>     ReplicaTag,\n <mask> )\n <mask> from ray.serve.config import BackendConfig, ReplicaConfig\n <mask> from ray.serve.constants import LongPollKey\n <mask> from ray.serve.kv_store import RayInternalKVStore\n <mask> from ray.serve.long_poll import LongPollHost\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove from ray.serve.long_poll import LongPollHost\nfrom ray.serve.utils import format_actor_name, get_random_letters, logger\n </s> add from ray.serve.long_poll import LongPollHost, LongPollNamespace\nfrom ray.serve.utils import (format_actor_name, get_random_letters, logger)\n\nimport ray </s> remove from ray.serve.constants import LongPollKey\n </s> add from ray.serve.common import BackendTag, EndpointTag, TrafficPolicy\nfrom ray.serve.config import BackendConfig </s> remove import ray\n </s> add  </s> remove                                  SERVE_CONTROLLER_NAME, HTTP_PROXY_TIMEOUT)\nfrom ray.serve.controller import ServeController, BackendTag, ReplicaTag\nfrom ray.serve.handle import RayServeHandle, RayServeSyncHandle\nfrom ray.serve.utils import (\n    block_until_http_ready, format_actor_name, get_random_letters, logger,\n    get_current_node_resource_key, register_custom_serializers)\n </s> add                                  HTTP_PROXY_TIMEOUT, SERVE_CONTROLLER_NAME)\nfrom ray.serve.controller import BackendTag, ReplicaTag, ServeController </s> remove import ray\n </s> add from ray.actor import ActorHandle\nfrom ray.serve.common import EndpointTag\nfrom ray.serve.config import (BackendConfig, BackendMetadata, HTTPOptions,\n                              ReplicaConfig)", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> )\n <mask> from ray.serve.config import BackendConfig, ReplicaConfig\n <mask> from ray.serve.constants import LongPollKey\n <mask> from ray.serve.kv_store import RayInternalKVStore\n <mask> from ray.serve.long_poll import LongPollHost\n <mask> from ray.serve.utils import format_actor_name, get_random_letters, logger\n <mask> \n <mask> CHECKPOINT_KEY = \"serve-backend-state-checkpoint\"\n <mask> SLOW_STARTUP_WARNING_S = 30\n <mask> SLOW_STARTUP_WARNING_PERIOD_S = 30\n <mask> \n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove from ray.serve.constants import LongPollKey\n </s> add  </s> remove from ray.serve.common import (\n    BackendInfo,\n    BackendTag,\n    Duration,\n    GoalId,\n    ReplicaTag,\n)\n </s> add from ray.serve.common import (BackendInfo, BackendTag, Duration, GoalId,\n                              ReplicaTag) </s> remove from ray.serve.long_poll import LongPollAsyncClient\nfrom ray.serve.utils import logger, compute_dict_delta, compute_iterable_delta\n </s> add from ray.serve.long_poll import LongPollClient, LongPollNamespace\nfrom ray.serve.utils import compute_iterable_delta, logger\n\nimport ray </s> remove from ray.actor import ActorHandle\n </s> add from ray.serve.utils import (block_until_http_ready, format_actor_name,\n                             get_current_node_resource_key, get_random_letters,\n                             logger, register_custom_serializers)\n\nimport ray </s> remove from ray.serve.constants import LongPollKey\n </s> add from ray.serve.common import BackendTag, EndpointTag, TrafficPolicy\nfrom ray.serve.config import BackendConfig </s> remove     LongPollKey,\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         assert isinstance(replica, VersionedReplica)\n <mask>         self._replicas[state].append(replica)\n <mask> \n <mask>     def get(self, states: Optional[List[ReplicaState]] = None\n <mask>             ) -> List[VersionedReplica]:\n <mask>         \"\"\"Get all replicas of the given states.\n <mask> \n <mask>         This does not remove them from the container. Replicas are returned\n <mask>         in order of state as passed in.\n <mask> \n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         logger.debug(\"Scaling backend '{}' to {} replicas\".format(\n            backend_tag, num_replicas))\n </s> add  </s> remove         self._sync_proxied_router = None\n        self._async_proxied_router = None\n </s> add         # TODO(simon): remove this when dropping router object and making\n        # ServeHandle sync only.\n        self._cached_routers = dict() </s> remove     async def listen_for_change(self, keys_to_snapshot_ids: Dict[str, int]\n                                ) -> Dict[str, UpdatedObject]:\n </s> add     async def listen_for_change(\n            self,\n            keys_to_snapshot_ids: Dict[KeyType, int],\n    ) -> Dict[KeyType, UpdatedObject]: </s> remove         self._notify_backend_configs_changed()\n </s> add         self._notify_backend_configs_changed(backend_tag) </s> remove         return object_ref\n\n    async def _do_long_poll(self):\n        while True:\n            try:\n                updates: Dict[str, UpdatedObject] = await self._poll_once()\n                logger.debug(\"LongPollClient received updates for keys: \"\n                             f\"{list(updates.keys())}.\")\n                for key, update in updates.items():\n                    self.object_snapshots[key] = update.object_snapshot\n                    self.snapshot_ids[key] = update.snapshot_id\n                    # NOTE(simon):\n                    # This blocks the loop from doing another poll. Consider\n                    # use loop.create_task here or poll first then call the\n                    # callbacks.\n                    callback = self.key_listeners[key]\n                    await callback(update.object_snapshot)\n            except ray.exceptions.RayActorError:\n                # This can happen during shutdown where the controller is\n                # intentionally killed, the client should just gracefully\n                # exit.\n                logger.debug(\"LongPollClient failed to connect to host. \"\n                             \"Shutting down.\")\n                break\n </s> add         self._current_ref._on_completed(\n            lambda update: self._process_update(update))\n\n    def _process_update(self, updates: Dict[str, UpdatedObject]):\n        if isinstance(updates, (ray.exceptions.RayActorError)):\n            # This can happen during shutdown where the controller is\n            # intentionally killed, the client should just gracefully\n            # exit.\n            logger.debug(\"LongPollClient failed to connect to host. \"\n                         \"Shutting down.\")\n            return\n\n        if isinstance(updates, (ray.exceptions.RayTaskError)):\n            # This can happen during shutdown where the controller doesn't\n            # contain this key, we will just repull.\n            # NOTE(simon): should we repull or just wait in the long poll\n            # host?\n            if not isinstance(updates.as_instanceof_cause(), ValueError):\n                logger.error(\"LongPollHost errored\\n\" + updates.traceback_str)\n            self._poll_next()\n            return\n\n        # Before we process the updates and calling callbacks, kick off\n        # another poll so we can pipeline the polling and processing.\n        self._poll_next()\n        logger.debug(\"LongPollClient received updates for keys: \"\n                     f\"{list(updates.keys())}.\")\n        for key, update in updates.items():\n            self.object_snapshots[key] = update.object_snapshot\n            self.snapshot_ids[key] = update.snapshot_id\n            callback = self.key_listeners[key]\n            if self.event_loop is None:\n                callback(update.object_snapshot)\n            else:\n                self.event_loop.call_soon_threadsafe(\n                    lambda: callback(update.object_snapshot)) </s> remove         self.long_poll_client = LongPollAsyncClient(controller_handle, {\n            LongPollKey.BACKEND_CONFIGS: self._update_backend_configs,\n        })\n </s> add         self.loop = asyncio.get_event_loop()\n        self.long_poll_client = LongPollClient(\n            controller_handle,\n            {\n                (LongPollNamespace.BACKEND_CONFIGS, self.backend_tag): self.\n                _update_backend_configs,\n            },\n            call_in_event_loop=self.loop,\n        )", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep replace replace replace replace replace replace replace replace replace replace replace keep keep replace keep keep keep keep", "code_tokens": " <mask> \n <mask>     def _notify_backend_configs_changed(self) -> None:\n <mask>         self._long_poll_host.notify_changed(LongPollKey.BACKEND_CONFIGS,\n <mask>                                             self.get_backend_configs())\n <mask> \n <mask>     def _notify_replica_handles_changed(self) -> None:\n <mask>         self._long_poll_host.notify_changed(\n <mask>             LongPollKey.REPLICA_HANDLES, {\n <mask>                 backend_tag: list(replica_dict.values())\n <mask>                 for backend_tag, replica_dict in\n <mask>                 self.get_running_replica_handles().items()\n <mask>             })\n <mask> \n <mask>     def get_running_replica_handles(\n <mask>             self) -> Dict[BackendTag, Dict[ReplicaTag, ActorHandle]]:\n <mask>         return {\n <mask>             backend_tag: {\n <mask>                 r.replica_tag: r.actor_handle\n <mask>                 for r in replicas.get(states=[ReplicaState.RUNNING])\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove                 r.replica_tag: r.actor_handle\n                for r in replicas.get(states=[ReplicaState.RUNNING])\n </s> add                 backend_replica.replica_tag: backend_replica.actor_handle\n                for backend_replica in replicas_container.get(\n                    [ReplicaState.RUNNING]) </s> remove             for backend_tag, replicas in self._replicas.items()\n </s> add             for backend_tag, replicas_container in self._replicas.items()\n            if filter_tag is None or backend_tag == filter_tag </s> remove     def get_backend_configs(self) -> Dict[BackendTag, BackendConfig]:\n </s> add     def _notify_replica_handles_changed(\n            self, key: Optional[BackendTag] = None) -> None:\n        for key, replica_dict in self.get_running_replica_handles(key).items():\n            self._long_poll_host.notify_changed(\n                (LongPollNamespace.REPLICA_HANDLES, key),\n                list(replica_dict.values()),\n            )\n\n    def get_backend_configs(\n            self,\n            filter_tag: Optional[BackendTag] = None,\n    ) -> Dict[BackendTag, BackendConfig]: </s> add             if filter_tag is None or tag == filter_tag </s> remove     async def _update_backend_configs(self, backend_configs):\n        # TODO(ilr) remove this loop when we poll per key\n        for backend_tag, config in backend_configs.items():\n            if backend_tag == self.backend_tag:\n                self._update_config(config)\n\n    def _update_config(self, new_config: BackendConfig) -> None:\n </s> add     def _update_backend_configs(self, new_config: BackendConfig) -> None:", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep keep replace replace keep replace", "code_tokens": " <mask>             self) -> Dict[BackendTag, Dict[ReplicaTag, ActorHandle]]:\n <mask>         return {\n <mask>             backend_tag: {\n <mask>                 r.replica_tag: r.actor_handle\n <mask>                 for r in replicas.get(states=[ReplicaState.RUNNING])\n <mask>             }\n <mask>             for backend_tag, replicas in self._replicas.items()\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove             self) -> Dict[BackendTag, Dict[ReplicaTag, ActorHandle]]:\n </s> add             self,\n            filter_tag: Optional[BackendTag] = None,\n    ) -> Dict[BackendTag, Dict[ReplicaTag, ActorHandle]]: </s> remove     def _notify_backend_configs_changed(self) -> None:\n        self._long_poll_host.notify_changed(LongPollKey.BACKEND_CONFIGS,\n                                            self.get_backend_configs())\n\n    def _notify_replica_handles_changed(self) -> None:\n        self._long_poll_host.notify_changed(\n            LongPollKey.REPLICA_HANDLES, {\n                backend_tag: list(replica_dict.values())\n                for backend_tag, replica_dict in\n                self.get_running_replica_handles().items()\n            })\n </s> add     def _notify_backend_configs_changed(\n            self, key: Optional[BackendTag] = None) -> None:\n        for key, config in self.get_backend_configs(key).items():\n            self._long_poll_host.notify_changed(\n                (LongPollNamespace.BACKEND_CONFIGS, key),\n                config,\n            ) </s> remove     def get_backend_configs(self) -> Dict[BackendTag, BackendConfig]:\n </s> add     def _notify_replica_handles_changed(\n            self, key: Optional[BackendTag] = None) -> None:\n        for key, replica_dict in self.get_running_replica_handles(key).items():\n            self._long_poll_host.notify_changed(\n                (LongPollNamespace.REPLICA_HANDLES, key),\n                list(replica_dict.values()),\n            )\n\n    def get_backend_configs(\n            self,\n            filter_tag: Optional[BackendTag] = None,\n    ) -> Dict[BackendTag, BackendConfig]: </s> add             if filter_tag is None or tag == filter_tag </s> remove         transition_triggered = False\n </s> add         transitioned_backend_tags = set()", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             }\n <mask>             for backend_tag, replicas in self._replicas.items()\n <mask>         }\n <mask> \n <mask>     def get_backend_configs(self) -> Dict[BackendTag, BackendConfig]:\n <mask>         return {\n <mask>             tag: info.backend_config\n <mask>             for tag, info in self._backend_metadata.items()\n <mask>         }\n <mask> \n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove             for backend_tag, replicas in self._replicas.items()\n </s> add             for backend_tag, replicas_container in self._replicas.items()\n            if filter_tag is None or backend_tag == filter_tag </s> add             if filter_tag is None or tag == filter_tag </s> remove                 r.replica_tag: r.actor_handle\n                for r in replicas.get(states=[ReplicaState.RUNNING])\n </s> add                 backend_replica.replica_tag: backend_replica.actor_handle\n                for backend_replica in replicas_container.get(\n                    [ReplicaState.RUNNING]) </s> remove             self) -> Dict[BackendTag, Dict[ReplicaTag, ActorHandle]]:\n </s> add             self,\n            filter_tag: Optional[BackendTag] = None,\n    ) -> Dict[BackendTag, Dict[ReplicaTag, ActorHandle]]: </s> remove     def _notify_backend_configs_changed(self) -> None:\n        self._long_poll_host.notify_changed(LongPollKey.BACKEND_CONFIGS,\n                                            self.get_backend_configs())\n\n    def _notify_replica_handles_changed(self) -> None:\n        self._long_poll_host.notify_changed(\n            LongPollKey.REPLICA_HANDLES, {\n                backend_tag: list(replica_dict.values())\n                for backend_tag, replica_dict in\n                self.get_running_replica_handles().items()\n            })\n </s> add     def _notify_backend_configs_changed(\n            self, key: Optional[BackendTag] = None) -> None:\n        for key, config in self.get_backend_configs(key).items():\n            self._long_poll_host.notify_changed(\n                (LongPollNamespace.BACKEND_CONFIGS, key),\n                config,\n            ) </s> remove         in_async_loop = asyncio.get_event_loop().is_running\n        assert in_async_loop, \"The client is only available in async context.\"\n        asyncio.get_event_loop().create_task(self._do_long_poll())\n </s> add     def update_key_listeners(\n            self,\n            key_listeners: Dict[KeyType, UpdateStateCallable],\n    ):\n        self.key_lisners = key_listeners\n        self._reset()", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>     ) -> Dict[BackendTag, BackendConfig]:\n <mask>         return {\n <mask>             tag: info.backend_config\n <mask>             for tag, info in self._backend_metadata.items()\n <mask>         }\n <mask> \n <mask>     def get_backend(self, backend_tag: BackendTag) -> Optional[BackendInfo]:\n <mask>         return self._backend_metadata.get(backend_tag)\n <mask> \n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove     def get_backend_configs(self) -> Dict[BackendTag, BackendConfig]:\n </s> add     def _notify_replica_handles_changed(\n            self, key: Optional[BackendTag] = None) -> None:\n        for key, replica_dict in self.get_running_replica_handles(key).items():\n            self._long_poll_host.notify_changed(\n                (LongPollNamespace.REPLICA_HANDLES, key),\n                list(replica_dict.values()),\n            )\n\n    def get_backend_configs(\n            self,\n            filter_tag: Optional[BackendTag] = None,\n    ) -> Dict[BackendTag, BackendConfig]: </s> remove             for backend_tag, replicas in self._replicas.items()\n </s> add             for backend_tag, replicas_container in self._replicas.items()\n            if filter_tag is None or backend_tag == filter_tag </s> remove                 r.replica_tag: r.actor_handle\n                for r in replicas.get(states=[ReplicaState.RUNNING])\n </s> add                 backend_replica.replica_tag: backend_replica.actor_handle\n                for backend_replica in replicas_container.get(\n                    [ReplicaState.RUNNING]) </s> remove             self) -> Dict[BackendTag, Dict[ReplicaTag, ActorHandle]]:\n </s> add             self,\n            filter_tag: Optional[BackendTag] = None,\n    ) -> Dict[BackendTag, Dict[ReplicaTag, ActorHandle]]: </s> remove     def _notify_backend_configs_changed(self) -> None:\n        self._long_poll_host.notify_changed(LongPollKey.BACKEND_CONFIGS,\n                                            self.get_backend_configs())\n\n    def _notify_replica_handles_changed(self) -> None:\n        self._long_poll_host.notify_changed(\n            LongPollKey.REPLICA_HANDLES, {\n                backend_tag: list(replica_dict.values())\n                for backend_tag, replica_dict in\n                self.get_running_replica_handles().items()\n            })\n </s> add     def _notify_backend_configs_changed(\n            self, key: Optional[BackendTag] = None) -> None:\n        for key, config in self.get_backend_configs(key).items():\n            self._long_poll_host.notify_changed(\n                (LongPollNamespace.BACKEND_CONFIGS, key),\n                config,\n            ) </s> remove         in_async_loop = asyncio.get_event_loop().is_running\n        assert in_async_loop, \"The client is only available in async context.\"\n        asyncio.get_event_loop().create_task(self._do_long_poll())\n </s> add     def update_key_listeners(\n            self,\n            key_listeners: Dict[KeyType, UpdateStateCallable],\n    ):\n        self.key_lisners = key_listeners\n        self._reset()", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         # NOTE(edoakes): we must write a checkpoint before starting new\n <mask>         # or pushing the updated config to avoid inconsistent state if we\n <mask>         # crash while making the change.\n <mask>         self._checkpoint()\n <mask>         self._notify_backend_configs_changed()\n <mask> \n <mask>         if existing_goal_id is not None:\n <mask>             self._goal_manager.complete_goal(existing_goal_id)\n <mask>         return new_goal_id\n <mask> \n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         self._sync_proxied_router = None\n        self._async_proxied_router = None\n </s> add         # TODO(simon): remove this when dropping router object and making\n        # ServeHandle sync only.\n        self._cached_routers = dict() </s> remove         return object_ref\n\n    async def _do_long_poll(self):\n        while True:\n            try:\n                updates: Dict[str, UpdatedObject] = await self._poll_once()\n                logger.debug(\"LongPollClient received updates for keys: \"\n                             f\"{list(updates.keys())}.\")\n                for key, update in updates.items():\n                    self.object_snapshots[key] = update.object_snapshot\n                    self.snapshot_ids[key] = update.snapshot_id\n                    # NOTE(simon):\n                    # This blocks the loop from doing another poll. Consider\n                    # use loop.create_task here or poll first then call the\n                    # callbacks.\n                    callback = self.key_listeners[key]\n                    await callback(update.object_snapshot)\n            except ray.exceptions.RayActorError:\n                # This can happen during shutdown where the controller is\n                # intentionally killed, the client should just gracefully\n                # exit.\n                logger.debug(\"LongPollClient failed to connect to host. \"\n                             \"Shutting down.\")\n                break\n </s> add         self._current_ref._on_completed(\n            lambda update: self._process_update(update))\n\n    def _process_update(self, updates: Dict[str, UpdatedObject]):\n        if isinstance(updates, (ray.exceptions.RayActorError)):\n            # This can happen during shutdown where the controller is\n            # intentionally killed, the client should just gracefully\n            # exit.\n            logger.debug(\"LongPollClient failed to connect to host. \"\n                         \"Shutting down.\")\n            return\n\n        if isinstance(updates, (ray.exceptions.RayTaskError)):\n            # This can happen during shutdown where the controller doesn't\n            # contain this key, we will just repull.\n            # NOTE(simon): should we repull or just wait in the long poll\n            # host?\n            if not isinstance(updates.as_instanceof_cause(), ValueError):\n                logger.error(\"LongPollHost errored\\n\" + updates.traceback_str)\n            self._poll_next()\n            return\n\n        # Before we process the updates and calling callbacks, kick off\n        # another poll so we can pipeline the polling and processing.\n        self._poll_next()\n        logger.debug(\"LongPollClient received updates for keys: \"\n                     f\"{list(updates.keys())}.\")\n        for key, update in updates.items():\n            self.object_snapshots[key] = update.object_snapshot\n            self.snapshot_ids[key] = update.snapshot_id\n            callback = self.key_listeners[key]\n            if self.event_loop is None:\n                callback(update.object_snapshot)\n            else:\n                self.event_loop.call_soon_threadsafe(\n                    lambda: callback(update.object_snapshot)) </s> remove         logger.debug(\"Scaling backend '{}' to {} replicas\".format(\n            backend_tag, num_replicas))\n </s> add  </s> add         # Inform the routers and backend replicas about config changes.\n        # TODO(edoakes): this should only happen if we change something other\n        # than num_replicas.\n        self._notify_backend_configs_changed(backend_tag) </s> remove     async def _update_backend_configs(self, backend_configs):\n        # TODO(ilr) remove this loop when we poll per key\n        for backend_tag, config in backend_configs.items():\n            if backend_tag == self.backend_tag:\n                self._update_config(config)\n\n    def _update_config(self, new_config: BackendConfig) -> None:\n </s> add     def _update_backend_configs(self, new_config: BackendConfig) -> None: </s> remove         self.config_updated_event = asyncio.Event()\n </s> add         self.config_updated_event = asyncio.Event(loop=event_loop)", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>             ])\n <mask> \n <mask>         if len(replicas_to_stop) > 0:\n <mask>             logger.info(f\"Stopping {len(replicas_to_stop)} replicas of \"\n <mask>                         f\"backend '{backend_tag}' with outdated versions.\")\n <mask> \n <mask>         for replica in replicas_to_stop:\n <mask>             replica.set_should_stop(graceful_shutdown_timeout_s)\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         elif delta_num_replicas > 0:\n </s> add         logger.debug(\n            f\"Scaling backend '{backend_tag}' from {current_num_replicas} \"\n            f\"to {num_replicas} replicas\")\n\n        if delta_num_replicas > 0: </s> remove         logger.debug(\"Scaling backend '{}' to {} replicas\".format(\n            backend_tag, num_replicas))\n </s> add  </s> remove         if transition_triggered:\n </s> add         if len(transitioned_backend_tags) > 0: </s> add     @property\n    def backend_tags(self):\n        return set(self.traffic_dict.keys()).union(\n            set(self.shadow_dict.keys()))\n </s> remove         transition_triggered = False\n </s> add         transitioned_backend_tags = set() </s> remove                     transition_triggered = True\n </s> add                     transitioned_backend_tags.add(backend_tag)", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         actually starting/stopping the intended replicas. This avoids\n <mask>         inconsistencies with starting/stopping a replica and then crashing\n <mask>         before writing a checkpoint.\n <mask>         \"\"\"\n <mask>         logger.debug(\"Scaling backend '{}' to {} replicas\".format(\n <mask>             backend_tag, num_replicas))\n <mask>         assert (backend_tag in self._backend_metadata\n <mask>                 ), \"Backend {} is not registered.\".format(backend_tag)\n <mask>         assert num_replicas >= 0, (\"Number of replicas must be\"\n <mask>                                    \" greater than or equal to 0.\")\n <mask> \n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         self._notify_backend_configs_changed()\n </s> add         self._notify_backend_configs_changed(backend_tag) </s> add     @property\n    def backend_tags(self):\n        return set(self.traffic_dict.keys()).union(\n            set(self.shadow_dict.keys()))\n </s> add         # Inform the routers and backend replicas about config changes.\n        # TODO(edoakes): this should only happen if we change something other\n        # than num_replicas.\n        self._notify_backend_configs_changed(backend_tag) </s> remove         self.config_updated_event = asyncio.Event()\n </s> add         self.config_updated_event = asyncio.Event(loop=event_loop) </s> remove         elif delta_num_replicas > 0:\n </s> add         logger.debug(\n            f\"Scaling backend '{backend_tag}' from {current_num_replicas} \"\n            f\"to {num_replicas} replicas\")\n\n        if delta_num_replicas > 0: </s> remove             ) -> List[VersionedReplica]:\n </s> add             ) -> List[BackendReplica]:", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         if delta_num_replicas == 0:\n <mask>             return False\n <mask> \n <mask>         elif delta_num_replicas > 0:\n <mask>             logger.debug(\"Adding {} replicas to backend {}\".format(\n <mask>                 delta_num_replicas, backend_tag))\n <mask>             for _ in range(delta_num_replicas):\n <mask>                 replica_tag = \"{}#{}\".format(backend_tag, get_random_letters())\n <mask>                 self._replicas[backend_tag].add(\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> add     @property\n    def backend_tags(self):\n        return set(self.traffic_dict.keys()).union(\n            set(self.shadow_dict.keys()))\n </s> add         # Inform the routers and backend replicas about config changes.\n        # TODO(edoakes): this should only happen if we change something other\n        # than num_replicas.\n        self._notify_backend_configs_changed(backend_tag) </s> remove         if transition_triggered:\n </s> add         if len(transitioned_backend_tags) > 0: </s> remove         logger.debug(\"Scaling backend '{}' to {} replicas\".format(\n            backend_tag, num_replicas))\n </s> add  </s> remove         transition_triggered = False\n </s> add         transitioned_backend_tags = set() </s> remove     refs = [(await router.assign_request.remote(query_param))\n            for _ in range(6)]\n </s> add     refs = [(await router.assign_request(query_param)) for _ in range(6)]", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         for goal_id in self._completed_goals():\n <mask>             self._goal_manager.complete_goal(goal_id)\n <mask> \n <mask>         transition_triggered = False\n <mask>         for backend_tag, replicas in self._replicas.items():\n <mask>             for replica in replicas.pop(states=[ReplicaState.SHOULD_START]):\n <mask>                 replica.start(self._backend_metadata[backend_tag])\n <mask>                 replicas.add(ReplicaState.STARTING, replica)\n <mask> \n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove                     transition_triggered = True\n </s> add                     transitioned_backend_tags.add(backend_tag) </s> remove                     transition_triggered = True\n </s> add                     transitioned_backend_tags.add(backend_tag) </s> remove             for backend_tag, replicas in self._replicas.items()\n </s> add             for backend_tag, replicas_container in self._replicas.items()\n            if filter_tag is None or backend_tag == filter_tag </s> remove     def get_backend_configs(self) -> Dict[BackendTag, BackendConfig]:\n </s> add     def _notify_replica_handles_changed(\n            self, key: Optional[BackendTag] = None) -> None:\n        for key, replica_dict in self.get_running_replica_handles(key).items():\n            self._long_poll_host.notify_changed(\n                (LongPollNamespace.REPLICA_HANDLES, key),\n                list(replica_dict.values()),\n            )\n\n    def get_backend_configs(\n            self,\n            filter_tag: Optional[BackendTag] = None,\n    ) -> Dict[BackendTag, BackendConfig]: </s> remove                 r.replica_tag: r.actor_handle\n                for r in replicas.get(states=[ReplicaState.RUNNING])\n </s> add                 backend_replica.replica_tag: backend_replica.actor_handle\n                for backend_replica in replicas_container.get(\n                    [ReplicaState.RUNNING]) </s> remove         elif delta_num_replicas > 0:\n </s> add         logger.debug(\n            f\"Scaling backend '{backend_tag}' from {current_num_replicas} \"\n            f\"to {num_replicas} replicas\")\n\n        if delta_num_replicas > 0:", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>             for replica in replicas.pop(states=[ReplicaState.STARTING]):\n <mask>                 if replica.check_started():\n <mask>                     replicas.add(ReplicaState.RUNNING, replica)\n <mask>                     transition_triggered = True\n <mask>                 else:\n <mask>                     replicas.add(ReplicaState.STARTING, replica)\n <mask> \n <mask>             for replica in replicas.pop(states=[ReplicaState.STOPPING]):\n <mask>                 if replica.check_stopped():\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove                     transition_triggered = True\n </s> add                     transitioned_backend_tags.add(backend_tag) </s> remove         transition_triggered = False\n </s> add         transitioned_backend_tags = set() </s> remove         if transition_triggered:\n </s> add         if len(transitioned_backend_tags) > 0: </s> remove             self._notify_replica_handles_changed()\n </s> add             [\n                self._notify_replica_handles_changed(tag)\n                for tag in transitioned_backend_tags\n            ] </s> remove         self.config_updated_event = asyncio.Event()\n </s> add         self.config_updated_event = asyncio.Event(loop=event_loop) </s> add         # Inform the routers and backend replicas about config changes.\n        # TODO(edoakes): this should only happen if we change something other\n        # than num_replicas.\n        self._notify_backend_configs_changed(backend_tag)", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep replace keep", "code_tokens": " <mask>                     replicas.add(ReplicaState.STARTING, replica)\n <mask> \n <mask>             for replica in replicas.pop(states=[ReplicaState.STOPPING]):\n <mask>                 if replica.check_stopped():\n <mask>                     transition_triggered = True\n <mask>                 else:\n <mask>                     replicas.add(ReplicaState.STOPPING, replica)\n <mask> \n <mask>         if transition_triggered:\n <mask>             self._checkpoint()\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove                     transition_triggered = True\n </s> add                     transitioned_backend_tags.add(backend_tag) </s> remove             self._notify_replica_handles_changed()\n </s> add             [\n                self._notify_replica_handles_changed(tag)\n                for tag in transitioned_backend_tags\n            ] </s> remove         transition_triggered = False\n </s> add         transitioned_backend_tags = set() </s> remove         self.config_updated_event = asyncio.Event()\n </s> add         self.config_updated_event = asyncio.Event(loop=event_loop) </s> remove             handle = RayServeHandle(\n                self._get_proxied_router(sync=sync), endpoint_name)\n </s> add             handle = RayServeHandle(router, endpoint_name)", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep keep keep replace", "code_tokens": " <mask>                     replicas.add(ReplicaState.STOPPING, replica)\n <mask> \n <mask>         if transition_triggered:\n <mask>             self._checkpoint()\n <mask>             self._notify_replica_handles_changed()\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         if transition_triggered:\n </s> add         if len(transitioned_backend_tags) > 0: </s> remove                     transition_triggered = True\n </s> add                     transitioned_backend_tags.add(backend_tag) </s> remove                     transition_triggered = True\n </s> add                     transitioned_backend_tags.add(backend_tag) </s> remove         self._notify_backend_configs_changed()\n </s> add         self._notify_backend_configs_changed(backend_tag) </s> remove         transition_triggered = False\n </s> add         transitioned_backend_tags = set() </s> remove     def _get_proxied_router(self, sync: bool):\n        if sync:\n            if self._sync_proxied_router is None:\n                self._sync_proxied_router = ThreadProxiedRouter(\n                    self._controller, sync=True)\n            return self._sync_proxied_router\n        else:\n            if self._async_proxied_router is None:\n                self._async_proxied_router = ThreadProxiedRouter(\n                    self._controller, sync=False)\n            return self._async_proxied_router\n </s> add     def _get_proxied_router(self, sync: bool, endpoint: EndpointTag):\n        key = (sync, endpoint)\n        if key not in self._cached_routers:\n            self._cached_routers[key] = ThreadProxiedRouter(\n                self._controller,\n                sync,\n                endpoint,\n            )\n        return self._cached_routers[key]", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep replace keep keep keep keep replace keep keep", "code_tokens": " <mask> from ray.util import metrics\n <mask> from ray.serve.config import BackendConfig\n <mask> from ray.serve.long_poll import LongPollAsyncClient\n <mask> from ray.serve.router import Query, RequestMetadata\n <mask> from ray.serve.constants import (\n <mask>     BACKEND_RECONFIGURE_METHOD,\n <mask>     DEFAULT_LATENCY_BUCKET_MS,\n <mask>     LongPollKey,\n <mask> )\n <mask> from ray.exceptions import RayTaskError\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove from ray.serve.constants import LongPollKey\n </s> add from ray.serve.common import BackendTag, EndpointTag, TrafficPolicy\nfrom ray.serve.config import BackendConfig </s> remove from ray.serve.long_poll import LongPollAsyncClient\nfrom ray.serve.utils import logger, compute_dict_delta, compute_iterable_delta\n </s> add from ray.serve.long_poll import LongPollClient, LongPollNamespace\nfrom ray.serve.utils import compute_iterable_delta, logger\n\nimport ray </s> remove from ray.serve.common import (\n    BackendInfo,\n    BackendTag,\n    Duration,\n    GoalId,\n    ReplicaTag,\n)\n </s> add from ray.serve.common import (BackendInfo, BackendTag, Duration, GoalId,\n                              ReplicaTag) </s> remove from ray.serve.constants import LongPollKey\n </s> add  </s> remove from ray.serve.long_poll import LongPollHost\nfrom ray.serve.utils import format_actor_name, get_random_letters, logger\n </s> add from ray.serve.long_poll import LongPollHost, LongPollNamespace\nfrom ray.serve.utils import (format_actor_name, get_random_letters, logger)\n\nimport ray", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_worker.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>                          \"processed in this replica.\"),\n <mask>             tag_keys=(\"backend\", ))\n <mask>         self.request_counter.set_default_tags({\"backend\": self.backend_tag})\n <mask> \n <mask>         self.long_poll_client = LongPollAsyncClient(controller_handle, {\n <mask>             LongPollKey.BACKEND_CONFIGS: self._update_backend_configs,\n <mask>         })\n <mask> \n <mask>         self.error_counter = metrics.Counter(\n <mask>             \"serve_backend_error_counter\",\n <mask>             description=(\"The number of exceptions that have \"\n <mask>                          \"occurred in the backend.\"),\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         self.config_updated_event = asyncio.Event()\n </s> add         self.config_updated_event = asyncio.Event(loop=event_loop) </s> remove     def set_max_concurrent_queries(self, new_value):\n </s> add         self.long_poll_client = LongPollClient(\n            controller_handle,\n            {\n                (LongPollNamespace.BACKEND_CONFIGS, backend_tag): self.\n                set_max_concurrent_queries,\n                (LongPollNamespace.REPLICA_HANDLES, backend_tag): self.\n                update_worker_replicas,\n            },\n            call_in_event_loop=event_loop,\n        )\n\n    def set_max_concurrent_queries(self, backend_config: BackendConfig):\n        new_value: int = backend_config.max_concurrent_queries </s> remove     def __init__(self, backend_tag):\n </s> add     def __init__(\n            self,\n            controller_handle,\n            backend_tag,\n            event_loop: asyncio.AbstractEventLoop,\n    ): </s> remove             self) -> Dict[BackendTag, Dict[ReplicaTag, ActorHandle]]:\n </s> add             self,\n            filter_tag: Optional[BackendTag] = None,\n    ) -> Dict[BackendTag, Dict[ReplicaTag, ActorHandle]]: </s> add         # NOTE(simon): this extra layer of router seems unnecessary\n        # BUT it's needed still because of the shared asyncio thread.\n        router = self._get_proxied_router(sync=sync, endpoint=endpoint_name) </s> remove         return object_ref\n\n    async def _do_long_poll(self):\n        while True:\n            try:\n                updates: Dict[str, UpdatedObject] = await self._poll_once()\n                logger.debug(\"LongPollClient received updates for keys: \"\n                             f\"{list(updates.keys())}.\")\n                for key, update in updates.items():\n                    self.object_snapshots[key] = update.object_snapshot\n                    self.snapshot_ids[key] = update.snapshot_id\n                    # NOTE(simon):\n                    # This blocks the loop from doing another poll. Consider\n                    # use loop.create_task here or poll first then call the\n                    # callbacks.\n                    callback = self.key_listeners[key]\n                    await callback(update.object_snapshot)\n            except ray.exceptions.RayActorError:\n                # This can happen during shutdown where the controller is\n                # intentionally killed, the client should just gracefully\n                # exit.\n                logger.debug(\"LongPollClient failed to connect to host. \"\n                             \"Shutting down.\")\n                break\n </s> add         self._current_ref._on_completed(\n            lambda update: self._process_update(update))\n\n    def _process_update(self, updates: Dict[str, UpdatedObject]):\n        if isinstance(updates, (ray.exceptions.RayActorError)):\n            # This can happen during shutdown where the controller is\n            # intentionally killed, the client should just gracefully\n            # exit.\n            logger.debug(\"LongPollClient failed to connect to host. \"\n                         \"Shutting down.\")\n            return\n\n        if isinstance(updates, (ray.exceptions.RayTaskError)):\n            # This can happen during shutdown where the controller doesn't\n            # contain this key, we will just repull.\n            # NOTE(simon): should we repull or just wait in the long poll\n            # host?\n            if not isinstance(updates.as_instanceof_cause(), ValueError):\n                logger.error(\"LongPollHost errored\\n\" + updates.traceback_str)\n            self._poll_next()\n            return\n\n        # Before we process the updates and calling callbacks, kick off\n        # another poll so we can pipeline the polling and processing.\n        self._poll_next()\n        logger.debug(\"LongPollClient received updates for keys: \"\n                     f\"{list(updates.keys())}.\")\n        for key, update in updates.items():\n            self.object_snapshots[key] = update.object_snapshot\n            self.snapshot_ids[key] = update.snapshot_id\n            callback = self.key_listeners[key]\n            if self.event_loop is None:\n                callback(update.object_snapshot)\n            else:\n                self.event_loop.call_soon_threadsafe(\n                    lambda: callback(update.object_snapshot))", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_worker.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>             reconfigure_method = getattr(self.callable,\n <mask>                                          BACKEND_RECONFIGURE_METHOD)\n <mask>             reconfigure_method(user_config)\n <mask> \n <mask>     async def _update_backend_configs(self, backend_configs):\n <mask>         # TODO(ilr) remove this loop when we poll per key\n <mask>         for backend_tag, config in backend_configs.items():\n <mask>             if backend_tag == self.backend_tag:\n <mask>                 self._update_config(config)\n <mask> \n <mask>     def _update_config(self, new_config: BackendConfig) -> None:\n <mask>         self.config = new_config\n <mask>         self.batch_queue.set_config(self.config.max_batch_size or 1,\n <mask>                                     self.config.batch_wait_timeout)\n <mask>         self.reconfigure(self.config.user_config)\n <mask> \n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         self._sync_proxied_router = None\n        self._async_proxied_router = None\n </s> add         # TODO(simon): remove this when dropping router object and making\n        # ServeHandle sync only.\n        self._cached_routers = dict() </s> remove             for backend_tag, replicas in self._replicas.items()\n </s> add             for backend_tag, replicas_container in self._replicas.items()\n            if filter_tag is None or backend_tag == filter_tag </s> remove         return object_ref\n\n    async def _do_long_poll(self):\n        while True:\n            try:\n                updates: Dict[str, UpdatedObject] = await self._poll_once()\n                logger.debug(\"LongPollClient received updates for keys: \"\n                             f\"{list(updates.keys())}.\")\n                for key, update in updates.items():\n                    self.object_snapshots[key] = update.object_snapshot\n                    self.snapshot_ids[key] = update.snapshot_id\n                    # NOTE(simon):\n                    # This blocks the loop from doing another poll. Consider\n                    # use loop.create_task here or poll first then call the\n                    # callbacks.\n                    callback = self.key_listeners[key]\n                    await callback(update.object_snapshot)\n            except ray.exceptions.RayActorError:\n                # This can happen during shutdown where the controller is\n                # intentionally killed, the client should just gracefully\n                # exit.\n                logger.debug(\"LongPollClient failed to connect to host. \"\n                             \"Shutting down.\")\n                break\n </s> add         self._current_ref._on_completed(\n            lambda update: self._process_update(update))\n\n    def _process_update(self, updates: Dict[str, UpdatedObject]):\n        if isinstance(updates, (ray.exceptions.RayActorError)):\n            # This can happen during shutdown where the controller is\n            # intentionally killed, the client should just gracefully\n            # exit.\n            logger.debug(\"LongPollClient failed to connect to host. \"\n                         \"Shutting down.\")\n            return\n\n        if isinstance(updates, (ray.exceptions.RayTaskError)):\n            # This can happen during shutdown where the controller doesn't\n            # contain this key, we will just repull.\n            # NOTE(simon): should we repull or just wait in the long poll\n            # host?\n            if not isinstance(updates.as_instanceof_cause(), ValueError):\n                logger.error(\"LongPollHost errored\\n\" + updates.traceback_str)\n            self._poll_next()\n            return\n\n        # Before we process the updates and calling callbacks, kick off\n        # another poll so we can pipeline the polling and processing.\n        self._poll_next()\n        logger.debug(\"LongPollClient received updates for keys: \"\n                     f\"{list(updates.keys())}.\")\n        for key, update in updates.items():\n            self.object_snapshots[key] = update.object_snapshot\n            self.snapshot_ids[key] = update.snapshot_id\n            callback = self.key_listeners[key]\n            if self.event_loop is None:\n                callback(update.object_snapshot)\n            else:\n                self.event_loop.call_soon_threadsafe(\n                    lambda: callback(update.object_snapshot)) </s> remove     def __init__(self, backend_tag):\n </s> add     def __init__(\n            self,\n            controller_handle,\n            backend_tag,\n            event_loop: asyncio.AbstractEventLoop,\n    ): </s> remove         self._notify_backend_configs_changed()\n </s> add         self._notify_backend_configs_changed(backend_tag) </s> remove     def _notify_backend_configs_changed(self) -> None:\n        self._long_poll_host.notify_changed(LongPollKey.BACKEND_CONFIGS,\n                                            self.get_backend_configs())\n\n    def _notify_replica_handles_changed(self) -> None:\n        self._long_poll_host.notify_changed(\n            LongPollKey.REPLICA_HANDLES, {\n                backend_tag: list(replica_dict.values())\n                for backend_tag, replica_dict in\n                self.get_running_replica_handles().items()\n            })\n </s> add     def _notify_backend_configs_changed(\n            self, key: Optional[BackendTag] = None) -> None:\n        for key, config in self.get_backend_configs(key).items():\n            self._long_poll_host.notify_changed(\n                (LongPollNamespace.BACKEND_CONFIGS, key),\n                config,\n            )", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/backend_worker.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>         self.set_traffic_dict(traffic_dict)\n <mask> \n <mask>     def set_traffic_dict(self, traffic_dict: Dict[str, float]) -> None:\n <mask>         prob = 0\n <mask>         for backend, weight in traffic_dict.items():\n <mask>             if weight < 0:\n <mask>                 raise ValueError(\n <mask>                     \"Attempted to assign a weight of {} to backend '{}'. \"\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         elif delta_num_replicas > 0:\n </s> add         logger.debug(\n            f\"Scaling backend '{backend_tag}' from {current_num_replicas} \"\n            f\"to {num_replicas} replicas\")\n\n        if delta_num_replicas > 0: </s> remove         logger.debug(\"Scaling backend '{}' to {} replicas\".format(\n            backend_tag, num_replicas))\n </s> add  </s> remove         self.config_updated_event = asyncio.Event()\n </s> add         self.config_updated_event = asyncio.Event(loop=event_loop) </s> remove     def _get_proxied_router(self, sync: bool):\n        if sync:\n            if self._sync_proxied_router is None:\n                self._sync_proxied_router = ThreadProxiedRouter(\n                    self._controller, sync=True)\n            return self._sync_proxied_router\n        else:\n            if self._async_proxied_router is None:\n                self._async_proxied_router = ThreadProxiedRouter(\n                    self._controller, sync=False)\n            return self._async_proxied_router\n </s> add     def _get_proxied_router(self, sync: bool, endpoint: EndpointTag):\n        key = (sync, endpoint)\n        if key not in self._cached_routers:\n            self._cached_routers[key] = ThreadProxiedRouter(\n                self._controller,\n                sync,\n                endpoint,\n            )\n        return self._cached_routers[key] </s> remove class _PendingEndpointFound(Enum):\n    \"\"\"Enum for the status of pending endpoint registration.\"\"\"\n    ADDED = 1\n    REMOVED = 2\n\n\n </s> add  </s> add         # Inform the routers and backend replicas about config changes.\n        # TODO(edoakes): this should only happen if we change something other\n        # than num_replicas.\n        self._notify_backend_configs_changed(backend_tag)", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/common.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> # Type signature for the update state callbacks. E.g.\n <mask> # async def update_state(updated_object: Any):\n <mask> #     do_something(updated_object)\n <mask> UpdateStateAsyncCallable = Callable[[Any], Awaitable[None]]\n <mask> \n <mask> \n <mask> class LongPollAsyncClient:\n <mask>     \"\"\"The asynchronous long polling client.\n <mask> \n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         return object_ref\n\n    async def _do_long_poll(self):\n        while True:\n            try:\n                updates: Dict[str, UpdatedObject] = await self._poll_once()\n                logger.debug(\"LongPollClient received updates for keys: \"\n                             f\"{list(updates.keys())}.\")\n                for key, update in updates.items():\n                    self.object_snapshots[key] = update.object_snapshot\n                    self.snapshot_ids[key] = update.snapshot_id\n                    # NOTE(simon):\n                    # This blocks the loop from doing another poll. Consider\n                    # use loop.create_task here or poll first then call the\n                    # callbacks.\n                    callback = self.key_listeners[key]\n                    await callback(update.object_snapshot)\n            except ray.exceptions.RayActorError:\n                # This can happen during shutdown where the controller is\n                # intentionally killed, the client should just gracefully\n                # exit.\n                logger.debug(\"LongPollClient failed to connect to host. \"\n                             \"Shutting down.\")\n                break\n </s> add         self._current_ref._on_completed(\n            lambda update: self._process_update(update))\n\n    def _process_update(self, updates: Dict[str, UpdatedObject]):\n        if isinstance(updates, (ray.exceptions.RayActorError)):\n            # This can happen during shutdown where the controller is\n            # intentionally killed, the client should just gracefully\n            # exit.\n            logger.debug(\"LongPollClient failed to connect to host. \"\n                         \"Shutting down.\")\n            return\n\n        if isinstance(updates, (ray.exceptions.RayTaskError)):\n            # This can happen during shutdown where the controller doesn't\n            # contain this key, we will just repull.\n            # NOTE(simon): should we repull or just wait in the long poll\n            # host?\n            if not isinstance(updates.as_instanceof_cause(), ValueError):\n                logger.error(\"LongPollHost errored\\n\" + updates.traceback_str)\n            self._poll_next()\n            return\n\n        # Before we process the updates and calling callbacks, kick off\n        # another poll so we can pipeline the polling and processing.\n        self._poll_next()\n        logger.debug(\"LongPollClient received updates for keys: \"\n                     f\"{list(updates.keys())}.\")\n        for key, update in updates.items():\n            self.object_snapshots[key] = update.object_snapshot\n            self.snapshot_ids[key] = update.snapshot_id\n            callback = self.key_listeners[key]\n            if self.event_loop is None:\n                callback(update.object_snapshot)\n            else:\n                self.event_loop.call_soon_threadsafe(\n                    lambda: callback(update.object_snapshot)) </s> remove     def __init__(self, backend_tag):\n </s> add     def __init__(\n            self,\n            controller_handle,\n            backend_tag,\n            event_loop: asyncio.AbstractEventLoop,\n    ): </s> remove         self._notify_backend_configs_changed()\n </s> add         self._notify_backend_configs_changed(backend_tag) </s> remove         self.object_snapshots: Dict[str, Any] = dict()\n </s> add         self.object_snapshots: Dict[KeyType, Any] = dict() </s> remove         self._sync_proxied_router = None\n        self._async_proxied_router = None\n </s> add         # TODO(simon): remove this when dropping router object and making\n        # ServeHandle sync only.\n        self._cached_routers = dict() </s> remove         self.notifier_events: DefaultDict[str, Set[\n </s> add         self.notifier_events: DefaultDict[KeyType, Set[", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/long_poll.py"}
{"docstring_tokens": "keep keep replace replace replace keep replace replace keep keep", "code_tokens": " <mask>         self.object_snapshots: Dict[str, Any] = dict()\n <mask> \n <mask>         in_async_loop = asyncio.get_event_loop().is_running\n <mask>         assert in_async_loop, \"The client is only available in async context.\"\n <mask>         asyncio.get_event_loop().create_task(self._do_long_poll())\n <mask> \n <mask>     def _poll_once(self) -> ray.ObjectRef:\n <mask>         object_ref = self.host_actor.listen_for_change.remote(\n <mask>             self.snapshot_ids)\n <mask>         return object_ref\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         self.object_snapshots: Dict[str, Any] = dict()\n </s> add         self.object_snapshots: Dict[KeyType, Any] = dict() </s> remove         self.notifier_events: DefaultDict[str, Set[\n </s> add         self.notifier_events: DefaultDict[KeyType, Set[ </s> remove         return object_ref\n\n    async def _do_long_poll(self):\n        while True:\n            try:\n                updates: Dict[str, UpdatedObject] = await self._poll_once()\n                logger.debug(\"LongPollClient received updates for keys: \"\n                             f\"{list(updates.keys())}.\")\n                for key, update in updates.items():\n                    self.object_snapshots[key] = update.object_snapshot\n                    self.snapshot_ids[key] = update.snapshot_id\n                    # NOTE(simon):\n                    # This blocks the loop from doing another poll. Consider\n                    # use loop.create_task here or poll first then call the\n                    # callbacks.\n                    callback = self.key_listeners[key]\n                    await callback(update.object_snapshot)\n            except ray.exceptions.RayActorError:\n                # This can happen during shutdown where the controller is\n                # intentionally killed, the client should just gracefully\n                # exit.\n                logger.debug(\"LongPollClient failed to connect to host. \"\n                             \"Shutting down.\")\n                break\n </s> add         self._current_ref._on_completed(\n            lambda update: self._process_update(update))\n\n    def _process_update(self, updates: Dict[str, UpdatedObject]):\n        if isinstance(updates, (ray.exceptions.RayActorError)):\n            # This can happen during shutdown where the controller is\n            # intentionally killed, the client should just gracefully\n            # exit.\n            logger.debug(\"LongPollClient failed to connect to host. \"\n                         \"Shutting down.\")\n            return\n\n        if isinstance(updates, (ray.exceptions.RayTaskError)):\n            # This can happen during shutdown where the controller doesn't\n            # contain this key, we will just repull.\n            # NOTE(simon): should we repull or just wait in the long poll\n            # host?\n            if not isinstance(updates.as_instanceof_cause(), ValueError):\n                logger.error(\"LongPollHost errored\\n\" + updates.traceback_str)\n            self._poll_next()\n            return\n\n        # Before we process the updates and calling callbacks, kick off\n        # another poll so we can pipeline the polling and processing.\n        self._poll_next()\n        logger.debug(\"LongPollClient received updates for keys: \"\n                     f\"{list(updates.keys())}.\")\n        for key, update in updates.items():\n            self.object_snapshots[key] = update.object_snapshot\n            self.snapshot_ids[key] = update.snapshot_id\n            callback = self.key_listeners[key]\n            if self.event_loop is None:\n                callback(update.object_snapshot)\n            else:\n                self.event_loop.call_soon_threadsafe(\n                    lambda: callback(update.object_snapshot)) </s> remove         self.snapshot_ids: DefaultDict[str, int] = defaultdict(\n </s> add         self.snapshot_ids: DefaultDict[KeyType, int] = defaultdict( </s> remove async def test_graceful_shutdown(serve_instance, router,\n                                 mock_controller_with_name):\n </s> add async def test_graceful_shutdown(serve_instance, mock_controller_with_name):", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/long_poll.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     def _poll_once(self) -> ray.ObjectRef:\n <mask>         object_ref = self.host_actor.listen_for_change.remote(\n <mask>             self.snapshot_ids)\n <mask>         return object_ref\n <mask> \n <mask>     async def _do_long_poll(self):\n <mask>         while True:\n <mask>             try:\n <mask>                 updates: Dict[str, UpdatedObject] = await self._poll_once()\n <mask>                 logger.debug(\"LongPollClient received updates for keys: \"\n <mask>                              f\"{list(updates.keys())}.\")\n <mask>                 for key, update in updates.items():\n <mask>                     self.object_snapshots[key] = update.object_snapshot\n <mask>                     self.snapshot_ids[key] = update.snapshot_id\n <mask>                     # NOTE(simon):\n <mask>                     # This blocks the loop from doing another poll. Consider\n <mask>                     # use loop.create_task here or poll first then call the\n <mask>                     # callbacks.\n <mask>                     callback = self.key_listeners[key]\n <mask>                     await callback(update.object_snapshot)\n <mask>             except ray.exceptions.RayActorError:\n <mask>                 # This can happen during shutdown where the controller is\n <mask>                 # intentionally killed, the client should just gracefully\n <mask>                 # exit.\n <mask>                 logger.debug(\"LongPollClient failed to connect to host. \"\n <mask>                              \"Shutting down.\")\n <mask>                 break\n <mask> \n <mask> \n <mask> class LongPollHost:\n <mask>     \"\"\"The server side object that manages long pulling requests.\n <mask> \n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove     def _poll_once(self) -> ray.ObjectRef:\n        object_ref = self.host_actor.listen_for_change.remote(\n </s> add     def _poll_next(self):\n        \"\"\"Poll the update. The callback is expected to scheduler another\n        _poll_next call.\n        \"\"\"\n        self._current_ref = self.host_actor.listen_for_change.remote( </s> remove UpdateStateAsyncCallable = Callable[[Any], Awaitable[None]]\n </s> add UpdateStateCallable = Callable[[Any], None]\nKeyType = Union[str, LongPollNamespace, Tuple[LongPollNamespace, str]] </s> remove         in_async_loop = asyncio.get_event_loop().is_running\n        assert in_async_loop, \"The client is only available in async context.\"\n        asyncio.get_event_loop().create_task(self._do_long_poll())\n </s> add     def update_key_listeners(\n            self,\n            key_listeners: Dict[KeyType, UpdateStateCallable],\n    ):\n        self.key_lisners = key_listeners\n        self._reset() </s> remove     def __init__(self, backend_tag):\n </s> add     def __init__(\n            self,\n            controller_handle,\n            backend_tag,\n            event_loop: asyncio.AbstractEventLoop,\n    ): </s> remove         self._sync_proxied_router = None\n        self._async_proxied_router = None\n </s> add         # TODO(simon): remove this when dropping router object and making\n        # ServeHandle sync only.\n        self._cached_routers = dict() </s> remove         self.config_updated_event = asyncio.Event()\n </s> add         self.config_updated_event = asyncio.Event(loop=event_loop)", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/long_poll.py"}
{"docstring_tokens": "keep keep keep replace keep keep replace keep keep keep", "code_tokens": " <mask> \n <mask>     def __init__(self):\n <mask>         # Map object_key -> int\n <mask>         self.snapshot_ids: DefaultDict[str, int] = defaultdict(\n <mask>             lambda: random.randint(0, 1_000_000))\n <mask>         # Map object_key -> object\n <mask>         self.object_snapshots: Dict[str, Any] = dict()\n <mask>         # Map object_key -> set(asyncio.Event waiting for updates)\n <mask>         self.notifier_events: DefaultDict[str, Set[\n <mask>             asyncio.Event]] = defaultdict(set)\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         self.notifier_events: DefaultDict[str, Set[\n </s> add         self.notifier_events: DefaultDict[KeyType, Set[ </s> remove     async def listen_for_change(self, keys_to_snapshot_ids: Dict[str, int]\n                                ) -> Dict[str, UpdatedObject]:\n </s> add     async def listen_for_change(\n            self,\n            keys_to_snapshot_ids: Dict[KeyType, int],\n    ) -> Dict[KeyType, UpdatedObject]: </s> remove         in_async_loop = asyncio.get_event_loop().is_running\n        assert in_async_loop, \"The client is only available in async context.\"\n        asyncio.get_event_loop().create_task(self._do_long_poll())\n </s> add     def update_key_listeners(\n            self,\n            key_listeners: Dict[KeyType, UpdateStateCallable],\n    ):\n        self.key_lisners = key_listeners\n        self._reset() </s> remove     def notify_changed(self, object_key: str, updated_object: Any):\n </s> add     def notify_changed(\n            self,\n            object_key: KeyType,\n            updated_object: Any,\n    ): </s> remove         self._sync_proxied_router = None\n        self._async_proxied_router = None\n </s> add         # TODO(simon): remove this when dropping router object and making\n        # ServeHandle sync only.\n        self._cached_routers = dict()", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/long_poll.py"}
{"docstring_tokens": "keep keep replace keep keep replace replace keep keep keep keep", "code_tokens": " <mask>         self.object_snapshots: Dict[str, Any] = dict()\n <mask>         # Map object_key -> set(asyncio.Event waiting for updates)\n <mask>         self.notifier_events: DefaultDict[str, Set[\n <mask>             asyncio.Event]] = defaultdict(set)\n <mask> \n <mask>     async def listen_for_change(self, keys_to_snapshot_ids: Dict[str, int]\n <mask>                                 ) -> Dict[str, UpdatedObject]:\n <mask>         \"\"\"Listen for changed objects.\n <mask> \n <mask>         This method will returns a dictionary of updated objects. It returns\n <mask>         immediately if the snapshot_ids are outdated, otherwise it will block\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         self.object_snapshots: Dict[str, Any] = dict()\n </s> add         self.object_snapshots: Dict[KeyType, Any] = dict() </s> remove         self.snapshot_ids: DefaultDict[str, int] = defaultdict(\n </s> add         self.snapshot_ids: DefaultDict[KeyType, int] = defaultdict( </s> remove         in_async_loop = asyncio.get_event_loop().is_running\n        assert in_async_loop, \"The client is only available in async context.\"\n        asyncio.get_event_loop().create_task(self._do_long_poll())\n </s> add     def update_key_listeners(\n            self,\n            key_listeners: Dict[KeyType, UpdateStateCallable],\n    ):\n        self.key_lisners = key_listeners\n        self._reset() </s> remove             ) -> List[VersionedReplica]:\n </s> add             ) -> List[BackendReplica]: </s> remove         return object_ref\n\n    async def _do_long_poll(self):\n        while True:\n            try:\n                updates: Dict[str, UpdatedObject] = await self._poll_once()\n                logger.debug(\"LongPollClient received updates for keys: \"\n                             f\"{list(updates.keys())}.\")\n                for key, update in updates.items():\n                    self.object_snapshots[key] = update.object_snapshot\n                    self.snapshot_ids[key] = update.snapshot_id\n                    # NOTE(simon):\n                    # This blocks the loop from doing another poll. Consider\n                    # use loop.create_task here or poll first then call the\n                    # callbacks.\n                    callback = self.key_listeners[key]\n                    await callback(update.object_snapshot)\n            except ray.exceptions.RayActorError:\n                # This can happen during shutdown where the controller is\n                # intentionally killed, the client should just gracefully\n                # exit.\n                logger.debug(\"LongPollClient failed to connect to host. \"\n                             \"Shutting down.\")\n                break\n </s> add         self._current_ref._on_completed(\n            lambda update: self._process_update(update))\n\n    def _process_update(self, updates: Dict[str, UpdatedObject]):\n        if isinstance(updates, (ray.exceptions.RayActorError)):\n            # This can happen during shutdown where the controller is\n            # intentionally killed, the client should just gracefully\n            # exit.\n            logger.debug(\"LongPollClient failed to connect to host. \"\n                         \"Shutting down.\")\n            return\n\n        if isinstance(updates, (ray.exceptions.RayTaskError)):\n            # This can happen during shutdown where the controller doesn't\n            # contain this key, we will just repull.\n            # NOTE(simon): should we repull or just wait in the long poll\n            # host?\n            if not isinstance(updates.as_instanceof_cause(), ValueError):\n                logger.error(\"LongPollHost errored\\n\" + updates.traceback_str)\n            self._poll_next()\n            return\n\n        # Before we process the updates and calling callbacks, kick off\n        # another poll so we can pipeline the polling and processing.\n        self._poll_next()\n        logger.debug(\"LongPollClient received updates for keys: \"\n                     f\"{list(updates.keys())}.\")\n        for key, update in updates.items():\n            self.object_snapshots[key] = update.object_snapshot\n            self.snapshot_ids[key] = update.snapshot_id\n            callback = self.key_listeners[key]\n            if self.event_loop is None:\n                callback(update.object_snapshot)\n            else:\n                self.event_loop.call_soon_threadsafe(\n                    lambda: callback(update.object_snapshot))", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/long_poll.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                 self.object_snapshots[updated_object_key],\n <mask>                 self.snapshot_ids[updated_object_key])\n <mask>         }\n <mask> \n <mask>     def notify_changed(self, object_key: str, updated_object: Any):\n <mask>         self.snapshot_ids[object_key] += 1\n <mask>         self.object_snapshots[object_key] = updated_object\n <mask>         logger.debug(f\"LongPollHost: Notify change for key {object_key}.\")\n <mask> \n <mask>         if object_key in self.notifier_events:\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         self.notifier_events: DefaultDict[str, Set[\n </s> add         self.notifier_events: DefaultDict[KeyType, Set[ </s> remove         self.snapshot_ids: DefaultDict[str, int] = defaultdict(\n </s> add         self.snapshot_ids: DefaultDict[KeyType, int] = defaultdict( </s> remove         self.object_snapshots: Dict[str, Any] = dict()\n </s> add         self.object_snapshots: Dict[KeyType, Any] = dict() </s> remove         in_async_loop = asyncio.get_event_loop().is_running\n        assert in_async_loop, \"The client is only available in async context.\"\n        asyncio.get_event_loop().create_task(self._do_long_poll())\n </s> add     def update_key_listeners(\n            self,\n            key_listeners: Dict[KeyType, UpdateStateCallable],\n    ):\n        self.key_lisners = key_listeners\n        self._reset() </s> remove     def get_backend_configs(self) -> Dict[BackendTag, BackendConfig]:\n </s> add     def _notify_replica_handles_changed(\n            self, key: Optional[BackendTag] = None) -> None:\n        for key, replica_dict in self.get_running_replica_handles(key).items():\n            self._long_poll_host.notify_changed(\n                (LongPollNamespace.REPLICA_HANDLES, key),\n                list(replica_dict.values()),\n            )\n\n    def get_backend_configs(\n            self,\n            filter_tag: Optional[BackendTag] = None,\n    ) -> Dict[BackendTag, BackendConfig]: </s> remove     async def listen_for_change(self, keys_to_snapshot_ids: Dict[str, int]\n                                ) -> Dict[str, UpdatedObject]:\n </s> add     async def listen_for_change(\n            self,\n            keys_to_snapshot_ids: Dict[KeyType, int],\n    ) -> Dict[KeyType, UpdatedObject]:", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/long_poll.py"}
{"docstring_tokens": "keep replace keep keep replace keep keep keep keep", "code_tokens": " <mask> import asyncio\n <mask> from enum import Enum\n <mask> import itertools\n <mask> from dataclasses import dataclass, field\n <mask> from typing import Any, ChainMap, Dict, Iterable, List, Optional\n <mask> \n <mask> from ray.serve.exceptions import RayServeException\n <mask> \n <mask> import ray\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove from ray.serve.exceptions import RayServeException\n\nimport ray\n </s> add  </s> remove import math\nimport time\n </s> add  </s> remove import ray\n </s> add  </s> add from functools import wraps\nfrom typing import (TYPE_CHECKING, Any, Callable, Coroutine, Dict, List,\n                    Optional, Type, Union)\nfrom uuid import UUID </s> remove from typing import (Any, Callable, Coroutine, Dict, List, Optional,\n                    TYPE_CHECKING, Type, Union)\n </s> add import time", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/router.py"}
{"docstring_tokens": "keep keep keep replace replace replace keep replace", "code_tokens": " <mask> from dataclasses import dataclass, field\n <mask> from typing import Any, ChainMap, Dict, Iterable, List, Optional\n <mask> \n <mask> from ray.serve.exceptions import RayServeException\n <mask> \n <mask> import ray\n <mask> from ray.actor import ActorHandle\n <mask> from ray.serve.constants import LongPollKey\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove from typing import Any, ChainMap, Dict, Iterable, List, Optional\n </s> add from typing import Any, Dict, Iterable, List, Optional </s> remove from enum import Enum\n </s> add  </s> remove import math\nimport time\n </s> add  </s> remove import ray\n </s> add  </s> add from functools import wraps\nfrom typing import (TYPE_CHECKING, Any, Callable, Coroutine, Dict, List,\n                    Optional, Type, Union)\nfrom uuid import UUID", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/router.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> import ray\n <mask> from ray.actor import ActorHandle\n <mask> from ray.serve.constants import LongPollKey\n <mask> from ray.serve.endpoint_policy import EndpointPolicy, RandomEndpointPolicy\n <mask> from ray.serve.long_poll import LongPollAsyncClient\n <mask> from ray.serve.utils import logger, compute_dict_delta, compute_iterable_delta\n <mask> from ray.util import metrics\n <mask> \n <mask> REPORT_QUEUE_LENGTH_PERIOD_S = 1.0\n <mask> \n <mask> \n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove from ray.serve.constants import LongPollKey\n </s> add from ray.serve.common import BackendTag, EndpointTag, TrafficPolicy\nfrom ray.serve.config import BackendConfig </s> remove from ray.serve.exceptions import RayServeException\n\nimport ray\n </s> add  </s> remove from ray.serve.long_poll import LongPollAsyncClient\n </s> add from ray.serve.long_poll import LongPollClient, LongPollNamespace </s> remove from ray.serve.long_poll import LongPollHost\nfrom ray.serve.utils import format_actor_name, get_random_letters, logger\n </s> add from ray.serve.long_poll import LongPollHost, LongPollNamespace\nfrom ray.serve.utils import (format_actor_name, get_random_letters, logger)\n\nimport ray </s> remove from ray.serve.constants import LongPollKey\n </s> add  </s> remove from ray.serve.common import (\n    BackendInfo,\n    BackendTag,\n    Duration,\n    GoalId,\n    ReplicaTag,\n)\n </s> add from ray.serve.common import (BackendInfo, BackendTag, Duration, GoalId,\n                              ReplicaTag)", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/router.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> class ReplicaSet:\n <mask>     \"\"\"Data structure representing a set of replica actor handles\"\"\"\n <mask> \n <mask>     def __init__(self, backend_tag):\n <mask>         self.backend_tag = backend_tag\n <mask>         # NOTE(simon): We have to do this because max_concurrent_queries\n <mask>         # and the replica handles come from different long poll keys.\n <mask>         self.max_concurrent_queries: int = 8\n <mask>         self.in_flight_queries: Dict[ActorHandle, set] = dict()\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         self.config_updated_event = asyncio.Event()\n </s> add         self.config_updated_event = asyncio.Event(loop=event_loop) </s> remove     def set_max_concurrent_queries(self, new_value):\n </s> add         self.long_poll_client = LongPollClient(\n            controller_handle,\n            {\n                (LongPollNamespace.BACKEND_CONFIGS, backend_tag): self.\n                set_max_concurrent_queries,\n                (LongPollNamespace.REPLICA_HANDLES, backend_tag): self.\n                update_worker_replicas,\n            },\n            call_in_event_loop=event_loop,\n        )\n\n    def set_max_concurrent_queries(self, backend_config: BackendConfig):\n        new_value: int = backend_config.max_concurrent_queries </s> remove         self._sync_proxied_router = None\n        self._async_proxied_router = None\n </s> add         # TODO(simon): remove this when dropping router object and making\n        # ServeHandle sync only.\n        self._cached_routers = dict() </s> add         # NOTE(simon): this extra layer of router seems unnecessary\n        # BUT it's needed still because of the shared asyncio thread.\n        router = self._get_proxied_router(sync=sync, endpoint=endpoint_name) </s> remove         return object_ref\n\n    async def _do_long_poll(self):\n        while True:\n            try:\n                updates: Dict[str, UpdatedObject] = await self._poll_once()\n                logger.debug(\"LongPollClient received updates for keys: \"\n                             f\"{list(updates.keys())}.\")\n                for key, update in updates.items():\n                    self.object_snapshots[key] = update.object_snapshot\n                    self.snapshot_ids[key] = update.snapshot_id\n                    # NOTE(simon):\n                    # This blocks the loop from doing another poll. Consider\n                    # use loop.create_task here or poll first then call the\n                    # callbacks.\n                    callback = self.key_listeners[key]\n                    await callback(update.object_snapshot)\n            except ray.exceptions.RayActorError:\n                # This can happen during shutdown where the controller is\n                # intentionally killed, the client should just gracefully\n                # exit.\n                logger.debug(\"LongPollClient failed to connect to host. \"\n                             \"Shutting down.\")\n                break\n </s> add         self._current_ref._on_completed(\n            lambda update: self._process_update(update))\n\n    def _process_update(self, updates: Dict[str, UpdatedObject]):\n        if isinstance(updates, (ray.exceptions.RayActorError)):\n            # This can happen during shutdown where the controller is\n            # intentionally killed, the client should just gracefully\n            # exit.\n            logger.debug(\"LongPollClient failed to connect to host. \"\n                         \"Shutting down.\")\n            return\n\n        if isinstance(updates, (ray.exceptions.RayTaskError)):\n            # This can happen during shutdown where the controller doesn't\n            # contain this key, we will just repull.\n            # NOTE(simon): should we repull or just wait in the long poll\n            # host?\n            if not isinstance(updates.as_instanceof_cause(), ValueError):\n                logger.error(\"LongPollHost errored\\n\" + updates.traceback_str)\n            self._poll_next()\n            return\n\n        # Before we process the updates and calling callbacks, kick off\n        # another poll so we can pipeline the polling and processing.\n        self._poll_next()\n        logger.debug(\"LongPollClient received updates for keys: \"\n                     f\"{list(updates.keys())}.\")\n        for key, update in updates.items():\n            self.object_snapshots[key] = update.object_snapshot\n            self.snapshot_ids[key] = update.snapshot_id\n            callback = self.key_listeners[key]\n            if self.event_loop is None:\n                callback(update.object_snapshot)\n            else:\n                self.event_loop.call_soon_threadsafe(\n                    lambda: callback(update.object_snapshot)) </s> remove class _PendingEndpointFound(Enum):\n    \"\"\"Enum for the status of pending endpoint registration.\"\"\"\n    ADDED = 1\n    REMOVED = 2\n\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/router.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         # Used to unblock this replica set waiting for free replicas. A newly\n <mask>         # added replica or updated max_concurrent_queries value means the\n <mask>         # query that waits on a free replica might be unblocked on.\n <mask>         self.config_updated_event = asyncio.Event()\n <mask>         self.num_queued_queries = 0\n <mask>         self.num_queued_queries_gauge = metrics.Gauge(\n <mask>             \"serve_backend_queued_queries\",\n <mask>             description=(\n <mask>                 \"The current number of queries to this backend waiting\"\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove     def __init__(self, backend_tag):\n </s> add     def __init__(\n            self,\n            controller_handle,\n            backend_tag,\n            event_loop: asyncio.AbstractEventLoop,\n    ): </s> remove         logger.debug(\"Scaling backend '{}' to {} replicas\".format(\n            backend_tag, num_replicas))\n </s> add  </s> remove         self._sync_proxied_router = None\n        self._async_proxied_router = None\n </s> add         # TODO(simon): remove this when dropping router object and making\n        # ServeHandle sync only.\n        self._cached_routers = dict() </s> add         # Inform the routers and backend replicas about config changes.\n        # TODO(edoakes): this should only happen if we change something other\n        # than num_replicas.\n        self._notify_backend_configs_changed(backend_tag) </s> remove         self._notify_backend_configs_changed()\n </s> add         self._notify_backend_configs_changed(backend_tag) </s> add     @property\n    def backend_tags(self):\n        return set(self.traffic_dict.keys()).union(\n            set(self.shadow_dict.keys()))\n", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/router.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         self.num_queued_queries_gauge.set_default_tags({\n <mask>             \"backend\": self.backend_tag\n <mask>         })\n <mask> \n <mask>     def set_max_concurrent_queries(self, new_value):\n <mask>         if new_value != self.max_concurrent_queries:\n <mask>             self.max_concurrent_queries = new_value\n <mask>             logger.debug(\n <mask>                 f\"ReplicaSet: changing max_concurrent_queries to {new_value}\")\n <mask>             self.config_updated_event.set()\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove     def __init__(self, backend_tag):\n </s> add     def __init__(\n            self,\n            controller_handle,\n            backend_tag,\n            event_loop: asyncio.AbstractEventLoop,\n    ): </s> remove         elif delta_num_replicas > 0:\n </s> add         logger.debug(\n            f\"Scaling backend '{backend_tag}' from {current_num_replicas} \"\n            f\"to {num_replicas} replicas\")\n\n        if delta_num_replicas > 0: </s> remove         self.config_updated_event = asyncio.Event()\n </s> add         self.config_updated_event = asyncio.Event(loop=event_loop) </s> remove             self) -> Dict[BackendTag, Dict[ReplicaTag, ActorHandle]]:\n </s> add             self,\n            filter_tag: Optional[BackendTag] = None,\n    ) -> Dict[BackendTag, Dict[ReplicaTag, ActorHandle]]: </s> add     @property\n    def backend_tags(self):\n        return set(self.traffic_dict.keys()).union(\n            set(self.shadow_dict.keys()))\n </s> remove         self.long_poll_client = LongPollAsyncClient(controller_handle, {\n            LongPollKey.BACKEND_CONFIGS: self._update_backend_configs,\n        })\n </s> add         self.loop = asyncio.get_event_loop()\n        self.long_poll_client = LongPollClient(\n            controller_handle,\n            {\n                (LongPollNamespace.BACKEND_CONFIGS, self.backend_tag): self.\n                _update_backend_configs,\n            },\n            call_in_event_loop=self.loop,\n        )", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/router.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>             self.num_queued_queries, tags={\"endpoint\": endpoint})\n <mask>         return assigned_ref\n <mask> \n <mask> \n <mask> class _PendingEndpointFound(Enum):\n <mask>     \"\"\"Enum for the status of pending endpoint registration.\"\"\"\n <mask>     ADDED = 1\n <mask>     REMOVED = 2\n <mask> \n <mask> \n <mask> class Router:\n <mask>     def __init__(self, controller_handle: ActorHandle):\n <mask>         \"\"\"Router process incoming queries: choose backend, and assign replica.\n <mask> \n <mask>         Args:\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove     def __init__(self, backend_tag):\n </s> add     def __init__(\n            self,\n            controller_handle,\n            backend_tag,\n            event_loop: asyncio.AbstractEventLoop,\n    ): </s> remove     def __init__(self, controller_handle, sync: bool):\n </s> add     def __init__(self, controller_handle, sync: bool,\n                 endpoint_tag: EndpointTag): </s> remove         serialized_data = (self.controller_handle, self.sync)\n </s> add         serialized_data = (\n            self.controller_handle,\n            self.sync,\n            self.endpoint_tag,\n        ) </s> add     @property\n    def backend_tags(self):\n        return set(self.traffic_dict.keys()).union(\n            set(self.shadow_dict.keys()))\n </s> remove         self.router = Router(controller_handle)\n\n </s> add         self.endpoint_tag = endpoint_tag </s> remove UpdateStateAsyncCallable = Callable[[Any], Awaitable[None]]\n </s> add UpdateStateCallable = Callable[[Any], None]\nKeyType = Union[str, LongPollNamespace, Tuple[LongPollNamespace, str]]", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/router.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>     for i in done:\n <mask>         assert await i == \"new_val\"\n <mask> \n <mask> \n <mask> async def test_graceful_shutdown(serve_instance, router,\n <mask>                                  mock_controller_with_name):\n <mask>     class KeepInflight:\n <mask>         def __init__(self):\n <mask>             self.events = []\n <mask> \n <mask>         def reconfigure(self, config):\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove     backend_worker = await add_servable_to_router(\n </s> add     backend_worker, router = await add_servable_to_router( </s> remove         router,\n        mock_controller_with_name[0],\n </s> add         *mock_controller_with_name, </s> remove     def _poll_once(self) -> ray.ObjectRef:\n        object_ref = self.host_actor.listen_for_change.remote(\n </s> add     def _poll_next(self):\n        \"\"\"Poll the update. The callback is expected to scheduler another\n        _poll_next call.\n        \"\"\"\n        self._current_ref = self.host_actor.listen_for_change.remote( </s> remove         in_async_loop = asyncio.get_event_loop().is_running\n        assert in_async_loop, \"The client is only available in async context.\"\n        asyncio.get_event_loop().create_task(self._do_long_poll())\n </s> add     def update_key_listeners(\n            self,\n            key_listeners: Dict[KeyType, UpdateStateCallable],\n    ):\n        self.key_lisners = key_listeners\n        self._reset() </s> remove     async def _update_backend_configs(self, backend_configs):\n        # TODO(ilr) remove this loop when we poll per key\n        for backend_tag, config in backend_configs.items():\n            if backend_tag == self.backend_tag:\n                self._update_config(config)\n\n    def _update_config(self, new_config: BackendConfig) -> None:\n </s> add     def _update_backend_configs(self, new_config: BackendConfig) -> None: </s> remove UpdateStateAsyncCallable = Callable[[Any], Awaitable[None]]\n </s> add UpdateStateCallable = Callable[[Any], None]\nKeyType = Union[str, LongPollNamespace, Tuple[LongPollNamespace, str]]", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/tests/test_backend_worker.py"}
{"docstring_tokens": "keep keep keep replace keep replace replace keep", "code_tokens": " <mask>             self.events.append(e)\n <mask>             await e.wait()\n <mask> \n <mask>     backend_worker = await add_servable_to_router(\n <mask>         KeepInflight,\n <mask>         router,\n <mask>         mock_controller_with_name[0],\n <mask>         backend_config=BackendConfig(\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove async def test_graceful_shutdown(serve_instance, router,\n                                 mock_controller_with_name):\n </s> add async def test_graceful_shutdown(serve_instance, mock_controller_with_name): </s> remove         return object_ref\n\n    async def _do_long_poll(self):\n        while True:\n            try:\n                updates: Dict[str, UpdatedObject] = await self._poll_once()\n                logger.debug(\"LongPollClient received updates for keys: \"\n                             f\"{list(updates.keys())}.\")\n                for key, update in updates.items():\n                    self.object_snapshots[key] = update.object_snapshot\n                    self.snapshot_ids[key] = update.snapshot_id\n                    # NOTE(simon):\n                    # This blocks the loop from doing another poll. Consider\n                    # use loop.create_task here or poll first then call the\n                    # callbacks.\n                    callback = self.key_listeners[key]\n                    await callback(update.object_snapshot)\n            except ray.exceptions.RayActorError:\n                # This can happen during shutdown where the controller is\n                # intentionally killed, the client should just gracefully\n                # exit.\n                logger.debug(\"LongPollClient failed to connect to host. \"\n                             \"Shutting down.\")\n                break\n </s> add         self._current_ref._on_completed(\n            lambda update: self._process_update(update))\n\n    def _process_update(self, updates: Dict[str, UpdatedObject]):\n        if isinstance(updates, (ray.exceptions.RayActorError)):\n            # This can happen during shutdown where the controller is\n            # intentionally killed, the client should just gracefully\n            # exit.\n            logger.debug(\"LongPollClient failed to connect to host. \"\n                         \"Shutting down.\")\n            return\n\n        if isinstance(updates, (ray.exceptions.RayTaskError)):\n            # This can happen during shutdown where the controller doesn't\n            # contain this key, we will just repull.\n            # NOTE(simon): should we repull or just wait in the long poll\n            # host?\n            if not isinstance(updates.as_instanceof_cause(), ValueError):\n                logger.error(\"LongPollHost errored\\n\" + updates.traceback_str)\n            self._poll_next()\n            return\n\n        # Before we process the updates and calling callbacks, kick off\n        # another poll so we can pipeline the polling and processing.\n        self._poll_next()\n        logger.debug(\"LongPollClient received updates for keys: \"\n                     f\"{list(updates.keys())}.\")\n        for key, update in updates.items():\n            self.object_snapshots[key] = update.object_snapshot\n            self.snapshot_ids[key] = update.snapshot_id\n            callback = self.key_listeners[key]\n            if self.event_loop is None:\n                callback(update.object_snapshot)\n            else:\n                self.event_loop.call_soon_threadsafe(\n                    lambda: callback(update.object_snapshot)) </s> remove         self.router = Router(controller_handle)\n\n </s> add         self.endpoint_tag = endpoint_tag </s> remove             self.async_loop = create_or_get_async_loop_in_thread()\n            asyncio.run_coroutine_threadsafe(\n                self.router.setup_in_async_loop(),\n                self.async_loop,\n            )\n </s> add             self._async_loop = create_or_get_async_loop_in_thread() </s> remove             handle = RayServeHandle(\n                self._get_proxied_router(sync=sync), endpoint_name)\n </s> add             handle = RayServeHandle(router, endpoint_name)", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/tests/test_backend_worker.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>             user_config={\"release\": False}))\n <mask> \n <mask>     query_param = make_request_param()\n <mask> \n <mask>     refs = [(await router.assign_request.remote(query_param))\n <mask>             for _ in range(6)]\n <mask> \n <mask>     shutdown_ref = backend_worker.drain_pending_queries.remote()\n <mask> \n <mask>     with pytest.raises(ray.exceptions.GetTimeoutError):\n <mask>         # Shutdown should block because there are still inflight queries.\n </s> [Serve] Change to use per-key long poll & synchronous callback (#14335) </s> remove         router,\n        mock_controller_with_name[0],\n </s> add         *mock_controller_with_name, </s> add         # NOTE(simon): this extra layer of router seems unnecessary\n        # BUT it's needed still because of the shared asyncio thread.\n        router = self._get_proxied_router(sync=sync, endpoint=endpoint_name) </s> remove         self._sync_proxied_router = None\n        self._async_proxied_router = None\n </s> add         # TODO(simon): remove this when dropping router object and making\n        # ServeHandle sync only.\n        self._cached_routers = dict() </s> remove     async def listen_for_change(self, keys_to_snapshot_ids: Dict[str, int]\n                                ) -> Dict[str, UpdatedObject]:\n </s> add     async def listen_for_change(\n            self,\n            keys_to_snapshot_ids: Dict[KeyType, int],\n    ) -> Dict[KeyType, UpdatedObject]: </s> add         # Inform the routers and backend replicas about config changes.\n        # TODO(edoakes): this should only happen if we change something other\n        # than num_replicas.\n        self._notify_backend_configs_changed(backend_tag) </s> remove         return object_ref\n\n    async def _do_long_poll(self):\n        while True:\n            try:\n                updates: Dict[str, UpdatedObject] = await self._poll_once()\n                logger.debug(\"LongPollClient received updates for keys: \"\n                             f\"{list(updates.keys())}.\")\n                for key, update in updates.items():\n                    self.object_snapshots[key] = update.object_snapshot\n                    self.snapshot_ids[key] = update.snapshot_id\n                    # NOTE(simon):\n                    # This blocks the loop from doing another poll. Consider\n                    # use loop.create_task here or poll first then call the\n                    # callbacks.\n                    callback = self.key_listeners[key]\n                    await callback(update.object_snapshot)\n            except ray.exceptions.RayActorError:\n                # This can happen during shutdown where the controller is\n                # intentionally killed, the client should just gracefully\n                # exit.\n                logger.debug(\"LongPollClient failed to connect to host. \"\n                             \"Shutting down.\")\n                break\n </s> add         self._current_ref._on_completed(\n            lambda update: self._process_update(update))\n\n    def _process_update(self, updates: Dict[str, UpdatedObject]):\n        if isinstance(updates, (ray.exceptions.RayActorError)):\n            # This can happen during shutdown where the controller is\n            # intentionally killed, the client should just gracefully\n            # exit.\n            logger.debug(\"LongPollClient failed to connect to host. \"\n                         \"Shutting down.\")\n            return\n\n        if isinstance(updates, (ray.exceptions.RayTaskError)):\n            # This can happen during shutdown where the controller doesn't\n            # contain this key, we will just repull.\n            # NOTE(simon): should we repull or just wait in the long poll\n            # host?\n            if not isinstance(updates.as_instanceof_cause(), ValueError):\n                logger.error(\"LongPollHost errored\\n\" + updates.traceback_str)\n            self._poll_next()\n            return\n\n        # Before we process the updates and calling callbacks, kick off\n        # another poll so we can pipeline the polling and processing.\n        self._poll_next()\n        logger.debug(\"LongPollClient received updates for keys: \"\n                     f\"{list(updates.keys())}.\")\n        for key, update in updates.items():\n            self.object_snapshots[key] = update.object_snapshot\n            self.snapshot_ids[key] = update.snapshot_id\n            callback = self.key_listeners[key]\n            if self.event_loop is None:\n                callback(update.object_snapshot)\n            else:\n                self.event_loop.call_soon_threadsafe(\n                    lambda: callback(update.object_snapshot))", "html_url": "https://github.com/ray-project/ray/commit/7e63090ca989a4771c90d1b9afa201e30d4702a1", "file_name": "python/ray/serve/tests/test_backend_worker.py"}
{"docstring_tokens": "keep keep keep replace replace replace replace replace replace replace replace keep replace replace keep keep keep", "code_tokens": " <mask>             for row in block.iter_rows():\n <mask>                 yield row\n <mask> \n <mask>     def to_batch_iterators(\n <mask>             self,\n <mask>             num_shards: int,\n <mask>             batch_size: int = None,\n <mask>             output_location_prefs: List[Any] = None,\n <mask>             batch_format: str = \"pandas\",\n <mask>             repeatable: bool = False) -> List[Iterator[BatchType]]:\n <mask>         \"\"\"Return a list of distributed iterators over record batches.\n <mask> \n <mask>         This returns a list of iterators that can be passed to Ray tasks\n <mask>         and actors, and used to read the dataset records in parallel.\n <mask> \n <mask>         Time complexity: O(1)\n <mask> \n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove             num_shards: Number of iterators to return.\n </s> add             prefetch_blocks: The number of blocks to prefetch ahead of the\n                current block during the scan. </s> remove             output_location_prefs: A list of Ray actor handles of size\n                ``num_shards``. The system will try to co-locate the objects\n                given to the ith iterator with the ith actor to maximize data\n                locality.\n </s> add  </s> remove             repeatable: Whether each iterator should loop over data forever\n                or stop after reading all records of the shard.\n </s> add  </s> remove     def to_pandas(self) -> Iterator[ObjectRef[\"pandas.DataFrame\"]]:\n </s> add     def to_pandas(self) -> List[ObjectRef[\"pandas.DataFrame\"]]: </s> remove     return Dataset(blocks)\n </s> add     return Dataset(BlockList(blocks, metadata))", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep replace replace replace replace keep keep keep keep", "code_tokens": " <mask> \n <mask>         Time complexity: O(1)\n <mask> \n <mask>         Args:\n <mask>             num_shards: Number of iterators to return.\n <mask>             batch_size: Record batch size, or None to let the system pick.\n <mask>             output_location_prefs: A list of Ray actor handles of size\n <mask>                 ``num_shards``. The system will try to co-locate the objects\n <mask>                 given to the ith iterator with the ith actor to maximize data\n <mask>                 locality.\n <mask>             batch_format: Specify \"pandas\" to select ``pandas.DataFrame`` as\n <mask>                 the batch format, or \"pyarrow\" to select ``pyarrow.Table``.\n <mask>             repeatable: Whether each iterator should loop over data forever\n <mask>                 or stop after reading all records of the shard.\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove             repeatable: Whether each iterator should loop over data forever\n                or stop after reading all records of the shard.\n </s> add  </s> remove         This returns a list of iterators that can be passed to Ray tasks\n        and actors, and used to read the dataset records in parallel.\n </s> add         Examples:\n            >>> for pandas_df in ray.data.range(1000000).iter_batches():\n            ...     print(pandas_df) </s> remove     def to_batch_iterators(\n            self,\n            num_shards: int,\n            batch_size: int = None,\n            output_location_prefs: List[Any] = None,\n            batch_format: str = \"pandas\",\n            repeatable: bool = False) -> List[Iterator[BatchType]]:\n        \"\"\"Return a list of distributed iterators over record batches.\n </s> add     def iter_batches(self,\n                     prefetch_blocks: int = 0,\n                     batch_size: int = None,\n                     batch_format: str = \"pandas\") -> Iterator[BatchType]:\n        \"\"\"Return a local batched iterator over the dataset. </s> remove     def to_pandas(self) -> Iterator[ObjectRef[\"pandas.DataFrame\"]]:\n </s> add     def to_pandas(self) -> List[ObjectRef[\"pandas.DataFrame\"]]: </s> remove     @ray.remote\n </s> add     @ray.remote(num_returns=2)", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/dataset.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>                 given to the ith iterator with the ith actor to maximize data\n <mask>                 locality.\n <mask>             batch_format: Specify \"pandas\" to select ``pandas.DataFrame`` as\n <mask>                 the batch format, or \"pyarrow\" to select ``pyarrow.Table``.\n <mask>             repeatable: Whether each iterator should loop over data forever\n <mask>                 or stop after reading all records of the shard.\n <mask> \n <mask>         Returns:\n <mask>             A list of iterators over record batches.\n <mask>         \"\"\"\n <mask>         raise NotImplementedError  # P1\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove             output_location_prefs: A list of Ray actor handles of size\n                ``num_shards``. The system will try to co-locate the objects\n                given to the ith iterator with the ith actor to maximize data\n                locality.\n </s> add  </s> remove             num_shards: Number of iterators to return.\n </s> add             prefetch_blocks: The number of blocks to prefetch ahead of the\n                current block during the scan. </s> remove         This returns a list of iterators that can be passed to Ray tasks\n        and actors, and used to read the dataset records in parallel.\n </s> add         Examples:\n            >>> for pandas_df in ray.data.range(1000000).iter_batches():\n            ...     print(pandas_df) </s> remove     def to_batch_iterators(\n            self,\n            num_shards: int,\n            batch_size: int = None,\n            output_location_prefs: List[Any] = None,\n            batch_format: str = \"pandas\",\n            repeatable: bool = False) -> List[Iterator[BatchType]]:\n        \"\"\"Return a list of distributed iterators over record batches.\n </s> add     def iter_batches(self,\n                     prefetch_blocks: int = 0,\n                     batch_size: int = None,\n                     batch_format: str = \"pandas\") -> Iterator[BatchType]:\n        \"\"\"Return a local batched iterator over the dataset. </s> remove     def to_pandas(self) -> Iterator[ObjectRef[\"pandas.DataFrame\"]]:\n </s> add     def to_pandas(self) -> List[ObjectRef[\"pandas.DataFrame\"]]: </s> remove     import numpy as np\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             A Modin dataframe created from this dataset.\n <mask>         \"\"\"\n <mask>         raise NotImplementedError  # P1\n <mask> \n <mask>     def to_pandas(self) -> Iterator[ObjectRef[\"pandas.DataFrame\"]]:\n <mask>         \"\"\"Convert this dataset into a set of Pandas dataframes.\n <mask> \n <mask>         This is only supported for datasets convertible to Arrow records.\n <mask> \n <mask>         Time complexity: O(1)\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove         This returns a list of iterators that can be passed to Ray tasks\n        and actors, and used to read the dataset records in parallel.\n </s> add         Examples:\n            >>> for pandas_df in ray.data.range(1000000).iter_batches():\n            ...     print(pandas_df) </s> remove     def to_batch_iterators(\n            self,\n            num_shards: int,\n            batch_size: int = None,\n            output_location_prefs: List[Any] = None,\n            batch_format: str = \"pandas\",\n            repeatable: bool = False) -> List[Iterator[BatchType]]:\n        \"\"\"Return a list of distributed iterators over record batches.\n </s> add     def iter_batches(self,\n                     prefetch_blocks: int = 0,\n                     batch_size: int = None,\n                     batch_format: str = \"pandas\") -> Iterator[BatchType]:\n        \"\"\"Return a local batched iterator over the dataset. </s> remove             repeatable: Whether each iterator should loop over data forever\n                or stop after reading all records of the shard.\n </s> add  </s> remove             num_shards: Number of iterators to return.\n </s> add             prefetch_blocks: The number of blocks to prefetch ahead of the\n                current block during the scan. </s> remove     @ray.remote\n </s> add     @ray.remote(num_returns=2) </s> remove     def slice(self, start: int, end: int) -> \"Block[T]\":\n </s> add     def slice(self, start: int, end: int, copy: bool) -> \"Block[T]\":", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         \"\"\"\n <mask>         raise NotImplementedError  # P2\n <mask> \n <mask>     def __repr__(self) -> str:\n <mask>         return \"Dataset({} blocks)\".format(len(self._blocks))\n <mask> \n <mask>     def __str__(self) -> str:\n <mask>         return repr(self)\n <mask> \n <mask>     def _block_sizes(self) -> List[int]:\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add     def schema(self) -> Any:\n        raise NotImplementedError\n\n    def get_metadata(self, input_files: List[str]) -> BlockMetadata:\n        return BlockMetadata(\n            num_rows=self.num_rows(),\n            size_bytes=self.size_bytes(),\n            schema=self.schema(),\n            input_files=input_files)\n </s> remove     def slice(self, start: int, end: int) -> \"Block[T]\":\n </s> add     def slice(self, start: int, end: int, copy: bool) -> \"Block[T]\": </s> remove     def to_pandas(self) -> Iterator[ObjectRef[\"pandas.DataFrame\"]]:\n </s> add     def to_pandas(self) -> List[ObjectRef[\"pandas.DataFrame\"]]: </s> remove class ListBlockBuilder(BlockBuilder[T]):\n </s> add class SimpleBlockBuilder(BlockBuilder[T]): </s> add class BlockMetadata:\n    def __init__(self, *, num_rows: Optional[int], size_bytes: Optional[int],\n                 schema: Union[type, \"pyarrow.lib.Schema\"],\n                 input_files: List[str]):\n        if input_files is None:\n            input_files = []\n        self.num_rows: Optional[int] = num_rows\n        self.size_bytes: Optional[int] = size_bytes\n        self.schema: Optional[Any] = schema\n        self.input_files: List[str] = input_files\n\n </s> remove     def apply(self, fn: Any, blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add     def apply(self, fn: Any,\n              blocks: Iterable[Block[T]]) -> Iterable[ObjectRef[Block]]:", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/dataset.py"}
{"docstring_tokens": "keep replace keep keep keep keep keep", "code_tokens": " <mask> import collections\n <mask> from typing import Iterator, Union, Tuple, Any, TypeVar, TYPE_CHECKING\n <mask> \n <mask> try:\n <mask>     import pyarrow\n <mask> except ImportError:\n <mask>     pyarrow = None\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove from typing import List, Any, Union, Optional, Tuple, TYPE_CHECKING\n </s> add import functools\nimport builtins\nfrom typing import List, Any, Union, Optional, Tuple, Callable, TYPE_CHECKING </s> remove import builtins\n </s> add  </s> remove from typing import TypeVar, List, Generic, Iterator, TYPE_CHECKING\n </s> add from typing import TypeVar, List, Generic, Iterator, Any, Union, Optional, \\\n    TYPE_CHECKING </s> remove     ListBlockBuilder\n </s> add     SimpleBlockBuilder </s> remove from typing import TypeVar, List, Any\n </s> add from typing import TypeVar, Iterable, Any </s> remove from ray.experimental.data.impl.block import Block, ObjectRef\n </s> add from ray.experimental.data.impl.block import Block, BlockMetadata", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/arrow_block.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> except ImportError:\n <mask>     pyarrow = None\n <mask> \n <mask> from ray.experimental.data.impl.block import Block, BlockBuilder, \\\n <mask>     ListBlockBuilder\n <mask> \n <mask> if TYPE_CHECKING:\n <mask>     import pandas\n <mask> \n <mask> T = TypeVar(\"T\")\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add     import pyarrow </s> remove from ray.experimental.data.impl.block import Block, ObjectRef\n </s> add from ray.experimental.data.impl.block import Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import Block, ObjectRef\n </s> add from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\nfrom ray.experimental.data.impl.block_list import BlockList </s> add from ray.experimental.data.impl.block_list import BlockList </s> remove from typing import Iterator, Union, Tuple, Any, TypeVar, TYPE_CHECKING\n </s> add from typing import Iterator, List, Union, Tuple, Any, TypeVar, TYPE_CHECKING </s> add from ray.experimental.data.impl.arrow_block import ArrowBlock, ArrowRow\nfrom ray.experimental.data.impl.block import ObjectRef, SimpleBlock, Block, \\\n    BlockMetadata\nfrom ray.experimental.data.impl.block_list import BlockList\nfrom ray.experimental.data.impl.lazy_block_list import LazyBlockList", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/arrow_block.py"}
{"docstring_tokens": "keep keep keep replace keep replace keep keep keep keep", "code_tokens": " <mask>                     check.build()\n <mask>                     self._builder = ArrowBlockBuilder()\n <mask>                 except (TypeError, pyarrow.lib.ArrowInvalid):\n <mask>                     self._builder = ListBlockBuilder()\n <mask>             else:\n <mask>                 self._builder = ListBlockBuilder()\n <mask>         self._builder.add(item)\n <mask> \n <mask>     def add_block(self, block: Block[T]) -> None:\n <mask>         if self._builder is None:\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     def add_block(self, block: \"ListBlock[T]\") -> None:\n </s> add     def add_block(self, block: \"SimpleBlock[T]\") -> None: </s> remove     def build(self) -> \"ListBlock[T]\":\n        return ListBlock(self._items)\n </s> add     def build(self) -> \"SimpleBlock[T]\":\n        return SimpleBlock(self._items) </s> add         self._tables: List[\"pyarrow.Table\"] = [] </s> remove         other_cols = block._table.to_pydict()\n        for k, vv in other_cols.items():\n            self._columns[k].extend(vv)\n </s> add         self._tables.append(block._table) </s> remove         return \"Dataset({} blocks)\".format(len(self._blocks))\n </s> add         try:\n            schema = self.schema()\n        except ValueError:\n            schema = \"Unknown schema\"\n        if hasattr(schema, \"names\"):\n            schema_str = []\n            for n, t in zip(schema.names, schema.types):\n                if hasattr(t, \"__name__\"):\n                    t = t.__name__\n                schema_str.append(\"{}: {}\".format(n, t))\n            schema_str = \", \".join(schema_str)\n            schema_str = \"{\" + schema_str + \"}\"\n        else:\n            schema_str = str(schema)\n        count = self._meta_count()\n        if count is None:\n            count = \"?\"\n        return \"Dataset(num_rows={}, num_blocks={}, schema={})\".format(\n            count, len(self._blocks), schema_str)", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/arrow_block.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>     def __init__(self):\n <mask>         if pyarrow is None:\n <mask>             raise ImportError(\"Run `pip install pyarrow` for Arrow support\")\n <mask>         self._columns = collections.defaultdict(list)\n <mask> \n <mask>     def add(self, item: Union[dict, ArrowRow]) -> None:\n <mask>         if isinstance(item, ArrowRow):\n <mask>             item = item.as_pydict()\n <mask>         if not isinstance(item, dict):\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove class ListBlockBuilder(BlockBuilder[T]):\n </s> add class SimpleBlockBuilder(BlockBuilder[T]): </s> remove     def add_block(self, block: \"ListBlock[T]\") -> None:\n </s> add     def add_block(self, block: \"SimpleBlock[T]\") -> None: </s> remove                 self._builder = ListBlockBuilder()\n </s> add                 self._builder = SimpleBlockBuilder() </s> remove         return ArrowBlock(pyarrow.Table.from_pydict(self._columns))\n </s> add         if self._columns:\n            tables = [pyarrow.Table.from_pydict(self._columns)]\n        else:\n            tables = []\n        tables.extend(self._tables)\n        if len(tables) > 1:\n            return ArrowBlock(pyarrow.concat_tables(tables))\n        elif len(tables) > 0:\n            return ArrowBlock(tables[0])\n        else:\n            return ArrowBlock(pyarrow.Table.from_pydict({})) </s> add class BlockMetadata:\n    def __init__(self, *, num_rows: Optional[int], size_bytes: Optional[int],\n                 schema: Union[type, \"pyarrow.lib.Schema\"],\n                 input_files: List[str]):\n        if input_files is None:\n            input_files = []\n        self.num_rows: Optional[int] = num_rows\n        self.size_bytes: Optional[int] = size_bytes\n        self.schema: Optional[Any] = schema\n        self.input_files: List[str] = input_files\n\n </s> remove         return \"Dataset({} blocks)\".format(len(self._blocks))\n </s> add         try:\n            schema = self.schema()\n        except ValueError:\n            schema = \"Unknown schema\"\n        if hasattr(schema, \"names\"):\n            schema_str = []\n            for n, t in zip(schema.names, schema.types):\n                if hasattr(t, \"__name__\"):\n                    t = t.__name__\n                schema_str.append(\"{}: {}\".format(n, t))\n            schema_str = \", \".join(schema_str)\n            schema_str = \"{\" + schema_str + \"}\"\n        else:\n            schema_str = str(schema)\n        count = self._meta_count()\n        if count is None:\n            count = \"?\"\n        return \"Dataset(num_rows={}, num_blocks={}, schema={})\".format(\n            count, len(self._blocks), schema_str)", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/arrow_block.py"}
{"docstring_tokens": "keep replace replace replace keep keep replace keep keep", "code_tokens": " <mask>     def add_block(self, block: \"ArrowBlock[T]\") -> None:\n <mask>         other_cols = block._table.to_pydict()\n <mask>         for k, vv in other_cols.items():\n <mask>             self._columns[k].extend(vv)\n <mask> \n <mask>     def build(self) -> \"ArrowBlock[T]\":\n <mask>         return ArrowBlock(pyarrow.Table.from_pydict(self._columns))\n <mask> \n <mask> \n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     def add_block(self, block: \"ListBlock[T]\") -> None:\n </s> add     def add_block(self, block: \"SimpleBlock[T]\") -> None: </s> remove     def build(self) -> \"ListBlock[T]\":\n        return ListBlock(self._items)\n </s> add     def build(self) -> \"SimpleBlock[T]\":\n        return SimpleBlock(self._items) </s> remove                 self._builder = ListBlockBuilder()\n </s> add                 self._builder = SimpleBlockBuilder() </s> remove                     self._builder = ListBlockBuilder()\n </s> add                     self._builder = SimpleBlockBuilder() </s> remove     def slice(self, start: int, end: int) -> \"ArrowBlock[T]\":\n        return ArrowBlock(self._table.slice(start, end - start))\n </s> add     def slice(self, start: int, end: int, copy: bool) -> \"ArrowBlock[T]\":\n        view = self._table.slice(start, end - start)\n        if copy:\n            # TODO(ekl) there must be a cleaner way to force a copy of a table.\n            copy = [c.to_pandas() for c in view.itercolumns()]\n            return ArrowBlock(\n                pyarrow.Table.from_arrays(copy, schema=self._table.schema))\n        else:\n            return ArrowBlock(view)\n\n    def schema(self) -> \"pyarrow.lib.Schema\":\n        return self._table.schema", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/arrow_block.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>                 raise StopIteration\n <mask> \n <mask>         return Iter()\n <mask> \n <mask>     def slice(self, start: int, end: int) -> \"ArrowBlock[T]\":\n <mask>         return ArrowBlock(self._table.slice(start, end - start))\n <mask> \n <mask>     def to_pandas(self) -> \"pandas.DataFrame\":\n <mask>         return self._table.to_pandas()\n <mask> \n <mask>     def num_rows(self) -> int:\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     def slice(self, start: int, end: int) -> \"Block[T]\":\n </s> add     def slice(self, start: int, end: int, copy: bool) -> \"Block[T]\": </s> remove     def slice(self, start: int, end: int) -> \"ListBlock[T]\":\n        return ListBlock(self._items[start:end])\n </s> add     def slice(self, start: int, end: int, copy: bool) -> \"SimpleBlock[T]\":\n        view = self._items[start:end]\n        if copy:\n            view = view.copy()\n        return SimpleBlock(view) </s> remove class ListBlock(Block):\n </s> add class SimpleBlock(Block): </s> add class BlockMetadata:\n    def __init__(self, *, num_rows: Optional[int], size_bytes: Optional[int],\n                 schema: Union[type, \"pyarrow.lib.Schema\"],\n                 input_files: List[str]):\n        if input_files is None:\n            input_files = []\n        self.num_rows: Optional[int] = num_rows\n        self.size_bytes: Optional[int] = size_bytes\n        self.schema: Optional[Any] = schema\n        self.input_files: List[str] = input_files\n\n </s> remove     def builder() -> ListBlockBuilder[T]:\n        return ListBlockBuilder()\n </s> add     def builder() -> SimpleBlockBuilder[T]:\n        return SimpleBlockBuilder() </s> remove         blocks.append(gen_block.remote(i, min(block_size, n - i)))\n </s> add         def make_call(start: int, count: int) -> ObjectRef[Block]:\n            return lambda: gen_block.remote(start, count)\n\n        count = min(block_size, n - i)\n        calls.append(make_call(i, count))\n        metadata.append(\n            BlockMetadata(\n                num_rows=count,\n                size_bytes=8 * count,\n                schema=int,\n                input_files=None))", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/arrow_block.py"}
{"docstring_tokens": "keep replace keep keep keep keep keep", "code_tokens": " <mask> import sys\n <mask> from typing import TypeVar, List, Generic, Iterator, TYPE_CHECKING\n <mask> \n <mask> if TYPE_CHECKING:\n <mask>     import pandas\n <mask> \n <mask> # TODO(ekl) shouldn't Ray provide an ObjectRef type natively?\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add     import pyarrow </s> remove from typing import Iterator, Union, Tuple, Any, TypeVar, TYPE_CHECKING\n </s> add from typing import Iterator, List, Union, Tuple, Any, TypeVar, TYPE_CHECKING </s> remove from typing import List, Any, Union, Optional, Tuple, TYPE_CHECKING\n </s> add import functools\nimport builtins\nfrom typing import List, Any, Union, Optional, Tuple, Callable, TYPE_CHECKING </s> remove from typing import TypeVar, List, Any\n </s> add from typing import TypeVar, Iterable, Any </s> remove import builtins\n </s> add  </s> remove from ray.experimental.data.impl.block import Block, ObjectRef\n </s> add from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\nfrom ray.experimental.data.impl.block_list import BlockList", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/block.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask> \n <mask> if TYPE_CHECKING:\n <mask>     import pandas\n <mask> \n <mask> # TODO(ekl) shouldn't Ray provide an ObjectRef type natively?\n <mask> ObjectRef = List\n <mask> T = TypeVar(\"T\")\n <mask> \n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove from typing import TypeVar, List, Generic, Iterator, TYPE_CHECKING\n </s> add from typing import TypeVar, List, Generic, Iterator, Any, Union, Optional, \\\n    TYPE_CHECKING </s> remove from ray.experimental.data.impl.block import Block, ObjectRef\n </s> add from ray.experimental.data.impl.block import Block, BlockMetadata </s> remove     ListBlockBuilder\n </s> add     SimpleBlockBuilder </s> remove from ray.experimental.data.impl.block import Block, ObjectRef\n </s> add from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\nfrom ray.experimental.data.impl.block_list import BlockList </s> add     read_tasks = [[] for _ in builtins.range(parallelism)]\n    # TODO(ekl) support reading row groups (maybe as an option)\n    for i, piece in enumerate(pq_ds.pieces):\n        read_tasks[i % len(read_tasks)].append(piece)\n    nonempty_tasks = [r for r in read_tasks if r] </s> add from ray.experimental.data.impl.block_list import BlockList", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/block.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask> \n <mask> \n <mask> class Block(Generic[T]):\n <mask>     def num_rows(self) -> int:\n <mask>         raise NotImplementedError\n <mask> \n <mask>     def iter_rows(self) -> Iterator[T]:\n <mask>         raise NotImplementedError\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     def slice(self, start: int, end: int) -> \"Block[T]\":\n </s> add     def slice(self, start: int, end: int, copy: bool) -> \"Block[T]\": </s> add     def schema(self) -> Any:\n        raise NotImplementedError\n\n    def get_metadata(self, input_files: List[str]) -> BlockMetadata:\n        return BlockMetadata(\n            num_rows=self.num_rows(),\n            size_bytes=self.size_bytes(),\n            schema=self.schema(),\n            input_files=input_files)\n </s> remove class ListBlockBuilder(BlockBuilder[T]):\n </s> add class SimpleBlockBuilder(BlockBuilder[T]): </s> remove     def apply(self, fn: Any, blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add     def apply(self, fn: Any,\n              blocks: Iterable[Block[T]]) -> Iterable[ObjectRef[Block]]: </s> remove class ListBlock(Block):\n </s> add class SimpleBlock(Block): </s> remove     def to_pandas(self) -> Iterator[ObjectRef[\"pandas.DataFrame\"]]:\n </s> add     def to_pandas(self) -> List[ObjectRef[\"pandas.DataFrame\"]]:", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/block.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     def iter_rows(self) -> Iterator[T]:\n <mask>         raise NotImplementedError\n <mask> \n <mask>     def slice(self, start: int, end: int) -> \"Block[T]\":\n <mask>         raise NotImplementedError\n <mask> \n <mask>     def to_pandas(self) -> \"pandas.DataFrame\":\n <mask>         raise NotImplementedError\n <mask> \n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     def slice(self, start: int, end: int) -> \"ListBlock[T]\":\n        return ListBlock(self._items[start:end])\n </s> add     def slice(self, start: int, end: int, copy: bool) -> \"SimpleBlock[T]\":\n        view = self._items[start:end]\n        if copy:\n            view = view.copy()\n        return SimpleBlock(view) </s> add     def schema(self) -> Any:\n        raise NotImplementedError\n\n    def get_metadata(self, input_files: List[str]) -> BlockMetadata:\n        return BlockMetadata(\n            num_rows=self.num_rows(),\n            size_bytes=self.size_bytes(),\n            schema=self.schema(),\n            input_files=input_files)\n </s> add class BlockMetadata:\n    def __init__(self, *, num_rows: Optional[int], size_bytes: Optional[int],\n                 schema: Union[type, \"pyarrow.lib.Schema\"],\n                 input_files: List[str]):\n        if input_files is None:\n            input_files = []\n        self.num_rows: Optional[int] = num_rows\n        self.size_bytes: Optional[int] = size_bytes\n        self.schema: Optional[Any] = schema\n        self.input_files: List[str] = input_files\n\n </s> remove     def slice(self, start: int, end: int) -> \"ArrowBlock[T]\":\n        return ArrowBlock(self._table.slice(start, end - start))\n </s> add     def slice(self, start: int, end: int, copy: bool) -> \"ArrowBlock[T]\":\n        view = self._table.slice(start, end - start)\n        if copy:\n            # TODO(ekl) there must be a cleaner way to force a copy of a table.\n            copy = [c.to_pandas() for c in view.itercolumns()]\n            return ArrowBlock(\n                pyarrow.Table.from_arrays(copy, schema=self._table.schema))\n        else:\n            return ArrowBlock(view)\n\n    def schema(self) -> \"pyarrow.lib.Schema\":\n        return self._table.schema </s> remove class ListBlockBuilder(BlockBuilder[T]):\n </s> add class SimpleBlockBuilder(BlockBuilder[T]): </s> remove     def to_pandas(self) -> Iterator[ObjectRef[\"pandas.DataFrame\"]]:\n </s> add     def to_pandas(self) -> List[ObjectRef[\"pandas.DataFrame\"]]:", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/block.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>         raise NotImplementedError\n <mask> \n <mask>     @staticmethod\n <mask>     def builder() -> BlockBuilder[T]:\n <mask>         raise NotImplementedError\n <mask> \n <mask> \n <mask> class SimpleBlockBuilder(BlockBuilder[T]):\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove class ListBlockBuilder(BlockBuilder[T]):\n </s> add class SimpleBlockBuilder(BlockBuilder[T]): </s> remove     def slice(self, start: int, end: int) -> \"Block[T]\":\n </s> add     def slice(self, start: int, end: int, copy: bool) -> \"Block[T]\": </s> add class BlockMetadata:\n    def __init__(self, *, num_rows: Optional[int], size_bytes: Optional[int],\n                 schema: Union[type, \"pyarrow.lib.Schema\"],\n                 input_files: List[str]):\n        if input_files is None:\n            input_files = []\n        self.num_rows: Optional[int] = num_rows\n        self.size_bytes: Optional[int] = size_bytes\n        self.schema: Optional[Any] = schema\n        self.input_files: List[str] = input_files\n\n </s> remove     def apply(self, fn: Any, blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add     def apply(self, fn: Any,\n              blocks: Iterable[Block[T]]) -> Iterable[ObjectRef[Block]]: </s> remove     def builder() -> ListBlockBuilder[T]:\n        return ListBlockBuilder()\n </s> add     def builder() -> SimpleBlockBuilder[T]:\n        return SimpleBlockBuilder() </s> remove     def to_pandas(self) -> Iterator[ObjectRef[\"pandas.DataFrame\"]]:\n </s> add     def to_pandas(self) -> List[ObjectRef[\"pandas.DataFrame\"]]:", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/block.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     def builder() -> BlockBuilder[T]:\n <mask>         raise NotImplementedError\n <mask> \n <mask> \n <mask> class ListBlockBuilder(BlockBuilder[T]):\n <mask>     def __init__(self):\n <mask>         self._items = []\n <mask> \n <mask>     def add(self, item: T) -> None:\n <mask>         self._items.append(item)\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     def add_block(self, block: \"ListBlock[T]\") -> None:\n </s> add     def add_block(self, block: \"SimpleBlock[T]\") -> None: </s> add     def schema(self) -> Any:\n        raise NotImplementedError\n\n    def get_metadata(self, input_files: List[str]) -> BlockMetadata:\n        return BlockMetadata(\n            num_rows=self.num_rows(),\n            size_bytes=self.size_bytes(),\n            schema=self.schema(),\n            input_files=input_files)\n </s> add         self._tables: List[\"pyarrow.Table\"] = [] </s> add class BlockMetadata:\n    def __init__(self, *, num_rows: Optional[int], size_bytes: Optional[int],\n                 schema: Union[type, \"pyarrow.lib.Schema\"],\n                 input_files: List[str]):\n        if input_files is None:\n            input_files = []\n        self.num_rows: Optional[int] = num_rows\n        self.size_bytes: Optional[int] = size_bytes\n        self.schema: Optional[Any] = schema\n        self.input_files: List[str] = input_files\n\n </s> remove     def build(self) -> \"ListBlock[T]\":\n        return ListBlock(self._items)\n </s> add     def build(self) -> \"SimpleBlock[T]\":\n        return SimpleBlock(self._items) </s> remove     def apply(self, fn: Any, blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add     def apply(self, fn: Any,\n              blocks: Iterable[Block[T]]) -> Iterable[ObjectRef[Block]]:", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/block.py"}
{"docstring_tokens": "keep keep keep replace keep keep keep keep keep keep keep keep replace replace keep keep keep", "code_tokens": " <mask>     def add(self, item: T) -> None:\n <mask>         self._items.append(item)\n <mask> \n <mask>     def add_block(self, block: \"ListBlock[T]\") -> None:\n <mask>         self._items.extend(block._items)\n <mask> \n <mask>     def build(self) -> \"ListBlock[T]\":\n <mask>         return ListBlock(self._items)\n <mask> \n <mask>     def add_block(self, block: \"ListBlock[T]\") -> None:\n <mask>         self._items.extend(block._items)\n <mask> \n <mask>     def build(self) -> \"ListBlock[T]\":\n <mask>         return ListBlock(self._items)\n <mask> \n <mask> \n <mask> class ListBlock(Block):\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove class ListBlock(Block):\n </s> add class SimpleBlock(Block): </s> remove class ListBlockBuilder(BlockBuilder[T]):\n </s> add class SimpleBlockBuilder(BlockBuilder[T]): </s> remove         other_cols = block._table.to_pydict()\n        for k, vv in other_cols.items():\n            self._columns[k].extend(vv)\n </s> add         self._tables.append(block._table) </s> remove                 self._builder = ListBlockBuilder()\n </s> add                 self._builder = SimpleBlockBuilder() </s> remove                     self._builder = ListBlockBuilder()\n </s> add                     self._builder = SimpleBlockBuilder()", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/block.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     def build(self) -> \"ListBlock[T]\":\n <mask>         return ListBlock(self._items)\n <mask> \n <mask> \n <mask> class ListBlock(Block):\n <mask>     def __init__(self, items):\n <mask>         self._items = items\n <mask> \n <mask>     def num_rows(self) -> int:\n <mask>         return len(self._items)\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     def build(self) -> \"ListBlock[T]\":\n        return ListBlock(self._items)\n </s> add     def build(self) -> \"SimpleBlock[T]\":\n        return SimpleBlock(self._items) </s> add class BlockMetadata:\n    def __init__(self, *, num_rows: Optional[int], size_bytes: Optional[int],\n                 schema: Union[type, \"pyarrow.lib.Schema\"],\n                 input_files: List[str]):\n        if input_files is None:\n            input_files = []\n        self.num_rows: Optional[int] = num_rows\n        self.size_bytes: Optional[int] = size_bytes\n        self.schema: Optional[Any] = schema\n        self.input_files: List[str] = input_files\n\n </s> remove     def add_block(self, block: \"ListBlock[T]\") -> None:\n </s> add     def add_block(self, block: \"SimpleBlock[T]\") -> None: </s> remove class ListBlockBuilder(BlockBuilder[T]):\n </s> add class SimpleBlockBuilder(BlockBuilder[T]): </s> remove         return ArrowBlock(pyarrow.Table.from_pydict(self._columns))\n </s> add         if self._columns:\n            tables = [pyarrow.Table.from_pydict(self._columns)]\n        else:\n            tables = []\n        tables.extend(self._tables)\n        if len(tables) > 1:\n            return ArrowBlock(pyarrow.concat_tables(tables))\n        elif len(tables) > 0:\n            return ArrowBlock(tables[0])\n        else:\n            return ArrowBlock(pyarrow.Table.from_pydict({})) </s> remove     def slice(self, start: int, end: int) -> \"ArrowBlock[T]\":\n        return ArrowBlock(self._table.slice(start, end - start))\n </s> add     def slice(self, start: int, end: int, copy: bool) -> \"ArrowBlock[T]\":\n        view = self._table.slice(start, end - start)\n        if copy:\n            # TODO(ekl) there must be a cleaner way to force a copy of a table.\n            copy = [c.to_pandas() for c in view.itercolumns()]\n            return ArrowBlock(\n                pyarrow.Table.from_arrays(copy, schema=self._table.schema))\n        else:\n            return ArrowBlock(view)\n\n    def schema(self) -> \"pyarrow.lib.Schema\":\n        return self._table.schema", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/block.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     def iter_rows(self) -> Iterator[T]:\n <mask>         return iter(self._items)\n <mask> \n <mask>     def slice(self, start: int, end: int) -> \"ListBlock[T]\":\n <mask>         return ListBlock(self._items[start:end])\n <mask> \n <mask>     def to_pandas(self) -> \"pandas.DataFrame\":\n <mask>         import pandas\n <mask>         return pandas.DataFrame(self._items)\n <mask> \n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     def slice(self, start: int, end: int) -> \"Block[T]\":\n </s> add     def slice(self, start: int, end: int, copy: bool) -> \"Block[T]\": </s> remove     def slice(self, start: int, end: int) -> \"ArrowBlock[T]\":\n        return ArrowBlock(self._table.slice(start, end - start))\n </s> add     def slice(self, start: int, end: int, copy: bool) -> \"ArrowBlock[T]\":\n        view = self._table.slice(start, end - start)\n        if copy:\n            # TODO(ekl) there must be a cleaner way to force a copy of a table.\n            copy = [c.to_pandas() for c in view.itercolumns()]\n            return ArrowBlock(\n                pyarrow.Table.from_arrays(copy, schema=self._table.schema))\n        else:\n            return ArrowBlock(view)\n\n    def schema(self) -> \"pyarrow.lib.Schema\":\n        return self._table.schema </s> add class BlockMetadata:\n    def __init__(self, *, num_rows: Optional[int], size_bytes: Optional[int],\n                 schema: Union[type, \"pyarrow.lib.Schema\"],\n                 input_files: List[str]):\n        if input_files is None:\n            input_files = []\n        self.num_rows: Optional[int] = num_rows\n        self.size_bytes: Optional[int] = size_bytes\n        self.schema: Optional[Any] = schema\n        self.input_files: List[str] = input_files\n\n </s> remove     def add_block(self, block: \"ListBlock[T]\") -> None:\n </s> add     def add_block(self, block: \"SimpleBlock[T]\") -> None: </s> remove     def build(self) -> \"ListBlock[T]\":\n        return ListBlock(self._items)\n </s> add     def build(self) -> \"SimpleBlock[T]\":\n        return SimpleBlock(self._items) </s> remove class ListBlock(Block):\n </s> add class SimpleBlock(Block):", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/block.py"}
{"docstring_tokens": "keep add keep keep keep", "code_tokens": " <mask>         return sys.getsizeof(self._items)\n <mask> \n <mask>     @staticmethod\n <mask>     def builder() -> SimpleBlockBuilder[T]:\n <mask>         return SimpleBlockBuilder()\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     def builder() -> ListBlockBuilder[T]:\n        return ListBlockBuilder()\n </s> add     def builder() -> SimpleBlockBuilder[T]:\n        return SimpleBlockBuilder() </s> add     def schema(self) -> Any:\n        raise NotImplementedError\n\n    def get_metadata(self, input_files: List[str]) -> BlockMetadata:\n        return BlockMetadata(\n            num_rows=self.num_rows(),\n            size_bytes=self.size_bytes(),\n            schema=self.schema(),\n            input_files=input_files)\n </s> remove class ListBlockBuilder(BlockBuilder[T]):\n </s> add class SimpleBlockBuilder(BlockBuilder[T]): </s> remove                     self._builder = ListBlockBuilder()\n </s> add                     self._builder = SimpleBlockBuilder() </s> remove                 self._builder = ListBlockBuilder()\n </s> add                 self._builder = SimpleBlockBuilder() </s> remove     def build(self) -> \"ListBlock[T]\":\n        return ListBlock(self._items)\n </s> add     def build(self) -> \"SimpleBlock[T]\":\n        return SimpleBlock(self._items)", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/block.py"}
{"docstring_tokens": "keep keep keep keep replace replace", "code_tokens": " <mask>     def size_bytes(self) -> int:\n <mask>         return sys.getsizeof(self._items)\n <mask> \n <mask>     @staticmethod\n <mask>     def builder() -> ListBlockBuilder[T]:\n <mask>         return ListBlockBuilder()\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add     def schema(self) -> Any:\n        if self._items:\n            return type(self._items[0])\n        else:\n            return None\n </s> add     def schema(self) -> Any:\n        raise NotImplementedError\n\n    def get_metadata(self, input_files: List[str]) -> BlockMetadata:\n        return BlockMetadata(\n            num_rows=self.num_rows(),\n            size_bytes=self.size_bytes(),\n            schema=self.schema(),\n            input_files=input_files)\n </s> remove class ListBlock(Block):\n </s> add class SimpleBlock(Block): </s> remove                     self._builder = ListBlockBuilder()\n </s> add                     self._builder = SimpleBlockBuilder() </s> remove class ListBlockBuilder(BlockBuilder[T]):\n </s> add class SimpleBlockBuilder(BlockBuilder[T]): </s> remove                 self._builder = ListBlockBuilder()\n </s> add                 self._builder = SimpleBlockBuilder()", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/block.py"}
{"docstring_tokens": "replace keep keep replace keep keep keep keep keep", "code_tokens": " <mask> from typing import TypeVar, List, Any\n <mask> \n <mask> import ray\n <mask> from ray.experimental.data.impl.block import Block, ObjectRef\n <mask> from ray.experimental.data.impl.progress_bar import ProgressBar\n <mask> \n <mask> T = TypeVar(\"T\")\n <mask> U = TypeVar(\"U\")\n <mask> \n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove from ray.experimental.data.impl.block import Block, ObjectRef\n </s> add from ray.experimental.data.impl.block import Block, BlockMetadata </s> add from ray.experimental.data.impl.block_list import BlockList </s> remove from typing import TypeVar, List, Generic, Iterator, TYPE_CHECKING\n </s> add from typing import TypeVar, List, Generic, Iterator, Any, Union, Optional, \\\n    TYPE_CHECKING </s> remove     ListBlockBuilder\n </s> add     SimpleBlockBuilder </s> remove from typing import Iterator, Union, Tuple, Any, TypeVar, TYPE_CHECKING\n </s> add from typing import Iterator, List, Union, Tuple, Any, TypeVar, TYPE_CHECKING", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/compute.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> U = TypeVar(\"U\")\n <mask> \n <mask> \n <mask> class ComputePool:\n <mask>     def apply(self, fn: Any, blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n <mask>         raise NotImplementedError\n <mask> \n <mask> \n <mask> class TaskPool(ComputePool):\n <mask>     def apply(self, fn: Any, remote_args: dict,\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove               blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add               blocks: BlockList[Any]) -> BlockList[Any]: </s> remove               blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add               blocks: Iterable[Block[T]]) -> Iterable[ObjectRef[Block]]: </s> remove         if remote_args:\n            fn = ray.remote(**remote_args)(fn)\n        else:\n            fn = ray.remote(fn)\n        blocks = [fn.remote(b) for b in blocks]\n </s> add         kwargs = remote_args.copy()\n        kwargs[\"num_returns\"] = 2 </s> remove         map_bar.block_until_complete(blocks)\n        return blocks\n </s> add         @ray.remote(**kwargs)\n        def wrapped_fn(block: Block, meta: BlockMetadata):\n            new_block = fn(block)\n            new_meta = BlockMetadata(\n                num_rows=new_block.num_rows(),\n                size_bytes=new_block.size_bytes(),\n                schema=new_block.schema(),\n                input_files=meta.input_files)\n            return new_block, new_meta\n\n        refs = [\n            wrapped_fn.remote(b, m)\n            for b, m in zip(blocks, blocks.get_metadata())\n        ]\n        new_blocks, new_metadata = zip(*refs)\n\n        map_bar.block_until_complete(list(new_blocks))\n        new_metadata = ray.get(list(new_metadata))\n        return BlockList(list(new_blocks), list(new_metadata)) </s> remove class ListBlockBuilder(BlockBuilder[T]):\n </s> add class SimpleBlockBuilder(BlockBuilder[T]): </s> add     def schema(self) -> Any:\n        raise NotImplementedError\n\n    def get_metadata(self, input_files: List[str]) -> BlockMetadata:\n        return BlockMetadata(\n            num_rows=self.num_rows(),\n            size_bytes=self.size_bytes(),\n            schema=self.schema(),\n            input_files=input_files)\n", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/compute.py"}
{"docstring_tokens": "keep keep keep replace keep keep replace replace replace replace replace keep", "code_tokens": " <mask> \n <mask> class TaskPool(ComputePool):\n <mask>     def apply(self, fn: Any, remote_args: dict,\n <mask>               blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n <mask>         map_bar = ProgressBar(\"Map Progress\", total=len(blocks))\n <mask> \n <mask>         if remote_args:\n <mask>             fn = ray.remote(**remote_args)(fn)\n <mask>         else:\n <mask>             fn = ray.remote(fn)\n <mask>         blocks = [fn.remote(b) for b in blocks]\n <mask> \n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove               blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add               blocks: Iterable[Block[T]]) -> Iterable[ObjectRef[Block]]: </s> remove         map_bar.block_until_complete(blocks)\n        return blocks\n </s> add         @ray.remote(**kwargs)\n        def wrapped_fn(block: Block, meta: BlockMetadata):\n            new_block = fn(block)\n            new_meta = BlockMetadata(\n                num_rows=new_block.num_rows(),\n                size_bytes=new_block.size_bytes(),\n                schema=new_block.schema(),\n                input_files=meta.input_files)\n            return new_block, new_meta\n\n        refs = [\n            wrapped_fn.remote(b, m)\n            for b, m in zip(blocks, blocks.get_metadata())\n        ]\n        new_blocks, new_metadata = zip(*refs)\n\n        map_bar.block_until_complete(list(new_blocks))\n        new_metadata = ray.get(list(new_metadata))\n        return BlockList(list(new_blocks), list(new_metadata)) </s> remove     def apply(self, fn: Any, blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add     def apply(self, fn: Any,\n              blocks: Iterable[Block[T]]) -> Iterable[ObjectRef[Block]]: </s> remove             def process_block(self, block: Block[T]) -> Block[U]:\n                return fn(block)\n </s> add             @ray.method(num_returns=2)\n            def process_block(self, block: Block[T], meta: BlockMetadata\n                              ) -> (Block[U], BlockMetadata):\n                new_block = fn(block)\n                new_metadata = BlockMetadata(\n                    num_rows=new_block.num_rows(),\n                    size_bytes=new_block.size_bytes(),\n                    schema=new_block.schema(),\n                    input_files=meta.input_files)\n                return new_block, new_metadata </s> add         new_metadata = ray.get([metadata_mapping[b] for b in blocks_out])", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/compute.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         else:\n <mask>             fn = ray.remote(fn)\n <mask>         blocks = [fn.remote(b) for b in blocks]\n <mask> \n <mask>         map_bar.block_until_complete(blocks)\n <mask>         return blocks\n <mask> \n <mask> \n <mask> class ActorPool(ComputePool):\n <mask>     def apply(self, fn: Any, remote_args: dict,\n <mask>               blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove         if remote_args:\n            fn = ray.remote(**remote_args)(fn)\n        else:\n            fn = ray.remote(fn)\n        blocks = [fn.remote(b) for b in blocks]\n </s> add         kwargs = remote_args.copy()\n        kwargs[\"num_returns\"] = 2 </s> remove               blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add               blocks: BlockList[Any]) -> BlockList[Any]: </s> remove               blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add               blocks: Iterable[Block[T]]) -> Iterable[ObjectRef[Block]]: </s> remove     def apply(self, fn: Any, blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add     def apply(self, fn: Any,\n              blocks: Iterable[Block[T]]) -> Iterable[ObjectRef[Block]]: </s> add         new_metadata = ray.get([metadata_mapping[b] for b in blocks_out]) </s> remove     blocks = []\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/compute.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> \n <mask> class ActorPool(ComputePool):\n <mask>     def apply(self, fn: Any, remote_args: dict,\n <mask>               blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n <mask> \n <mask>         map_bar = ProgressBar(\"Map Progress\", total=len(blocks))\n <mask> \n <mask>         class Worker:\n <mask>             def ready(self):\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove               blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add               blocks: BlockList[Any]) -> BlockList[Any]: </s> remove     def apply(self, fn: Any, blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add     def apply(self, fn: Any,\n              blocks: Iterable[Block[T]]) -> Iterable[ObjectRef[Block]]: </s> remove         if remote_args:\n            fn = ray.remote(**remote_args)(fn)\n        else:\n            fn = ray.remote(fn)\n        blocks = [fn.remote(b) for b in blocks]\n </s> add         kwargs = remote_args.copy()\n        kwargs[\"num_returns\"] = 2 </s> remove         map_bar.block_until_complete(blocks)\n        return blocks\n </s> add         @ray.remote(**kwargs)\n        def wrapped_fn(block: Block, meta: BlockMetadata):\n            new_block = fn(block)\n            new_meta = BlockMetadata(\n                num_rows=new_block.num_rows(),\n                size_bytes=new_block.size_bytes(),\n                schema=new_block.schema(),\n                input_files=meta.input_files)\n            return new_block, new_meta\n\n        refs = [\n            wrapped_fn.remote(b, m)\n            for b, m in zip(blocks, blocks.get_metadata())\n        ]\n        new_blocks, new_metadata = zip(*refs)\n\n        map_bar.block_until_complete(list(new_blocks))\n        new_metadata = ray.get(list(new_metadata))\n        return BlockList(list(new_blocks), list(new_metadata)) </s> remove             def process_block(self, block: Block[T]) -> Block[U]:\n                return fn(block)\n </s> add             @ray.method(num_returns=2)\n            def process_block(self, block: Block[T], meta: BlockMetadata\n                              ) -> (Block[U], BlockMetadata):\n                new_block = fn(block)\n                new_metadata = BlockMetadata(\n                    num_rows=new_block.num_rows(),\n                    size_bytes=new_block.size_bytes(),\n                    schema=new_block.schema(),\n                    input_files=meta.input_files)\n                return new_block, new_metadata </s> remove class ListBlock(Block):\n </s> add class SimpleBlock(Block):", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/compute.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         class Worker:\n <mask>             def ready(self):\n <mask>                 return \"ok\"\n <mask> \n <mask>             def process_block(self, block: Block[T]) -> Block[U]:\n <mask>                 return fn(block)\n <mask> \n <mask>         if \"num_cpus\" not in remote_args:\n <mask>             remote_args[\"num_cpus\"] = 1\n <mask>         Worker = ray.remote(**remote_args)(Worker)\n <mask> \n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove               blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add               blocks: Iterable[Block[T]]) -> Iterable[ObjectRef[Block]]: </s> remove         map_bar.block_until_complete(blocks)\n        return blocks\n </s> add         @ray.remote(**kwargs)\n        def wrapped_fn(block: Block, meta: BlockMetadata):\n            new_block = fn(block)\n            new_meta = BlockMetadata(\n                num_rows=new_block.num_rows(),\n                size_bytes=new_block.size_bytes(),\n                schema=new_block.schema(),\n                input_files=meta.input_files)\n            return new_block, new_meta\n\n        refs = [\n            wrapped_fn.remote(b, m)\n            for b, m in zip(blocks, blocks.get_metadata())\n        ]\n        new_blocks, new_metadata = zip(*refs)\n\n        map_bar.block_until_complete(list(new_blocks))\n        new_metadata = ray.get(list(new_metadata))\n        return BlockList(list(new_blocks), list(new_metadata)) </s> remove               blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add               blocks: BlockList[Any]) -> BlockList[Any]: </s> remove                 self._builder = ListBlockBuilder()\n </s> add                 self._builder = SimpleBlockBuilder() </s> remove                     self._builder = ListBlockBuilder()\n </s> add                     self._builder = SimpleBlockBuilder() </s> remove     def build(self) -> \"ListBlock[T]\":\n        return ListBlock(self._items)\n </s> add     def build(self) -> \"SimpleBlock[T]\":\n        return SimpleBlock(self._items)", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/compute.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         workers = [Worker.remote()]\n <mask>         tasks = {w.ready.remote(): w for w in workers}\n <mask>         ready_workers = set()\n <mask>         blocks_in = [(b, m) for (b, m) in zip(blocks, blocks.get_metadata())]\n <mask>         blocks_out = []\n <mask> \n <mask>         while len(blocks_out) < len(blocks):\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove         blocks_in = blocks.copy()\n </s> add         blocks_in = [(b, m) for (b, m) in zip(blocks, blocks.get_metadata())] </s> add     metadata: List[BlockMetadata] = [] </s> remove         map_bar.block_until_complete(blocks)\n        return blocks\n </s> add         @ray.remote(**kwargs)\n        def wrapped_fn(block: Block, meta: BlockMetadata):\n            new_block = fn(block)\n            new_meta = BlockMetadata(\n                num_rows=new_block.num_rows(),\n                size_bytes=new_block.size_bytes(),\n                schema=new_block.schema(),\n                input_files=meta.input_files)\n            return new_block, new_meta\n\n        refs = [\n            wrapped_fn.remote(b, m)\n            for b, m in zip(blocks, blocks.get_metadata())\n        ]\n        new_blocks, new_metadata = zip(*refs)\n\n        map_bar.block_until_complete(list(new_blocks))\n        new_metadata = ray.get(list(new_metadata))\n        return BlockList(list(new_blocks), list(new_metadata)) </s> remove         builder = ListBlock.builder()\n </s> add         builder = SimpleBlock.builder() </s> remove         blocks.append(ray.put(builder.build()))\n </s> add         block = builder.build()\n        blocks.append(ray.put(block))\n        metadata.append(\n            BlockMetadata(\n                num_rows=block.num_rows(),\n                size_bytes=block.size_bytes(),\n                schema=type(items[0]),\n                input_files=None)) </s> remove     return Dataset([\n        gen_read.remote(ps) for ps in np.array_split(pieces, parallelism)\n        if len(ps) > 0\n    ])\n </s> add     calls: List[Callable[[], ObjectRef[Block]]] = []\n    metadata: List[BlockMetadata] = []\n    for pieces in nonempty_tasks:\n\n        def make_call(\n                pieces: List[pq.ParquetDatasetPiece]) -> ObjectRef[Block]:\n            return lambda: gen_read.remote(pieces)\n\n        calls.append(make_call(pieces))\n        piece_metadata = [p.get_metadata() for p in pieces]\n        metadata.append(\n            BlockMetadata(\n                num_rows=sum(m.num_rows for m in piece_metadata),\n                size_bytes=sum(\n                    sum(\n                        m.row_group(i).total_byte_size\n                        for i in builtins.range(m.num_row_groups))\n                    for m in piece_metadata),\n                schema=piece_metadata[0].schema.to_arrow_schema(),\n                input_files=[p.path for p in pieces]))\n\n    return Dataset(LazyBlockList(calls, metadata))", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/compute.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         workers = [Worker.remote()]\n <mask>         tasks = {w.ready.remote(): w for w in workers}\n <mask>         ready_workers = set()\n <mask>         blocks_in = blocks.copy()\n <mask>         blocks_out = []\n <mask> \n <mask>         while len(blocks_out) < len(blocks):\n <mask>             ready, _ = ray.wait(\n <mask>                 list(tasks), timeout=0.01, num_returns=1, fetch_local=False)\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add         metadata_mapping = {} </s> add     metadata: List[BlockMetadata] = [] </s> remove         builder = ListBlock.builder()\n </s> add         builder = SimpleBlock.builder() </s> remove         blocks.append(ray.put(builder.build()))\n </s> add         block = builder.build()\n        blocks.append(ray.put(block))\n        metadata.append(\n            BlockMetadata(\n                num_rows=block.num_rows(),\n                size_bytes=block.size_bytes(),\n                schema=type(items[0]),\n                input_files=None)) </s> add     read_tasks = [[] for _ in builtins.range(parallelism)]\n    # TODO(ekl) support reading row groups (maybe as an option)\n    for i, piece in enumerate(pq_ds.pieces):\n        read_tasks[i % len(read_tasks)].append(piece)\n    nonempty_tasks = [r for r in read_tasks if r] </s> remove         for i, item in enumerate(ds.to_local_iterator()):\n </s> add         for i, item in enumerate(ds.iter_rows()):", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/compute.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                 ready_workers.add(worker)\n <mask> \n <mask>             # Schedule a new task.\n <mask>             if blocks_in:\n <mask>                 tasks[worker.process_block.remote(blocks_in.pop())] = worker\n <mask> \n <mask>         map_bar.close()\n <mask>         return blocks_out\n <mask> \n <mask> \n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove         return blocks_out\n </s> add         return BlockList(blocks_out, new_metadata) </s> add         new_metadata = ray.get([metadata_mapping[b] for b in blocks_out]) </s> add     if output_num_blocks == 1:\n        # Handle the num_returns=1 edge case which doesn't return a list.\n        shuffle_map_out = [[x] for x in shuffle_map_out] </s> add     map_bar.close() </s> remove     def slice(self, start: int, end: int) -> \"ArrowBlock[T]\":\n        return ArrowBlock(self._table.slice(start, end - start))\n </s> add     def slice(self, start: int, end: int, copy: bool) -> \"ArrowBlock[T]\":\n        view = self._table.slice(start, end - start)\n        if copy:\n            # TODO(ekl) there must be a cleaner way to force a copy of a table.\n            copy = [c.to_pandas() for c in view.itercolumns()]\n            return ArrowBlock(\n                pyarrow.Table.from_arrays(copy, schema=self._table.schema))\n        else:\n            return ArrowBlock(view)\n\n    def schema(self) -> \"pyarrow.lib.Schema\":\n        return self._table.schema </s> add         metadata_mapping = {}", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/compute.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>                 metadata_mapping[block_ref] = meta_ref\n <mask>                 tasks[block_ref] = worker\n <mask> \n <mask>         map_bar.close()\n <mask>         return BlockList(blocks_out, new_metadata)\n <mask> \n <mask> \n <mask> def get_compute(compute_spec: str) -> ComputePool:\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove         return blocks_out\n </s> add         return BlockList(blocks_out, new_metadata) </s> remove                 tasks[worker.process_block.remote(blocks_in.pop())] = worker\n </s> add                 block_ref, meta_ref = worker.process_block.remote(\n                    *blocks_in.pop())\n                metadata_mapping[block_ref] = meta_ref\n                tasks[block_ref] = worker </s> remove     def apply(self, fn: Any, blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add     def apply(self, fn: Any,\n              blocks: Iterable[Block[T]]) -> Iterable[ObjectRef[Block]]: </s> add     reduce_bar = ProgressBar(\n        \"Shuffle Reduce\", position=0, total=output_num_blocks) </s> remove     reduce_bar.block_until_complete(shuffle_reduce_out)\n\n    map_bar.close()\n </s> add     new_blocks, new_metadata = zip(*shuffle_reduce_out)\n    reduce_bar.block_until_complete(list(new_blocks))\n    new_metadata = ray.get(list(new_metadata)) </s> add     map_bar.close()", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/compute.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             if blocks_in:\n <mask>                 tasks[worker.process_block.remote(blocks_in.pop())] = worker\n <mask> \n <mask>         map_bar.close()\n <mask>         return blocks_out\n <mask> \n <mask> \n <mask> def get_compute(compute_spec: str) -> ComputePool:\n <mask>     if compute_spec == \"tasks\":\n <mask>         return TaskPool()\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove                 tasks[worker.process_block.remote(blocks_in.pop())] = worker\n </s> add                 block_ref, meta_ref = worker.process_block.remote(\n                    *blocks_in.pop())\n                metadata_mapping[block_ref] = meta_ref\n                tasks[block_ref] = worker </s> add         new_metadata = ray.get([metadata_mapping[b] for b in blocks_out]) </s> add     if output_num_blocks == 1:\n        # Handle the num_returns=1 edge case which doesn't return a list.\n        shuffle_map_out = [[x] for x in shuffle_map_out] </s> remove     def apply(self, fn: Any, blocks: List[Block[T]]) -> List[ObjectRef[Block]]:\n </s> add     def apply(self, fn: Any,\n              blocks: Iterable[Block[T]]) -> Iterable[ObjectRef[Block]]: </s> remove             slices.append(block.slice(i * slice_sz, (i + 1) * slice_sz))\n        return slices\n </s> add             slices.append(\n                block.slice(i * slice_sz, (i + 1) * slice_sz, copy=True))\n        num_rows = sum(s.num_rows() for s in slices)\n        assert num_rows == block.num_rows(), (num_rows, block.num_rows())\n        # Needed to handle num_returns=1 edge case in Ray API.\n        if len(slices) == 1:\n            return slices[0]\n        else:\n            return slices </s> remove     assert sorted(ds.to_local_iterator()) == [0, 1, 2, 3, 4]\n </s> add     assert sorted(ds.iter_rows()) == [0, 1, 2, 3, 4]\n\n\ndef test_empty_dataset(ray_start_regular_shared):\n    ds = ray.experimental.data.range(0)\n    assert ds.count() == 0\n    with pytest.raises(ValueError):\n        ds.size_bytes()\n    with pytest.raises(ValueError):\n        ds.schema()\n\n    ds = ray.experimental.data.range(1)\n    ds = ds.filter(lambda x: x > 1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=0, num_blocks=1, schema=Unknown schema)\"\n\n\ndef test_schema(ray_start_regular_shared):\n    ds = ray.experimental.data.range(10)\n    ds2 = ray.experimental.data.range_arrow(10)\n    ds3 = ds2.repartition(5)\n    ds4 = ds3.map(lambda x: {\"a\": \"hi\", \"b\": 1.0}).limit(5).repartition(1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema=<class 'int'>)\"\n    assert str(ds2) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema={value: int64})\"\n    assert str(ds3) == \\\n        \"Dataset(num_rows=10, num_blocks=5, schema={value: int64})\"\n    assert str(ds4) == \\\n        \"Dataset(num_rows=5, num_blocks=1, schema={a: string, b: double})\"\n\n\ndef test_lazy_loading_exponential_rampup(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    assert len(ds._blocks._blocks) == 1\n    assert ds.take(10) == list(range(10))\n    assert len(ds._blocks._blocks) == 2\n    assert ds.take(20) == list(range(20))\n    assert len(ds._blocks._blocks) == 4\n    assert ds.take(30) == list(range(30))\n    assert len(ds._blocks._blocks) == 8\n    assert ds.take(50) == list(range(50))\n    assert len(ds._blocks._blocks) == 16\n    assert ds.take(100) == list(range(100))\n    assert len(ds._blocks._blocks) == 20\n\n\ndef test_limit(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    for i in range(100):\n        assert ds.limit(i).take(200) == list(range(i))", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/compute.py"}
{"docstring_tokens": "keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> from typing import TypeVar, List\n <mask> \n <mask> import ray\n <mask> from ray.experimental.data.impl.block import Block, ObjectRef\n <mask> from ray.experimental.data.impl.progress_bar import ProgressBar\n <mask> from ray.experimental.data.impl.arrow_block import DelegatingArrowBlockBuilder\n <mask> \n <mask> T = TypeVar(\"T\")\n <mask> \n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove from ray.experimental.data.impl.block import Block, ObjectRef\n </s> add from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\nfrom ray.experimental.data.impl.block_list import BlockList </s> remove from typing import TypeVar, List, Any\n </s> add from typing import TypeVar, Iterable, Any </s> add from ray.experimental.data.impl.block_list import BlockList </s> remove from ray.experimental.data.impl.arrow_block import ArrowBlock, ArrowRow\nfrom ray.experimental.data.impl.block import ObjectRef, ListBlock, Block\n </s> add  </s> add from ray.experimental.data.impl.arrow_block import ArrowBlock, ArrowRow\nfrom ray.experimental.data.impl.block import ObjectRef, SimpleBlock, Block, \\\n    BlockMetadata\nfrom ray.experimental.data.impl.block_list import BlockList\nfrom ray.experimental.data.impl.lazy_block_list import LazyBlockList </s> remove     ListBlockBuilder\n </s> add     SimpleBlockBuilder", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/shuffle.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask> from ray.experimental.data.impl.block import Block, BlockMetadata\n <mask> from ray.experimental.data.impl.progress_bar import ProgressBar\n <mask> from ray.experimental.data.impl.arrow_block import DelegatingArrowBlockBuilder\n <mask> \n <mask> T = TypeVar(\"T\")\n <mask> \n <mask> \n <mask> def simple_shuffle(input_blocks: BlockList[T],\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove from ray.experimental.data.impl.block import Block, ObjectRef\n </s> add from ray.experimental.data.impl.block import Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import Block, ObjectRef\n </s> add from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\nfrom ray.experimental.data.impl.block_list import BlockList </s> remove from typing import TypeVar, List, Any\n </s> add from typing import TypeVar, Iterable, Any </s> add from ray.experimental.data.impl.arrow_block import ArrowBlock, ArrowRow\nfrom ray.experimental.data.impl.block import ObjectRef, SimpleBlock, Block, \\\n    BlockMetadata\nfrom ray.experimental.data.impl.block_list import BlockList\nfrom ray.experimental.data.impl.lazy_block_list import LazyBlockList </s> remove     ListBlockBuilder\n </s> add     SimpleBlockBuilder </s> remove from ray.experimental.data.impl.arrow_block import ArrowBlock, ArrowRow\nfrom ray.experimental.data.impl.block import ObjectRef, ListBlock, Block\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/shuffle.py"}
{"docstring_tokens": "keep keep replace replace keep keep keep keep replace keep keep", "code_tokens": " <mask> \n <mask> \n <mask> def simple_shuffle(input_blocks: List[ObjectRef[Block[T]]],\n <mask>                    output_num_blocks: int) -> List[ObjectRef[Block[T]]]:\n <mask>     input_num_blocks = len(input_blocks)\n <mask> \n <mask>     @ray.remote(num_returns=output_num_blocks)\n <mask>     def shuffle_map(block: Block[T]) -> List[Block[T]]:\n <mask>         slice_sz = max(1, block.num_rows() // output_num_blocks)\n <mask>         slices = []\n <mask>         for i in range(output_num_blocks):\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove             slices.append(block.slice(i * slice_sz, (i + 1) * slice_sz))\n        return slices\n </s> add             slices.append(\n                block.slice(i * slice_sz, (i + 1) * slice_sz, copy=True))\n        num_rows = sum(s.num_rows() for s in slices)\n        assert num_rows == block.num_rows(), (num_rows, block.num_rows())\n        # Needed to handle num_returns=1 edge case in Ray API.\n        if len(slices) == 1:\n            return slices[0]\n        else:\n            return slices </s> remove     def gen_block(start: int, count: int) -> ListBlock:\n        builder = ListBlock.builder()\n </s> add     def gen_block(start: int, count: int) -> SimpleBlock:\n        builder = SimpleBlock.builder() </s> remove     @ray.remote\n    def shuffle_reduce(*mapper_outputs: List[Block[T]]) -> Block[T]:\n </s> add     @ray.remote(num_returns=2)\n    def shuffle_reduce(\n            *mapper_outputs: List[Block[T]]) -> (Block[T], BlockMetadata): </s> add     calls: List[Callable[[], ObjectRef[Block]]] = []\n    metadata: List[BlockMetadata] = [] </s> remove     blocks = []\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/shuffle.py"}
{"docstring_tokens": "keep replace replace keep replace replace keep keep keep keep", "code_tokens": " <mask>         for i in range(output_num_blocks):\n <mask>             slices.append(block.slice(i * slice_sz, (i + 1) * slice_sz))\n <mask>         return slices\n <mask> \n <mask>     @ray.remote\n <mask>     def shuffle_reduce(*mapper_outputs: List[Block[T]]) -> Block[T]:\n <mask>         builder = DelegatingArrowBlockBuilder()\n <mask>         assert len(mapper_outputs) == input_num_blocks\n <mask>         for block in mapper_outputs:\n <mask>             builder.add_block(block)\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove         slice_sz = max(1, block.num_rows() // output_num_blocks)\n </s> add         slice_sz = max(1, math.ceil(block.num_rows() / output_num_blocks)) </s> remove         return builder.build()\n </s> add         new_block = builder.build()\n        new_metadata = BlockMetadata(\n            num_rows=new_block.num_rows(),\n            size_bytes=new_block.size_bytes(),\n            schema=new_block.schema(),\n            input_files=None)\n        return new_block, new_metadata </s> add     metadata: List[BlockMetadata] = [] </s> remove         blocks.append(ray.put(builder.build()))\n </s> add         block = builder.build()\n        blocks.append(ray.put(block))\n        metadata.append(\n            BlockMetadata(\n                num_rows=block.num_rows(),\n                size_bytes=block.size_bytes(),\n                schema=type(items[0]),\n                input_files=None)) </s> remove     def gen_block(start: int, count: int) -> ListBlock:\n        builder = ListBlock.builder()\n </s> add     def gen_block(start: int, count: int) -> SimpleBlock:\n        builder = SimpleBlock.builder()", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/shuffle.py"}
{"docstring_tokens": "keep keep keep replace keep keep replace replace keep keep keep", "code_tokens": " <mask>         assert len(mapper_outputs) == input_num_blocks\n <mask>         for block in mapper_outputs:\n <mask>             builder.add_block(block)\n <mask>         return builder.build()\n <mask> \n <mask>     map_bar = ProgressBar(\"Shuffle Map\", position=0, total=input_num_blocks)\n <mask>     reduce_bar = ProgressBar(\n <mask>         \"Shuffle Reduce\", position=1, total=output_num_blocks)\n <mask> \n <mask>     shuffle_map_out = [shuffle_map.remote(block) for block in input_blocks]\n <mask>     map_bar.block_until_complete([x[0] for x in shuffle_map_out])\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add     if output_num_blocks == 1:\n        # Handle the num_returns=1 edge case which doesn't return a list.\n        shuffle_map_out = [[x] for x in shuffle_map_out] </s> add     map_bar.close() </s> add     reduce_bar = ProgressBar(\n        \"Shuffle Reduce\", position=0, total=output_num_blocks) </s> remove     @ray.remote\n    def shuffle_reduce(*mapper_outputs: List[Block[T]]) -> Block[T]:\n </s> add     @ray.remote(num_returns=2)\n    def shuffle_reduce(\n            *mapper_outputs: List[Block[T]]) -> (Block[T], BlockMetadata): </s> add     metadata: List[BlockMetadata] = []", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/shuffle.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask> \n <mask>     map_bar = ProgressBar(\"Shuffle Map\", position=0, total=input_num_blocks)\n <mask> \n <mask>     shuffle_map_out = [shuffle_map.remote(block) for block in input_blocks]\n <mask>     map_bar.block_until_complete([x[0] for x in shuffle_map_out])\n <mask>     map_bar.close()\n <mask> \n <mask>     reduce_bar = ProgressBar(\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     reduce_bar = ProgressBar(\n        \"Shuffle Reduce\", position=1, total=output_num_blocks)\n </s> add  </s> add     map_bar.close() </s> remove         return builder.build()\n </s> add         new_block = builder.build()\n        new_metadata = BlockMetadata(\n            num_rows=new_block.num_rows(),\n            size_bytes=new_block.size_bytes(),\n            schema=new_block.schema(),\n            input_files=None)\n        return new_block, new_metadata </s> add     reduce_bar = ProgressBar(\n        \"Shuffle Reduce\", position=0, total=output_num_blocks) </s> remove     reduce_bar.block_until_complete(shuffle_reduce_out)\n\n    map_bar.close()\n </s> add     new_blocks, new_metadata = zip(*shuffle_reduce_out)\n    reduce_bar.block_until_complete(list(new_blocks))\n    new_metadata = ray.get(list(new_metadata)) </s> add         new_metadata = ray.get([metadata_mapping[b] for b in blocks_out])", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/shuffle.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>         # Handle the num_returns=1 edge case which doesn't return a list.\n <mask>         shuffle_map_out = [[x] for x in shuffle_map_out]\n <mask>     map_bar.block_until_complete([x[0] for x in shuffle_map_out])\n <mask> \n <mask>     reduce_bar = ProgressBar(\n <mask>         \"Shuffle Reduce\", position=0, total=output_num_blocks)\n <mask>     shuffle_reduce_out = [\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add     if output_num_blocks == 1:\n        # Handle the num_returns=1 edge case which doesn't return a list.\n        shuffle_map_out = [[x] for x in shuffle_map_out] </s> remove     reduce_bar = ProgressBar(\n        \"Shuffle Reduce\", position=1, total=output_num_blocks)\n </s> add  </s> add     reduce_bar = ProgressBar(\n        \"Shuffle Reduce\", position=0, total=output_num_blocks) </s> remove         return builder.build()\n </s> add         new_block = builder.build()\n        new_metadata = BlockMetadata(\n            num_rows=new_block.num_rows(),\n            size_bytes=new_block.size_bytes(),\n            schema=new_block.schema(),\n            input_files=None)\n        return new_block, new_metadata </s> remove             slices.append(block.slice(i * slice_sz, (i + 1) * slice_sz))\n        return slices\n </s> add             slices.append(\n                block.slice(i * slice_sz, (i + 1) * slice_sz, copy=True))\n        num_rows = sum(s.num_rows() for s in slices)\n        assert num_rows == block.num_rows(), (num_rows, block.num_rows())\n        # Needed to handle num_returns=1 edge case in Ray API.\n        if len(slices) == 1:\n            return slices[0]\n        else:\n            return slices </s> remove     reduce_bar.block_until_complete(shuffle_reduce_out)\n\n    map_bar.close()\n </s> add     new_blocks, new_metadata = zip(*shuffle_reduce_out)\n    reduce_bar.block_until_complete(list(new_blocks))\n    new_metadata = ray.get(list(new_metadata))", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/shuffle.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>     map_bar.close()\n <mask> \n <mask>     shuffle_reduce_out = [\n <mask>         shuffle_reduce.remote(\n <mask>             *[shuffle_map_out[i][j] for i in range(input_num_blocks)])\n <mask>         for j in range(output_num_blocks)\n <mask>     ]\n <mask>     new_blocks, new_metadata = zip(*shuffle_reduce_out)\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     reduce_bar.block_until_complete(shuffle_reduce_out)\n\n    map_bar.close()\n </s> add     new_blocks, new_metadata = zip(*shuffle_reduce_out)\n    reduce_bar.block_until_complete(list(new_blocks))\n    new_metadata = ray.get(list(new_metadata)) </s> remove         map_bar.block_until_complete(blocks)\n        return blocks\n </s> add         @ray.remote(**kwargs)\n        def wrapped_fn(block: Block, meta: BlockMetadata):\n            new_block = fn(block)\n            new_meta = BlockMetadata(\n                num_rows=new_block.num_rows(),\n                size_bytes=new_block.size_bytes(),\n                schema=new_block.schema(),\n                input_files=meta.input_files)\n            return new_block, new_meta\n\n        refs = [\n            wrapped_fn.remote(b, m)\n            for b, m in zip(blocks, blocks.get_metadata())\n        ]\n        new_blocks, new_metadata = zip(*refs)\n\n        map_bar.block_until_complete(list(new_blocks))\n        new_metadata = ray.get(list(new_metadata))\n        return BlockList(list(new_blocks), list(new_metadata)) </s> add     map_bar.close() </s> remove     reduce_bar = ProgressBar(\n        \"Shuffle Reduce\", position=1, total=output_num_blocks)\n </s> add  </s> add         new_metadata = ray.get([metadata_mapping[b] for b in blocks_out]) </s> remove         table = piece.read(\n            columns=columns, use_threads=False, partitions=partitions)\n </s> add         tables = [\n            piece.read(\n                columns=columns, use_threads=False, partitions=partitions)\n            for piece in pieces\n        ]\n        if len(tables) > 1:\n            table = pyarrow.concat_tables(tables)\n        else:\n            table = tables[0]", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/shuffle.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep replace", "code_tokens": " <mask>         shuffle_reduce.remote(\n <mask>             *[shuffle_map_out[i][j] for i in range(input_num_blocks)])\n <mask>         for j in range(output_num_blocks)\n <mask>     ]\n <mask>     reduce_bar.block_until_complete(shuffle_reduce_out)\n <mask> \n <mask>     map_bar.close()\n <mask>     reduce_bar.close()\n <mask>     return shuffle_reduce_out\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add     reduce_bar = ProgressBar(\n        \"Shuffle Reduce\", position=0, total=output_num_blocks) </s> add     map_bar.close() </s> remove     reduce_bar = ProgressBar(\n        \"Shuffle Reduce\", position=1, total=output_num_blocks)\n </s> add  </s> add     if output_num_blocks == 1:\n        # Handle the num_returns=1 edge case which doesn't return a list.\n        shuffle_map_out = [[x] for x in shuffle_map_out] </s> add         new_metadata = ray.get([metadata_mapping[b] for b in blocks_out])", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/impl/shuffle.py"}
{"docstring_tokens": "replace keep replace keep keep keep keep keep", "code_tokens": " <mask> import builtins\n <mask> import logging\n <mask> from typing import List, Any, Union, Optional, Tuple, TYPE_CHECKING\n <mask> \n <mask> if TYPE_CHECKING:\n <mask>     import pyarrow\n <mask>     import pandas\n <mask>     import dask\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove from typing import Iterator, Union, Tuple, Any, TypeVar, TYPE_CHECKING\n </s> add from typing import Iterator, List, Union, Tuple, Any, TypeVar, TYPE_CHECKING </s> remove from typing import TypeVar, List, Generic, Iterator, TYPE_CHECKING\n </s> add from typing import TypeVar, List, Generic, Iterator, Any, Union, Optional, \\\n    TYPE_CHECKING </s> remove from typing import TypeVar, List, Any\n </s> add from typing import TypeVar, Iterable, Any </s> remove     ListBlockBuilder\n </s> add     SimpleBlockBuilder </s> remove from ray.experimental.data.impl.block import Block, ObjectRef\n </s> add from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\nfrom ray.experimental.data.impl.block_list import BlockList", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>     import pyspark\n <mask> \n <mask> import ray\n <mask> from ray.experimental.data.dataset import Dataset\n <mask> from ray.experimental.data.impl.arrow_block import ArrowBlock, ArrowRow\n <mask> from ray.experimental.data.impl.block import ObjectRef, ListBlock, Block\n <mask> from ray.experimental.data.impl import reader as _reader\n <mask> \n <mask> logger = logging.getLogger(__name__)\n <mask> \n <mask> \n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add from ray.experimental.data.impl.arrow_block import ArrowBlock, ArrowRow\nfrom ray.experimental.data.impl.block import ObjectRef, SimpleBlock, Block, \\\n    BlockMetadata\nfrom ray.experimental.data.impl.block_list import BlockList\nfrom ray.experimental.data.impl.lazy_block_list import LazyBlockList </s> remove from ray.experimental.data.impl.block import Block, ObjectRef\n </s> add from ray.experimental.data.impl.block import Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import Block, ObjectRef\n </s> add from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\nfrom ray.experimental.data.impl.block_list import BlockList </s> add from ray.experimental.data.impl.block_list import BlockList </s> remove from typing import TypeVar, List, Any\n </s> add from typing import TypeVar, Iterable, Any </s> remove     import numpy as np\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> from ray.experimental.data.dataset import Dataset\n <mask> from ray.experimental.data.impl import reader as _reader\n <mask> \n <mask> logger = logging.getLogger(__name__)\n <mask> \n <mask> \n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove from ray.experimental.data.impl.arrow_block import ArrowBlock, ArrowRow\nfrom ray.experimental.data.impl.block import ObjectRef, ListBlock, Block\n </s> add  </s> remove     import numpy as np\n </s> add  </s> remove from ray.experimental.data.impl.block import Block, ObjectRef\n </s> add from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\nfrom ray.experimental.data.impl.block_list import BlockList </s> remove from ray.experimental.data.impl.block import Block, ObjectRef\n </s> add from ray.experimental.data.impl.block import Block, BlockMetadata </s> add from ray.experimental.data.impl.block_list import BlockList </s> remove     @ray.remote\n </s> add     @ray.remote(num_returns=2)", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask> \n <mask> \n <mask> def autoinit_ray(f):\n <mask>     def wrapped(*a, **kw):\n <mask>         if not ray.is_initialized():\n <mask>             ray.client().connect()\n <mask>         return f(*a, **kw)\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add         self._tables: List[\"pyarrow.Table\"] = [] </s> remove             def process_block(self, block: Block[T]) -> Block[U]:\n                return fn(block)\n </s> add             @ray.method(num_returns=2)\n            def process_block(self, block: Block[T], meta: BlockMetadata\n                              ) -> (Block[U], BlockMetadata):\n                new_block = fn(block)\n                new_metadata = BlockMetadata(\n                    num_rows=new_block.num_rows(),\n                    size_bytes=new_block.size_bytes(),\n                    schema=new_block.schema(),\n                    input_files=meta.input_files)\n                return new_block, new_metadata </s> add     def schema(self) -> Any:\n        if self._items:\n            return type(self._items[0])\n        else:\n            return None\n </s> remove     def slice(self, start: int, end: int) -> \"ListBlock[T]\":\n        return ListBlock(self._items[start:end])\n </s> add     def slice(self, start: int, end: int, copy: bool) -> \"SimpleBlock[T]\":\n        view = self._items[start:end]\n        if copy:\n            view = view.copy()\n        return SimpleBlock(view) </s> remove         return ArrowBlock(pyarrow.Table.from_pydict(self._columns))\n </s> add         if self._columns:\n            tables = [pyarrow.Table.from_pydict(self._columns)]\n        else:\n            tables = []\n        tables.extend(self._tables)\n        if len(tables) > 1:\n            return ArrowBlock(pyarrow.concat_tables(tables))\n        elif len(tables) > 0:\n            return ArrowBlock(tables[0])\n        else:\n            return ArrowBlock(pyarrow.Table.from_pydict({})) </s> remove         return blocks_out\n </s> add         return BlockList(blocks_out, new_metadata)", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     blocks: List[ObjectRef[Block]] = []\n <mask>     i = 0\n <mask>     while i < len(items):\n <mask>         builder = SimpleBlock.builder()\n <mask>         for item in items[i:i + block_size]:\n <mask>             builder.add(item)\n <mask>         block = builder.build()\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove         builder = ListBlock.builder()\n </s> add         builder = SimpleBlock.builder() </s> remove         blocks.append(ray.put(builder.build()))\n </s> add         block = builder.build()\n        blocks.append(ray.put(block))\n        metadata.append(\n            BlockMetadata(\n                num_rows=block.num_rows(),\n                size_bytes=block.size_bytes(),\n                schema=type(items[0]),\n                input_files=None)) </s> remove     def gen_block(start: int, count: int) -> ListBlock:\n        builder = ListBlock.builder()\n </s> add     def gen_block(start: int, count: int) -> SimpleBlock:\n        builder = SimpleBlock.builder() </s> remove     blocks: List[ObjectRef[Block]] = []\n </s> add  </s> add     calls: List[Callable[[], ObjectRef[Block]]] = []\n    metadata: List[BlockMetadata] = [] </s> remove         blocks.append(gen_block.remote(i, min(block_size, n - i)))\n </s> add         def make_call(start: int, count: int) -> ObjectRef[Block]:\n            return lambda: gen_block.remote(start, count)\n\n        count = min(block_size, n - i)\n        calls.append(make_call(i, count))\n        metadata.append(\n            BlockMetadata(\n                num_rows=count,\n                size_bytes=8 * count,\n                schema=int,\n                input_files=None))", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep replace keep keep replace keep keep keep keep", "code_tokens": " <mask>     while i < len(items):\n <mask>         builder = ListBlock.builder()\n <mask>         for item in items[i:i + block_size]:\n <mask>             builder.add(item)\n <mask>         blocks.append(ray.put(builder.build()))\n <mask>         i += block_size\n <mask> \n <mask>     return Dataset(blocks)\n <mask> \n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add     metadata: List[BlockMetadata] = [] </s> remove     return Dataset(blocks)\n </s> add     return Dataset(BlockList(blocks, metadata)) </s> remove     return Dataset(blocks)\n </s> add     return Dataset(LazyBlockList(calls, metadata)) </s> remove         blocks.append(gen_block.remote(block_size * i, min(block_size, n - i)))\n </s> add         def make_call(start: int, count: int) -> ObjectRef[Block]:\n            return lambda: gen_block.remote(start, count)\n\n        start = block_size * i\n        count = min(block_size, n - i)\n        calls.append(make_call(start, count))\n        schema = pyarrow.Table.from_pydict({\"value\": [0]}).schema\n        metadata.append(\n            BlockMetadata(\n                num_rows=count,\n                size_bytes=8 * count,\n                schema=schema,\n                input_files=None)) </s> remove     return Dataset(blocks)\n </s> add     return Dataset(LazyBlockList(calls, metadata))", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             builder.add(item)\n <mask>         blocks.append(ray.put(builder.build()))\n <mask>         i += block_size\n <mask> \n <mask>     return Dataset(blocks)\n <mask> \n <mask> \n <mask> @autoinit_ray\n <mask> def range(n: int, parallelism: int = 200) -> Dataset[int]:\n <mask>     \"\"\"Create a dataset from a range of integers [0..n).\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     return Dataset(blocks)\n </s> add     return Dataset(LazyBlockList(calls, metadata)) </s> remove     return Dataset([df_to_block.remote(df) for df in dfs])\n </s> add     res = [df_to_block.remote(df) for df in dfs]\n    blocks, metadata = zip(*res)\n    return Dataset(BlockList(blocks, ray.get(list(metadata)))) </s> remove         blocks.append(ray.put(builder.build()))\n </s> add         block = builder.build()\n        blocks.append(ray.put(block))\n        metadata.append(\n            BlockMetadata(\n                num_rows=block.num_rows(),\n                size_bytes=block.size_bytes(),\n                schema=type(items[0]),\n                input_files=None)) </s> remove     def to_batch_iterators(\n            self,\n            num_shards: int,\n            batch_size: int = None,\n            output_location_prefs: List[Any] = None,\n            batch_format: str = \"pandas\",\n            repeatable: bool = False) -> List[Iterator[BatchType]]:\n        \"\"\"Return a list of distributed iterators over record batches.\n </s> add     def iter_batches(self,\n                     prefetch_blocks: int = 0,\n                     batch_size: int = None,\n                     batch_format: str = \"pandas\") -> Iterator[BatchType]:\n        \"\"\"Return a local batched iterator over the dataset. </s> remove         builder = ListBlock.builder()\n </s> add         builder = SimpleBlock.builder() </s> remove     return Dataset(blocks)\n </s> add     return Dataset(LazyBlockList(calls, metadata))", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>     Returns:\n <mask>         Dataset holding the integers.\n <mask>     \"\"\"\n <mask>     block_size = max(1, n // parallelism)\n <mask> \n <mask>     @ray.remote\n <mask>     def gen_block(start: int, count: int) -> SimpleBlock:\n <mask>         builder = SimpleBlock.builder()\n <mask>         for value in builtins.range(start, start + count):\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     blocks: List[ObjectRef[Block]] = []\n </s> add  </s> remove     def gen_block(start: int, count: int) -> ListBlock:\n        builder = ListBlock.builder()\n </s> add     def gen_block(start: int, count: int) -> SimpleBlock:\n        builder = SimpleBlock.builder() </s> remove     blocks = []\n </s> add  </s> add     import pyarrow\n\n    calls: List[Callable[[], ObjectRef[Block]]] = []\n    metadata: List[BlockMetadata] = [] </s> remove         import pyarrow\n\n </s> add  </s> remove         blocks.append(gen_block.remote(block_size * i, min(block_size, n - i)))\n </s> add         def make_call(start: int, count: int) -> ObjectRef[Block]:\n            return lambda: gen_block.remote(start, count)\n\n        start = block_size * i\n        count = min(block_size, n - i)\n        calls.append(make_call(start, count))\n        schema = pyarrow.Table.from_pydict({\"value\": [0]}).schema\n        metadata.append(\n            BlockMetadata(\n                num_rows=count,\n                size_bytes=8 * count,\n                schema=schema,\n                input_files=None))", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep replace keep keep replace replace keep keep", "code_tokens": " <mask>     block_size = max(1, n // parallelism)\n <mask>     blocks: List[ObjectRef[Block]] = []\n <mask> \n <mask>     @ray.remote\n <mask>     def gen_block(start: int, count: int) -> ListBlock:\n <mask>         builder = ListBlock.builder()\n <mask>         for value in builtins.range(start, start + count):\n <mask>             builder.add(value)\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add     calls: List[Callable[[], ObjectRef[Block]]] = []\n    metadata: List[BlockMetadata] = [] </s> remove     blocks = []\n </s> add  </s> add     import pyarrow\n\n    calls: List[Callable[[], ObjectRef[Block]]] = []\n    metadata: List[BlockMetadata] = [] </s> remove         builder = ListBlock.builder()\n </s> add         builder = SimpleBlock.builder() </s> remove         import pyarrow\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep replace keep keep replace keep keep", "code_tokens": " <mask>     while i < n:\n <mask>         blocks.append(gen_block.remote(i, min(block_size, n - i)))\n <mask>         i += block_size\n <mask> \n <mask>     return Dataset(blocks)\n <mask> \n <mask> \n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     return Dataset(blocks)\n </s> add     return Dataset(LazyBlockList(calls, metadata)) </s> remove         blocks.append(gen_block.remote(block_size * i, min(block_size, n - i)))\n </s> add         def make_call(start: int, count: int) -> ObjectRef[Block]:\n            return lambda: gen_block.remote(start, count)\n\n        start = block_size * i\n        count = min(block_size, n - i)\n        calls.append(make_call(start, count))\n        schema = pyarrow.Table.from_pydict({\"value\": [0]}).schema\n        metadata.append(\n            BlockMetadata(\n                num_rows=count,\n                size_bytes=8 * count,\n                schema=schema,\n                input_files=None)) </s> remove         blocks.append(ray.put(builder.build()))\n </s> add         block = builder.build()\n        blocks.append(ray.put(block))\n        metadata.append(\n            BlockMetadata(\n                num_rows=block.num_rows(),\n                size_bytes=block.size_bytes(),\n                schema=type(items[0]),\n                input_files=None)) </s> remove         builder = ListBlock.builder()\n </s> add         builder = SimpleBlock.builder() </s> remove     return Dataset(blocks)\n </s> add     return Dataset(BlockList(blocks, metadata))", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     Returns:\n <mask>         Dataset holding the integers as Arrow records.\n <mask>     \"\"\"\n <mask>     block_size = max(1, n // parallelism)\n <mask>     i = 0\n <mask> \n <mask>     @ray.remote\n <mask>     def gen_block(start: int, count: int) -> \"ArrowBlock\":\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     blocks = []\n </s> add  </s> remove     blocks: List[ObjectRef[Block]] = []\n </s> add  </s> add     calls: List[Callable[[], ObjectRef[Block]]] = []\n    metadata: List[BlockMetadata] = [] </s> remove     def gen_block(start: int, count: int) -> ListBlock:\n        builder = ListBlock.builder()\n </s> add     def gen_block(start: int, count: int) -> SimpleBlock:\n        builder = SimpleBlock.builder() </s> remove         import pyarrow\n\n </s> add  </s> remove     import numpy as np\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep replace keep keep keep keep replace replace keep keep keep keep", "code_tokens": " <mask>     \"\"\"\n <mask>     block_size = max(1, n // parallelism)\n <mask>     blocks = []\n <mask>     i = 0\n <mask> \n <mask>     @ray.remote\n <mask>     def gen_block(start: int, count: int) -> \"ArrowBlock\":\n <mask>         import pyarrow\n <mask> \n <mask>         return ArrowBlock(\n <mask>             pyarrow.Table.from_pydict({\n <mask>                 \"value\": list(builtins.range(start, start + count))\n <mask>             }))\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add     import pyarrow\n\n    calls: List[Callable[[], ObjectRef[Block]]] = []\n    metadata: List[BlockMetadata] = [] </s> remove     def gen_block(start: int, count: int) -> ListBlock:\n        builder = ListBlock.builder()\n </s> add     def gen_block(start: int, count: int) -> SimpleBlock:\n        builder = SimpleBlock.builder() </s> remove     blocks: List[ObjectRef[Block]] = []\n </s> add  </s> add     calls: List[Callable[[], ObjectRef[Block]]] = []\n    metadata: List[BlockMetadata] = [] </s> remove         blocks.append(gen_block.remote(block_size * i, min(block_size, n - i)))\n </s> add         def make_call(start: int, count: int) -> ObjectRef[Block]:\n            return lambda: gen_block.remote(start, count)\n\n        start = block_size * i\n        count = min(block_size, n - i)\n        calls.append(make_call(start, count))\n        schema = pyarrow.Table.from_pydict({\"value\": [0]}).schema\n        metadata.append(\n            BlockMetadata(\n                num_rows=count,\n                size_bytes=8 * count,\n                schema=schema,\n                input_files=None))", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep keep replace keep keep replace keep keep keep keep", "code_tokens": " <mask>             }))\n <mask> \n <mask>     while i < n:\n <mask>         blocks.append(gen_block.remote(block_size * i, min(block_size, n - i)))\n <mask>         i += block_size\n <mask> \n <mask>     return Dataset(blocks)\n <mask> \n <mask> \n <mask> @autoinit_ray\n <mask> def read_parquet(paths: Union[str, List[str]],\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove         blocks.append(gen_block.remote(i, min(block_size, n - i)))\n </s> add         def make_call(start: int, count: int) -> ObjectRef[Block]:\n            return lambda: gen_block.remote(start, count)\n\n        count = min(block_size, n - i)\n        calls.append(make_call(i, count))\n        metadata.append(\n            BlockMetadata(\n                num_rows=count,\n                size_bytes=8 * count,\n                schema=int,\n                input_files=None)) </s> remove     return Dataset(blocks)\n </s> add     return Dataset(LazyBlockList(calls, metadata)) </s> remove         blocks.append(ray.put(builder.build()))\n </s> add         block = builder.build()\n        blocks.append(ray.put(block))\n        metadata.append(\n            BlockMetadata(\n                num_rows=block.num_rows(),\n                size_bytes=block.size_bytes(),\n                schema=type(items[0]),\n                input_files=None)) </s> remove     return Dataset(blocks)\n </s> add     return Dataset(BlockList(blocks, metadata)) </s> remove         builder = ListBlock.builder()\n </s> add         builder = SimpleBlock.builder()", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep replace keep keep replace replace replace replace replace replace replace replace replace replace", "code_tokens": " <mask>     \"\"\"\n <mask>     import pyarrow.parquet as pq\n <mask>     import numpy as np\n <mask> \n <mask>     pq_ds = pq.ParquetDataset(paths, **arrow_parquet_args)\n <mask>     pieces = pq_ds.pieces\n <mask>     data_pieces = []\n <mask> \n <mask>     for piece in pieces:\n <mask>         num_row_groups = piece.get_metadata().to_dict()[\"num_row_groups\"]\n <mask>         for i in builtins.range(num_row_groups):\n <mask>             data_pieces.append(\n <mask>                 pq.ParquetDatasetPiece(piece.path, piece.open_file_func,\n <mask>                                        piece.file_options, i,\n <mask>                                        piece.partition_keys))\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add     read_tasks = [[] for _ in builtins.range(parallelism)]\n    # TODO(ekl) support reading row groups (maybe as an option)\n    for i, piece in enumerate(pq_ds.pieces):\n        read_tasks[i % len(read_tasks)].append(piece)\n    nonempty_tasks = [r for r in read_tasks if r] </s> remove     @ray.remote\n </s> add     @ray.remote(num_returns=2) </s> remove     @ray.remote\n </s> add     @ray.remote(num_returns=2) </s> remove     return Dataset([\n        gen_read.remote(ps) for ps in np.array_split(pieces, parallelism)\n        if len(ps) > 0\n    ])\n </s> add     calls: List[Callable[[], ObjectRef[Block]]] = []\n    metadata: List[BlockMetadata] = []\n    for pieces in nonempty_tasks:\n\n        def make_call(\n                pieces: List[pq.ParquetDatasetPiece]) -> ObjectRef[Block]:\n            return lambda: gen_read.remote(pieces)\n\n        calls.append(make_call(pieces))\n        piece_metadata = [p.get_metadata() for p in pieces]\n        metadata.append(\n            BlockMetadata(\n                num_rows=sum(m.num_rows for m in piece_metadata),\n                size_bytes=sum(\n                    sum(\n                        m.row_group(i).total_byte_size\n                        for i in builtins.range(m.num_row_groups))\n                    for m in piece_metadata),\n                schema=piece_metadata[0].schema.to_arrow_schema(),\n                input_files=[p.path for p in pieces]))\n\n    return Dataset(LazyBlockList(calls, metadata)) </s> remove         for i, item in enumerate(ds.to_local_iterator()):\n </s> add         for i, item in enumerate(ds.iter_rows()):", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>     import pyarrow.parquet as pq\n <mask> \n <mask>     pq_ds = pq.ParquetDataset(paths, **arrow_parquet_args)\n <mask> \n <mask>     partitions = pq_ds.partitions\n <mask> \n <mask>     @ray.remote\n <mask>     def gen_read(pieces: List[pq.ParquetDatasetPiece]):\n <mask>         import pyarrow\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     pieces = pq_ds.pieces\n    data_pieces = []\n\n    for piece in pieces:\n        num_row_groups = piece.get_metadata().to_dict()[\"num_row_groups\"]\n        for i in builtins.range(num_row_groups):\n            data_pieces.append(\n                pq.ParquetDatasetPiece(piece.path, piece.open_file_func,\n                                       piece.file_options, i,\n                                       piece.partition_keys))\n </s> add  </s> remove     import numpy as np\n </s> add  </s> add         import pyarrow </s> remove         return ArrowBlock(pa.table(df))\n </s> add         block = ArrowBlock(pa.table(df))\n        return block, block.get_metadata(input_files=None) </s> remove     blocks = []\n </s> add  </s> add     import pyarrow\n\n    calls: List[Callable[[], ObjectRef[Block]]] = []\n    metadata: List[BlockMetadata] = []", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask>     @ray.remote\n <mask>     def gen_read(pieces: List[pq.ParquetDatasetPiece]):\n <mask>         logger.debug(\"Reading {} parquet pieces\".format(len(pieces)))\n <mask>         tables = [\n <mask>             piece.read(\n <mask>                 columns=columns, use_threads=False, partitions=partitions)\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove         table = piece.read(\n            columns=columns, use_threads=False, partitions=partitions)\n </s> add         tables = [\n            piece.read(\n                columns=columns, use_threads=False, partitions=partitions)\n            for piece in pieces\n        ]\n        if len(tables) > 1:\n            table = pyarrow.concat_tables(tables)\n        else:\n            table = tables[0] </s> remove     return Dataset([\n        gen_read.remote(ps) for ps in np.array_split(pieces, parallelism)\n        if len(ps) > 0\n    ])\n </s> add     calls: List[Callable[[], ObjectRef[Block]]] = []\n    metadata: List[BlockMetadata] = []\n    for pieces in nonempty_tasks:\n\n        def make_call(\n                pieces: List[pq.ParquetDatasetPiece]) -> ObjectRef[Block]:\n            return lambda: gen_read.remote(pieces)\n\n        calls.append(make_call(pieces))\n        piece_metadata = [p.get_metadata() for p in pieces]\n        metadata.append(\n            BlockMetadata(\n                num_rows=sum(m.num_rows for m in piece_metadata),\n                size_bytes=sum(\n                    sum(\n                        m.row_group(i).total_byte_size\n                        for i in builtins.range(m.num_row_groups))\n                    for m in piece_metadata),\n                schema=piece_metadata[0].schema.to_arrow_schema(),\n                input_files=[p.path for p in pieces]))\n\n    return Dataset(LazyBlockList(calls, metadata)) </s> remove     pieces = pq_ds.pieces\n    data_pieces = []\n\n    for piece in pieces:\n        num_row_groups = piece.get_metadata().to_dict()[\"num_row_groups\"]\n        for i in builtins.range(num_row_groups):\n            data_pieces.append(\n                pq.ParquetDatasetPiece(piece.path, piece.open_file_func,\n                                       piece.file_options, i,\n                                       piece.partition_keys))\n </s> add  </s> add     read_tasks = [[] for _ in builtins.range(parallelism)]\n    # TODO(ekl) support reading row groups (maybe as an option)\n    for i, piece in enumerate(pq_ds.pieces):\n        read_tasks[i % len(read_tasks)].append(piece)\n    nonempty_tasks = [r for r in read_tasks if r] </s> remove     @ray.remote\n </s> add     @ray.remote(num_returns=2) </s> remove     @ray.remote\n </s> add     @ray.remote(num_returns=2)", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep keep replace replace keep keep replace replace replace replace keep keep keep", "code_tokens": " <mask>     @ray.remote\n <mask>     def gen_read(pieces: List[pq.ParquetDatasetPiece]):\n <mask>         logger.debug(\"Reading {} parquet pieces\".format(len(pieces)))\n <mask>         table = piece.read(\n <mask>             columns=columns, use_threads=False, partitions=partitions)\n <mask>         return ArrowBlock(table)\n <mask> \n <mask>     return Dataset([\n <mask>         gen_read.remote(ps) for ps in np.array_split(pieces, parallelism)\n <mask>         if len(ps) > 0\n <mask>     ])\n <mask> \n <mask> \n <mask> @autoinit_ray\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add         import pyarrow </s> remove     ])\n </s> add     ]\n\n    blocks, metadata = zip(*res)\n    return Dataset(BlockList(blocks, ray.get(list(metadata)))) </s> remove     ])\n </s> add     ]\n\n    blocks, metadata = zip(*res)\n    return Dataset(BlockList(blocks, ray.get(list(metadata)))) </s> remove     return Dataset([\n </s> add     res = [ </s> remove     return Dataset([\n </s> add     res = [", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     import numpy as np\n <mask> \n <mask>     paths, filesystem = _resolve_paths_and_filesystem(paths, filesystem)\n <mask> \n <mask>     @ray.remote\n <mask>     def json_read(read_paths: List[str]):\n <mask>         logger.debug(f\"Reading {len(read_paths)} files.\")\n <mask>         tables = []\n <mask>         for read_path in read_paths:\n <mask>             with filesystem.open_input_file(read_path) as f:\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     @ray.remote\n </s> add     @ray.remote(num_returns=2) </s> remove     pieces = pq_ds.pieces\n    data_pieces = []\n\n    for piece in pieces:\n        num_row_groups = piece.get_metadata().to_dict()[\"num_row_groups\"]\n        for i in builtins.range(num_row_groups):\n            data_pieces.append(\n                pq.ParquetDatasetPiece(piece.path, piece.open_file_func,\n                                       piece.file_options, i,\n                                       piece.partition_keys))\n </s> add  </s> remove     import numpy as np\n </s> add  </s> remove         for i, (path, item) in enumerate(ds.to_local_iterator()):\n </s> add         for i, (path, item) in enumerate(ds.iter_rows()): </s> add     read_tasks = [[] for _ in builtins.range(parallelism)]\n    # TODO(ekl) support reading row groups (maybe as an option)\n    for i, piece in enumerate(pq_ds.pieces):\n        read_tasks[i % len(read_tasks)].append(piece)\n    nonempty_tasks = [r for r in read_tasks if r] </s> add         # Test metadata ops.\n        assert ds.count() == 10\n        assert \"bytes\" in str(ds.schema()), ds\n        assert \"bytes\" in str(ds), ds", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep replace keep replace keep keep keep", "code_tokens": " <mask>                         **arrow_json_args))\n <mask>         return ArrowBlock(pa.concat_tables(tables))\n <mask> \n <mask>     return Dataset([\n <mask>         json_read.remote(read_paths)\n <mask>         for read_paths in np.array_split(paths, parallelism)\n <mask>         if len(read_paths) > 0\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     return Dataset([\n </s> add     res = [ </s> remove         return ArrowBlock(pa.concat_tables(tables))\n </s> add         block = ArrowBlock(pa.concat_tables(tables))\n        return block, block.get_metadata(input_files=read_paths) </s> remove     ])\n </s> add     ]\n\n    blocks, metadata = zip(*res)\n    return Dataset(BlockList(blocks, ray.get(list(metadata)))) </s> remove     ])\n </s> add     ]\n\n    blocks, metadata = zip(*res)\n    return Dataset(BlockList(blocks, ray.get(list(metadata)))) </s> remove         table = piece.read(\n            columns=columns, use_threads=False, partitions=partitions)\n </s> add         tables = [\n            piece.read(\n                columns=columns, use_threads=False, partitions=partitions)\n            for piece in pieces\n        ]\n        if len(tables) > 1:\n            table = pyarrow.concat_tables(tables)\n        else:\n            table = tables[0]", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     return Dataset([\n <mask>         json_read.remote(read_paths)\n <mask>         for read_paths in np.array_split(paths, parallelism)\n <mask>         if len(read_paths) > 0\n <mask>     ])\n <mask> \n <mask> \n <mask> @autoinit_ray\n <mask> def read_csv(paths: Union[str, List[str]],\n <mask>              filesystem: Optional[\"pyarrow.fs.FileSystem\"] = None,\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     ])\n </s> add     ]\n\n    blocks, metadata = zip(*res)\n    return Dataset(BlockList(blocks, ray.get(list(metadata)))) </s> remove     return Dataset([\n </s> add     res = [ </s> remove     return Dataset([\n </s> add     res = [ </s> remove         return ArrowBlock(pa.concat_tables(tables))\n </s> add         block = ArrowBlock(pa.concat_tables(tables))\n        return block, block.get_metadata(input_files=read_paths) </s> remove     return Dataset([\n        gen_read.remote(ps) for ps in np.array_split(pieces, parallelism)\n        if len(ps) > 0\n    ])\n </s> add     calls: List[Callable[[], ObjectRef[Block]]] = []\n    metadata: List[BlockMetadata] = []\n    for pieces in nonempty_tasks:\n\n        def make_call(\n                pieces: List[pq.ParquetDatasetPiece]) -> ObjectRef[Block]:\n            return lambda: gen_read.remote(pieces)\n\n        calls.append(make_call(pieces))\n        piece_metadata = [p.get_metadata() for p in pieces]\n        metadata.append(\n            BlockMetadata(\n                num_rows=sum(m.num_rows for m in piece_metadata),\n                size_bytes=sum(\n                    sum(\n                        m.row_group(i).total_byte_size\n                        for i in builtins.range(m.num_row_groups))\n                    for m in piece_metadata),\n                schema=piece_metadata[0].schema.to_arrow_schema(),\n                input_files=[p.path for p in pieces]))\n\n    return Dataset(LazyBlockList(calls, metadata)) </s> remove         return ArrowBlock(pa.concat_tables(tables))\n </s> add         block = ArrowBlock(pa.concat_tables(tables))\n        return block, block.get_metadata(input_files=read_paths)", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     import numpy as np\n <mask> \n <mask>     paths, filesystem = _resolve_paths_and_filesystem(paths, filesystem)\n <mask> \n <mask>     @ray.remote\n <mask>     def csv_read(read_paths: List[str]):\n <mask>         logger.debug(f\"Reading {len(read_paths)} files.\")\n <mask>         tables = []\n <mask>         for read_path in read_paths:\n <mask>             with filesystem.open_input_file(read_path) as f:\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     @ray.remote\n </s> add     @ray.remote(num_returns=2) </s> remove     pieces = pq_ds.pieces\n    data_pieces = []\n\n    for piece in pieces:\n        num_row_groups = piece.get_metadata().to_dict()[\"num_row_groups\"]\n        for i in builtins.range(num_row_groups):\n            data_pieces.append(\n                pq.ParquetDatasetPiece(piece.path, piece.open_file_func,\n                                       piece.file_options, i,\n                                       piece.partition_keys))\n </s> add  </s> remove     import numpy as np\n </s> add  </s> remove         for i, (path, item) in enumerate(ds.to_local_iterator()):\n </s> add         for i, (path, item) in enumerate(ds.iter_rows()): </s> add     read_tasks = [[] for _ in builtins.range(parallelism)]\n    # TODO(ekl) support reading row groups (maybe as an option)\n    for i, piece in enumerate(pq_ds.pieces):\n        read_tasks[i % len(read_tasks)].append(piece)\n    nonempty_tasks = [r for r in read_tasks if r] </s> add         # Test metadata ops.\n        assert ds.count() == 10\n        assert \"bytes\" in str(ds.schema()), ds\n        assert \"bytes\" in str(ds), ds", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep keep replace keep replace keep keep", "code_tokens": " <mask>                         f,\n <mask>                         read_options=csv.ReadOptions(use_threads=False),\n <mask>                         **arrow_csv_args))\n <mask>         return ArrowBlock(pa.concat_tables(tables))\n <mask> \n <mask>     return Dataset([\n <mask>         csv_read.remote(read_paths)\n <mask>         for read_paths in np.array_split(paths, parallelism)\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove         return ArrowBlock(pa.concat_tables(tables))\n </s> add         block = ArrowBlock(pa.concat_tables(tables))\n        return block, block.get_metadata(input_files=read_paths) </s> remove     return Dataset([\n </s> add     res = [ </s> remove     ])\n </s> add     ]\n\n    blocks, metadata = zip(*res)\n    return Dataset(BlockList(blocks, ray.get(list(metadata)))) </s> remove     ])\n </s> add     ]\n\n    blocks, metadata = zip(*res)\n    return Dataset(BlockList(blocks, ray.get(list(metadata)))) </s> remove         table = piece.read(\n            columns=columns, use_threads=False, partitions=partitions)\n </s> add         tables = [\n            piece.read(\n                columns=columns, use_threads=False, partitions=partitions)\n            for piece in pieces\n        ]\n        if len(tables) > 1:\n            table = pyarrow.concat_tables(tables)\n        else:\n            table = tables[0]", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     return Dataset([\n <mask>         csv_read.remote(read_paths)\n <mask>         for read_paths in np.array_split(paths, parallelism)\n <mask>         if len(read_paths) > 0\n <mask>     ])\n <mask> \n <mask> \n <mask> @autoinit_ray\n <mask> def read_binary_files(\n <mask>         paths: Union[str, List[str]],\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     ])\n </s> add     ]\n\n    blocks, metadata = zip(*res)\n    return Dataset(BlockList(blocks, ray.get(list(metadata)))) </s> remove     return Dataset([\n </s> add     res = [ </s> remove     return Dataset([\n </s> add     res = [ </s> remove         return ArrowBlock(pa.concat_tables(tables))\n </s> add         block = ArrowBlock(pa.concat_tables(tables))\n        return block, block.get_metadata(input_files=read_paths) </s> remove         return ArrowBlock(pa.concat_tables(tables))\n </s> add         block = ArrowBlock(pa.concat_tables(tables))\n        return block, block.get_metadata(input_files=read_paths) </s> remove     return Dataset([\n        gen_read.remote(ps) for ps in np.array_split(pieces, parallelism)\n        if len(ps) > 0\n    ])\n </s> add     calls: List[Callable[[], ObjectRef[Block]]] = []\n    metadata: List[BlockMetadata] = []\n    for pieces in nonempty_tasks:\n\n        def make_call(\n                pieces: List[pq.ParquetDatasetPiece]) -> ObjectRef[Block]:\n            return lambda: gen_read.remote(pieces)\n\n        calls.append(make_call(pieces))\n        piece_metadata = [p.get_metadata() for p in pieces]\n        metadata.append(\n            BlockMetadata(\n                num_rows=sum(m.num_rows for m in piece_metadata),\n                size_bytes=sum(\n                    sum(\n                        m.row_group(i).total_byte_size\n                        for i in builtins.range(m.num_row_groups))\n                    for m in piece_metadata),\n                schema=piece_metadata[0].schema.to_arrow_schema(),\n                input_files=[p.path for p in pieces]))\n\n    return Dataset(LazyBlockList(calls, metadata))", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep keep replace keep replace", "code_tokens": " <mask>     \"\"\"\n <mask>     import pyarrow as pa\n <mask> \n <mask>     @ray.remote\n <mask>     def df_to_block(df: \"pandas.DataFrame\"):\n <mask>         return ArrowBlock(pa.table(df))\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     return Dataset([df_to_block.remote(df) for df in dfs])\n </s> add     res = [df_to_block.remote(df) for df in dfs]\n    blocks, metadata = zip(*res)\n    return Dataset(BlockList(blocks, ray.get(list(metadata)))) </s> remove     blocks = []\n </s> add  </s> add     import pyarrow\n\n    calls: List[Callable[[], ObjectRef[Block]]] = []\n    metadata: List[BlockMetadata] = [] </s> add     read_tasks = [[] for _ in builtins.range(parallelism)]\n    # TODO(ekl) support reading row groups (maybe as an option)\n    for i, piece in enumerate(pq_ds.pieces):\n        read_tasks[i % len(read_tasks)].append(piece)\n    nonempty_tasks = [r for r in read_tasks if r] </s> add         import pyarrow", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     @ray.remote\n <mask>     def df_to_block(df: \"pandas.DataFrame\"):\n <mask>         return ArrowBlock(pa.table(df))\n <mask> \n <mask>     return Dataset([df_to_block.remote(df) for df in dfs])\n <mask> \n <mask> \n <mask> def from_spark(df: \"pyspark.sql.DataFrame\",\n <mask>                parallelism: int = 200) -> Dataset[ArrowRow]:\n <mask>     \"\"\"Create a dataset from a Spark dataframe.\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove         return ArrowBlock(pa.table(df))\n </s> add         block = ArrowBlock(pa.table(df))\n        return block, block.get_metadata(input_files=None) </s> remove     @ray.remote\n </s> add     @ray.remote(num_returns=2) </s> remove     return Dataset(blocks)\n </s> add     return Dataset(BlockList(blocks, metadata)) </s> remove     return Dataset(blocks)\n </s> add     return Dataset(LazyBlockList(calls, metadata)) </s> remove     def to_batch_iterators(\n            self,\n            num_shards: int,\n            batch_size: int = None,\n            output_location_prefs: List[Any] = None,\n            batch_format: str = \"pandas\",\n            repeatable: bool = False) -> List[Iterator[BatchType]]:\n        \"\"\"Return a list of distributed iterators over record batches.\n </s> add     def iter_batches(self,\n                     prefetch_blocks: int = 0,\n                     batch_size: int = None,\n                     batch_format: str = \"pandas\") -> Iterator[BatchType]:\n        \"\"\"Return a local batched iterator over the dataset. </s> remove     def to_pandas(self) -> Iterator[ObjectRef[\"pandas.DataFrame\"]]:\n </s> add     def to_pandas(self) -> List[ObjectRef[\"pandas.DataFrame\"]]:", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask> import ray.experimental.data.tests.util as util\n <mask> \n <mask> \n <mask> def test_basic(ray_start_regular_shared):\n <mask>     ds = ray.experimental.data.range(5)\n <mask>     assert sorted(ds.map(lambda x: x + 1).take()) == [1, 2, 3, 4, 5]\n <mask>     assert ds.count() == 5\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove     assert sorted(ds.to_local_iterator()) == [0, 1, 2, 3, 4]\n </s> add     assert sorted(ds.iter_rows()) == [0, 1, 2, 3, 4]\n\n\ndef test_empty_dataset(ray_start_regular_shared):\n    ds = ray.experimental.data.range(0)\n    assert ds.count() == 0\n    with pytest.raises(ValueError):\n        ds.size_bytes()\n    with pytest.raises(ValueError):\n        ds.schema()\n\n    ds = ray.experimental.data.range(1)\n    ds = ds.filter(lambda x: x > 1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=0, num_blocks=1, schema=Unknown schema)\"\n\n\ndef test_schema(ray_start_regular_shared):\n    ds = ray.experimental.data.range(10)\n    ds2 = ray.experimental.data.range_arrow(10)\n    ds3 = ds2.repartition(5)\n    ds4 = ds3.map(lambda x: {\"a\": \"hi\", \"b\": 1.0}).limit(5).repartition(1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema=<class 'int'>)\"\n    assert str(ds2) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema={value: int64})\"\n    assert str(ds3) == \\\n        \"Dataset(num_rows=10, num_blocks=5, schema={value: int64})\"\n    assert str(ds4) == \\\n        \"Dataset(num_rows=5, num_blocks=1, schema={a: string, b: double})\"\n\n\ndef test_lazy_loading_exponential_rampup(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    assert len(ds._blocks._blocks) == 1\n    assert ds.take(10) == list(range(10))\n    assert len(ds._blocks._blocks) == 2\n    assert ds.take(20) == list(range(20))\n    assert len(ds._blocks._blocks) == 4\n    assert ds.take(30) == list(range(30))\n    assert len(ds._blocks._blocks) == 8\n    assert ds.take(50) == list(range(50))\n    assert len(ds._blocks._blocks) == 16\n    assert ds.take(100) == list(range(100))\n    assert len(ds._blocks._blocks) == 20\n\n\ndef test_limit(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    for i in range(100):\n        assert ds.limit(i).take(200) == list(range(i)) </s> remove     assert sorted(values) == [[4, \"e\"], [4, \"e\"], [5, \"f\"], [5, \"f\"], [6, \"g\"],\n </s> add     # Test metadata-only parquet ops.\n    assert len(ds._blocks._blocks) == 1\n    assert ds.count() == 6\n    assert ds.size_bytes() > 0\n    assert ds.schema() is not None\n    input_files = ds.input_files()\n    assert len(input_files) == 2, input_files\n    assert \"test1.parquet\" in str(input_files)\n    assert \"test2.parquet\" in str(input_files)\n    assert str(ds) == \\\n        \"Dataset(num_rows=6, num_blocks=2, \" \\\n        \"schema={one: int64, two: string})\", ds\n    assert repr(ds) == \\\n        \"Dataset(num_rows=6, num_blocks=2, \" \\\n        \"schema={one: int64, two: string})\", ds\n    assert len(ds._blocks._blocks) == 1\n\n    # Forces a data read.\n    values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\n    assert len(ds._blocks._blocks) == 2\n    assert sorted(values) == [[1, \"a\"], [2, \"b\"], [3, \"c\"], [4, \"e\"], [5, \"f\"], </s> add         # Test metadata ops.\n        assert ds.count() == 10\n        assert \"bytes\" in str(ds.schema()), ds\n        assert \"bytes\" in str(ds), ds </s> remove         for i, (path, item) in enumerate(ds.to_local_iterator()):\n </s> add         for i, (path, item) in enumerate(ds.iter_rows()): </s> add     # Test metadata ops.\n    assert ds.count() == 3\n    assert \"two\" in str(ds.schema())\n    assert \"two\" in str(ds) </s> add     # Test metadata ops.\n    assert ds.count() == 3\n    assert \"two\" in str(ds.schema())\n    assert \"two\" in str(ds)", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> def test_basic(ray_start_regular_shared):\n <mask>     ds = ray.experimental.data.range(5)\n <mask>     assert sorted(ds.map(lambda x: x + 1).take()) == [1, 2, 3, 4, 5]\n <mask>     assert ds.count() == 5\n <mask>     assert sorted(ds.to_local_iterator()) == [0, 1, 2, 3, 4]\n <mask> \n <mask> \n <mask> def test_convert_types(ray_start_regular_shared):\n <mask>     plain_ds = ray.experimental.data.range(1)\n <mask>     arrow_ds = plain_ds.map(lambda x: {\"a\": x})\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add def test_basic_actors(shutdown_only):\n    ray.init(num_cpus=2)\n    ds = ray.experimental.data.range(5)\n    assert sorted(ds.map(lambda x: x + 1,\n                         compute=\"actors\").take()) == [1, 2, 3, 4, 5]\n\n </s> remove     assert sorted(values) == [[4, \"e\"], [4, \"e\"], [5, \"f\"], [5, \"f\"], [6, \"g\"],\n </s> add     # Test metadata-only parquet ops.\n    assert len(ds._blocks._blocks) == 1\n    assert ds.count() == 6\n    assert ds.size_bytes() > 0\n    assert ds.schema() is not None\n    input_files = ds.input_files()\n    assert len(input_files) == 2, input_files\n    assert \"test1.parquet\" in str(input_files)\n    assert \"test2.parquet\" in str(input_files)\n    assert str(ds) == \\\n        \"Dataset(num_rows=6, num_blocks=2, \" \\\n        \"schema={one: int64, two: string})\", ds\n    assert repr(ds) == \\\n        \"Dataset(num_rows=6, num_blocks=2, \" \\\n        \"schema={one: int64, two: string})\", ds\n    assert len(ds._blocks._blocks) == 1\n\n    # Forces a data read.\n    values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\n    assert len(ds._blocks._blocks) == 2\n    assert sorted(values) == [[1, \"a\"], [2, \"b\"], [3, \"c\"], [4, \"e\"], [5, \"f\"], </s> add         # Test metadata ops.\n        assert ds.count() == 10\n        assert \"bytes\" in str(ds.schema()), ds\n        assert \"bytes\" in str(ds), ds </s> add     # Test metadata ops.\n    assert ds.count() == 3\n    assert \"two\" in str(ds.schema())\n    assert \"two\" in str(ds) </s> remove         for i, (path, item) in enumerate(ds.to_local_iterator()):\n </s> add         for i, (path, item) in enumerate(ds.iter_rows()): </s> add     # Test metadata ops.\n    assert ds.count() == 3\n    assert \"two\" in str(ds.schema())\n    assert \"two\" in str(ds)", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep replace keep keep keep keep keep keep keep keep replace", "code_tokens": " <mask> \n <mask>     ds = ray.experimental.data.read_parquet(tmp_path)\n <mask>     values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\n <mask> \n <mask>     assert sorted(values) == [[4, \"e\"], [4, \"e\"], [5, \"f\"], [5, \"f\"], [6, \"g\"],\n <mask>                               [6, \"g\"]]\n <mask> \n <mask> \n <mask>     ds = ray.experimental.data.read_parquet(tmp_path)\n <mask>     values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\n <mask> \n <mask>     assert sorted(values) == [[4, \"e\"], [4, \"e\"], [5, \"f\"], [5, \"f\"], [6, \"g\"],\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add     # Test metadata ops.\n    assert ds.count() == 3\n    assert \"two\" in str(ds.schema())\n    assert \"two\" in str(ds) </s> add         # Test metadata ops.\n        assert ds.count() == 10\n        assert \"bytes\" in str(ds.schema()), ds\n        assert \"bytes\" in str(ds), ds </s> add     # Test metadata ops.\n    assert ds.count() == 3\n    assert \"two\" in str(ds.schema())\n    assert \"two\" in str(ds) </s> remove     assert sorted(ds.to_local_iterator()) == [0, 1, 2, 3, 4]\n </s> add     assert sorted(ds.iter_rows()) == [0, 1, 2, 3, 4]\n\n\ndef test_empty_dataset(ray_start_regular_shared):\n    ds = ray.experimental.data.range(0)\n    assert ds.count() == 0\n    with pytest.raises(ValueError):\n        ds.size_bytes()\n    with pytest.raises(ValueError):\n        ds.schema()\n\n    ds = ray.experimental.data.range(1)\n    ds = ds.filter(lambda x: x > 1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=0, num_blocks=1, schema=Unknown schema)\"\n\n\ndef test_schema(ray_start_regular_shared):\n    ds = ray.experimental.data.range(10)\n    ds2 = ray.experimental.data.range_arrow(10)\n    ds3 = ds2.repartition(5)\n    ds4 = ds3.map(lambda x: {\"a\": \"hi\", \"b\": 1.0}).limit(5).repartition(1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema=<class 'int'>)\"\n    assert str(ds2) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema={value: int64})\"\n    assert str(ds3) == \\\n        \"Dataset(num_rows=10, num_blocks=5, schema={value: int64})\"\n    assert str(ds4) == \\\n        \"Dataset(num_rows=5, num_blocks=1, schema={a: string, b: double})\"\n\n\ndef test_lazy_loading_exponential_rampup(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    assert len(ds._blocks._blocks) == 1\n    assert ds.take(10) == list(range(10))\n    assert len(ds._blocks._blocks) == 2\n    assert ds.take(20) == list(range(20))\n    assert len(ds._blocks._blocks) == 4\n    assert ds.take(30) == list(range(30))\n    assert len(ds._blocks._blocks) == 8\n    assert ds.take(50) == list(range(50))\n    assert len(ds._blocks._blocks) == 16\n    assert ds.take(100) == list(range(100))\n    assert len(ds._blocks._blocks) == 20\n\n\ndef test_limit(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    for i in range(100):\n        assert ds.limit(i).take(200) == list(range(i)) </s> remove         for i, (path, item) in enumerate(ds.to_local_iterator()):\n </s> add         for i, (path, item) in enumerate(ds.iter_rows()):", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> def test_read_binary_files(ray_start_regular_shared):\n <mask>     with util.gen_bin_files(10) as (_, paths):\n <mask>         ds = ray.experimental.data.read_binary_files(paths, parallelism=10)\n <mask>         for i, item in enumerate(ds.to_local_iterator()):\n <mask>             expected = open(paths[i], \"rb\").read()\n <mask>             assert expected == item\n <mask> \n <mask> \n <mask> def test_read_binary_files_with_paths(ray_start_regular_shared):\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove         for i, (path, item) in enumerate(ds.to_local_iterator()):\n </s> add         for i, (path, item) in enumerate(ds.iter_rows()): </s> add         # Test metadata ops.\n        assert ds.count() == 10\n        assert \"bytes\" in str(ds.schema()), ds\n        assert \"bytes\" in str(ds), ds </s> remove         for i, item in enumerate(ds.to_local_iterator()):\n </s> add         for i, item in enumerate(ds.iter_rows()): </s> remove     assert sorted(ds.to_local_iterator()) == [0, 1, 2, 3, 4]\n </s> add     assert sorted(ds.iter_rows()) == [0, 1, 2, 3, 4]\n\n\ndef test_empty_dataset(ray_start_regular_shared):\n    ds = ray.experimental.data.range(0)\n    assert ds.count() == 0\n    with pytest.raises(ValueError):\n        ds.size_bytes()\n    with pytest.raises(ValueError):\n        ds.schema()\n\n    ds = ray.experimental.data.range(1)\n    ds = ds.filter(lambda x: x > 1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=0, num_blocks=1, schema=Unknown schema)\"\n\n\ndef test_schema(ray_start_regular_shared):\n    ds = ray.experimental.data.range(10)\n    ds2 = ray.experimental.data.range_arrow(10)\n    ds3 = ds2.repartition(5)\n    ds4 = ds3.map(lambda x: {\"a\": \"hi\", \"b\": 1.0}).limit(5).repartition(1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema=<class 'int'>)\"\n    assert str(ds2) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema={value: int64})\"\n    assert str(ds3) == \\\n        \"Dataset(num_rows=10, num_blocks=5, schema={value: int64})\"\n    assert str(ds4) == \\\n        \"Dataset(num_rows=5, num_blocks=1, schema={a: string, b: double})\"\n\n\ndef test_lazy_loading_exponential_rampup(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    assert len(ds._blocks._blocks) == 1\n    assert ds.take(10) == list(range(10))\n    assert len(ds._blocks._blocks) == 2\n    assert ds.take(20) == list(range(20))\n    assert len(ds._blocks._blocks) == 4\n    assert ds.take(30) == list(range(30))\n    assert len(ds._blocks._blocks) == 8\n    assert ds.take(50) == list(range(50))\n    assert len(ds._blocks._blocks) == 16\n    assert ds.take(100) == list(range(100))\n    assert len(ds._blocks._blocks) == 20\n\n\ndef test_limit(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    for i in range(100):\n        assert ds.limit(i).take(200) == list(range(i)) </s> add def test_basic_actors(shutdown_only):\n    ray.init(num_cpus=2)\n    ds = ray.experimental.data.range(5)\n    assert sorted(ds.map(lambda x: x + 1,\n                         compute=\"actors\").take()) == [1, 2, 3, 4, 5]\n\n </s> remove     assert sorted(values) == [[4, \"e\"], [4, \"e\"], [5, \"f\"], [5, \"f\"], [6, \"g\"],\n </s> add     # Test metadata-only parquet ops.\n    assert len(ds._blocks._blocks) == 1\n    assert ds.count() == 6\n    assert ds.size_bytes() > 0\n    assert ds.schema() is not None\n    input_files = ds.input_files()\n    assert len(input_files) == 2, input_files\n    assert \"test1.parquet\" in str(input_files)\n    assert \"test2.parquet\" in str(input_files)\n    assert str(ds) == \\\n        \"Dataset(num_rows=6, num_blocks=2, \" \\\n        \"schema={one: int64, two: string})\", ds\n    assert repr(ds) == \\\n        \"Dataset(num_rows=6, num_blocks=2, \" \\\n        \"schema={one: int64, two: string})\", ds\n    assert len(ds._blocks._blocks) == 1\n\n    # Forces a data read.\n    values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\n    assert len(ds._blocks._blocks) == 2\n    assert sorted(values) == [[1, \"a\"], [2, \"b\"], [3, \"c\"], [4, \"e\"], [5, \"f\"],", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>         for i, item in enumerate(ds.iter_rows()):\n <mask>             expected = open(paths[i], \"rb\").read()\n <mask>             assert expected == item\n <mask> \n <mask> \n <mask> def test_read_binary_files_with_paths(ray_start_regular_shared):\n <mask>     with util.gen_bin_files(10) as (_, paths):\n <mask>         ds = ray.experimental.data.read_binary_files(\n <mask>             paths, include_paths=True, parallelism=10)\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove         for i, (path, item) in enumerate(ds.to_local_iterator()):\n </s> add         for i, (path, item) in enumerate(ds.iter_rows()): </s> remove         for i, item in enumerate(ds.to_local_iterator()):\n </s> add         for i, item in enumerate(ds.iter_rows()): </s> remove         for i, item in enumerate(ds.to_local_iterator()):\n </s> add         for i, item in enumerate(ds.iter_rows()): </s> remove     assert sorted(ds.to_local_iterator()) == [0, 1, 2, 3, 4]\n </s> add     assert sorted(ds.iter_rows()) == [0, 1, 2, 3, 4]\n\n\ndef test_empty_dataset(ray_start_regular_shared):\n    ds = ray.experimental.data.range(0)\n    assert ds.count() == 0\n    with pytest.raises(ValueError):\n        ds.size_bytes()\n    with pytest.raises(ValueError):\n        ds.schema()\n\n    ds = ray.experimental.data.range(1)\n    ds = ds.filter(lambda x: x > 1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=0, num_blocks=1, schema=Unknown schema)\"\n\n\ndef test_schema(ray_start_regular_shared):\n    ds = ray.experimental.data.range(10)\n    ds2 = ray.experimental.data.range_arrow(10)\n    ds3 = ds2.repartition(5)\n    ds4 = ds3.map(lambda x: {\"a\": \"hi\", \"b\": 1.0}).limit(5).repartition(1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema=<class 'int'>)\"\n    assert str(ds2) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema={value: int64})\"\n    assert str(ds3) == \\\n        \"Dataset(num_rows=10, num_blocks=5, schema={value: int64})\"\n    assert str(ds4) == \\\n        \"Dataset(num_rows=5, num_blocks=1, schema={a: string, b: double})\"\n\n\ndef test_lazy_loading_exponential_rampup(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    assert len(ds._blocks._blocks) == 1\n    assert ds.take(10) == list(range(10))\n    assert len(ds._blocks._blocks) == 2\n    assert ds.take(20) == list(range(20))\n    assert len(ds._blocks._blocks) == 4\n    assert ds.take(30) == list(range(30))\n    assert len(ds._blocks._blocks) == 8\n    assert ds.take(50) == list(range(50))\n    assert len(ds._blocks._blocks) == 16\n    assert ds.take(100) == list(range(100))\n    assert len(ds._blocks._blocks) == 20\n\n\ndef test_limit(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    for i in range(100):\n        assert ds.limit(i).take(200) == list(range(i)) </s> remove     @ray.remote\n </s> add     @ray.remote(num_returns=2) </s> remove     @ray.remote\n </s> add     @ray.remote(num_returns=2)", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> def test_read_binary_files_with_paths(ray_start_regular_shared):\n <mask>     with util.gen_bin_files(10) as (_, paths):\n <mask>         ds = ray.experimental.data.read_binary_files(\n <mask>             paths, include_paths=True, parallelism=10)\n <mask>         for i, (path, item) in enumerate(ds.to_local_iterator()):\n <mask>             assert path == paths[i]\n <mask>             expected = open(paths[i], \"rb\").read()\n <mask>             assert expected == item\n <mask> \n <mask> \n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add         # Test metadata ops.\n        assert ds.count() == 10\n        assert \"bytes\" in str(ds.schema()), ds\n        assert \"bytes\" in str(ds), ds </s> remove         for i, item in enumerate(ds.to_local_iterator()):\n </s> add         for i, item in enumerate(ds.iter_rows()): </s> remove         for i, item in enumerate(ds.to_local_iterator()):\n </s> add         for i, item in enumerate(ds.iter_rows()): </s> remove     assert sorted(ds.to_local_iterator()) == [0, 1, 2, 3, 4]\n </s> add     assert sorted(ds.iter_rows()) == [0, 1, 2, 3, 4]\n\n\ndef test_empty_dataset(ray_start_regular_shared):\n    ds = ray.experimental.data.range(0)\n    assert ds.count() == 0\n    with pytest.raises(ValueError):\n        ds.size_bytes()\n    with pytest.raises(ValueError):\n        ds.schema()\n\n    ds = ray.experimental.data.range(1)\n    ds = ds.filter(lambda x: x > 1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=0, num_blocks=1, schema=Unknown schema)\"\n\n\ndef test_schema(ray_start_regular_shared):\n    ds = ray.experimental.data.range(10)\n    ds2 = ray.experimental.data.range_arrow(10)\n    ds3 = ds2.repartition(5)\n    ds4 = ds3.map(lambda x: {\"a\": \"hi\", \"b\": 1.0}).limit(5).repartition(1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema=<class 'int'>)\"\n    assert str(ds2) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema={value: int64})\"\n    assert str(ds3) == \\\n        \"Dataset(num_rows=10, num_blocks=5, schema={value: int64})\"\n    assert str(ds4) == \\\n        \"Dataset(num_rows=5, num_blocks=1, schema={a: string, b: double})\"\n\n\ndef test_lazy_loading_exponential_rampup(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    assert len(ds._blocks._blocks) == 1\n    assert ds.take(10) == list(range(10))\n    assert len(ds._blocks._blocks) == 2\n    assert ds.take(20) == list(range(20))\n    assert len(ds._blocks._blocks) == 4\n    assert ds.take(30) == list(range(30))\n    assert len(ds._blocks._blocks) == 8\n    assert ds.take(50) == list(range(50))\n    assert len(ds._blocks._blocks) == 16\n    assert ds.take(100) == list(range(100))\n    assert len(ds._blocks._blocks) == 20\n\n\ndef test_limit(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    for i in range(100):\n        assert ds.limit(i).take(200) == list(range(i)) </s> add def test_basic_actors(shutdown_only):\n    ray.init(num_cpus=2)\n    ds = ray.experimental.data.range(5)\n    assert sorted(ds.map(lambda x: x + 1,\n                         compute=\"actors\").take()) == [1, 2, 3, 4, 5]\n\n </s> remove     assert sorted(values) == [[4, \"e\"], [4, \"e\"], [5, \"f\"], [5, \"f\"], [6, \"g\"],\n </s> add     # Test metadata-only parquet ops.\n    assert len(ds._blocks._blocks) == 1\n    assert ds.count() == 6\n    assert ds.size_bytes() > 0\n    assert ds.schema() is not None\n    input_files = ds.input_files()\n    assert len(input_files) == 2, input_files\n    assert \"test1.parquet\" in str(input_files)\n    assert \"test2.parquet\" in str(input_files)\n    assert str(ds) == \\\n        \"Dataset(num_rows=6, num_blocks=2, \" \\\n        \"schema={one: int64, two: string})\", ds\n    assert repr(ds) == \\\n        \"Dataset(num_rows=6, num_blocks=2, \" \\\n        \"schema={one: int64, two: string})\", ds\n    assert len(ds._blocks._blocks) == 1\n\n    # Forces a data read.\n    values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\n    assert len(ds._blocks._blocks) == 2\n    assert sorted(values) == [[1, \"a\"], [2, \"b\"], [3, \"c\"], [4, \"e\"], [5, \"f\"],", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         # All the paths are absolute, so we want the root file system.\n <mask>         fs, _ = pa.fs.FileSystem.from_uri(\"/\")\n <mask>         ds = ray.experimental.data.read_binary_files(\n <mask>             paths, filesystem=fs, parallelism=10)\n <mask>         for i, item in enumerate(ds.to_local_iterator()):\n <mask>             expected = open(paths[i], \"rb\").read()\n <mask>             assert expected == item\n <mask> \n <mask> \n <mask> def test_map_batch(ray_start_regular_shared, tmp_path):\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> remove         for i, (path, item) in enumerate(ds.to_local_iterator()):\n </s> add         for i, (path, item) in enumerate(ds.iter_rows()): </s> remove         for i, item in enumerate(ds.to_local_iterator()):\n </s> add         for i, item in enumerate(ds.iter_rows()): </s> add         # Test metadata ops.\n        assert ds.count() == 10\n        assert \"bytes\" in str(ds.schema()), ds\n        assert \"bytes\" in str(ds), ds </s> remove     assert sorted(values) == [[4, \"e\"], [4, \"e\"], [5, \"f\"], [5, \"f\"], [6, \"g\"],\n </s> add     # Test metadata-only parquet ops.\n    assert len(ds._blocks._blocks) == 1\n    assert ds.count() == 6\n    assert ds.size_bytes() > 0\n    assert ds.schema() is not None\n    input_files = ds.input_files()\n    assert len(input_files) == 2, input_files\n    assert \"test1.parquet\" in str(input_files)\n    assert \"test2.parquet\" in str(input_files)\n    assert str(ds) == \\\n        \"Dataset(num_rows=6, num_blocks=2, \" \\\n        \"schema={one: int64, two: string})\", ds\n    assert repr(ds) == \\\n        \"Dataset(num_rows=6, num_blocks=2, \" \\\n        \"schema={one: int64, two: string})\", ds\n    assert len(ds._blocks._blocks) == 1\n\n    # Forces a data read.\n    values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\n    assert len(ds._blocks._blocks) == 2\n    assert sorted(values) == [[1, \"a\"], [2, \"b\"], [3, \"c\"], [4, \"e\"], [5, \"f\"], </s> remove     assert sorted(ds.to_local_iterator()) == [0, 1, 2, 3, 4]\n </s> add     assert sorted(ds.iter_rows()) == [0, 1, 2, 3, 4]\n\n\ndef test_empty_dataset(ray_start_regular_shared):\n    ds = ray.experimental.data.range(0)\n    assert ds.count() == 0\n    with pytest.raises(ValueError):\n        ds.size_bytes()\n    with pytest.raises(ValueError):\n        ds.schema()\n\n    ds = ray.experimental.data.range(1)\n    ds = ds.filter(lambda x: x > 1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=0, num_blocks=1, schema=Unknown schema)\"\n\n\ndef test_schema(ray_start_regular_shared):\n    ds = ray.experimental.data.range(10)\n    ds2 = ray.experimental.data.range_arrow(10)\n    ds3 = ds2.repartition(5)\n    ds4 = ds3.map(lambda x: {\"a\": \"hi\", \"b\": 1.0}).limit(5).repartition(1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema=<class 'int'>)\"\n    assert str(ds2) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema={value: int64})\"\n    assert str(ds3) == \\\n        \"Dataset(num_rows=10, num_blocks=5, schema={value: int64})\"\n    assert str(ds4) == \\\n        \"Dataset(num_rows=5, num_blocks=1, schema={a: string, b: double})\"\n\n\ndef test_lazy_loading_exponential_rampup(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    assert len(ds._blocks._blocks) == 1\n    assert ds.take(10) == list(range(10))\n    assert len(ds._blocks._blocks) == 2\n    assert ds.take(20) == list(range(20))\n    assert len(ds._blocks._blocks) == 4\n    assert ds.take(30) == list(range(30))\n    assert len(ds._blocks._blocks) == 8\n    assert ds.take(50) == list(range(50))\n    assert len(ds._blocks._blocks) == 16\n    assert ds.take(100) == list(range(100))\n    assert len(ds._blocks._blocks) == 20\n\n\ndef test_limit(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    for i in range(100):\n        assert ds.limit(i).take(200) == list(range(i)) </s> add     # Test metadata ops.\n    assert ds.count() == 3\n    assert \"two\" in str(ds.schema())\n    assert \"two\" in str(ds)", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>     df1.to_json(path1, orient=\"records\", lines=True)\n <mask>     ds = ray.experimental.data.read_json(path1)\n <mask>     assert df1.equals(ray.get(ds.to_pandas())[0])\n <mask> \n <mask>     # Two files, parallelism=2.\n <mask>     df2 = pd.DataFrame({\"one\": [4, 5, 6], \"two\": [\"e\", \"f\", \"g\"]})\n <mask>     path2 = os.path.join(tmp_path, \"test2.json\")\n <mask>     df2.to_json(path2, orient=\"records\", lines=True)\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add     # Test metadata ops.\n    assert ds.count() == 3\n    assert \"two\" in str(ds.schema())\n    assert \"two\" in str(ds) </s> remove     assert sorted(values) == [[4, \"e\"], [4, \"e\"], [5, \"f\"], [5, \"f\"], [6, \"g\"],\n </s> add     # Test metadata-only parquet ops.\n    assert len(ds._blocks._blocks) == 1\n    assert ds.count() == 6\n    assert ds.size_bytes() > 0\n    assert ds.schema() is not None\n    input_files = ds.input_files()\n    assert len(input_files) == 2, input_files\n    assert \"test1.parquet\" in str(input_files)\n    assert \"test2.parquet\" in str(input_files)\n    assert str(ds) == \\\n        \"Dataset(num_rows=6, num_blocks=2, \" \\\n        \"schema={one: int64, two: string})\", ds\n    assert repr(ds) == \\\n        \"Dataset(num_rows=6, num_blocks=2, \" \\\n        \"schema={one: int64, two: string})\", ds\n    assert len(ds._blocks._blocks) == 1\n\n    # Forces a data read.\n    values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\n    assert len(ds._blocks._blocks) == 2\n    assert sorted(values) == [[1, \"a\"], [2, \"b\"], [3, \"c\"], [4, \"e\"], [5, \"f\"], </s> remove     values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\n </s> add  </s> add         # Test metadata ops.\n        assert ds.count() == 10\n        assert \"bytes\" in str(ds.schema()), ds\n        assert \"bytes\" in str(ds), ds </s> remove     assert sorted(ds.to_local_iterator()) == [0, 1, 2, 3, 4]\n </s> add     assert sorted(ds.iter_rows()) == [0, 1, 2, 3, 4]\n\n\ndef test_empty_dataset(ray_start_regular_shared):\n    ds = ray.experimental.data.range(0)\n    assert ds.count() == 0\n    with pytest.raises(ValueError):\n        ds.size_bytes()\n    with pytest.raises(ValueError):\n        ds.schema()\n\n    ds = ray.experimental.data.range(1)\n    ds = ds.filter(lambda x: x > 1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=0, num_blocks=1, schema=Unknown schema)\"\n\n\ndef test_schema(ray_start_regular_shared):\n    ds = ray.experimental.data.range(10)\n    ds2 = ray.experimental.data.range_arrow(10)\n    ds3 = ds2.repartition(5)\n    ds4 = ds3.map(lambda x: {\"a\": \"hi\", \"b\": 1.0}).limit(5).repartition(1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema=<class 'int'>)\"\n    assert str(ds2) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema={value: int64})\"\n    assert str(ds3) == \\\n        \"Dataset(num_rows=10, num_blocks=5, schema={value: int64})\"\n    assert str(ds4) == \\\n        \"Dataset(num_rows=5, num_blocks=1, schema={a: string, b: double})\"\n\n\ndef test_lazy_loading_exponential_rampup(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    assert len(ds._blocks._blocks) == 1\n    assert ds.take(10) == list(range(10))\n    assert len(ds._blocks._blocks) == 2\n    assert ds.take(20) == list(range(20))\n    assert len(ds._blocks._blocks) == 4\n    assert ds.take(30) == list(range(30))\n    assert len(ds._blocks._blocks) == 8\n    assert ds.take(50) == list(range(50))\n    assert len(ds._blocks._blocks) == 16\n    assert ds.take(100) == list(range(100))\n    assert len(ds._blocks._blocks) == 20\n\n\ndef test_limit(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    for i in range(100):\n        assert ds.limit(i).take(200) == list(range(i)) </s> remove         for i, item in enumerate(ds.to_local_iterator()):\n </s> add         for i, item in enumerate(ds.iter_rows()):", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>     ds = ray.experimental.data.read_csv(path1)\n <mask>     dsdf = ray.get(ds.to_pandas())[0]\n <mask>     assert df1.equals(dsdf)\n <mask> \n <mask>     # Two files, parallelism=2.\n <mask>     df2 = pd.DataFrame({\"one\": [4, 5, 6], \"two\": [\"e\", \"f\", \"g\"]})\n <mask>     path2 = os.path.join(tmp_path, \"test2.csv\")\n </s> [data] Support block metadata and lazy loading of blocks from the filesystem (#16689) </s> add     # Test metadata ops.\n    assert ds.count() == 3\n    assert \"two\" in str(ds.schema())\n    assert \"two\" in str(ds) </s> remove     values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\n </s> add  </s> remove     assert sorted(values) == [[4, \"e\"], [4, \"e\"], [5, \"f\"], [5, \"f\"], [6, \"g\"],\n </s> add     # Test metadata-only parquet ops.\n    assert len(ds._blocks._blocks) == 1\n    assert ds.count() == 6\n    assert ds.size_bytes() > 0\n    assert ds.schema() is not None\n    input_files = ds.input_files()\n    assert len(input_files) == 2, input_files\n    assert \"test1.parquet\" in str(input_files)\n    assert \"test2.parquet\" in str(input_files)\n    assert str(ds) == \\\n        \"Dataset(num_rows=6, num_blocks=2, \" \\\n        \"schema={one: int64, two: string})\", ds\n    assert repr(ds) == \\\n        \"Dataset(num_rows=6, num_blocks=2, \" \\\n        \"schema={one: int64, two: string})\", ds\n    assert len(ds._blocks._blocks) == 1\n\n    # Forces a data read.\n    values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\n    assert len(ds._blocks._blocks) == 2\n    assert sorted(values) == [[1, \"a\"], [2, \"b\"], [3, \"c\"], [4, \"e\"], [5, \"f\"], </s> add         # Test metadata ops.\n        assert ds.count() == 10\n        assert \"bytes\" in str(ds.schema()), ds\n        assert \"bytes\" in str(ds), ds </s> remove     assert sorted(ds.to_local_iterator()) == [0, 1, 2, 3, 4]\n </s> add     assert sorted(ds.iter_rows()) == [0, 1, 2, 3, 4]\n\n\ndef test_empty_dataset(ray_start_regular_shared):\n    ds = ray.experimental.data.range(0)\n    assert ds.count() == 0\n    with pytest.raises(ValueError):\n        ds.size_bytes()\n    with pytest.raises(ValueError):\n        ds.schema()\n\n    ds = ray.experimental.data.range(1)\n    ds = ds.filter(lambda x: x > 1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=0, num_blocks=1, schema=Unknown schema)\"\n\n\ndef test_schema(ray_start_regular_shared):\n    ds = ray.experimental.data.range(10)\n    ds2 = ray.experimental.data.range_arrow(10)\n    ds3 = ds2.repartition(5)\n    ds4 = ds3.map(lambda x: {\"a\": \"hi\", \"b\": 1.0}).limit(5).repartition(1)\n    assert str(ds) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema=<class 'int'>)\"\n    assert str(ds2) == \\\n        \"Dataset(num_rows=10, num_blocks=10, schema={value: int64})\"\n    assert str(ds3) == \\\n        \"Dataset(num_rows=10, num_blocks=5, schema={value: int64})\"\n    assert str(ds4) == \\\n        \"Dataset(num_rows=5, num_blocks=1, schema={a: string, b: double})\"\n\n\ndef test_lazy_loading_exponential_rampup(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    assert len(ds._blocks._blocks) == 1\n    assert ds.take(10) == list(range(10))\n    assert len(ds._blocks._blocks) == 2\n    assert ds.take(20) == list(range(20))\n    assert len(ds._blocks._blocks) == 4\n    assert ds.take(30) == list(range(30))\n    assert len(ds._blocks._blocks) == 8\n    assert ds.take(50) == list(range(50))\n    assert len(ds._blocks._blocks) == 16\n    assert ds.take(100) == list(range(100))\n    assert len(ds._blocks._blocks) == 20\n\n\ndef test_limit(ray_start_regular_shared):\n    ds = ray.experimental.data.range(100, parallelism=20)\n    for i in range(100):\n        assert ds.limit(i).take(200) == list(range(i)) </s> remove         for i, item in enumerate(ds.to_local_iterator()):\n </s> add         for i, item in enumerate(ds.iter_rows()):", "html_url": "https://github.com/ray-project/ray/commit/7eec197d311a2013bcaba692125d4a23b2e539bb", "file_name": "python/ray/experimental/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> import boto3\n <mask> from botocore.config import Config\n <mask> \n <mask> from ray.autoscaler.node_provider import NodeProvider\n <mask> from ray.autoscaler.tags import TAG_RAY_CLUSTER_NAME\n <mask> from ray.ray_constants import BOTO_MAX_RETRIES\n <mask> \n <mask> \n <mask> class AWSNodeProvider(NodeProvider):\n <mask>     def __init__(self, provider_config, cluster_name):\n </s> [autoscaler] Translate to/from AWS 'Name' tag (#2219)\n\n* fix tag\r\n\r\n* fix </s> add         tags = to_aws_format(tags) </s> add         tag_filters = to_aws_format(tag_filters) </s> add         tags = to_aws_format(tags) </s> remove         return tags\n </s> add         return from_aws_format(tags)", "html_url": "https://github.com/ray-project/ray/commit/7fcaad264a32f7be6e70ab636d68c9f5ee78c09b", "file_name": "python/ray/autoscaler/aws/node_provider.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>         self.internal_ip_cache = {}\n <mask>         self.external_ip_cache = {}\n <mask> \n <mask>     def nodes(self, tag_filters):\n <mask>         filters = [\n <mask>             {\n <mask>                 \"Name\": \"instance-state-name\",\n <mask>                 \"Values\": [\"pending\", \"running\"],\n <mask>             },\n </s> [autoscaler] Translate to/from AWS 'Name' tag (#2219)\n\n* fix tag\r\n\r\n* fix </s> remove         return tags\n </s> add         return from_aws_format(tags) </s> add         tags = to_aws_format(tags) </s> add         tags = to_aws_format(tags) </s> remove from ray.autoscaler.tags import TAG_RAY_CLUSTER_NAME\n </s> add from ray.autoscaler.tags import TAG_RAY_CLUSTER_NAME, TAG_RAY_NODE_NAME", "html_url": "https://github.com/ray-project/ray/commit/7fcaad264a32f7be6e70ab636d68c9f5ee78c09b", "file_name": "python/ray/autoscaler/aws/node_provider.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         node = self._node(node_id)\n <mask>         tags = {}\n <mask>         for tag in node.tags:\n <mask>             tags[tag[\"Key\"]] = tag[\"Value\"]\n <mask>         return tags\n <mask> \n <mask>     def external_ip(self, node_id):\n <mask>         if node_id in self.external_ip_cache:\n <mask>             return self.external_ip_cache[node_id]\n <mask>         node = self._node(node_id)\n </s> [autoscaler] Translate to/from AWS 'Name' tag (#2219)\n\n* fix tag\r\n\r\n* fix </s> add         tags = to_aws_format(tags) </s> add         tag_filters = to_aws_format(tag_filters) </s> add         tags = to_aws_format(tags) </s> remove from ray.autoscaler.tags import TAG_RAY_CLUSTER_NAME\n </s> add from ray.autoscaler.tags import TAG_RAY_CLUSTER_NAME, TAG_RAY_NODE_NAME", "html_url": "https://github.com/ray-project/ray/commit/7fcaad264a32f7be6e70ab636d68c9f5ee78c09b", "file_name": "python/ray/autoscaler/aws/node_provider.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     def set_node_tags(self, node_id, tags):\n <mask>         node = self._node(node_id)\n <mask>         tag_pairs = []\n <mask>         for k, v in tags.items():\n <mask>             tag_pairs.append({\n <mask>                 \"Key\": k,\n <mask>                 \"Value\": v,\n </s> [autoscaler] Translate to/from AWS 'Name' tag (#2219)\n\n* fix tag\r\n\r\n* fix </s> remove         return tags\n </s> add         return from_aws_format(tags) </s> add         tags = to_aws_format(tags) </s> add         tag_filters = to_aws_format(tag_filters) </s> remove from ray.autoscaler.tags import TAG_RAY_CLUSTER_NAME\n </s> add from ray.autoscaler.tags import TAG_RAY_CLUSTER_NAME, TAG_RAY_NODE_NAME", "html_url": "https://github.com/ray-project/ray/commit/7fcaad264a32f7be6e70ab636d68c9f5ee78c09b", "file_name": "python/ray/autoscaler/aws/node_provider.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> \n <mask>     def create_node(self, node_config, tags, count):\n <mask>         conf = node_config.copy()\n <mask>         tag_pairs = [{\n <mask>             \"Key\": TAG_RAY_CLUSTER_NAME,\n <mask>             \"Value\": self.cluster_name,\n </s> [autoscaler] Translate to/from AWS 'Name' tag (#2219)\n\n* fix tag\r\n\r\n* fix </s> add         tags = to_aws_format(tags) </s> add         tag_filters = to_aws_format(tag_filters) </s> remove         return tags\n </s> add         return from_aws_format(tags) </s> remove from ray.autoscaler.tags import TAG_RAY_CLUSTER_NAME\n </s> add from ray.autoscaler.tags import TAG_RAY_CLUSTER_NAME, TAG_RAY_NODE_NAME", "html_url": "https://github.com/ray-project/ray/commit/7fcaad264a32f7be6e70ab636d68c9f5ee78c09b", "file_name": "python/ray/autoscaler/aws/node_provider.py"}
{"docstring_tokens": "keep keep add keep keep keep", "code_tokens": " <mask>             \"net.java.dev.jna:jna:5.5.0\",\n <mask>         ],\n <mask>         repositories = [\n <mask>             \"https://repo1.maven.org/maven2/\",\n <mask>         ],\n <mask>     )\n </s> [Java] add maven repo (#10109) </s> add   <repositories>\n    <repository>\n      <id>spring</id>\n      <url>https://repo.spring.io/plugins-release/</url>\n    </repository>\n    <repository>\n      <id>central</id>\n      <url>https://repo1.maven.org/maven2/</url>\n    </repository>\n  </repositories>\n </s> add             \"https://repo.spring.io/plugins-release/\", </s> add   <repositories>\n    <repository>\n      <id>spring</id>\n      <url>https://repo.spring.io/plugins-release/</url>\n    </repository>\n    <repository>\n      <id>central</id>\n      <url>https://repo1.maven.org/maven2/</url>\n    </repository>\n  </repositories>\n", "html_url": "https://github.com/ray-project/ray/commit/7ffb37f711f0045f1d1199db139e31e58fcbbe00", "file_name": "java/dependencies.bzl"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>       <url>https://oss.sonatype.org/service/local/staging/deploy/maven2/</url>\n <mask>     </repository>\n <mask>   </distributionManagement>\n <mask> \n <mask>   <modules>\n <mask>     <module>api</module>\n <mask>     <module>runtime</module>\n <mask>     <module>test</module>\n <mask>     <module>tutorial</module>\n <mask>   </modules>\n </s> [Java] add maven repo (#10109) </s> add   <repositories>\n    <repository>\n      <id>spring</id>\n      <url>https://repo.spring.io/plugins-release/</url>\n    </repository>\n    <repository>\n      <id>central</id>\n      <url>https://repo1.maven.org/maven2/</url>\n    </repository>\n  </repositories>\n </s> add             \"https://repo.spring.io/plugins-release/\", </s> add             \"https://repo.spring.io/plugins-release/\",", "html_url": "https://github.com/ray-project/ray/commit/7ffb37f711f0045f1d1199db139e31e58fcbbe00", "file_name": "java/pom.xml"}
{"docstring_tokens": "keep keep keep add keep keep keep", "code_tokens": " <mask> \t        \"org.powermock:powermock-module-testng:1.6.6\",\n <mask> \t        \"org.powermock:powermock-api-mockito:1.6.6\",\n <mask>         ],\n <mask>         repositories = [\n <mask>             \"https://repo1.maven.org/maven2/\",\n <mask>         ],\n <mask>     )\n </s> [Java] add maven repo (#10109) </s> add   <repositories>\n    <repository>\n      <id>spring</id>\n      <url>https://repo.spring.io/plugins-release/</url>\n    </repository>\n    <repository>\n      <id>central</id>\n      <url>https://repo1.maven.org/maven2/</url>\n    </repository>\n  </repositories>\n </s> add   <repositories>\n    <repository>\n      <id>spring</id>\n      <url>https://repo.spring.io/plugins-release/</url>\n    </repository>\n    <repository>\n      <id>central</id>\n      <url>https://repo1.maven.org/maven2/</url>\n    </repository>\n  </repositories>\n </s> add             \"https://repo.spring.io/plugins-release/\",", "html_url": "https://github.com/ray-project/ray/commit/7ffb37f711f0045f1d1199db139e31e58fcbbe00", "file_name": "streaming/java/dependencies.bzl"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask>       <url>https://oss.sonatype.org/service/local/staging/deploy/maven2/</url>\n <mask>     </repository>\n <mask>   </distributionManagement>\n <mask> \n <mask>   <modules>\n <mask>     <module>streaming-api</module>\n <mask>     <module>streaming-runtime</module>\n <mask>     <module>streaming-state</module>\n </s> [Java] add maven repo (#10109) </s> add             \"https://repo.spring.io/plugins-release/\", </s> add   <repositories>\n    <repository>\n      <id>spring</id>\n      <url>https://repo.spring.io/plugins-release/</url>\n    </repository>\n    <repository>\n      <id>central</id>\n      <url>https://repo1.maven.org/maven2/</url>\n    </repository>\n  </repositories>\n </s> add             \"https://repo.spring.io/plugins-release/\",", "html_url": "https://github.com/ray-project/ray/commit/7ffb37f711f0045f1d1199db139e31e58fcbbe00", "file_name": "streaming/java/pom.xml"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         L3_cache_size_bytes_(100000000),\n <mask>         max_tasks_to_spillback_(10),\n <mask>         actor_creation_num_spillbacks_warning_(100),\n <mask>         node_manager_forward_task_retry_timeout_milliseconds_(1000),\n <mask>         // TODO: Setting this to large values results in latency, which needs to\n <mask>         // be addressed. This timeout is often on the critical path for object\n <mask>         // transfers.\n <mask>         object_manager_pull_timeout_ms_(20),\n <mask>         object_manager_push_timeout_ms_(10000),\n <mask>         object_manager_default_chunk_size_(1000000),\n <mask>         num_workers_per_process_(1) {}\n <mask> \n <mask>   ~RayConfig() {}\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   ClientID client_id_;\n  const ObjectManagerConfig config_;\n  std::unique_ptr<ObjectDirectoryInterface> object_directory_;\n  ObjectStoreNotificationManager store_notification_;\n  ObjectBufferPool buffer_pool_;\n\n  /// This runs on a thread pool dedicated to sending objects.\n  boost::asio::io_service send_service_;\n  /// This runs on a thread pool dedicated to receiving objects.\n  boost::asio::io_service receive_service_;\n\n  /// Weak reference to main service. We ensure this object is destroyed before\n  /// main_service_ is stopped.\n  boost::asio::io_service *main_service_;\n\n  /// Used to create \"work\" for send_service_.\n  /// Without this, if send_service_ has no more sends to process, it will stop.\n  boost::asio::io_service::work send_work_;\n  /// Used to create \"work\" for receive_service_.\n  /// Without this, if receive_service_ has no more receives to process, it will stop.\n  boost::asio::io_service::work receive_work_;\n\n  /// Runs the send service, which handle\n  /// all outgoing object transfers.\n  std::vector<std::thread> send_threads_;\n  /// Runs the receive service, which handle\n  /// all incoming object transfers.\n  std::vector<std::thread> receive_threads_;\n\n  /// Connection pool for reusing outgoing connections to remote object managers.\n  ConnectionPool connection_pool_;\n\n  /// Cache of locally available objects.\n  std::unordered_map<ObjectID, ObjectInfoT> local_objects_;\n\n  /// This is used as the callback identifier in Pull for\n  /// SubscribeObjectLocations. We only need one identifier because we never need to\n  /// subscribe multiple times to the same object during Pull.\n  UniqueID object_directory_pull_callback_id_ = UniqueID::from_random();\n </s> add   struct PullRequest {\n    PullRequest() : retry_timer(nullptr), timer_set(false), client_locations() {}\n    std::unique_ptr<boost::asio::deadline_timer> retry_timer;\n    bool timer_set;\n    std::vector<ClientID> client_locations;\n  }; </s> add   // Try pulling from the client. </s> remove         RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n            object_directory_pull_callback_id_, object_id));\n        GetLocationsSuccess(client_ids, object_id);\n </s> add         // Exit if the Pull request has already been fulfilled or canceled.\n        auto it = pull_requests_.find(object_id);\n        if (it == pull_requests_.end()) {\n          return;\n        }\n        // Reset the list of clients that are now expected to have the object.\n        // NOTE(swang): Since we are overwriting the previous list of clients,\n        // we may end up sending a duplicate request to the same client as\n        // before.\n        it->second.client_locations = client_ids;\n        if (it->second.client_locations.empty()) {\n          // The object locations are now empty, so we should wait for the next\n          // notification about a new object location.  Cancel the timer until\n          // the next Pull attempt since there are no more clients to try.\n          if (it->second.retry_timer != nullptr) {\n            it->second.retry_timer->cancel();\n            it->second.timer_set = false;\n          }\n        } else {\n          // New object locations were found.\n          if (!it->second.timer_set) {\n            // The timer was not set, which means that we weren't trying any\n            // clients. We now have some clients to try, so begin trying to\n            // Pull from one.  If we fail to receive an object within the pull\n            // timeout, then this will try the rest of the clients in the list\n            // in succession.\n            TryPull(object_id);\n          }\n        } </s> remove   ray::Status status_code = object_directory_->SubscribeObjectLocations(\n </s> add   if (pull_requests_.find(object_id) != pull_requests_.end()) {\n    return ray::Status::OK();\n  }\n\n  pull_requests_.emplace(object_id, PullRequest());\n  // Subscribe to object notifications. A notification will be received every\n  // time the set of client IDs for the object changes. Notifications will also\n  // be received if the list of locations is empty. The set of client IDs has\n  // no ordering guarantee between notifications.\n  return object_directory_->SubscribeObjectLocations( </s> remove   /// The callback will be invoked whenever locations are obtained for the\n  /// specified object. The callback provided to this method may fire immediately,\n  /// within the call to this method, if any other listener is subscribed to the same\n  /// object: This occurs when location data for the object has already been obtained.\n  ///\n </s> add   /// The callback will be invoked with the complete list of known locations\n  /// whenever the set of locations changes. The callback will also be fired if\n  /// the list of known locations is empty. The callback provided to this\n  /// method may fire immediately, within the call to this method, if any other\n  /// listener is subscribed to the same object: This occurs when location data\n  /// for the object has already been obtained.\n  // </s> remove   /// Discover ClientID via ObjectDirectory, then pull object\n  /// from ClientID associated with ObjectID.\n </s> add   /// Try to Pull an object from one of its expected client locations. If there\n  /// are more client locations to try after this attempt, then this method\n  /// will try each of the other clients in succession, with a timeout between\n  /// each attempt. If the object is received or if the Pull is Canceled before\n  /// the timeout, then no more Pull requests for this object will be sent\n  /// to other node managers until TryPull is called again.", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/common/state/ray_config.h"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     // Update entries for this object.\n <mask>     std::vector<ClientID> client_id_vec =\n <mask>         UpdateObjectLocations(object_id_listener_pair->second.current_object_locations,\n <mask>                               location_history, gcs_client_->client_table());\n <mask>     if (!client_id_vec.empty()) {\n <mask>       // Copy the callbacks so that the callbacks can unsubscribe without interrupting\n <mask>       // looping over the callbacks.\n <mask>       auto callbacks = object_id_listener_pair->second.callbacks;\n <mask>       // Call all callbacks associated with the object id locations we have received.\n <mask>       for (const auto &callback_pair : callbacks) {\n <mask>         callback_pair.second(client_id_vec, object_id);\n <mask>       }\n <mask>     }\n <mask>   };\n <mask>   RAY_CHECK_OK(gcs_client_->object_table().Subscribe(\n <mask>       UniqueID::nil(), gcs_client_->client_table().GetLocalClientId(),\n <mask>       object_notification_callback, nullptr));\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove         RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n            object_directory_pull_callback_id_, object_id));\n        GetLocationsSuccess(client_ids, object_id);\n </s> add         // Exit if the Pull request has already been fulfilled or canceled.\n        auto it = pull_requests_.find(object_id);\n        if (it == pull_requests_.end()) {\n          return;\n        }\n        // Reset the list of clients that are now expected to have the object.\n        // NOTE(swang): Since we are overwriting the previous list of clients,\n        // we may end up sending a duplicate request to the same client as\n        // before.\n        it->second.client_locations = client_ids;\n        if (it->second.client_locations.empty()) {\n          // The object locations are now empty, so we should wait for the next\n          // notification about a new object location.  Cancel the timer until\n          // the next Pull attempt since there are no more clients to try.\n          if (it->second.retry_timer != nullptr) {\n            it->second.retry_timer->cancel();\n            it->second.timer_set = false;\n          }\n        } else {\n          // New object locations were found.\n          if (!it->second.timer_set) {\n            // The timer was not set, which means that we weren't trying any\n            // clients. We now have some clients to try, so begin trying to\n            // Pull from one.  If we fail to receive an object within the pull\n            // timeout, then this will try the rest of the clients in the list\n            // in succession.\n            TryPull(object_id);\n          }\n        } </s> remove   // Immediately notify of found object locations.\n  if (!listener_state.current_object_locations.empty()) {\n    std::vector<ClientID> client_id_vec(listener_state.current_object_locations.begin(),\n                                        listener_state.current_object_locations.end());\n    callback(client_id_vec, object_id);\n  }\n </s> add   // Immediately notify of object locations. This notifies the client even if\n  // the list of locations is empty, since this may indicate that the objects\n  // have been evicted from all nodes.\n  std::vector<ClientID> client_id_vec(listener_state.current_object_locations.begin(),\n                                      listener_state.current_object_locations.end());\n  callback(client_id_vec, object_id); </s> remove   virtual ray::Status Cancel(const ObjectID &object_id) = 0;\n </s> add   virtual void CancelPull(const ObjectID &object_id) = 0; </s> remove void ObjectManager::HandleUnfulfilledPushRequests(const ObjectInfoT &object_info) {\n  ObjectID object_id = ObjectID::from_binary(object_info.object_id);\n </s> add  </s> remove   ray::Status status_code = object_directory_->SubscribeObjectLocations(\n </s> add   if (pull_requests_.find(object_id) != pull_requests_.end()) {\n    return ray::Status::OK();\n  }\n\n  pull_requests_.emplace(object_id, PullRequest());\n  // Subscribe to object notifications. A notification will be received every\n  // time the set of client IDs for the object changes. Notifications will also\n  // be received if the list of locations is empty. The set of client IDs has\n  // no ordering guarantee between notifications.\n  return object_directory_->SubscribeObjectLocations( </s> remove             auto object_id_wait_state = active_wait_requests_.find(wait_id);\n            // We never expect to handle a subscription notification for a wait that has\n            // already completed.\n            RAY_CHECK(object_id_wait_state != active_wait_requests_.end());\n            auto &wait_state = object_id_wait_state->second;\n            RAY_CHECK(wait_state.remaining.erase(subscribe_object_id));\n            wait_state.found.insert(subscribe_object_id);\n            wait_state.requested_objects.erase(subscribe_object_id);\n            RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n                wait_id, subscribe_object_id));\n            if (wait_state.found.size() >= wait_state.num_required_objects) {\n              WaitComplete(wait_id);\n </s> add             if (!client_ids.empty()) {\n              auto object_id_wait_state = active_wait_requests_.find(wait_id);\n              // We never expect to handle a subscription notification for a wait that has\n              // already completed.\n              RAY_CHECK(object_id_wait_state != active_wait_requests_.end());\n              auto &wait_state = object_id_wait_state->second;\n              RAY_CHECK(wait_state.remaining.erase(subscribe_object_id));\n              wait_state.found.insert(subscribe_object_id);\n              wait_state.requested_objects.erase(subscribe_object_id);\n              RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n                  wait_id, subscribe_object_id));\n              if (wait_state.found.size() >= wait_state.num_required_objects) {\n                WaitComplete(wait_id);\n              }", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_directory.cc"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>   if (listener_state.callbacks.count(callback_id) > 0) {\n <mask>     return ray::Status::OK();\n <mask>   }\n <mask>   listener_state.callbacks.emplace(callback_id, callback);\n <mask>   // Immediately notify of found object locations.\n <mask>   if (!listener_state.current_object_locations.empty()) {\n <mask>     std::vector<ClientID> client_id_vec(listener_state.current_object_locations.begin(),\n <mask>                                         listener_state.current_object_locations.end());\n <mask>     callback(client_id_vec, object_id);\n <mask>   }\n <mask>   return status;\n <mask> }\n <mask> \n <mask> ray::Status ObjectDirectory::UnsubscribeObjectLocations(const UniqueID &callback_id,\n <mask>                                                         const ObjectID &object_id) {\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   ray::Status status_code = object_directory_->SubscribeObjectLocations(\n </s> add   if (pull_requests_.find(object_id) != pull_requests_.end()) {\n    return ray::Status::OK();\n  }\n\n  pull_requests_.emplace(object_id, PullRequest());\n  // Subscribe to object notifications. A notification will be received every\n  // time the set of client IDs for the object changes. Notifications will also\n  // be received if the list of locations is empty. The set of client IDs has\n  // no ordering guarantee between notifications.\n  return object_directory_->SubscribeObjectLocations( </s> remove ray::Status ObjectManager::Cancel(const ObjectID &object_id) {\n  ray::Status status = object_directory_->UnsubscribeObjectLocations(\n      object_directory_pull_callback_id_, object_id);\n  return status;\n </s> add void ObjectManager::CancelPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) {\n    return;\n  }\n\n  RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n      object_directory_pull_callback_id_, object_id));\n  pull_requests_.erase(it); </s> remove   return status_code;\n </s> add  </s> remove void ObjectManager::GetLocationsSuccess(const std::vector<ray::ClientID> &client_ids,\n                                        const ray::ObjectID &object_id) {\n  if (local_objects_.count(object_id) == 0) {\n    // Only pull objects that aren't local.\n    RAY_CHECK(!client_ids.empty());\n    ClientID client_id = client_ids.front();\n    Pull(object_id, client_id);\n  }\n}\n\nvoid ObjectManager::Pull(const ObjectID &object_id, const ClientID &client_id) {\n  // Check if object is already local.\n  if (local_objects_.count(object_id) != 0) {\n    RAY_LOG(ERROR) << object_id << \" attempted to pull an object that's already local.\";\n </s> add void ObjectManager::TryPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) { </s> remove         RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n            object_directory_pull_callback_id_, object_id));\n        GetLocationsSuccess(client_ids, object_id);\n </s> add         // Exit if the Pull request has already been fulfilled or canceled.\n        auto it = pull_requests_.find(object_id);\n        if (it == pull_requests_.end()) {\n          return;\n        }\n        // Reset the list of clients that are now expected to have the object.\n        // NOTE(swang): Since we are overwriting the previous list of clients,\n        // we may end up sending a duplicate request to the same client as\n        // before.\n        it->second.client_locations = client_ids;\n        if (it->second.client_locations.empty()) {\n          // The object locations are now empty, so we should wait for the next\n          // notification about a new object location.  Cancel the timer until\n          // the next Pull attempt since there are no more clients to try.\n          if (it->second.retry_timer != nullptr) {\n            it->second.retry_timer->cancel();\n            it->second.timer_set = false;\n          }\n        } else {\n          // New object locations were found.\n          if (!it->second.timer_set) {\n            // The timer was not set, which means that we weren't trying any\n            // clients. We now have some clients to try, so begin trying to\n            // Pull from one.  If we fail to receive an object within the pull\n            // timeout, then this will try the rest of the clients in the list\n            // in succession.\n            TryPull(object_id);\n          }\n        } </s> remove   // Check if we're pulling from self.\n </s> add   // The timer should never fire if there are no expected client locations.\n  RAY_CHECK(!it->second.client_locations.empty());\n  RAY_CHECK(local_objects_.count(object_id) == 0);\n\n  // Get the next client to try.\n  const ClientID client_id = std::move(it->second.client_locations.back());\n  it->second.client_locations.pop_back();", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_directory.cc"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>   virtual ray::Status LookupLocations(const ObjectID &object_id,\n <mask>                                       const OnLocationsFound &callback) = 0;\n <mask> \n <mask>   /// Subscribe to be notified of locations (ClientID) of the given object.\n <mask>   /// The callback will be invoked whenever locations are obtained for the\n <mask>   /// specified object. The callback provided to this method may fire immediately,\n <mask>   /// within the call to this method, if any other listener is subscribed to the same\n <mask>   /// object: This occurs when location data for the object has already been obtained.\n <mask>   ///\n <mask>   /// \\param callback_id The id associated with the specified callback. This is\n <mask>   /// needed when UnsubscribeObjectLocations is called.\n <mask>   /// \\param object_id The required object's ObjectID.\n <mask>   /// \\param success_cb Invoked with non-empty list of client ids and object_id.\n <mask>   /// \\return Status of whether subscription succeeded.\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   /// Discover ClientID via ObjectDirectory, then pull object\n  /// from ClientID associated with ObjectID.\n </s> add   /// Try to Pull an object from one of its expected client locations. If there\n  /// are more client locations to try after this attempt, then this method\n  /// will try each of the other clients in succession, with a timeout between\n  /// each attempt. If the object is received or if the Pull is Canceled before\n  /// the timeout, then no more Pull requests for this object will be sent\n  /// to other node managers until TryPull is called again. </s> remove   /// Cancels all requests (Push/Pull) associated with the given ObjectID.\n </s> add   /// Cancels all requests (Push/Pull) associated with the given ObjectID. This\n  /// method is idempotent. </s> remove   /// \\return Status of whether requests were successfully cancelled.\n  ray::Status Cancel(const ObjectID &object_id);\n </s> add   /// \\return Void.\n  void CancelPull(const ObjectID &object_id); </s> remove   /// Pull an object from ClientID. Returns UniqueID asociated with\n  /// an invocation of this method.\n </s> add   /// Pull an object from ClientID. </s> remove   /// \\param client_id The remote node's client id.\n </s> add  </s> remove   void Pull(const ObjectID &object_id, const ClientID &client_id);\n </s> add   void TryPull(const ObjectID &object_id);", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_directory.h"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>       connection_pool_() {\n <mask>   RAY_CHECK(config_.max_sends > 0);\n <mask>   RAY_CHECK(config_.max_receives > 0);\n <mask>   main_service_ = &main_service;\n <mask>   store_notification_.SubscribeObjAdded([this](const ObjectInfoT &object_info) {\n <mask>     NotifyDirectoryObjectAdd(object_info);\n <mask>     HandleUnfulfilledPushRequests(object_info);\n <mask>   });\n <mask>   store_notification_.SubscribeObjDeleted(\n <mask>       [this](const ObjectID &oid) { NotifyDirectoryObjectDeleted(oid); });\n <mask>   StartIOService();\n <mask> }\n <mask> \n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   store_notification_.SubscribeObjAdded([this](const ObjectInfoT &object_info) {\n    NotifyDirectoryObjectAdd(object_info);\n    HandleUnfulfilledPushRequests(object_info);\n  });\n </s> add   store_notification_.SubscribeObjAdded(\n      [this](const ObjectInfoT &object_info) { HandleObjectAdded(object_info); }); </s> remove void ObjectManager::NotifyDirectoryObjectAdd(const ObjectInfoT &object_info) {\n </s> add void ObjectManager::HandleObjectAdded(const ObjectInfoT &object_info) {\n  // Notify the object directory that the object has been added to this node. </s> remove void ObjectManager::HandleUnfulfilledPushRequests(const ObjectInfoT &object_info) {\n  ObjectID object_id = ObjectID::from_binary(object_info.object_id);\n </s> add  </s> remove   return status_code;\n </s> add  </s> remove   // Immediately notify of found object locations.\n  if (!listener_state.current_object_locations.empty()) {\n    std::vector<ClientID> client_id_vec(listener_state.current_object_locations.begin(),\n                                        listener_state.current_object_locations.end());\n    callback(client_id_vec, object_id);\n  }\n </s> add   // Immediately notify of object locations. This notifies the client even if\n  // the list of locations is empty, since this may indicate that the objects\n  // have been evicted from all nodes.\n  std::vector<ClientID> client_id_vec(listener_state.current_object_locations.begin(),\n                                      listener_state.current_object_locations.end());\n  callback(client_id_vec, object_id); </s> remove void ObjectManager::GetLocationsSuccess(const std::vector<ray::ClientID> &client_ids,\n                                        const ray::ObjectID &object_id) {\n  if (local_objects_.count(object_id) == 0) {\n    // Only pull objects that aren't local.\n    RAY_CHECK(!client_ids.empty());\n    ClientID client_id = client_ids.front();\n    Pull(object_id, client_id);\n  }\n}\n\nvoid ObjectManager::Pull(const ObjectID &object_id, const ClientID &client_id) {\n  // Check if object is already local.\n  if (local_objects_.count(object_id) != 0) {\n    RAY_LOG(ERROR) << object_id << \" attempted to pull an object that's already local.\";\n </s> add void ObjectManager::TryPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) {", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.cc"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>   RAY_CHECK(config_.max_sends > 0);\n <mask>   RAY_CHECK(config_.max_receives > 0);\n <mask>   // TODO(hme) Client ID is never set with this constructor.\n <mask>   main_service_ = &main_service;\n <mask>   store_notification_.SubscribeObjAdded([this](const ObjectInfoT &object_info) {\n <mask>     NotifyDirectoryObjectAdd(object_info);\n <mask>     HandleUnfulfilledPushRequests(object_info);\n <mask>   });\n <mask>   store_notification_.SubscribeObjDeleted(\n <mask>       [this](const ObjectID &oid) { NotifyDirectoryObjectDeleted(oid); });\n <mask>   StartIOService();\n <mask> }\n <mask> \n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   store_notification_.SubscribeObjAdded([this](const ObjectInfoT &object_info) {\n    NotifyDirectoryObjectAdd(object_info);\n    HandleUnfulfilledPushRequests(object_info);\n  });\n </s> add   store_notification_.SubscribeObjAdded(\n      [this](const ObjectInfoT &object_info) { HandleObjectAdded(object_info); }); </s> remove void ObjectManager::NotifyDirectoryObjectAdd(const ObjectInfoT &object_info) {\n </s> add void ObjectManager::HandleObjectAdded(const ObjectInfoT &object_info) {\n  // Notify the object directory that the object has been added to this node. </s> remove   // Check if we're pulling from self.\n </s> add   // The timer should never fire if there are no expected client locations.\n  RAY_CHECK(!it->second.client_locations.empty());\n  RAY_CHECK(local_objects_.count(object_id) == 0);\n\n  // Get the next client to try.\n  const ClientID client_id = std::move(it->second.client_locations.back());\n  it->second.client_locations.pop_back(); </s> remove   // Immediately notify of found object locations.\n  if (!listener_state.current_object_locations.empty()) {\n    std::vector<ClientID> client_id_vec(listener_state.current_object_locations.begin(),\n                                        listener_state.current_object_locations.end());\n    callback(client_id_vec, object_id);\n  }\n </s> add   // Immediately notify of object locations. This notifies the client even if\n  // the list of locations is empty, since this may indicate that the objects\n  // have been evicted from all nodes.\n  std::vector<ClientID> client_id_vec(listener_state.current_object_locations.begin(),\n                                      listener_state.current_object_locations.end());\n  callback(client_id_vec, object_id); </s> remove void ObjectManager::HandleUnfulfilledPushRequests(const ObjectInfoT &object_info) {\n  ObjectID object_id = ObjectID::from_binary(object_info.object_id);\n </s> add  </s> remove }\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.cc"}
{"docstring_tokens": "keep keep replace keep keep keep keep replace keep", "code_tokens": " <mask> }\n <mask> \n <mask> void ObjectManager::NotifyDirectoryObjectAdd(const ObjectInfoT &object_info) {\n <mask>   ObjectID object_id = ObjectID::from_binary(object_info.object_id);\n <mask>   local_objects_[object_id] = object_info;\n <mask>   ray::Status status =\n <mask>       object_directory_->ReportObjectAdded(object_id, client_id_, object_info);\n <mask> }\n <mask> \n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove void ObjectManager::HandleUnfulfilledPushRequests(const ObjectInfoT &object_info) {\n  ObjectID object_id = ObjectID::from_binary(object_info.object_id);\n </s> add  </s> remove   store_notification_.SubscribeObjAdded([this](const ObjectInfoT &object_info) {\n    NotifyDirectoryObjectAdd(object_info);\n    HandleUnfulfilledPushRequests(object_info);\n  });\n </s> add   store_notification_.SubscribeObjAdded(\n      [this](const ObjectInfoT &object_info) { HandleObjectAdded(object_info); }); </s> remove   store_notification_.SubscribeObjAdded([this](const ObjectInfoT &object_info) {\n    NotifyDirectoryObjectAdd(object_info);\n    HandleUnfulfilledPushRequests(object_info);\n  });\n </s> add   store_notification_.SubscribeObjAdded(\n      [this](const ObjectInfoT &object_info) { HandleObjectAdded(object_info); }); </s> remove ray::Status ObjectManager::Cancel(const ObjectID &object_id) {\n  ray::Status status = object_directory_->UnsubscribeObjectLocations(\n      object_directory_pull_callback_id_, object_id);\n  return status;\n </s> add void ObjectManager::CancelPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) {\n    return;\n  }\n\n  RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n      object_directory_pull_callback_id_, object_id));\n  pull_requests_.erase(it); </s> add   // The object is local, so we no longer need to Pull it from a remote\n  // manager. Cancel any outstanding Pull requests for this object.\n  CancelPull(object_id);", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.cc"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>   ray::Status status =\n <mask>       object_directory_->ReportObjectAdded(object_id, client_id_, object_info);\n <mask> }\n <mask> \n <mask> void ObjectManager::HandleUnfulfilledPushRequests(const ObjectInfoT &object_info) {\n <mask>   ObjectID object_id = ObjectID::from_binary(object_info.object_id);\n <mask>   // Handle the unfulfilled_push_requests_ which contains the push request that is not\n <mask>   // completed due to unsatisfied local objects.\n <mask>   auto iter = unfulfilled_push_requests_.find(object_id);\n <mask>   if (iter != unfulfilled_push_requests_.end()) {\n <mask>     for (auto &pair : iter->second) {\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove }\n </s> add  </s> remove void ObjectManager::NotifyDirectoryObjectAdd(const ObjectInfoT &object_info) {\n </s> add void ObjectManager::HandleObjectAdded(const ObjectInfoT &object_info) {\n  // Notify the object directory that the object has been added to this node. </s> remove         RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n            object_directory_pull_callback_id_, object_id));\n        GetLocationsSuccess(client_ids, object_id);\n </s> add         // Exit if the Pull request has already been fulfilled or canceled.\n        auto it = pull_requests_.find(object_id);\n        if (it == pull_requests_.end()) {\n          return;\n        }\n        // Reset the list of clients that are now expected to have the object.\n        // NOTE(swang): Since we are overwriting the previous list of clients,\n        // we may end up sending a duplicate request to the same client as\n        // before.\n        it->second.client_locations = client_ids;\n        if (it->second.client_locations.empty()) {\n          // The object locations are now empty, so we should wait for the next\n          // notification about a new object location.  Cancel the timer until\n          // the next Pull attempt since there are no more clients to try.\n          if (it->second.retry_timer != nullptr) {\n            it->second.retry_timer->cancel();\n            it->second.timer_set = false;\n          }\n        } else {\n          // New object locations were found.\n          if (!it->second.timer_set) {\n            // The timer was not set, which means that we weren't trying any\n            // clients. We now have some clients to try, so begin trying to\n            // Pull from one.  If we fail to receive an object within the pull\n            // timeout, then this will try the rest of the clients in the list\n            // in succession.\n            TryPull(object_id);\n          }\n        } </s> remove     if (!client_id_vec.empty()) {\n      // Copy the callbacks so that the callbacks can unsubscribe without interrupting\n      // looping over the callbacks.\n      auto callbacks = object_id_listener_pair->second.callbacks;\n      // Call all callbacks associated with the object id locations we have received.\n      for (const auto &callback_pair : callbacks) {\n        callback_pair.second(client_id_vec, object_id);\n      }\n </s> add     // Copy the callbacks so that the callbacks can unsubscribe without interrupting\n    // looping over the callbacks.\n    auto callbacks = object_id_listener_pair->second.callbacks;\n    // Call all callbacks associated with the object id locations we have\n    // received.  This notifies the client even if the list of locations is\n    // empty, since this may indicate that the objects have been evicted from\n    // all nodes.\n    for (const auto &callback_pair : callbacks) {\n      callback_pair.second(client_id_vec, object_id); </s> remove             auto object_id_wait_state = active_wait_requests_.find(wait_id);\n            // We never expect to handle a subscription notification for a wait that has\n            // already completed.\n            RAY_CHECK(object_id_wait_state != active_wait_requests_.end());\n            auto &wait_state = object_id_wait_state->second;\n            RAY_CHECK(wait_state.remaining.erase(subscribe_object_id));\n            wait_state.found.insert(subscribe_object_id);\n            wait_state.requested_objects.erase(subscribe_object_id);\n            RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n                wait_id, subscribe_object_id));\n            if (wait_state.found.size() >= wait_state.num_required_objects) {\n              WaitComplete(wait_id);\n </s> add             if (!client_ids.empty()) {\n              auto object_id_wait_state = active_wait_requests_.find(wait_id);\n              // We never expect to handle a subscription notification for a wait that has\n              // already completed.\n              RAY_CHECK(object_id_wait_state != active_wait_requests_.end());\n              auto &wait_state = object_id_wait_state->second;\n              RAY_CHECK(wait_state.remaining.erase(subscribe_object_id));\n              wait_state.found.insert(subscribe_object_id);\n              wait_state.requested_objects.erase(subscribe_object_id);\n              RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n                  wait_id, subscribe_object_id));\n              if (wait_state.found.size() >= wait_state.num_required_objects) {\n                WaitComplete(wait_id);\n              } </s> remove void ObjectManager::GetLocationsSuccess(const std::vector<ray::ClientID> &client_ids,\n                                        const ray::ObjectID &object_id) {\n  if (local_objects_.count(object_id) == 0) {\n    // Only pull objects that aren't local.\n    RAY_CHECK(!client_ids.empty());\n    ClientID client_id = client_ids.front();\n    Pull(object_id, client_id);\n  }\n}\n\nvoid ObjectManager::Pull(const ObjectID &object_id, const ClientID &client_id) {\n  // Check if object is already local.\n  if (local_objects_.count(object_id) != 0) {\n    RAY_LOG(ERROR) << object_id << \" attempted to pull an object that's already local.\";\n </s> add void ObjectManager::TryPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) {", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.cc"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>       }\n <mask>     }\n <mask>     unfulfilled_push_requests_.erase(iter);\n <mask>   }\n <mask> }\n <mask> \n <mask> void ObjectManager::NotifyDirectoryObjectDeleted(const ObjectID &object_id) {\n <mask>   local_objects_.erase(object_id);\n <mask>   ray::Status status = object_directory_->ReportObjectRemoved(object_id, client_id_);\n <mask> }\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove ray::Status ObjectManager::Cancel(const ObjectID &object_id) {\n  ray::Status status = object_directory_->UnsubscribeObjectLocations(\n      object_directory_pull_callback_id_, object_id);\n  return status;\n </s> add void ObjectManager::CancelPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) {\n    return;\n  }\n\n  RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n      object_directory_pull_callback_id_, object_id));\n  pull_requests_.erase(it); </s> remove void ObjectManager::NotifyDirectoryObjectAdd(const ObjectInfoT &object_info) {\n </s> add void ObjectManager::HandleObjectAdded(const ObjectInfoT &object_info) {\n  // Notify the object directory that the object has been added to this node. </s> remove     return;\n </s> add     const ClientID client_id = std::move(it->second.client_locations.back());\n    it->second.client_locations.pop_back();\n    RAY_CHECK(client_id != client_id_); </s> remove void ObjectManager::GetLocationsSuccess(const std::vector<ray::ClientID> &client_ids,\n                                        const ray::ObjectID &object_id) {\n  if (local_objects_.count(object_id) == 0) {\n    // Only pull objects that aren't local.\n    RAY_CHECK(!client_ids.empty());\n    ClientID client_id = client_ids.front();\n    Pull(object_id, client_id);\n  }\n}\n\nvoid ObjectManager::Pull(const ObjectID &object_id, const ClientID &client_id) {\n  // Check if object is already local.\n  if (local_objects_.count(object_id) != 0) {\n    RAY_LOG(ERROR) << object_id << \" attempted to pull an object that's already local.\";\n </s> add void ObjectManager::TryPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) { </s> remove }\n </s> add  </s> remove void ObjectManager::HandleUnfulfilledPushRequests(const ObjectInfoT &object_info) {\n  ObjectID object_id = ObjectID::from_binary(object_info.object_id);\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.cc"}
{"docstring_tokens": "keep replace keep keep replace replace replace", "code_tokens": " <mask>   }\n <mask>   ray::Status status_code = object_directory_->SubscribeObjectLocations(\n <mask>       object_directory_pull_callback_id_, object_id,\n <mask>       [this](const std::vector<ClientID> &client_ids, const ObjectID &object_id) {\n <mask>         RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n <mask>             object_directory_pull_callback_id_, object_id));\n <mask>         GetLocationsSuccess(client_ids, object_id);\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   return status_code;\n </s> add  </s> remove ray::Status ObjectManager::Cancel(const ObjectID &object_id) {\n  ray::Status status = object_directory_->UnsubscribeObjectLocations(\n      object_directory_pull_callback_id_, object_id);\n  return status;\n </s> add void ObjectManager::CancelPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) {\n    return;\n  }\n\n  RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n      object_directory_pull_callback_id_, object_id));\n  pull_requests_.erase(it); </s> remove             auto object_id_wait_state = active_wait_requests_.find(wait_id);\n            // We never expect to handle a subscription notification for a wait that has\n            // already completed.\n            RAY_CHECK(object_id_wait_state != active_wait_requests_.end());\n            auto &wait_state = object_id_wait_state->second;\n            RAY_CHECK(wait_state.remaining.erase(subscribe_object_id));\n            wait_state.found.insert(subscribe_object_id);\n            wait_state.requested_objects.erase(subscribe_object_id);\n            RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n                wait_id, subscribe_object_id));\n            if (wait_state.found.size() >= wait_state.num_required_objects) {\n              WaitComplete(wait_id);\n </s> add             if (!client_ids.empty()) {\n              auto object_id_wait_state = active_wait_requests_.find(wait_id);\n              // We never expect to handle a subscription notification for a wait that has\n              // already completed.\n              RAY_CHECK(object_id_wait_state != active_wait_requests_.end());\n              auto &wait_state = object_id_wait_state->second;\n              RAY_CHECK(wait_state.remaining.erase(subscribe_object_id));\n              wait_state.found.insert(subscribe_object_id);\n              wait_state.requested_objects.erase(subscribe_object_id);\n              RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n                  wait_id, subscribe_object_id));\n              if (wait_state.found.size() >= wait_state.num_required_objects) {\n                WaitComplete(wait_id);\n              } </s> remove   // Immediately notify of found object locations.\n  if (!listener_state.current_object_locations.empty()) {\n    std::vector<ClientID> client_id_vec(listener_state.current_object_locations.begin(),\n                                        listener_state.current_object_locations.end());\n    callback(client_id_vec, object_id);\n  }\n </s> add   // Immediately notify of object locations. This notifies the client even if\n  // the list of locations is empty, since this may indicate that the objects\n  // have been evicted from all nodes.\n  std::vector<ClientID> client_id_vec(listener_state.current_object_locations.begin(),\n                                      listener_state.current_object_locations.end());\n  callback(client_id_vec, object_id); </s> remove void ObjectManager::GetLocationsSuccess(const std::vector<ray::ClientID> &client_ids,\n                                        const ray::ObjectID &object_id) {\n  if (local_objects_.count(object_id) == 0) {\n    // Only pull objects that aren't local.\n    RAY_CHECK(!client_ids.empty());\n    ClientID client_id = client_ids.front();\n    Pull(object_id, client_id);\n  }\n}\n\nvoid ObjectManager::Pull(const ObjectID &object_id, const ClientID &client_id) {\n  // Check if object is already local.\n  if (local_objects_.count(object_id) != 0) {\n    RAY_LOG(ERROR) << object_id << \" attempted to pull an object that's already local.\";\n </s> add void ObjectManager::TryPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) {", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.cc"}
{"docstring_tokens": "keep keep replace keep keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep keep", "code_tokens": " <mask>         GetLocationsSuccess(client_ids, object_id);\n <mask>       });\n <mask>   return status_code;\n <mask> }\n <mask> \n <mask> void ObjectManager::GetLocationsSuccess(const std::vector<ray::ClientID> &client_ids,\n <mask>                                         const ray::ObjectID &object_id) {\n <mask>   if (local_objects_.count(object_id) == 0) {\n <mask>     // Only pull objects that aren't local.\n <mask>     RAY_CHECK(!client_ids.empty());\n <mask>     ClientID client_id = client_ids.front();\n <mask>     Pull(object_id, client_id);\n <mask>   }\n <mask> }\n <mask> \n <mask> void ObjectManager::Pull(const ObjectID &object_id, const ClientID &client_id) {\n <mask>   // Check if object is already local.\n <mask>   if (local_objects_.count(object_id) != 0) {\n <mask>     RAY_LOG(ERROR) << object_id << \" attempted to pull an object that's already local.\";\n <mask>     return;\n <mask>   }\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   // Check if we're pulling from self.\n </s> add   // The timer should never fire if there are no expected client locations.\n  RAY_CHECK(!it->second.client_locations.empty());\n  RAY_CHECK(local_objects_.count(object_id) == 0);\n\n  // Get the next client to try.\n  const ClientID client_id = std::move(it->second.client_locations.back());\n  it->second.client_locations.pop_back(); </s> remove   ray::Status status_code = object_directory_->SubscribeObjectLocations(\n </s> add   if (pull_requests_.find(object_id) != pull_requests_.end()) {\n    return ray::Status::OK();\n  }\n\n  pull_requests_.emplace(object_id, PullRequest());\n  // Subscribe to object notifications. A notification will be received every\n  // time the set of client IDs for the object changes. Notifications will also\n  // be received if the list of locations is empty. The set of client IDs has\n  // no ordering guarantee between notifications.\n  return object_directory_->SubscribeObjectLocations( </s> remove     return;\n </s> add     const ClientID client_id = std::move(it->second.client_locations.back());\n    it->second.client_locations.pop_back();\n    RAY_CHECK(client_id != client_id_); </s> add     // If we're trying to pull from ourselves, skip this client and try the\n    // next one. </s> remove         RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n            object_directory_pull_callback_id_, object_id));\n        GetLocationsSuccess(client_ids, object_id);\n </s> add         // Exit if the Pull request has already been fulfilled or canceled.\n        auto it = pull_requests_.find(object_id);\n        if (it == pull_requests_.end()) {\n          return;\n        }\n        // Reset the list of clients that are now expected to have the object.\n        // NOTE(swang): Since we are overwriting the previous list of clients,\n        // we may end up sending a duplicate request to the same client as\n        // before.\n        it->second.client_locations = client_ids;\n        if (it->second.client_locations.empty()) {\n          // The object locations are now empty, so we should wait for the next\n          // notification about a new object location.  Cancel the timer until\n          // the next Pull attempt since there are no more clients to try.\n          if (it->second.retry_timer != nullptr) {\n            it->second.retry_timer->cancel();\n            it->second.timer_set = false;\n          }\n        } else {\n          // New object locations were found.\n          if (!it->second.timer_set) {\n            // The timer was not set, which means that we weren't trying any\n            // clients. We now have some clients to try, so begin trying to\n            // Pull from one.  If we fail to receive an object within the pull\n            // timeout, then this will try the rest of the clients in the list\n            // in succession.\n            TryPull(object_id);\n          }\n        }", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   if (local_objects_.count(object_id) != 0) {\n <mask>     RAY_LOG(ERROR) << object_id << \" attempted to pull an object that's already local.\";\n <mask>     return;\n <mask>   }\n <mask>   // Check if we're pulling from self.\n <mask>   if (client_id == client_id_) {\n <mask>     RAY_LOG(ERROR) << client_id_ << \" attempted to pull an object from itself.\";\n <mask>     return;\n <mask>   }\n <mask>   PullEstablishConnection(object_id, client_id);\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove void ObjectManager::GetLocationsSuccess(const std::vector<ray::ClientID> &client_ids,\n                                        const ray::ObjectID &object_id) {\n  if (local_objects_.count(object_id) == 0) {\n    // Only pull objects that aren't local.\n    RAY_CHECK(!client_ids.empty());\n    ClientID client_id = client_ids.front();\n    Pull(object_id, client_id);\n  }\n}\n\nvoid ObjectManager::Pull(const ObjectID &object_id, const ClientID &client_id) {\n  // Check if object is already local.\n  if (local_objects_.count(object_id) != 0) {\n    RAY_LOG(ERROR) << object_id << \" attempted to pull an object that's already local.\";\n </s> add void ObjectManager::TryPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) { </s> remove     return;\n </s> add     const ClientID client_id = std::move(it->second.client_locations.back());\n    it->second.client_locations.pop_back();\n    RAY_CHECK(client_id != client_id_); </s> add     // If we're trying to pull from ourselves, skip this client and try the\n    // next one. </s> remove   ray::Status status_code = object_directory_->SubscribeObjectLocations(\n </s> add   if (pull_requests_.find(object_id) != pull_requests_.end()) {\n    return ray::Status::OK();\n  }\n\n  pull_requests_.emplace(object_id, PullRequest());\n  // Subscribe to object notifications. A notification will be received every\n  // time the set of client IDs for the object changes. Notifications will also\n  // be received if the list of locations is empty. The set of client IDs has\n  // no ordering guarantee between notifications.\n  return object_directory_->SubscribeObjectLocations( </s> remove         RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n            object_directory_pull_callback_id_, object_id));\n        GetLocationsSuccess(client_ids, object_id);\n </s> add         // Exit if the Pull request has already been fulfilled or canceled.\n        auto it = pull_requests_.find(object_id);\n        if (it == pull_requests_.end()) {\n          return;\n        }\n        // Reset the list of clients that are now expected to have the object.\n        // NOTE(swang): Since we are overwriting the previous list of clients,\n        // we may end up sending a duplicate request to the same client as\n        // before.\n        it->second.client_locations = client_ids;\n        if (it->second.client_locations.empty()) {\n          // The object locations are now empty, so we should wait for the next\n          // notification about a new object location.  Cancel the timer until\n          // the next Pull attempt since there are no more clients to try.\n          if (it->second.retry_timer != nullptr) {\n            it->second.retry_timer->cancel();\n            it->second.timer_set = false;\n          }\n        } else {\n          // New object locations were found.\n          if (!it->second.timer_set) {\n            // The timer was not set, which means that we weren't trying any\n            // clients. We now have some clients to try, so begin trying to\n            // Pull from one.  If we fail to receive an object within the pull\n            // timeout, then this will try the rest of the clients in the list\n            // in succession.\n            TryPull(object_id);\n          }\n        } </s> add       auto timeout = boost::posix_time::milliseconds(wait_state.timeout_ms);\n      wait_state.timeout_timer->expires_from_now(timeout);", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.cc"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>   it->second.client_locations.pop_back();\n <mask>   if (client_id == client_id_) {\n <mask>     RAY_LOG(ERROR) << client_id_ << \" attempted to pull an object from itself.\";\n <mask>     const ClientID client_id = std::move(it->second.client_locations.back());\n <mask>     it->second.client_locations.pop_back();\n <mask>     RAY_CHECK(client_id != client_id_);\n <mask>   }\n <mask> \n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove     return;\n </s> add     const ClientID client_id = std::move(it->second.client_locations.back());\n    it->second.client_locations.pop_back();\n    RAY_CHECK(client_id != client_id_); </s> remove   // Check if we're pulling from self.\n </s> add   // The timer should never fire if there are no expected client locations.\n  RAY_CHECK(!it->second.client_locations.empty());\n  RAY_CHECK(local_objects_.count(object_id) == 0);\n\n  // Get the next client to try.\n  const ClientID client_id = std::move(it->second.client_locations.back());\n  it->second.client_locations.pop_back(); </s> remove void ObjectManager::GetLocationsSuccess(const std::vector<ray::ClientID> &client_ids,\n                                        const ray::ObjectID &object_id) {\n  if (local_objects_.count(object_id) == 0) {\n    // Only pull objects that aren't local.\n    RAY_CHECK(!client_ids.empty());\n    ClientID client_id = client_ids.front();\n    Pull(object_id, client_id);\n  }\n}\n\nvoid ObjectManager::Pull(const ObjectID &object_id, const ClientID &client_id) {\n  // Check if object is already local.\n  if (local_objects_.count(object_id) != 0) {\n    RAY_LOG(ERROR) << object_id << \" attempted to pull an object that's already local.\";\n </s> add void ObjectManager::TryPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) { </s> add   // Try pulling from the client. </s> remove   ray::Status status_code = object_directory_->SubscribeObjectLocations(\n </s> add   if (pull_requests_.find(object_id) != pull_requests_.end()) {\n    return ray::Status::OK();\n  }\n\n  pull_requests_.emplace(object_id, PullRequest());\n  // Subscribe to object notifications. A notification will be received every\n  // time the set of client IDs for the object changes. Notifications will also\n  // be received if the list of locations is empty. The set of client IDs has\n  // no ordering guarantee between notifications.\n  return object_directory_->SubscribeObjectLocations( </s> remove         RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n            object_directory_pull_callback_id_, object_id));\n        GetLocationsSuccess(client_ids, object_id);\n </s> add         // Exit if the Pull request has already been fulfilled or canceled.\n        auto it = pull_requests_.find(object_id);\n        if (it == pull_requests_.end()) {\n          return;\n        }\n        // Reset the list of clients that are now expected to have the object.\n        // NOTE(swang): Since we are overwriting the previous list of clients,\n        // we may end up sending a duplicate request to the same client as\n        // before.\n        it->second.client_locations = client_ids;\n        if (it->second.client_locations.empty()) {\n          // The object locations are now empty, so we should wait for the next\n          // notification about a new object location.  Cancel the timer until\n          // the next Pull attempt since there are no more clients to try.\n          if (it->second.retry_timer != nullptr) {\n            it->second.retry_timer->cancel();\n            it->second.timer_set = false;\n          }\n        } else {\n          // New object locations were found.\n          if (!it->second.timer_set) {\n            // The timer was not set, which means that we weren't trying any\n            // clients. We now have some clients to try, so begin trying to\n            // Pull from one.  If we fail to receive an object within the pull\n            // timeout, then this will try the rest of the clients in the list\n            // in succession.\n            TryPull(object_id);\n          }\n        }", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   }\n <mask>   // Check if we're pulling from self.\n <mask>   if (client_id == client_id_) {\n <mask>     RAY_LOG(ERROR) << client_id_ << \" attempted to pull an object from itself.\";\n <mask>     return;\n <mask>   }\n <mask>   PullEstablishConnection(object_id, client_id);\n <mask> };\n <mask> \n <mask> void ObjectManager::PullEstablishConnection(const ObjectID &object_id,\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   // Check if we're pulling from self.\n </s> add   // The timer should never fire if there are no expected client locations.\n  RAY_CHECK(!it->second.client_locations.empty());\n  RAY_CHECK(local_objects_.count(object_id) == 0);\n\n  // Get the next client to try.\n  const ClientID client_id = std::move(it->second.client_locations.back());\n  it->second.client_locations.pop_back(); </s> remove void ObjectManager::GetLocationsSuccess(const std::vector<ray::ClientID> &client_ids,\n                                        const ray::ObjectID &object_id) {\n  if (local_objects_.count(object_id) == 0) {\n    // Only pull objects that aren't local.\n    RAY_CHECK(!client_ids.empty());\n    ClientID client_id = client_ids.front();\n    Pull(object_id, client_id);\n  }\n}\n\nvoid ObjectManager::Pull(const ObjectID &object_id, const ClientID &client_id) {\n  // Check if object is already local.\n  if (local_objects_.count(object_id) != 0) {\n    RAY_LOG(ERROR) << object_id << \" attempted to pull an object that's already local.\";\n </s> add void ObjectManager::TryPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) { </s> add     // If we're trying to pull from ourselves, skip this client and try the\n    // next one. </s> remove   ray::Status status_code = object_directory_->SubscribeObjectLocations(\n </s> add   if (pull_requests_.find(object_id) != pull_requests_.end()) {\n    return ray::Status::OK();\n  }\n\n  pull_requests_.emplace(object_id, PullRequest());\n  // Subscribe to object notifications. A notification will be received every\n  // time the set of client IDs for the object changes. Notifications will also\n  // be received if the list of locations is empty. The set of client IDs has\n  // no ordering guarantee between notifications.\n  return object_directory_->SubscribeObjectLocations( </s> add   // Try pulling from the client. </s> remove         RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n            object_directory_pull_callback_id_, object_id));\n        GetLocationsSuccess(client_ids, object_id);\n </s> add         // Exit if the Pull request has already been fulfilled or canceled.\n        auto it = pull_requests_.find(object_id);\n        if (it == pull_requests_.end()) {\n          return;\n        }\n        // Reset the list of clients that are now expected to have the object.\n        // NOTE(swang): Since we are overwriting the previous list of clients,\n        // we may end up sending a duplicate request to the same client as\n        // before.\n        it->second.client_locations = client_ids;\n        if (it->second.client_locations.empty()) {\n          // The object locations are now empty, so we should wait for the next\n          // notification about a new object location.  Cancel the timer until\n          // the next Pull attempt since there are no more clients to try.\n          if (it->second.retry_timer != nullptr) {\n            it->second.retry_timer->cancel();\n            it->second.timer_set = false;\n          }\n        } else {\n          // New object locations were found.\n          if (!it->second.timer_set) {\n            // The timer was not set, which means that we weren't trying any\n            // clients. We now have some clients to try, so begin trying to\n            // Pull from one.  If we fail to receive an object within the pull\n            // timeout, then this will try the rest of the clients in the list\n            // in succession.\n            TryPull(object_id);\n          }\n        }", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.cc"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask>     const ClientID client_id = std::move(it->second.client_locations.back());\n <mask>     it->second.client_locations.pop_back();\n <mask>     RAY_CHECK(client_id != client_id_);\n <mask>   }\n <mask>   PullEstablishConnection(object_id, client_id);\n <mask> \n <mask>   // If there are more clients to try, try them in succession, with a timeout\n <mask>   // in between each try.\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   /// Discover ClientID via ObjectDirectory, then pull object\n  /// from ClientID associated with ObjectID.\n </s> add   /// Try to Pull an object from one of its expected client locations. If there\n  /// are more client locations to try after this attempt, then this method\n  /// will try each of the other clients in succession, with a timeout between\n  /// each attempt. If the object is received or if the Pull is Canceled before\n  /// the timeout, then no more Pull requests for this object will be sent\n  /// to other node managers until TryPull is called again. </s> add     // If we're trying to pull from ourselves, skip this client and try the\n    // next one. </s> remove     return;\n </s> add     const ClientID client_id = std::move(it->second.client_locations.back());\n    it->second.client_locations.pop_back();\n    RAY_CHECK(client_id != client_id_); </s> remove         RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n            object_directory_pull_callback_id_, object_id));\n        GetLocationsSuccess(client_ids, object_id);\n </s> add         // Exit if the Pull request has already been fulfilled or canceled.\n        auto it = pull_requests_.find(object_id);\n        if (it == pull_requests_.end()) {\n          return;\n        }\n        // Reset the list of clients that are now expected to have the object.\n        // NOTE(swang): Since we are overwriting the previous list of clients,\n        // we may end up sending a duplicate request to the same client as\n        // before.\n        it->second.client_locations = client_ids;\n        if (it->second.client_locations.empty()) {\n          // The object locations are now empty, so we should wait for the next\n          // notification about a new object location.  Cancel the timer until\n          // the next Pull attempt since there are no more clients to try.\n          if (it->second.retry_timer != nullptr) {\n            it->second.retry_timer->cancel();\n            it->second.timer_set = false;\n          }\n        } else {\n          // New object locations were found.\n          if (!it->second.timer_set) {\n            // The timer was not set, which means that we weren't trying any\n            // clients. We now have some clients to try, so begin trying to\n            // Pull from one.  If we fail to receive an object within the pull\n            // timeout, then this will try the rest of the clients in the list\n            // in succession.\n            TryPull(object_id);\n          }\n        } </s> remove   // Check if we're pulling from self.\n </s> add   // The timer should never fire if there are no expected client locations.\n  RAY_CHECK(!it->second.client_locations.empty());\n  RAY_CHECK(local_objects_.count(object_id) == 0);\n\n  // Get the next client to try.\n  const ClientID client_id = std::move(it->second.client_locations.back());\n  it->second.client_locations.pop_back(); </s> remove         // TODO: Setting this to large values results in latency, which needs to\n        // be addressed. This timeout is often on the critical path for object\n        // transfers.\n        object_manager_pull_timeout_ms_(20),\n </s> add         object_manager_pull_timeout_ms_(100),", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.cc"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>   }\n <mask>   return status;\n <mask> }\n <mask> \n <mask> ray::Status ObjectManager::Cancel(const ObjectID &object_id) {\n <mask>   ray::Status status = object_directory_->UnsubscribeObjectLocations(\n <mask>       object_directory_pull_callback_id_, object_id);\n <mask>   return status;\n <mask> }\n <mask> \n <mask> ray::Status ObjectManager::Wait(const std::vector<ObjectID> &object_ids,\n <mask>                                 int64_t timeout_ms, uint64_t num_required_objects,\n <mask>                                 bool wait_local, const WaitCallback &callback) {\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   /// A set of active wait requests.\n  std::unordered_map<UniqueID, WaitState> active_wait_requests_;\n\n </s> add  </s> remove   // Immediately notify of found object locations.\n  if (!listener_state.current_object_locations.empty()) {\n    std::vector<ClientID> client_id_vec(listener_state.current_object_locations.begin(),\n                                        listener_state.current_object_locations.end());\n    callback(client_id_vec, object_id);\n  }\n </s> add   // Immediately notify of object locations. This notifies the client even if\n  // the list of locations is empty, since this may indicate that the objects\n  // have been evicted from all nodes.\n  std::vector<ClientID> client_id_vec(listener_state.current_object_locations.begin(),\n                                      listener_state.current_object_locations.end());\n  callback(client_id_vec, object_id); </s> remove   return status_code;\n </s> add  </s> remove   ray::Status status_code = object_directory_->SubscribeObjectLocations(\n </s> add   if (pull_requests_.find(object_id) != pull_requests_.end()) {\n    return ray::Status::OK();\n  }\n\n  pull_requests_.emplace(object_id, PullRequest());\n  // Subscribe to object notifications. A notification will be received every\n  // time the set of client IDs for the object changes. Notifications will also\n  // be received if the list of locations is empty. The set of client IDs has\n  // no ordering guarantee between notifications.\n  return object_directory_->SubscribeObjectLocations( </s> remove void ObjectManager::GetLocationsSuccess(const std::vector<ray::ClientID> &client_ids,\n                                        const ray::ObjectID &object_id) {\n  if (local_objects_.count(object_id) == 0) {\n    // Only pull objects that aren't local.\n    RAY_CHECK(!client_ids.empty());\n    ClientID client_id = client_ids.front();\n    Pull(object_id, client_id);\n  }\n}\n\nvoid ObjectManager::Pull(const ObjectID &object_id, const ClientID &client_id) {\n  // Check if object is already local.\n  if (local_objects_.count(object_id) != 0) {\n    RAY_LOG(ERROR) << object_id << \" attempted to pull an object that's already local.\";\n </s> add void ObjectManager::TryPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) { </s> remove         RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n            object_directory_pull_callback_id_, object_id));\n        GetLocationsSuccess(client_ids, object_id);\n </s> add         // Exit if the Pull request has already been fulfilled or canceled.\n        auto it = pull_requests_.find(object_id);\n        if (it == pull_requests_.end()) {\n          return;\n        }\n        // Reset the list of clients that are now expected to have the object.\n        // NOTE(swang): Since we are overwriting the previous list of clients,\n        // we may end up sending a duplicate request to the same client as\n        // before.\n        it->second.client_locations = client_ids;\n        if (it->second.client_locations.empty()) {\n          // The object locations are now empty, so we should wait for the next\n          // notification about a new object location.  Cancel the timer until\n          // the next Pull attempt since there are no more clients to try.\n          if (it->second.retry_timer != nullptr) {\n            it->second.retry_timer->cancel();\n            it->second.timer_set = false;\n          }\n        } else {\n          // New object locations were found.\n          if (!it->second.timer_set) {\n            // The timer was not set, which means that we weren't trying any\n            // clients. We now have some clients to try, so begin trying to\n            // Pull from one.  If we fail to receive an object within the pull\n            // timeout, then this will try the rest of the clients in the list\n            // in succession.\n            TryPull(object_id);\n          }\n        }", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.cc"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>       // Subscribe to object notifications.\n <mask>       RAY_CHECK_OK(object_directory_->SubscribeObjectLocations(\n <mask>           wait_id, object_id, [this, wait_id](const std::vector<ClientID> &client_ids,\n <mask>                                               const ObjectID &subscribe_object_id) {\n <mask>             auto object_id_wait_state = active_wait_requests_.find(wait_id);\n <mask>             // We never expect to handle a subscription notification for a wait that has\n <mask>             // already completed.\n <mask>             RAY_CHECK(object_id_wait_state != active_wait_requests_.end());\n <mask>             auto &wait_state = object_id_wait_state->second;\n <mask>             RAY_CHECK(wait_state.remaining.erase(subscribe_object_id));\n <mask>             wait_state.found.insert(subscribe_object_id);\n <mask>             wait_state.requested_objects.erase(subscribe_object_id);\n <mask>             RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n <mask>                 wait_id, subscribe_object_id));\n <mask>             if (wait_state.found.size() >= wait_state.num_required_objects) {\n <mask>               WaitComplete(wait_id);\n <mask>             }\n <mask>           }));\n <mask>     }\n <mask>     if (wait_state.timeout_ms != -1) {\n <mask>       wait_state.timeout_timer->async_wait(\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> add       auto timeout = boost::posix_time::milliseconds(wait_state.timeout_ms);\n      wait_state.timeout_timer->expires_from_now(timeout); </s> remove   ray::Status status_code = object_directory_->SubscribeObjectLocations(\n </s> add   if (pull_requests_.find(object_id) != pull_requests_.end()) {\n    return ray::Status::OK();\n  }\n\n  pull_requests_.emplace(object_id, PullRequest());\n  // Subscribe to object notifications. A notification will be received every\n  // time the set of client IDs for the object changes. Notifications will also\n  // be received if the list of locations is empty. The set of client IDs has\n  // no ordering guarantee between notifications.\n  return object_directory_->SubscribeObjectLocations( </s> remove         RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n            object_directory_pull_callback_id_, object_id));\n        GetLocationsSuccess(client_ids, object_id);\n </s> add         // Exit if the Pull request has already been fulfilled or canceled.\n        auto it = pull_requests_.find(object_id);\n        if (it == pull_requests_.end()) {\n          return;\n        }\n        // Reset the list of clients that are now expected to have the object.\n        // NOTE(swang): Since we are overwriting the previous list of clients,\n        // we may end up sending a duplicate request to the same client as\n        // before.\n        it->second.client_locations = client_ids;\n        if (it->second.client_locations.empty()) {\n          // The object locations are now empty, so we should wait for the next\n          // notification about a new object location.  Cancel the timer until\n          // the next Pull attempt since there are no more clients to try.\n          if (it->second.retry_timer != nullptr) {\n            it->second.retry_timer->cancel();\n            it->second.timer_set = false;\n          }\n        } else {\n          // New object locations were found.\n          if (!it->second.timer_set) {\n            // The timer was not set, which means that we weren't trying any\n            // clients. We now have some clients to try, so begin trying to\n            // Pull from one.  If we fail to receive an object within the pull\n            // timeout, then this will try the rest of the clients in the list\n            // in succession.\n            TryPull(object_id);\n          }\n        } </s> remove void ObjectManager::GetLocationsSuccess(const std::vector<ray::ClientID> &client_ids,\n                                        const ray::ObjectID &object_id) {\n  if (local_objects_.count(object_id) == 0) {\n    // Only pull objects that aren't local.\n    RAY_CHECK(!client_ids.empty());\n    ClientID client_id = client_ids.front();\n    Pull(object_id, client_id);\n  }\n}\n\nvoid ObjectManager::Pull(const ObjectID &object_id, const ClientID &client_id) {\n  // Check if object is already local.\n  if (local_objects_.count(object_id) != 0) {\n    RAY_LOG(ERROR) << object_id << \" attempted to pull an object that's already local.\";\n </s> add void ObjectManager::TryPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) { </s> remove void ObjectManager::HandleUnfulfilledPushRequests(const ObjectInfoT &object_info) {\n  ObjectID object_id = ObjectID::from_binary(object_info.object_id);\n </s> add  </s> remove   // Check if we're pulling from self.\n </s> add   // The timer should never fire if there are no expected client locations.\n  RAY_CHECK(!it->second.client_locations.empty());\n  RAY_CHECK(local_objects_.count(object_id) == 0);\n\n  // Get the next client to try.\n  const ClientID client_id = std::move(it->second.client_locations.back());\n  it->second.client_locations.pop_back();", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.cc"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>     }\n <mask>     if (wait_state.timeout_ms != -1) {\n <mask>       wait_state.timeout_timer->async_wait(\n <mask>           [this, wait_id](const boost::system::error_code &error_code) {\n <mask>             if (error_code.value() != 0) {\n <mask>               return;\n <mask>             }\n <mask>             WaitComplete(wait_id);\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove             auto object_id_wait_state = active_wait_requests_.find(wait_id);\n            // We never expect to handle a subscription notification for a wait that has\n            // already completed.\n            RAY_CHECK(object_id_wait_state != active_wait_requests_.end());\n            auto &wait_state = object_id_wait_state->second;\n            RAY_CHECK(wait_state.remaining.erase(subscribe_object_id));\n            wait_state.found.insert(subscribe_object_id);\n            wait_state.requested_objects.erase(subscribe_object_id);\n            RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n                wait_id, subscribe_object_id));\n            if (wait_state.found.size() >= wait_state.num_required_objects) {\n              WaitComplete(wait_id);\n </s> add             if (!client_ids.empty()) {\n              auto object_id_wait_state = active_wait_requests_.find(wait_id);\n              // We never expect to handle a subscription notification for a wait that has\n              // already completed.\n              RAY_CHECK(object_id_wait_state != active_wait_requests_.end());\n              auto &wait_state = object_id_wait_state->second;\n              RAY_CHECK(wait_state.remaining.erase(subscribe_object_id));\n              wait_state.found.insert(subscribe_object_id);\n              wait_state.requested_objects.erase(subscribe_object_id);\n              RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n                  wait_id, subscribe_object_id));\n              if (wait_state.found.size() >= wait_state.num_required_objects) {\n                WaitComplete(wait_id);\n              } </s> remove void ObjectManager::GetLocationsSuccess(const std::vector<ray::ClientID> &client_ids,\n                                        const ray::ObjectID &object_id) {\n  if (local_objects_.count(object_id) == 0) {\n    // Only pull objects that aren't local.\n    RAY_CHECK(!client_ids.empty());\n    ClientID client_id = client_ids.front();\n    Pull(object_id, client_id);\n  }\n}\n\nvoid ObjectManager::Pull(const ObjectID &object_id, const ClientID &client_id) {\n  // Check if object is already local.\n  if (local_objects_.count(object_id) != 0) {\n    RAY_LOG(ERROR) << object_id << \" attempted to pull an object that's already local.\";\n </s> add void ObjectManager::TryPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) { </s> remove   // Check if we're pulling from self.\n </s> add   // The timer should never fire if there are no expected client locations.\n  RAY_CHECK(!it->second.client_locations.empty());\n  RAY_CHECK(local_objects_.count(object_id) == 0);\n\n  // Get the next client to try.\n  const ClientID client_id = std::move(it->second.client_locations.back());\n  it->second.client_locations.pop_back(); </s> remove     return;\n </s> add     const ClientID client_id = std::move(it->second.client_locations.back());\n    it->second.client_locations.pop_back();\n    RAY_CHECK(client_id != client_id_); </s> remove   ray::Status status_code = object_directory_->SubscribeObjectLocations(\n </s> add   if (pull_requests_.find(object_id) != pull_requests_.end()) {\n    return ray::Status::OK();\n  }\n\n  pull_requests_.emplace(object_id, PullRequest());\n  // Subscribe to object notifications. A notification will be received every\n  // time the set of client IDs for the object changes. Notifications will also\n  // be received if the list of locations is empty. The set of client IDs has\n  // no ordering guarantee between notifications.\n  return object_directory_->SubscribeObjectLocations( </s> remove   return status_code;\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> class ObjectManagerInterface {\n <mask>  public:\n <mask>   virtual ray::Status Pull(const ObjectID &object_id) = 0;\n <mask>   virtual ray::Status Cancel(const ObjectID &object_id) = 0;\n <mask>   virtual ~ObjectManagerInterface(){};\n <mask> };\n <mask> \n <mask> // TODO(hme): Add success/failure callbacks for push and pull.\n <mask> class ObjectManager : public ObjectManagerInterface {\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove     if (!client_id_vec.empty()) {\n      // Copy the callbacks so that the callbacks can unsubscribe without interrupting\n      // looping over the callbacks.\n      auto callbacks = object_id_listener_pair->second.callbacks;\n      // Call all callbacks associated with the object id locations we have received.\n      for (const auto &callback_pair : callbacks) {\n        callback_pair.second(client_id_vec, object_id);\n      }\n </s> add     // Copy the callbacks so that the callbacks can unsubscribe without interrupting\n    // looping over the callbacks.\n    auto callbacks = object_id_listener_pair->second.callbacks;\n    // Call all callbacks associated with the object id locations we have\n    // received.  This notifies the client even if the list of locations is\n    // empty, since this may indicate that the objects have been evicted from\n    // all nodes.\n    for (const auto &callback_pair : callbacks) {\n      callback_pair.second(client_id_vec, object_id); </s> remove void ObjectManager::HandleUnfulfilledPushRequests(const ObjectInfoT &object_info) {\n  ObjectID object_id = ObjectID::from_binary(object_info.object_id);\n </s> add  </s> remove ray::Status ObjectManager::Cancel(const ObjectID &object_id) {\n  ray::Status status = object_directory_->UnsubscribeObjectLocations(\n      object_directory_pull_callback_id_, object_id);\n  return status;\n </s> add void ObjectManager::CancelPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) {\n    return;\n  }\n\n  RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n      object_directory_pull_callback_id_, object_id));\n  pull_requests_.erase(it); </s> remove   /// The callback will be invoked whenever locations are obtained for the\n  /// specified object. The callback provided to this method may fire immediately,\n  /// within the call to this method, if any other listener is subscribed to the same\n  /// object: This occurs when location data for the object has already been obtained.\n  ///\n </s> add   /// The callback will be invoked with the complete list of known locations\n  /// whenever the set of locations changes. The callback will also be fired if\n  /// the list of known locations is empty. The callback provided to this\n  /// method may fire immediately, within the call to this method, if any other\n  /// listener is subscribed to the same object: This occurs when location data\n  /// for the object has already been obtained.\n  // </s> remove }\n </s> add  </s> remove void ObjectManager::GetLocationsSuccess(const std::vector<ray::ClientID> &client_ids,\n                                        const ray::ObjectID &object_id) {\n  if (local_objects_.count(object_id) == 0) {\n    // Only pull objects that aren't local.\n    RAY_CHECK(!client_ids.empty());\n    ClientID client_id = client_ids.front();\n    Pull(object_id, client_id);\n  }\n}\n\nvoid ObjectManager::Pull(const ObjectID &object_id, const ClientID &client_id) {\n  // Check if object is already local.\n  if (local_objects_.count(object_id) != 0) {\n    RAY_LOG(ERROR) << object_id << \" attempted to pull an object that's already local.\";\n </s> add void ObjectManager::TryPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) {", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.h"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>   /// \\param client_id The remote node's client id.\n <mask>   /// \\return Void.\n <mask>   void Push(const ObjectID &object_id, const ClientID &client_id);\n <mask> \n <mask>   /// Pull an object from ClientID. Returns UniqueID asociated with\n <mask>   /// an invocation of this method.\n <mask>   ///\n <mask>   /// \\param object_id The object's object id.\n <mask>   /// \\return Status of whether the pull request successfully initiated.\n <mask>   ray::Status Pull(const ObjectID &object_id);\n <mask> \n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   /// Discover ClientID via ObjectDirectory, then pull object\n  /// from ClientID associated with ObjectID.\n </s> add   /// Try to Pull an object from one of its expected client locations. If there\n  /// are more client locations to try after this attempt, then this method\n  /// will try each of the other clients in succession, with a timeout between\n  /// each attempt. If the object is received or if the Pull is Canceled before\n  /// the timeout, then no more Pull requests for this object will be sent\n  /// to other node managers until TryPull is called again. </s> remove   void Pull(const ObjectID &object_id, const ClientID &client_id);\n </s> add   void TryPull(const ObjectID &object_id); </s> remove   /// \\param client_id The remote node's client id.\n </s> add  </s> remove   /// \\return Status of whether requests were successfully cancelled.\n  ray::Status Cancel(const ObjectID &object_id);\n </s> add   /// \\return Void.\n  void CancelPull(const ObjectID &object_id); </s> remove   /// Cancels all requests (Push/Pull) associated with the given ObjectID.\n </s> add   /// Cancels all requests (Push/Pull) associated with the given ObjectID. This\n  /// method is idempotent. </s> remove   /// Handle any push requests that were made before an object was available.\n  /// This is invoked when an \"object added\" notification is received from the store.\n  void HandleUnfulfilledPushRequests(const ObjectInfoT &object_info);\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.h"}
{"docstring_tokens": "keep keep keep replace replace keep keep replace", "code_tokens": " <mask>   /// \\return Status of whether the pull request successfully initiated.\n <mask>   ray::Status Pull(const ObjectID &object_id);\n <mask> \n <mask>   /// Discover ClientID via ObjectDirectory, then pull object\n <mask>   /// from ClientID associated with ObjectID.\n <mask>   ///\n <mask>   /// \\param object_id The object's object id.\n <mask>   /// \\param client_id The remote node's client id.\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   /// Pull an object from ClientID. Returns UniqueID asociated with\n  /// an invocation of this method.\n </s> add   /// Pull an object from ClientID. </s> remove   void Pull(const ObjectID &object_id, const ClientID &client_id);\n </s> add   void TryPull(const ObjectID &object_id); </s> remove   /// Cancels all requests (Push/Pull) associated with the given ObjectID.\n </s> add   /// Cancels all requests (Push/Pull) associated with the given ObjectID. This\n  /// method is idempotent. </s> remove   /// \\return Status of whether requests were successfully cancelled.\n  ray::Status Cancel(const ObjectID &object_id);\n </s> add   /// \\return Void.\n  void CancelPull(const ObjectID &object_id); </s> remove   /// The callback will be invoked whenever locations are obtained for the\n  /// specified object. The callback provided to this method may fire immediately,\n  /// within the call to this method, if any other listener is subscribed to the same\n  /// object: This occurs when location data for the object has already been obtained.\n  ///\n </s> add   /// The callback will be invoked with the complete list of known locations\n  /// whenever the set of locations changes. The callback will also be fired if\n  /// the list of known locations is empty. The callback provided to this\n  /// method may fire immediately, within the call to this method, if any other\n  /// listener is subscribed to the same object: This occurs when location data\n  /// for the object has already been obtained.\n  //", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.h"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   ///\n <mask>   /// \\param object_id The object's object id.\n <mask>   /// \\param client_id The remote node's client id.\n <mask>   /// \\return Void.\n <mask>   void Pull(const ObjectID &object_id, const ClientID &client_id);\n <mask> \n <mask>   /// Add a connection to a remote object manager.\n <mask>   /// This is invoked by an external server.\n <mask>   ///\n <mask>   /// \\param conn The connection.\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   /// \\param client_id The remote node's client id.\n </s> add  </s> remove   /// Pull an object from ClientID. Returns UniqueID asociated with\n  /// an invocation of this method.\n </s> add   /// Pull an object from ClientID. </s> remove   /// Discover ClientID via ObjectDirectory, then pull object\n  /// from ClientID associated with ObjectID.\n </s> add   /// Try to Pull an object from one of its expected client locations. If there\n  /// are more client locations to try after this attempt, then this method\n  /// will try each of the other clients in succession, with a timeout between\n  /// each attempt. If the object is received or if the Pull is Canceled before\n  /// the timeout, then no more Pull requests for this object will be sent\n  /// to other node managers until TryPull is called again. </s> remove   /// Handle any push requests that were made before an object was available.\n  /// This is invoked when an \"object added\" notification is received from the store.\n  void HandleUnfulfilledPushRequests(const ObjectInfoT &object_info);\n\n </s> add  </s> remove   /// The callback will be invoked whenever locations are obtained for the\n  /// specified object. The callback provided to this method may fire immediately,\n  /// within the call to this method, if any other listener is subscribed to the same\n  /// object: This occurs when location data for the object has already been obtained.\n  ///\n </s> add   /// The callback will be invoked with the complete list of known locations\n  /// whenever the set of locations changes. The callback will also be fired if\n  /// the list of known locations is empty. The callback provided to this\n  /// method may fire immediately, within the call to this method, if any other\n  /// listener is subscribed to the same object: This occurs when location data\n  /// for the object has already been obtained.\n  // </s> remove   /// Cancels all requests (Push/Pull) associated with the given ObjectID.\n </s> add   /// Cancels all requests (Push/Pull) associated with the given ObjectID. This\n  /// method is idempotent.", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.h"}
{"docstring_tokens": "keep replace keep keep keep keep keep keep keep keep replace replace keep keep keep", "code_tokens": " <mask> \n <mask>   /// Cancels all requests (Push/Pull) associated with the given ObjectID.\n <mask>   ///\n <mask>   /// \\param object_id The ObjectID.\n <mask>   /// \\return Status of whether requests were successfully cancelled.\n <mask>   ray::Status Cancel(const ObjectID &object_id);\n <mask> \n <mask>   /// Cancels all requests (Push/Pull) associated with the given ObjectID.\n <mask>   ///\n <mask>   /// \\param object_id The ObjectID.\n <mask>   /// \\return Status of whether requests were successfully cancelled.\n <mask>   ray::Status Cancel(const ObjectID &object_id);\n <mask> \n <mask>   /// Callback definition for wait.\n <mask>   using WaitCallback = std::function<void(const std::vector<ray::ObjectID> &found,\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   /// Discover ClientID via ObjectDirectory, then pull object\n  /// from ClientID associated with ObjectID.\n </s> add   /// Try to Pull an object from one of its expected client locations. If there\n  /// are more client locations to try after this attempt, then this method\n  /// will try each of the other clients in succession, with a timeout between\n  /// each attempt. If the object is received or if the Pull is Canceled before\n  /// the timeout, then no more Pull requests for this object will be sent\n  /// to other node managers until TryPull is called again. </s> remove   /// Pull an object from ClientID. Returns UniqueID asociated with\n  /// an invocation of this method.\n </s> add   /// Pull an object from ClientID. </s> remove   /// The callback will be invoked whenever locations are obtained for the\n  /// specified object. The callback provided to this method may fire immediately,\n  /// within the call to this method, if any other listener is subscribed to the same\n  /// object: This occurs when location data for the object has already been obtained.\n  ///\n </s> add   /// The callback will be invoked with the complete list of known locations\n  /// whenever the set of locations changes. The callback will also be fired if\n  /// the list of known locations is empty. The callback provided to this\n  /// method may fire immediately, within the call to this method, if any other\n  /// listener is subscribed to the same object: This occurs when location data\n  /// for the object has already been obtained.\n  // </s> remove   /// \\param client_id The remote node's client id.\n </s> add  </s> remove   /// Register object add with directory.\n  void NotifyDirectoryObjectAdd(const ObjectInfoT &object_info);\n </s> add   /// Handle an object being added to this node. This adds the object to the\n  /// directory, pushes the object to other nodes if necessary, and cancels any\n  /// outstanding Pull requests for the object.\n  void HandleObjectAdded(const ObjectInfoT &object_info);", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.h"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>  private:\n <mask>   friend class TestObjectManager;\n <mask> \n <mask>   ClientID client_id_;\n <mask>   const ObjectManagerConfig config_;\n <mask>   std::unique_ptr<ObjectDirectoryInterface> object_directory_;\n <mask>   ObjectStoreNotificationManager store_notification_;\n <mask>   ObjectBufferPool buffer_pool_;\n <mask> \n <mask>   /// This runs on a thread pool dedicated to sending objects.\n <mask>   boost::asio::io_service send_service_;\n <mask>   /// This runs on a thread pool dedicated to receiving objects.\n <mask>   boost::asio::io_service receive_service_;\n <mask> \n <mask>   /// Weak reference to main service. We ensure this object is destroyed before\n <mask>   /// main_service_ is stopped.\n <mask>   boost::asio::io_service *main_service_;\n <mask> \n <mask>   /// Used to create \"work\" for send_service_.\n <mask>   /// Without this, if send_service_ has no more sends to process, it will stop.\n <mask>   boost::asio::io_service::work send_work_;\n <mask>   /// Used to create \"work\" for receive_service_.\n <mask>   /// Without this, if receive_service_ has no more receives to process, it will stop.\n <mask>   boost::asio::io_service::work receive_work_;\n <mask> \n <mask>   /// Runs the send service, which handle\n <mask>   /// all outgoing object transfers.\n <mask>   std::vector<std::thread> send_threads_;\n <mask>   /// Runs the receive service, which handle\n <mask>   /// all incoming object transfers.\n <mask>   std::vector<std::thread> receive_threads_;\n <mask> \n <mask>   /// Connection pool for reusing outgoing connections to remote object managers.\n <mask>   ConnectionPool connection_pool_;\n <mask> \n <mask>   /// Cache of locally available objects.\n <mask>   std::unordered_map<ObjectID, ObjectInfoT> local_objects_;\n <mask> \n <mask>   /// This is used as the callback identifier in Pull for\n <mask>   /// SubscribeObjectLocations. We only need one identifier because we never need to\n <mask>   /// subscribe multiple times to the same object during Pull.\n <mask>   UniqueID object_directory_pull_callback_id_ = UniqueID::from_random();\n <mask> \n <mask>   struct WaitState {\n <mask>     WaitState(asio::io_service &service, int64_t timeout_ms, const WaitCallback &callback)\n <mask>         : timeout_ms(timeout_ms),\n <mask>           timeout_timer(std::unique_ptr<boost::asio::deadline_timer>(\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   /// Discover ClientID via ObjectDirectory, then pull object\n  /// from ClientID associated with ObjectID.\n </s> add   /// Try to Pull an object from one of its expected client locations. If there\n  /// are more client locations to try after this attempt, then this method\n  /// will try each of the other clients in succession, with a timeout between\n  /// each attempt. If the object is received or if the Pull is Canceled before\n  /// the timeout, then no more Pull requests for this object will be sent\n  /// to other node managers until TryPull is called again. </s> remove   /// The callback will be invoked whenever locations are obtained for the\n  /// specified object. The callback provided to this method may fire immediately,\n  /// within the call to this method, if any other listener is subscribed to the same\n  /// object: This occurs when location data for the object has already been obtained.\n  ///\n </s> add   /// The callback will be invoked with the complete list of known locations\n  /// whenever the set of locations changes. The callback will also be fired if\n  /// the list of known locations is empty. The callback provided to this\n  /// method may fire immediately, within the call to this method, if any other\n  /// listener is subscribed to the same object: This occurs when location data\n  /// for the object has already been obtained.\n  // </s> remove         RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n            object_directory_pull_callback_id_, object_id));\n        GetLocationsSuccess(client_ids, object_id);\n </s> add         // Exit if the Pull request has already been fulfilled or canceled.\n        auto it = pull_requests_.find(object_id);\n        if (it == pull_requests_.end()) {\n          return;\n        }\n        // Reset the list of clients that are now expected to have the object.\n        // NOTE(swang): Since we are overwriting the previous list of clients,\n        // we may end up sending a duplicate request to the same client as\n        // before.\n        it->second.client_locations = client_ids;\n        if (it->second.client_locations.empty()) {\n          // The object locations are now empty, so we should wait for the next\n          // notification about a new object location.  Cancel the timer until\n          // the next Pull attempt since there are no more clients to try.\n          if (it->second.retry_timer != nullptr) {\n            it->second.retry_timer->cancel();\n            it->second.timer_set = false;\n          }\n        } else {\n          // New object locations were found.\n          if (!it->second.timer_set) {\n            // The timer was not set, which means that we weren't trying any\n            // clients. We now have some clients to try, so begin trying to\n            // Pull from one.  If we fail to receive an object within the pull\n            // timeout, then this will try the rest of the clients in the list\n            // in succession.\n            TryPull(object_id);\n          }\n        } </s> remove   /// A set of active wait requests.\n  std::unordered_map<UniqueID, WaitState> active_wait_requests_;\n\n </s> add  </s> remove   /// Handle any push requests that were made before an object was available.\n  /// This is invoked when an \"object added\" notification is received from the store.\n  void HandleUnfulfilledPushRequests(const ObjectInfoT &object_info);\n\n </s> add  </s> remove   /// Register object add with directory.\n  void NotifyDirectoryObjectAdd(const ObjectInfoT &object_info);\n </s> add   /// Handle an object being added to this node. This adds the object to the\n  /// directory, pushes the object to other nodes if necessary, and cancels any\n  /// outstanding Pull requests for the object.\n  void HandleObjectAdded(const ObjectInfoT &object_info);", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.h"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     /// The number of required objects.\n <mask>     uint64_t num_required_objects;\n <mask>   };\n <mask> \n <mask>   /// A set of active wait requests.\n <mask>   std::unordered_map<UniqueID, WaitState> active_wait_requests_;\n <mask> \n <mask>   /// Creates a wait request and adds it to active_wait_requests_.\n <mask>   ray::Status AddWaitRequest(const UniqueID &wait_id,\n <mask>                              const std::vector<ObjectID> &object_ids, int64_t timeout_ms,\n <mask>                              uint64_t num_required_objects, bool wait_local,\n <mask>                              const WaitCallback &callback);\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove ray::Status ObjectManager::Cancel(const ObjectID &object_id) {\n  ray::Status status = object_directory_->UnsubscribeObjectLocations(\n      object_directory_pull_callback_id_, object_id);\n  return status;\n </s> add void ObjectManager::CancelPull(const ObjectID &object_id) {\n  auto it = pull_requests_.find(object_id);\n  if (it == pull_requests_.end()) {\n    return;\n  }\n\n  RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n      object_directory_pull_callback_id_, object_id));\n  pull_requests_.erase(it); </s> remove   ClientID client_id_;\n  const ObjectManagerConfig config_;\n  std::unique_ptr<ObjectDirectoryInterface> object_directory_;\n  ObjectStoreNotificationManager store_notification_;\n  ObjectBufferPool buffer_pool_;\n\n  /// This runs on a thread pool dedicated to sending objects.\n  boost::asio::io_service send_service_;\n  /// This runs on a thread pool dedicated to receiving objects.\n  boost::asio::io_service receive_service_;\n\n  /// Weak reference to main service. We ensure this object is destroyed before\n  /// main_service_ is stopped.\n  boost::asio::io_service *main_service_;\n\n  /// Used to create \"work\" for send_service_.\n  /// Without this, if send_service_ has no more sends to process, it will stop.\n  boost::asio::io_service::work send_work_;\n  /// Used to create \"work\" for receive_service_.\n  /// Without this, if receive_service_ has no more receives to process, it will stop.\n  boost::asio::io_service::work receive_work_;\n\n  /// Runs the send service, which handle\n  /// all outgoing object transfers.\n  std::vector<std::thread> send_threads_;\n  /// Runs the receive service, which handle\n  /// all incoming object transfers.\n  std::vector<std::thread> receive_threads_;\n\n  /// Connection pool for reusing outgoing connections to remote object managers.\n  ConnectionPool connection_pool_;\n\n  /// Cache of locally available objects.\n  std::unordered_map<ObjectID, ObjectInfoT> local_objects_;\n\n  /// This is used as the callback identifier in Pull for\n  /// SubscribeObjectLocations. We only need one identifier because we never need to\n  /// subscribe multiple times to the same object during Pull.\n  UniqueID object_directory_pull_callback_id_ = UniqueID::from_random();\n </s> add   struct PullRequest {\n    PullRequest() : retry_timer(nullptr), timer_set(false), client_locations() {}\n    std::unique_ptr<boost::asio::deadline_timer> retry_timer;\n    bool timer_set;\n    std::vector<ClientID> client_locations;\n  }; </s> remove   /// Cancels all requests (Push/Pull) associated with the given ObjectID.\n </s> add   /// Cancels all requests (Push/Pull) associated with the given ObjectID. This\n  /// method is idempotent. </s> remove   /// The callback will be invoked whenever locations are obtained for the\n  /// specified object. The callback provided to this method may fire immediately,\n  /// within the call to this method, if any other listener is subscribed to the same\n  /// object: This occurs when location data for the object has already been obtained.\n  ///\n </s> add   /// The callback will be invoked with the complete list of known locations\n  /// whenever the set of locations changes. The callback will also be fired if\n  /// the list of known locations is empty. The callback provided to this\n  /// method may fire immediately, within the call to this method, if any other\n  /// listener is subscribed to the same object: This occurs when location data\n  /// for the object has already been obtained.\n  // </s> remove   /// Pull an object from ClientID. Returns UniqueID asociated with\n  /// an invocation of this method.\n </s> add   /// Pull an object from ClientID. </s> remove   /// Maintains a map of push requests that have not been fulfilled due to an object not\n  /// being local. Objects are removed from this map after push_timeout_ms have elapsed.\n  std::unordered_map<\n      ObjectID,\n      std::unordered_map<ClientID, std::unique_ptr<boost::asio::deadline_timer>>>\n      unfulfilled_push_requests_;\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.h"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>   void SubscribeRemainingWaitObjects(const UniqueID &wait_id);\n <mask>   /// Completion handler for Wait.\n <mask>   void WaitComplete(const UniqueID &wait_id);\n <mask> \n <mask>   /// Maintains a map of push requests that have not been fulfilled due to an object not\n <mask>   /// being local. Objects are removed from this map after push_timeout_ms have elapsed.\n <mask>   std::unordered_map<\n <mask>       ObjectID,\n <mask>       std::unordered_map<ClientID, std::unique_ptr<boost::asio::deadline_timer>>>\n <mask>       unfulfilled_push_requests_;\n <mask> \n <mask>   /// Handle starting, running, and stopping asio io_service.\n <mask>   void StartIOService();\n <mask>   void RunSendService();\n <mask>   void RunReceiveService();\n <mask>   void StopIOService();\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   /// Register object add with directory.\n  void NotifyDirectoryObjectAdd(const ObjectInfoT &object_info);\n </s> add   /// Handle an object being added to this node. This adds the object to the\n  /// directory, pushes the object to other nodes if necessary, and cancels any\n  /// outstanding Pull requests for the object.\n  void HandleObjectAdded(const ObjectInfoT &object_info); </s> remove   /// Handle any push requests that were made before an object was available.\n  /// This is invoked when an \"object added\" notification is received from the store.\n  void HandleUnfulfilledPushRequests(const ObjectInfoT &object_info);\n\n </s> add  </s> remove }\n </s> add  </s> remove void ObjectManager::HandleUnfulfilledPushRequests(const ObjectInfoT &object_info) {\n  ObjectID object_id = ObjectID::from_binary(object_info.object_id);\n </s> add  </s> remove   /// Pull an object from ClientID. Returns UniqueID asociated with\n  /// an invocation of this method.\n </s> add   /// Pull an object from ClientID. </s> remove         RAY_CHECK_OK(object_directory_->UnsubscribeObjectLocations(\n            object_directory_pull_callback_id_, object_id));\n        GetLocationsSuccess(client_ids, object_id);\n </s> add         // Exit if the Pull request has already been fulfilled or canceled.\n        auto it = pull_requests_.find(object_id);\n        if (it == pull_requests_.end()) {\n          return;\n        }\n        // Reset the list of clients that are now expected to have the object.\n        // NOTE(swang): Since we are overwriting the previous list of clients,\n        // we may end up sending a duplicate request to the same client as\n        // before.\n        it->second.client_locations = client_ids;\n        if (it->second.client_locations.empty()) {\n          // The object locations are now empty, so we should wait for the next\n          // notification about a new object location.  Cancel the timer until\n          // the next Pull attempt since there are no more clients to try.\n          if (it->second.retry_timer != nullptr) {\n            it->second.retry_timer->cancel();\n            it->second.timer_set = false;\n          }\n        } else {\n          // New object locations were found.\n          if (!it->second.timer_set) {\n            // The timer was not set, which means that we weren't trying any\n            // clients. We now have some clients to try, so begin trying to\n            // Pull from one.  If we fail to receive an object within the pull\n            // timeout, then this will try the rest of the clients in the list\n            // in succession.\n            TryPull(object_id);\n          }\n        }", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.h"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>   void RunSendService();\n <mask>   void RunReceiveService();\n <mask>   void StopIOService();\n <mask> \n <mask>   /// Register object add with directory.\n <mask>   void NotifyDirectoryObjectAdd(const ObjectInfoT &object_info);\n <mask> \n <mask>   /// Register object remove with directory.\n <mask>   void NotifyDirectoryObjectDeleted(const ObjectID &object_id);\n <mask> \n <mask>   /// Handle any push requests that were made before an object was available.\n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   /// Handle any push requests that were made before an object was available.\n  /// This is invoked when an \"object added\" notification is received from the store.\n  void HandleUnfulfilledPushRequests(const ObjectInfoT &object_info);\n\n </s> add  </s> remove   /// Maintains a map of push requests that have not been fulfilled due to an object not\n  /// being local. Objects are removed from this map after push_timeout_ms have elapsed.\n  std::unordered_map<\n      ObjectID,\n      std::unordered_map<ClientID, std::unique_ptr<boost::asio::deadline_timer>>>\n      unfulfilled_push_requests_;\n\n </s> add  </s> remove   /// \\return Status of whether requests were successfully cancelled.\n  ray::Status Cancel(const ObjectID &object_id);\n </s> add   /// \\return Void.\n  void CancelPull(const ObjectID &object_id); </s> remove   /// Pull an object from ClientID. Returns UniqueID asociated with\n  /// an invocation of this method.\n </s> add   /// Pull an object from ClientID. </s> remove   /// Cancels all requests (Push/Pull) associated with the given ObjectID.\n </s> add   /// Cancels all requests (Push/Pull) associated with the given ObjectID. This\n  /// method is idempotent. </s> remove   /// Discover ClientID via ObjectDirectory, then pull object\n  /// from ClientID associated with ObjectID.\n </s> add   /// Try to Pull an object from one of its expected client locations. If there\n  /// are more client locations to try after this attempt, then this method\n  /// will try each of the other clients in succession, with a timeout between\n  /// each attempt. If the object is received or if the Pull is Canceled before\n  /// the timeout, then no more Pull requests for this object will be sent\n  /// to other node managers until TryPull is called again.", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.h"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>   /// Register object remove with directory.\n <mask>   void NotifyDirectoryObjectDeleted(const ObjectID &object_id);\n <mask> \n <mask>   /// Handle any push requests that were made before an object was available.\n <mask>   /// This is invoked when an \"object added\" notification is received from the store.\n <mask>   void HandleUnfulfilledPushRequests(const ObjectInfoT &object_info);\n <mask> \n <mask>   /// Part of an asynchronous sequence of Pull methods.\n <mask>   /// Uses an existing connection or creates a connection to ClientID.\n <mask>   /// Executes on main_service_ thread.\n <mask>   void PullEstablishConnection(const ObjectID &object_id, const ClientID &client_id);\n <mask> \n </s> [xray] Object manager retries Pull requests (#2630)\n\n* Move all ObjectManager members to bottom of class def\r\n\r\n* Better Pull requests\r\n- suppress duplicate Pulls\r\n- retry the Pull at the next client after a timeout\r\n- cancel a Pull if the object no longer appears on any clients\r\n\r\n* increase object manager Pull timeout\r\n\r\n* Make the component failure test harder.\r\n\r\n* note\r\n\r\n* Notify SubscribeObjectLocations caller of empty list\r\n\r\n* Address melih's comments\r\n\r\n* Fix wait...\r\n\r\n* Make component failure test easier for legacy ray\r\n\r\n* lint </s> remove   /// Register object add with directory.\n  void NotifyDirectoryObjectAdd(const ObjectInfoT &object_info);\n </s> add   /// Handle an object being added to this node. This adds the object to the\n  /// directory, pushes the object to other nodes if necessary, and cancels any\n  /// outstanding Pull requests for the object.\n  void HandleObjectAdded(const ObjectInfoT &object_info); </s> remove   void Pull(const ObjectID &object_id, const ClientID &client_id);\n </s> add   void TryPull(const ObjectID &object_id); </s> remove   /// Pull an object from ClientID. Returns UniqueID asociated with\n  /// an invocation of this method.\n </s> add   /// Pull an object from ClientID. </s> remove   /// \\param client_id The remote node's client id.\n </s> add  </s> remove   /// Discover ClientID via ObjectDirectory, then pull object\n  /// from ClientID associated with ObjectID.\n </s> add   /// Try to Pull an object from one of its expected client locations. If there\n  /// are more client locations to try after this attempt, then this method\n  /// will try each of the other clients in succession, with a timeout between\n  /// each attempt. If the object is received or if the Pull is Canceled before\n  /// the timeout, then no more Pull requests for this object will be sent\n  /// to other node managers until TryPull is called again. </s> remove   /// The callback will be invoked whenever locations are obtained for the\n  /// specified object. The callback provided to this method may fire immediately,\n  /// within the call to this method, if any other listener is subscribed to the same\n  /// object: This occurs when location data for the object has already been obtained.\n  ///\n </s> add   /// The callback will be invoked with the complete list of known locations\n  /// whenever the set of locations changes. The callback will also be fired if\n  /// the list of known locations is empty. The callback provided to this\n  /// method may fire immediately, within the call to this method, if any other\n  /// listener is subscribed to the same object: This occurs when location data\n  /// for the object has already been obtained.\n  //", "html_url": "https://github.com/ray-project/ray/commit/806fdf2f051c3ce694b227cb8ae3b540fb59c701", "file_name": "src/ray/object_manager/object_manager.h"}
{"docstring_tokens": "keep keep replace keep replace replace replace keep keep", "code_tokens": " <mask>         config[\"ignore_worker_failures\"] = True\n <mask>         # Make worker idx=1 fail. Other workers will be ok.\n <mask>         config[\"env_config\"] = {\"bad_indices\": [1]}\n <mask>         if fail_eval:\n <mask>             config[\"evaluation_num_workers\"] = 2\n <mask>             config[\"evaluation_interval\"] = 1\n <mask>             config[\"evaluation_config\"] = {\n <mask>                 \"ignore_worker_failures\": True,\n <mask>                 \"env_config\": {\n </s> [RLlib] Sync policy specs from local_worker_for_synching while recovering rollout/eval workers. (#28422) </s> remove             config[\"evaluation_num_workers\"] = 2\n            config[\"evaluation_interval\"] = 1\n            config[\"evaluation_config\"] = {\n </s> add             config.evaluation_num_workers = 2\n            config.evaluation_interval = 1\n            config.evaluation_config = { </s> remove         config[\"env_config\"] = {\"bad_indices\": [1, 2]}\n </s> add         config.env_config = {\"bad_indices\": [1, 2]} </s> remove         config[\"num_workers\"] = 1\n        config[\"evaluation_num_workers\"] = 1\n        config[\"evaluation_interval\"] = 1\n        config[\"evaluation_config\"] = {\n </s> add         config.num_workers = 1\n        config.evaluation_num_workers = 1\n        config.evaluation_interval = 1\n        config.env = \"fault_env\"\n        config.evaluation_config = { </s> remove         config[\"num_workers\"] = 2\n        config[\"ignore_worker_failures\"] = False\n </s> add         config.num_workers = 2\n        config.ignore_worker_failures = False\n        config.env = \"fault_env\" </s> remove     def _do_test_fault_fatal(self, alg, config, fail_eval=False):\n        agent_cls = get_algorithm_class(alg)\n\n </s> add     def _do_test_fault_fatal(self, config, fail_eval=False):", "html_url": "https://github.com/ray-project/ray/commit/80c8032f00facea7702de3513ee905ed33962eeb", "file_name": "rllib/tests/test_worker_failures.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                 },\n <mask>             }\n <mask> \n <mask>         for _ in framework_iterator(config, frameworks=(\"tf2\", \"torch\")):\n <mask>             algo = algo_cls(config=config, env=\"fault_env\")\n <mask>             result = algo.train()\n <mask> \n <mask>             # Both rollout workers are healthy.\n <mask>             self.assertTrue(result[\"num_healthy_workers\"] == 1)\n <mask>             if fail_eval:\n </s> [RLlib] Sync policy specs from local_worker_for_synching while recovering rollout/eval workers. (#28422) </s> remove             a = agent_cls(config=config, env=\"fault_env\")\n </s> add             a = config.build() </s> remove             a = agent_cls(config=config, env=\"fault_env\")\n </s> add             a = config.build() </s> remove     def _do_test_fault_fatal(self, alg, config, fail_eval=False):\n        agent_cls = get_algorithm_class(alg)\n\n </s> add     def _do_test_fault_fatal(self, config, fail_eval=False): </s> remove             config[\"evaluation_num_workers\"] = 2\n            config[\"evaluation_interval\"] = 1\n            config[\"evaluation_config\"] = {\n </s> add             config.evaluation_num_workers = 2\n            config.evaluation_interval = 1\n            config.evaluation_config = { </s> remove         config[\"num_workers\"] = 1\n        config[\"evaluation_num_workers\"] = 1\n        config[\"evaluation_interval\"] = 1\n        config[\"evaluation_config\"] = {\n </s> add         config.num_workers = 1\n        config.evaluation_num_workers = 1\n        config.evaluation_interval = 1\n        config.env = \"fault_env\"\n        config.evaluation_config = { </s> remove         config[\"env_config\"] = {\"bad_indices\": [1]}\n </s> add         config.env_config = {\"bad_indices\": [1]}", "html_url": "https://github.com/ray-project/ray/commit/80c8032f00facea7702de3513ee905ed33962eeb", "file_name": "rllib/tests/test_worker_failures.py"}
{"docstring_tokens": "keep replace replace replace keep replace replace keep keep", "code_tokens": " <mask> \n <mask>     def _do_test_fault_fatal(self, alg, config, fail_eval=False):\n <mask>         agent_cls = get_algorithm_class(alg)\n <mask> \n <mask>         # Test raises real error when out of workers.\n <mask>         config[\"num_workers\"] = 2\n <mask>         config[\"ignore_worker_failures\"] = False\n <mask>         # Make both worker idx=1 and 2 fail.\n <mask>         config[\"env_config\"] = {\"bad_indices\": [1, 2]}\n </s> [RLlib] Sync policy specs from local_worker_for_synching while recovering rollout/eval workers. (#28422) </s> remove         config[\"env_config\"] = {\"bad_indices\": [1, 2]}\n </s> add         config.env_config = {\"bad_indices\": [1, 2]} </s> remove     def _do_test_fault_fatal_but_recreate(self, alg, config):\n        register_env(\"fault_env\", lambda c: FaultInjectEnv(c))\n        agent_cls = get_algorithm_class(alg)\n\n </s> add     def _do_test_fault_fatal_but_recreate(self, config): </s> remove             config[\"evaluation_num_workers\"] = 2\n            config[\"evaluation_interval\"] = 1\n            config[\"evaluation_config\"] = {\n </s> add             config.evaluation_num_workers = 2\n            config.evaluation_interval = 1\n            config.evaluation_config = { </s> remove         config[\"num_workers\"] = 1\n        config[\"evaluation_num_workers\"] = 1\n        config[\"evaluation_interval\"] = 1\n        config[\"evaluation_config\"] = {\n </s> add         config.num_workers = 1\n        config.evaluation_num_workers = 1\n        config.evaluation_interval = 1\n        config.env = \"fault_env\"\n        config.evaluation_config = { </s> remove         config[\"env_config\"] = {\"bad_indices\": [1]}\n </s> add         config.env_config = {\"bad_indices\": [1]}", "html_url": "https://github.com/ray-project/ray/commit/80c8032f00facea7702de3513ee905ed33962eeb", "file_name": "rllib/tests/test_worker_failures.py"}
{"docstring_tokens": "keep replace keep replace replace replace", "code_tokens": " <mask>         # Make both worker idx=1 and 2 fail.\n <mask>         config[\"env_config\"] = {\"bad_indices\": [1, 2]}\n <mask>         if fail_eval:\n <mask>             config[\"evaluation_num_workers\"] = 2\n <mask>             config[\"evaluation_interval\"] = 1\n <mask>             config[\"evaluation_config\"] = {\n </s> [RLlib] Sync policy specs from local_worker_for_synching while recovering rollout/eval workers. (#28422) </s> remove         config[\"num_workers\"] = 2\n        config[\"ignore_worker_failures\"] = False\n </s> add         config.num_workers = 2\n        config.ignore_worker_failures = False\n        config.env = \"fault_env\" </s> remove     def _do_test_fault_fatal(self, alg, config, fail_eval=False):\n        agent_cls = get_algorithm_class(alg)\n\n </s> add     def _do_test_fault_fatal(self, config, fail_eval=False): </s> remove         config[\"env_config\"] = {\"bad_indices\": [1]}\n </s> add         config.env_config = {\"bad_indices\": [1]} </s> remove             config[\"evaluation_num_workers\"] = 2\n            config[\"evaluation_interval\"] = 1\n            config[\"evaluation_config\"] = {\n </s> add             config.evaluation_num_workers = 2\n            config.evaluation_interval = 1\n            config.evaluation_config = { </s> remove         config[\"num_workers\"] = 1\n        config[\"evaluation_num_workers\"] = 1\n        config[\"evaluation_interval\"] = 1\n        config[\"evaluation_config\"] = {\n </s> add         config.num_workers = 1\n        config.evaluation_num_workers = 1\n        config.evaluation_interval = 1\n        config.env = \"fault_env\"\n        config.evaluation_config = {", "html_url": "https://github.com/ray-project/ray/commit/80c8032f00facea7702de3513ee905ed33962eeb", "file_name": "rllib/tests/test_worker_failures.py"}
{"docstring_tokens": "keep keep keep replace keep keep keep replace replace replace replace keep", "code_tokens": " <mask>             }\n <mask> \n <mask>         for _ in framework_iterator(config, frameworks=(\"torch\", \"tf\")):\n <mask>             a = agent_cls(config=config, env=\"fault_env\")\n <mask>             self.assertRaises(Exception, lambda: a.train())\n <mask>             a.stop()\n <mask> \n <mask>     def _do_test_fault_fatal_but_recreate(self, alg, config):\n <mask>         register_env(\"fault_env\", lambda c: FaultInjectEnv(c))\n <mask>         agent_cls = get_algorithm_class(alg)\n <mask> \n <mask>         # Test raises real error when out of workers.\n </s> [RLlib] Sync policy specs from local_worker_for_synching while recovering rollout/eval workers. (#28422) </s> remove             a = agent_cls(config=config, env=\"fault_env\")\n </s> add             a = config.build() </s> remove         config[\"num_workers\"] = 1\n        config[\"evaluation_num_workers\"] = 1\n        config[\"evaluation_interval\"] = 1\n        config[\"evaluation_config\"] = {\n </s> add         config.num_workers = 1\n        config.evaluation_num_workers = 1\n        config.evaluation_interval = 1\n        config.env = \"fault_env\"\n        config.evaluation_config = { </s> remove             algo = algo_cls(config=config, env=\"fault_env\")\n </s> add             algo = config.build() </s> remove     def _do_test_fault_fatal(self, alg, config, fail_eval=False):\n        agent_cls = get_algorithm_class(alg)\n\n </s> add     def _do_test_fault_fatal(self, config, fail_eval=False): </s> remove         config[\"num_workers\"] = 2\n        config[\"ignore_worker_failures\"] = False\n </s> add         config.num_workers = 2\n        config.ignore_worker_failures = False\n        config.env = \"fault_env\"", "html_url": "https://github.com/ray-project/ray/commit/80c8032f00facea7702de3513ee905ed33962eeb", "file_name": "rllib/tests/test_worker_failures.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         register_env(\"fault_env\", lambda c: FaultInjectEnv(c))\n <mask>         agent_cls = get_algorithm_class(alg)\n <mask> \n <mask>         # Test raises real error when out of workers.\n <mask>         config[\"num_workers\"] = 1\n <mask>         config[\"evaluation_num_workers\"] = 1\n <mask>         config[\"evaluation_interval\"] = 1\n <mask>         config[\"evaluation_config\"] = {\n <mask>             \"recreate_failed_workers\": True,\n <mask>             # Make eval worker (index 1) fail.\n <mask>             \"env_config\": {\n <mask>                 \"bad_indices\": [1],\n <mask>             },\n </s> [RLlib] Sync policy specs from local_worker_for_synching while recovering rollout/eval workers. (#28422) </s> remove     def _do_test_fault_fatal_but_recreate(self, alg, config):\n        register_env(\"fault_env\", lambda c: FaultInjectEnv(c))\n        agent_cls = get_algorithm_class(alg)\n\n </s> add     def _do_test_fault_fatal_but_recreate(self, config): </s> remove             config[\"evaluation_num_workers\"] = 2\n            config[\"evaluation_interval\"] = 1\n            config[\"evaluation_config\"] = {\n </s> add             config.evaluation_num_workers = 2\n            config.evaluation_interval = 1\n            config.evaluation_config = { </s> remove     def _do_test_fault_fatal(self, alg, config, fail_eval=False):\n        agent_cls = get_algorithm_class(alg)\n\n </s> add     def _do_test_fault_fatal(self, config, fail_eval=False): </s> remove         config[\"num_workers\"] = 2\n        config[\"ignore_worker_failures\"] = False\n </s> add         config.num_workers = 2\n        config.ignore_worker_failures = False\n        config.env = \"fault_env\" </s> remove             config[\"evaluation_num_workers\"] = 2\n            config[\"evaluation_interval\"] = 1\n            config[\"evaluation_config\"] = {\n </s> add             config.evaluation_num_workers = 2\n            config.evaluation_interval = 1\n            config.evaluation_config = { </s> remove         config[\"env_config\"] = {\"bad_indices\": [1, 2]}\n </s> add         config.env_config = {\"bad_indices\": [1, 2]}", "html_url": "https://github.com/ray-project/ray/commit/80c8032f00facea7702de3513ee905ed33962eeb", "file_name": "rllib/tests/test_worker_failures.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             },\n <mask>         }\n <mask> \n <mask>         for _ in framework_iterator(config, frameworks=(\"tf\", \"tf2\", \"torch\")):\n <mask>             a = agent_cls(config=config, env=\"fault_env\")\n <mask>             # Expect this to go well and all faulty workers are recovered.\n <mask>             self.assertTrue(\n <mask>                 not any(\n <mask>                     ray.get(\n <mask>                         worker.apply.remote(\n </s> [RLlib] Sync policy specs from local_worker_for_synching while recovering rollout/eval workers. (#28422) </s> remove             algo = algo_cls(config=config, env=\"fault_env\")\n </s> add             algo = config.build() </s> remove             a = agent_cls(config=config, env=\"fault_env\")\n </s> add             a = config.build() </s> remove     def _do_test_fault_fatal_but_recreate(self, alg, config):\n        register_env(\"fault_env\", lambda c: FaultInjectEnv(c))\n        agent_cls = get_algorithm_class(alg)\n\n </s> add     def _do_test_fault_fatal_but_recreate(self, config): </s> remove         self._do_test_fault_fatal(\"PG\", {\"optimizer\": {}})\n </s> add         self._do_test_fault_fatal(PGConfig().training(optimizer={})) </s> remove         self._do_test_fault_ignore(\"A3C\", {\"optimizer\": {\"grads_per_step\": 1}})\n </s> add         self._do_test_fault_ignore(\n            A3CConfig().training(optimizer={\"grads_per_step\": 1})\n        ) </s> remove         config[\"env_config\"] = {\"bad_indices\": [1, 2]}\n </s> add         config.env_config = {\"bad_indices\": [1, 2]}", "html_url": "https://github.com/ray-project/ray/commit/80c8032f00facea7702de3513ee905ed33962eeb", "file_name": "rllib/tests/test_worker_failures.py"}
{"docstring_tokens": "keep keep keep replace keep keep replace keep", "code_tokens": " <mask> \n <mask>     def test_fatal(self):\n <mask>         # Test the case where all workers fail (w/o recovery).\n <mask>         self._do_test_fault_fatal(\"PG\", {\"optimizer\": {}})\n <mask> \n <mask>     def test_async_grads(self):\n <mask>         self._do_test_fault_ignore(\"A3C\", {\"optimizer\": {\"grads_per_step\": 1}})\n <mask> \n </s> [RLlib] Sync policy specs from local_worker_for_synching while recovering rollout/eval workers. (#28422) </s> remove         self._do_test_fault_ignore(\n            \"APEX\",\n            {\n                \"num_gpus\": 0,\n                \"min_sample_timesteps_per_iteration\": 1000,\n                \"min_time_s_per_iteration\": 1,\n                \"explore\": False,\n                \"num_steps_sampled_before_learning_starts\": 1000,\n                \"target_network_update_freq\": 100,\n                \"optimizer\": {\n </s> add         config = (\n            ApexDQNConfig()\n            .training(\n                optimizer={ </s> remove             a = agent_cls(config=config, env=\"fault_env\")\n </s> add             a = config.build() </s> remove         config[\"env_config\"] = {\"bad_indices\": [1]}\n </s> add         config.env_config = {\"bad_indices\": [1]} </s> remove     def _do_test_fault_fatal(self, alg, config, fail_eval=False):\n        agent_cls = get_algorithm_class(alg)\n\n </s> add     def _do_test_fault_fatal(self, config, fail_eval=False): </s> remove         config[\"env_config\"] = {\"bad_indices\": [1, 2]}\n </s> add         config.env_config = {\"bad_indices\": [1, 2]}", "html_url": "https://github.com/ray-project/ray/commit/80c8032f00facea7702de3513ee905ed33962eeb", "file_name": "rllib/tests/test_worker_failures.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     def test_async_grads(self):\n <mask>         self._do_test_fault_ignore(\"A3C\", {\"optimizer\": {\"grads_per_step\": 1}})\n <mask> \n <mask>     def test_async_replay(self):\n <mask>         self._do_test_fault_ignore(\n <mask>             \"APEX\",\n <mask>             {\n <mask>                 \"num_gpus\": 0,\n <mask>                 \"min_sample_timesteps_per_iteration\": 1000,\n <mask>                 \"min_time_s_per_iteration\": 1,\n <mask>                 \"explore\": False,\n <mask>                 \"num_steps_sampled_before_learning_starts\": 1000,\n <mask>                 \"target_network_update_freq\": 100,\n <mask>                 \"optimizer\": {\n <mask>                     \"num_replay_buffer_shards\": 1,\n <mask>                 },\n <mask>             },\n <mask>         )\n <mask> \n </s> [RLlib] Sync policy specs from local_worker_for_synching while recovering rollout/eval workers. (#28422) </s> remove         self._do_test_fault_ignore(\"A3C\", {\"optimizer\": {\"grads_per_step\": 1}})\n </s> add         self._do_test_fault_ignore(\n            A3CConfig().training(optimizer={\"grads_per_step\": 1})\n        ) </s> remove         self._do_test_fault_fatal(\"PG\", {\"optimizer\": {}})\n </s> add         self._do_test_fault_fatal(PGConfig().training(optimizer={})) </s> remove             config[\"evaluation_num_workers\"] = 2\n            config[\"evaluation_interval\"] = 1\n            config[\"evaluation_config\"] = {\n </s> add             config.evaluation_num_workers = 2\n            config.evaluation_interval = 1\n            config.evaluation_config = { </s> remove         config[\"num_workers\"] = 1\n        config[\"evaluation_num_workers\"] = 1\n        config[\"evaluation_interval\"] = 1\n        config[\"evaluation_config\"] = {\n </s> add         config.num_workers = 1\n        config.evaluation_num_workers = 1\n        config.evaluation_interval = 1\n        config.env = \"fault_env\"\n        config.evaluation_config = { </s> remove         config[\"env_config\"] = {\"bad_indices\": [1, 2]}\n </s> add         config.env_config = {\"bad_indices\": [1, 2]} </s> remove             a = agent_cls(config=config, env=\"fault_env\")\n </s> add             a = config.build()", "html_url": "https://github.com/ray-project/ray/commit/80c8032f00facea7702de3513ee905ed33962eeb", "file_name": "rllib/tests/test_worker_failures.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         raise TypeError(f\"Remote function cannot be called directly. \"\n <mask>                         \"Use {self._name}.remote method instead\")\n <mask> \n <mask>     def remote(self, *args, **kwargs):\n <mask>         return ClientObjectRef(ray.call_remote(self, *args, **kwargs))\n <mask> \n <mask>     def __repr__(self):\n <mask>         return \"ClientRemoteFunc(%s, %s)\" % (self._name, self._ref)\n <mask> \n <mask>     def _ensure_ref(self):\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove         return ClientObjectRef(ray.call_remote(self, *args, **kwargs))\n </s> add         return return_refs(ray.call_remote(self, *args, **kwargs)) </s> add         self._options = validate_options(options) </s> remove         ref_id = ray.call_remote(self, *args, **kwargs)\n        return ClientActorHandle(ClientActorRef(ref_id), self)\n </s> add         ref_ids = ray.call_remote(self, *args, **kwargs)\n        assert len(ref_ids) == 1\n        return ClientActorHandle(ClientActorRef(ref_ids[0]), self)\n\n    def options(self, **kwargs):\n        return ActorOptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs) </s> remove     def remote(self, function_or_class, *args, **kwargs):\n        # TODO(barakmich): Arguments to ray.remote\n        # get captured here.\n        if (inspect.isfunction(function_or_class)\n                or is_cython(function_or_class)):\n            return ClientRemoteFunc(function_or_class)\n        elif inspect.isclass(function_or_class):\n            return ClientActorClass(function_or_class)\n        else:\n            raise TypeError(\"The @ray.remote decorator must be applied to \"\n                            \"either a function or to a class.\")\n </s> add     def remote(self, *args, **kwargs):\n        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n            # This is the case where the decorator is just @ray.remote.\n            return remote_decorator(options=None)(args[0])\n        error_string = (\"The @ray.remote decorator must be applied either \"\n                        \"with no arguments and no parentheses, for example \"\n                        \"'@ray.remote', or it must be applied using some of \"\n                        \"the arguments 'num_returns', 'num_cpus', 'num_gpus', \"\n                        \"'memory', 'object_store_memory', 'resources', \"\n                        \"'max_calls', or 'max_restarts', like \"\n                        \"'@ray.remote(num_returns=2, \"\n                        \"resources={\\\"CustomResource\\\": 1})'.\")\n        assert len(args) == 0 and len(kwargs) > 0, error_string\n        return remote_decorator(options=kwargs) </s> remove     def call_remote(self, instance, *args, **kwargs) -> bytes:\n </s> add     def call_remote(self, instance, *args, **kwargs) -> List[bytes]: </s> remove         if self._ref is None:\n            # While calling ray.put() on our function, if\n            # our function is recursive, it will attempt to\n            # encode the ClientRemoteFunc -- itself -- and\n            # infinitely recurse on _ensure_ref.\n            #\n            # So we set the state of the reference to be an\n            # in-progress self reference value, which\n            # the encoding can detect and handle correctly.\n            self._ref = SelfReferenceSentinel()\n            self._ref = ray.put(self._func)\n </s> add         with self._lock:\n            if self._ref is None:\n                # While calling ray.put() on our function, if\n                # our function is recursive, it will attempt to\n                # encode the ClientRemoteFunc -- itself -- and\n                # infinitely recurse on _ensure_ref.\n                #\n                # So we set the state of the reference to be an\n                # in-progress self reference value, which\n                # the encoding can detect and handle correctly.\n                self._ref = SelfReferenceSentinel()\n                self._ref = ray.put(self._func)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/common.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     def __repr__(self):\n <mask>         return \"ClientRemoteFunc(%s, %s)\" % (self._name, self._ref)\n <mask> \n <mask>     def _ensure_ref(self):\n <mask>         if self._ref is None:\n <mask>             # While calling ray.put() on our function, if\n <mask>             # our function is recursive, it will attempt to\n <mask>             # encode the ClientRemoteFunc -- itself -- and\n <mask>             # infinitely recurse on _ensure_ref.\n <mask>             #\n <mask>             # So we set the state of the reference to be an\n <mask>             # in-progress self reference value, which\n <mask>             # the encoding can detect and handle correctly.\n <mask>             self._ref = SelfReferenceSentinel()\n <mask>             self._ref = ray.put(self._func)\n <mask> \n <mask>     def _prepare_client_task(self) -> ray_client_pb2.ClientTask:\n <mask>         self._ensure_ref()\n <mask>         task = ray_client_pb2.ClientTask()\n <mask>         task.type = ray_client_pb2.ClientTask.FUNCTION\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\") </s> remove         ref_id = ray.call_remote(self, *args, **kwargs)\n        return ClientActorHandle(ClientActorRef(ref_id), self)\n </s> add         ref_ids = ray.call_remote(self, *args, **kwargs)\n        assert len(ref_ids) == 1\n        return ClientActorHandle(ClientActorRef(ref_ids[0]), self)\n\n    def options(self, **kwargs):\n        return ActorOptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs) </s> remove     def remote(self, function_or_class, *args, **kwargs):\n        # TODO(barakmich): Arguments to ray.remote\n        # get captured here.\n        if (inspect.isfunction(function_or_class)\n                or is_cython(function_or_class)):\n            return ClientRemoteFunc(function_or_class)\n        elif inspect.isclass(function_or_class):\n            return ClientActorClass(function_or_class)\n        else:\n            raise TypeError(\"The @ray.remote decorator must be applied to \"\n                            \"either a function or to a class.\")\n </s> add     def remote(self, *args, **kwargs):\n        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n            # This is the case where the decorator is just @ray.remote.\n            return remote_decorator(options=None)(args[0])\n        error_string = (\"The @ray.remote decorator must be applied either \"\n                        \"with no arguments and no parentheses, for example \"\n                        \"'@ray.remote', or it must be applied using some of \"\n                        \"the arguments 'num_returns', 'num_cpus', 'num_gpus', \"\n                        \"'memory', 'object_store_memory', 'resources', \"\n                        \"'max_calls', or 'max_restarts', like \"\n                        \"'@ray.remote(num_returns=2, \"\n                        \"resources={\\\"CustomResource\\\": 1})'.\")\n        assert len(args) == 0 and len(kwargs) > 0, error_string\n        return remote_decorator(options=kwargs) </s> add         if isinstance(val, ClientObjectRef):\n            raise TypeError(\n                \"Calling 'put' on an ObjectRef is not allowed \"\n                \"(similarly, returning an ObjectRef from a remote \"\n                \"function is not allowed). If you really want to \"\n                \"do this, you can wrap the ObjectRef in a list and \"\n                \"call 'put' on it (or return it).\") </s> remove   // A reference to the returned value from the execution.\n  bytes return_id = 2;\n </s> add   // A reference to the returned values from the execution.\n  repeated bytes return_ids = 2; </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/common.py"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask>         task.name = self._name\n <mask>         task.payload_id = self._ref.id\n <mask>         return task\n <mask> \n <mask> \n <mask> class ClientActorClass(ClientStub):\n <mask>     \"\"\" A stub created on the Ray Client to represent an actor class.\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add         set_task_options(task, self._options, \"baseline_options\") </s> remove   // A reference to the returned value from the execution.\n  bytes return_id = 2;\n </s> add   // A reference to the returned values from the execution.\n  repeated bytes return_ids = 2; </s> remove     def __init__(self, actor_cls):\n </s> add     def __init__(self, actor_cls, options=None): </s> add     def options(self, **kwargs):\n        return OptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs)\n </s> remove   // A name parameter, if the payload can be called in more than one way (like a method on\n  // a payload object).\n </s> add   // A name parameter, if the payload can be called in more than one way\n  // (like a method on a payload object). </s> remove         if self._ref is None:\n            # While calling ray.put() on our function, if\n            # our function is recursive, it will attempt to\n            # encode the ClientRemoteFunc -- itself -- and\n            # infinitely recurse on _ensure_ref.\n            #\n            # So we set the state of the reference to be an\n            # in-progress self reference value, which\n            # the encoding can detect and handle correctly.\n            self._ref = SelfReferenceSentinel()\n            self._ref = ray.put(self._func)\n </s> add         with self._lock:\n            if self._ref is None:\n                # While calling ray.put() on our function, if\n                # our function is recursive, it will attempt to\n                # encode the ClientRemoteFunc -- itself -- and\n                # infinitely recurse on _ensure_ref.\n                #\n                # So we set the state of the reference to be an\n                # in-progress self reference value, which\n                # the encoding can detect and handle correctly.\n                self._ref = SelfReferenceSentinel()\n                self._ref = ray.put(self._func)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/common.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         _name: The original name of the class\n <mask>         _ref: The ClientObjectRef of the pickled `actor_cls`\n <mask>     \"\"\"\n <mask> \n <mask>     def __init__(self, actor_cls):\n <mask>         self.actor_cls = actor_cls\n <mask>         self._name = actor_cls.__name__\n <mask>         self._ref = None\n <mask> \n <mask>     def __call__(self, *args, **kwargs):\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add         self._options = validate_options(options) </s> add                 baseline_options=None, </s> remove   // The ID of the client namespace associated with the Datapath stream making this\n  // request.\n </s> add   // The ID of the client namespace associated with the Datapath stream\n  // making this request. </s> add         set_task_options(task, self._options, \"baseline_options\") </s> remove         ref_id = ray.call_remote(self, *args, **kwargs)\n        return ClientActorHandle(ClientActorRef(ref_id), self)\n </s> add         ref_ids = ray.call_remote(self, *args, **kwargs)\n        assert len(ref_ids) == 1\n        return ClientActorHandle(ClientActorRef(ref_ids[0]), self)\n\n    def options(self, **kwargs):\n        return ActorOptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs) </s> remove         if self._ref is None:\n            # While calling ray.put() on our function, if\n            # our function is recursive, it will attempt to\n            # encode the ClientRemoteFunc -- itself -- and\n            # infinitely recurse on _ensure_ref.\n            #\n            # So we set the state of the reference to be an\n            # in-progress self reference value, which\n            # the encoding can detect and handle correctly.\n            self._ref = SelfReferenceSentinel()\n            self._ref = ray.put(self._func)\n </s> add         with self._lock:\n            if self._ref is None:\n                # While calling ray.put() on our function, if\n                # our function is recursive, it will attempt to\n                # encode the ClientRemoteFunc -- itself -- and\n                # infinitely recurse on _ensure_ref.\n                #\n                # So we set the state of the reference to be an\n                # in-progress self reference value, which\n                # the encoding can detect and handle correctly.\n                self._ref = SelfReferenceSentinel()\n                self._ref = ray.put(self._func)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/common.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>         self.actor_cls = actor_cls\n <mask>         self._name = actor_cls.__name__\n <mask>         self._ref = None\n <mask> \n <mask>     def __call__(self, *args, **kwargs):\n <mask>         raise TypeError(f\"Remote actor cannot be instantiated directly. \"\n <mask>                         \"Use {self._name}.remote() instead\")\n <mask> \n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove     def __init__(self, actor_cls):\n </s> add     def __init__(self, actor_cls, options=None): </s> remove         return ClientObjectRef(ray.call_remote(self, *args, **kwargs))\n </s> add         return return_refs(ray.call_remote(self, *args, **kwargs)) </s> remove         return ClientObjectRef(ray.call_remote(self, *args, **kwargs))\n </s> add         return return_refs(ray.call_remote(self, *args, **kwargs))\n\n    def options(self, **kwargs):\n        return OptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs) </s> remove         ref_id = ray.call_remote(self, *args, **kwargs)\n        return ClientActorHandle(ClientActorRef(ref_id), self)\n </s> add         ref_ids = ray.call_remote(self, *args, **kwargs)\n        assert len(ref_ids) == 1\n        return ClientActorHandle(ClientActorRef(ref_ids[0]), self)\n\n    def options(self, **kwargs):\n        return ActorOptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs) </s> add         set_task_options(task, self._options, \"baseline_options\") </s> remove             reg_class = ray.remote(actor_class)\n </s> add             if options is None or len(options) == 0:\n                reg_class = ray.remote(actor_class)\n            else:\n                reg_class = ray.remote(**options)(actor_class)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/common.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>             self._ref = ray.put(self.actor_cls)\n <mask> \n <mask>     def remote(self, *args, **kwargs) -> \"ClientActorHandle\":\n <mask>         # Actually instantiate the actor\n <mask>         ref_id = ray.call_remote(self, *args, **kwargs)\n <mask>         return ClientActorHandle(ClientActorRef(ref_id), self)\n <mask> \n <mask>     def __repr__(self):\n <mask>         return \"ClientActorClass(%s, %s)\" % (self._name, self._ref)\n <mask> \n <mask>     def __getattr__(self, key):\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove         return ClientObjectRef(ray.call_remote(self, *args, **kwargs))\n </s> add         return return_refs(ray.call_remote(self, *args, **kwargs))\n\n    def options(self, **kwargs):\n        return OptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs) </s> remove         return ClientObjectRef(ray.call_remote(self, *args, **kwargs))\n </s> add         return return_refs(ray.call_remote(self, *args, **kwargs)) </s> remove         if self._ref is None:\n            # While calling ray.put() on our function, if\n            # our function is recursive, it will attempt to\n            # encode the ClientRemoteFunc -- itself -- and\n            # infinitely recurse on _ensure_ref.\n            #\n            # So we set the state of the reference to be an\n            # in-progress self reference value, which\n            # the encoding can detect and handle correctly.\n            self._ref = SelfReferenceSentinel()\n            self._ref = ray.put(self._func)\n </s> add         with self._lock:\n            if self._ref is None:\n                # While calling ray.put() on our function, if\n                # our function is recursive, it will attempt to\n                # encode the ClientRemoteFunc -- itself -- and\n                # infinitely recurse on _ensure_ref.\n                #\n                # So we set the state of the reference to be an\n                # in-progress self reference value, which\n                # the encoding can detect and handle correctly.\n                self._ref = SelfReferenceSentinel()\n                self._ref = ray.put(self._func) </s> remove     def call_remote(self, instance, *args, **kwargs) -> bytes:\n </s> add     def call_remote(self, instance, *args, **kwargs) -> List[bytes]: </s> remove     def __init__(self, actor_cls):\n </s> add     def __init__(self, actor_cls, options=None): </s> add         self._options = validate_options(options)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/common.py"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask>         task.name = self._name\n <mask>         task.payload_id = self._ref.id\n <mask>         return task\n <mask> \n <mask> \n <mask> class ClientActorHandle(ClientStub):\n <mask>     \"\"\"Client-side stub for instantiated actor.\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add         set_task_options(task, self._options, \"baseline_options\") </s> add     def options(self, **kwargs):\n        return OptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs)\n </s> add         self._options = validate_options(options) </s> remove     def __init__(self, actor_cls):\n </s> add     def __init__(self, actor_cls, options=None): </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> add   // Options for modifying the remote task execution environment.\n  TaskOptions options = 7;\n  // Options passed to create the default remote task excution environment.\n  TaskOptions baseline_options = 8;", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/common.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                  actor_class: ClientActorClass):\n <mask>         self.actor_ref = actor_ref\n <mask> \n <mask>     def __del__(self) -> None:\n <mask>         ray.call_release(self.actor_ref.id)\n <mask> \n <mask>     @property\n <mask>     def _actor_id(self):\n <mask>         return self.actor_ref.id\n <mask> \n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove             raise e.details()\n </s> add             raise decode_exception(e.details) </s> add     def unify_and_track_outputs(self, output, client_id):\n        if output is None:\n            outputs = []\n        elif isinstance(output, list):\n            outputs = output\n        else:\n            outputs = [output]\n        for out in outputs:\n            if out.binary() in self.object_refs[client_id]:\n                logger.warning(f\"Already saw object_ref {out}\")\n            self.object_refs[client_id][out.binary()] = out\n        return [out.binary() for out in outputs]\n </s> remove         output = getattr(actor_handle, task.name).remote(*arglist, **kwargs)\n        self.object_refs[task.client_id][output.binary()] = output\n        return ray_client_pb2.ClientTaskTicket(return_id=output.binary())\n </s> add         method = getattr(actor_handle, task.name)\n        opts = decode_options(task.options)\n        if opts is not None:\n            method = method.options(**opts)\n        output = method.remote(*arglist, **kwargs)\n        ids = self.unify_and_track_outputs(output, task.client_id)\n        return ray_client_pb2.ClientTaskTicket(return_ids=ids) </s> add     def options(self, **kwargs):\n        return OptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs)\n </s> remove         return ticket.return_id\n </s> add         return ticket.return_ids </s> add def decode_options(\n        options: ray_client_pb2.TaskOptions) -> Optional[Dict[str, Any]]:\n    if options.json_options == \"\":\n        return None\n    opts = json.loads(options.json_options)\n    assert isinstance(opts, dict)\n    return opts\n\n", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/common.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         raise TypeError(f\"Remote method cannot be called directly. \"\n <mask>                         f\"Use {self._name}.remote() instead\")\n <mask> \n <mask>     def remote(self, *args, **kwargs):\n <mask>         return ClientObjectRef(ray.call_remote(self, *args, **kwargs))\n <mask> \n <mask>     def __repr__(self):\n <mask>         return \"ClientRemoteMethod(%s, %s)\" % (self.method_name,\n <mask>                                                self.actor_handle)\n <mask> \n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove         return ClientObjectRef(ray.call_remote(self, *args, **kwargs))\n </s> add         return return_refs(ray.call_remote(self, *args, **kwargs))\n\n    def options(self, **kwargs):\n        return OptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs) </s> add         self._options = validate_options(options) </s> remove         ref_id = ray.call_remote(self, *args, **kwargs)\n        return ClientActorHandle(ClientActorRef(ref_id), self)\n </s> add         ref_ids = ray.call_remote(self, *args, **kwargs)\n        assert len(ref_ids) == 1\n        return ClientActorHandle(ClientActorRef(ref_ids[0]), self)\n\n    def options(self, **kwargs):\n        return ActorOptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs) </s> remove     def remote(self, function_or_class, *args, **kwargs):\n        # TODO(barakmich): Arguments to ray.remote\n        # get captured here.\n        if (inspect.isfunction(function_or_class)\n                or is_cython(function_or_class)):\n            return ClientRemoteFunc(function_or_class)\n        elif inspect.isclass(function_or_class):\n            return ClientActorClass(function_or_class)\n        else:\n            raise TypeError(\"The @ray.remote decorator must be applied to \"\n                            \"either a function or to a class.\")\n </s> add     def remote(self, *args, **kwargs):\n        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n            # This is the case where the decorator is just @ray.remote.\n            return remote_decorator(options=None)(args[0])\n        error_string = (\"The @ray.remote decorator must be applied either \"\n                        \"with no arguments and no parentheses, for example \"\n                        \"'@ray.remote', or it must be applied using some of \"\n                        \"the arguments 'num_returns', 'num_cpus', 'num_gpus', \"\n                        \"'memory', 'object_store_memory', 'resources', \"\n                        \"'max_calls', or 'max_restarts', like \"\n                        \"'@ray.remote(num_returns=2, \"\n                        \"resources={\\\"CustomResource\\\": 1})'.\")\n        assert len(args) == 0 and len(kwargs) > 0, error_string\n        return remote_decorator(options=kwargs) </s> remove     def call_remote(self, instance, *args, **kwargs) -> bytes:\n </s> add     def call_remote(self, instance, *args, **kwargs) -> List[bytes]: </s> add                 baseline_options=None,", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/common.py"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask>                                                self.actor_handle)\n <mask> \n <mask>     def _prepare_client_task(self) -> ray_client_pb2.ClientTask:\n <mask>         task = ray_client_pb2.ClientTask()\n <mask>         task.type = ray_client_pb2.ClientTask.METHOD\n <mask>         task.name = self.method_name\n <mask>         task.payload_id = self.actor_handle.actor_ref.id\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add         set_task_options(task, self._options, \"baseline_options\") </s> add         set_task_options(task, self._options, \"baseline_options\") </s> remove         if self._ref is None:\n            # While calling ray.put() on our function, if\n            # our function is recursive, it will attempt to\n            # encode the ClientRemoteFunc -- itself -- and\n            # infinitely recurse on _ensure_ref.\n            #\n            # So we set the state of the reference to be an\n            # in-progress self reference value, which\n            # the encoding can detect and handle correctly.\n            self._ref = SelfReferenceSentinel()\n            self._ref = ray.put(self._func)\n </s> add         with self._lock:\n            if self._ref is None:\n                # While calling ray.put() on our function, if\n                # our function is recursive, it will attempt to\n                # encode the ClientRemoteFunc -- itself -- and\n                # infinitely recurse on _ensure_ref.\n                #\n                # So we set the state of the reference to be an\n                # in-progress self reference value, which\n                # the encoding can detect and handle correctly.\n                self._ref = SelfReferenceSentinel()\n                self._ref = ray.put(self._func) </s> remove         logger.info(\"schedule: %s %s\" %\n                    (task.name,\n                     ray_client_pb2.ClientTask.RemoteExecType.Name(task.type)))\n </s> add         logger.debug(\n            \"schedule: %s %s\" % (task.name,\n                                 ray_client_pb2.ClientTask.RemoteExecType.Name(\n                                     task.type))) </s> remove         remote_class = self.lookup_or_register_actor(task.payload_id,\n                                                     task.client_id)\n </s> add         remote_class = self.lookup_or_register_actor(\n            task.payload_id, task.client_id,\n            decode_options(task.baseline_options)) </s> add def decode_options(\n        options: ray_client_pb2.TaskOptions) -> Optional[Dict[str, Any]]:\n    if options.json_options == \"\":\n        return None\n    opts = json.loads(options.json_options)\n    assert isinstance(opts, dict)\n    return opts\n\n", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/common.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask> import base64\n <mask> from collections import defaultdict\n <mask> \n <mask> from typing import Dict\n <mask> from typing import Set\n <mask> from typing import Optional\n <mask> \n <mask> from ray import cloudpickle\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add from typing import Optional </s> remove from ray.experimental.client.client_pickler import loads_from_server\n </s> add  </s> add from ray.test_utils import client_test_enabled </s> remove from ray.experimental.client.common import ClientObjectRef\n </s> add from ray.experimental.client.client_pickler import loads_from_server </s> add from ray.experimental.client.common import ClientObjectRef </s> add from ray.experimental.client.common import ClientStub", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server.py"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask> from typing import Dict\n <mask> from typing import Set\n <mask> \n <mask> from ray import cloudpickle\n <mask> import ray\n <mask> import ray.state\n <mask> import ray.core.generated.ray_client_pb2 as ray_client_pb2\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add from typing import Any </s> remove from ray.experimental.client.client_pickler import loads_from_server\n </s> add  </s> remove import ray\n </s> add  </s> add from ray.test_utils import client_test_enabled </s> add if client_test_enabled():\n    from ray.experimental.client import ray\nelse:\n    import ray\n </s> remove from ray.experimental.client.common import ClientObjectRef\n </s> add from ray.experimental.client.client_pickler import loads_from_server", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         try:\n <mask>             ready_object_refs, remaining_object_refs = ray.wait(\n <mask>                 object_refs,\n <mask>                 num_returns=num_returns,\n <mask>                 timeout=timeout if timeout != -1 else None)\n <mask>         except Exception:\n <mask>             # TODO(ameer): improve exception messages.\n <mask>             return ray_client_pb2.WaitResponse(valid=False)\n <mask>         logger.debug(\"wait: %s %s\" % (str(ready_object_refs),\n <mask>                                       str(remaining_object_refs)))\n <mask>         ready_object_ids = [\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add             logger.error(f\"Exception {e}\") </s> remove         logger.info(\"schedule: %s %s\" %\n                    (task.name,\n                     ray_client_pb2.ClientTask.RemoteExecType.Name(task.type)))\n </s> add         logger.debug(\n            \"schedule: %s %s\" % (task.name,\n                                 ray_client_pb2.ClientTask.RemoteExecType.Name(\n                                     task.type))) </s> remove             raise e.details()\n </s> add             raise decode_exception(e.details) </s> add     \"test_advanced.py\", </s> add                 raise e </s> remove             raise cloudpickle.loads(data.error)\n </s> add             err = cloudpickle.loads(data.error)\n            logger.error(err)\n            raise err", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>             )\n <mask>         except Exception as e:\n <mask>             # TODO(ameer): improve exception messages.\n <mask>             return ray_client_pb2.WaitResponse(valid=False)\n <mask>         logger.debug(\"wait: %s %s\" % (str(ready_object_refs),\n <mask>                                       str(remaining_object_refs)))\n <mask>         ready_object_ids = [\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove                 timeout=timeout if timeout != -1 else None)\n        except Exception:\n </s> add                 timeout=timeout if timeout != -1 else None,\n            )\n        except Exception as e: </s> add                 raise e </s> remove             raise e.details()\n </s> add             raise decode_exception(e.details) </s> remove         logger.info(\"schedule: %s %s\" %\n                    (task.name,\n                     ray_client_pb2.ClientTask.RemoteExecType.Name(task.type)))\n </s> add         logger.debug(\n            \"schedule: %s %s\" % (task.name,\n                                 ray_client_pb2.ClientTask.RemoteExecType.Name(\n                                     task.type))) </s> remove             raise cloudpickle.loads(data.error)\n </s> add             err = cloudpickle.loads(data.error)\n            logger.error(err)\n            raise err </s> remove         return ticket.return_id\n </s> add         return ticket.return_ids", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>             ready_object_ids=ready_object_ids,\n <mask>             remaining_object_ids=remaining_object_ids)\n <mask> \n <mask>     def Schedule(self, task, context=None) -> ray_client_pb2.ClientTaskTicket:\n <mask>         logger.info(\"schedule: %s %s\" %\n <mask>                     (task.name,\n <mask>                      ray_client_pb2.ClientTask.RemoteExecType.Name(task.type)))\n <mask>         with stash_api_for_tests(self._test_mode):\n <mask>             try:\n <mask>                 if task.type == ray_client_pb2.ClientTask.FUNCTION:\n <mask>                     result = self._schedule_function(task, context)\n <mask>                 elif task.type == ray_client_pb2.ClientTask.ACTOR:\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove             raise e.details()\n </s> add             raise decode_exception(e.details) </s> remove                 timeout=timeout if timeout != -1 else None)\n        except Exception:\n </s> add                 timeout=timeout if timeout != -1 else None,\n            )\n        except Exception as e: </s> add     def options(self, **kwargs):\n        return OptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs)\n </s> remove         remote_class = self.lookup_or_register_actor(task.payload_id,\n                                                     task.client_id)\n </s> add         remote_class = self.lookup_or_register_actor(\n            task.payload_id, task.client_id,\n            decode_options(task.baseline_options)) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove         remote_func = self.lookup_or_register_func(task.payload_id,\n                                                   task.client_id)\n </s> add         remote_func = self.lookup_or_register_func(\n            task.payload_id, task.client_id,\n            decode_options(task.baseline_options))", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask>             except Exception as e:\n <mask>                 logger.error(f\"Caught schedule exception {e}\")\n <mask>                 return ray_client_pb2.ClientTaskTicket(\n <mask>                     valid=False, error=cloudpickle.dumps(e))\n <mask> \n <mask>     def _schedule_method(self, task: ray_client_pb2.ClientTask,\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add             logger.error(f\"Exception {e}\") </s> remove                 timeout=timeout if timeout != -1 else None)\n        except Exception:\n </s> add                 timeout=timeout if timeout != -1 else None,\n            )\n        except Exception as e: </s> remove             return_id=actor._actor_id.binary())\n </s> add             return_ids=[actor._actor_id.binary()]) </s> remove         return ticket.return_id\n </s> add         return ticket.return_ids </s> remove             raise e.details()\n </s> add             raise decode_exception(e.details) </s> remove             raise cloudpickle.loads(data.error)\n </s> add             err = cloudpickle.loads(data.error)\n            logger.error(err)\n            raise err", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server.py"}
{"docstring_tokens": "keep replace replace replace keep keep keep replace replace keep", "code_tokens": " <mask>         arglist, kwargs = self._convert_args(task.args, task.kwargs)\n <mask>         output = getattr(actor_handle, task.name).remote(*arglist, **kwargs)\n <mask>         self.object_refs[task.client_id][output.binary()] = output\n <mask>         return ray_client_pb2.ClientTaskTicket(return_id=output.binary())\n <mask> \n <mask>     def _schedule_actor(self, task: ray_client_pb2.ClientTask,\n <mask>                         context=None) -> ray_client_pb2.ClientTaskTicket:\n <mask>         remote_class = self.lookup_or_register_actor(task.payload_id,\n <mask>                                                      task.client_id)\n <mask> \n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove         remote_func = self.lookup_or_register_func(task.payload_id,\n                                                   task.client_id)\n </s> add         remote_func = self.lookup_or_register_func(\n            task.payload_id, task.client_id,\n            decode_options(task.baseline_options)) </s> remove         if output.binary() in self.object_refs[task.client_id]:\n            raise Exception(\"already found it\")\n        self.object_refs[task.client_id][output.binary()] = output\n        return ray_client_pb2.ClientTaskTicket(return_id=output.binary())\n </s> add         ids = self.unify_and_track_outputs(output, task.client_id)\n        return ray_client_pb2.ClientTaskTicket(return_ids=ids) </s> add         opts = decode_options(task.options)\n        if opts is not None:\n            remote_func = remote_func.options(**opts) </s> remove             return_id=actor._actor_id.binary())\n </s> add             return_ids=[actor._actor_id.binary()]) </s> add         opts = decode_options(task.options)\n        if opts is not None:\n            remote_class = remote_class.options(**opts)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>             decode_options(task.baseline_options))\n <mask> \n <mask>         arglist, kwargs = self._convert_args(task.args, task.kwargs)\n <mask>         with current_remote(remote_class):\n <mask>             actor = remote_class.remote(*arglist, **kwargs)\n <mask>         self.actor_refs[actor._actor_id.binary()] = actor\n <mask>         self.actor_owners[task.client_id].add(actor._actor_id.binary())\n <mask>         return ray_client_pb2.ClientTaskTicket(\n <mask>             return_ids=[actor._actor_id.binary()])\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove         remote_class = self.lookup_or_register_actor(task.payload_id,\n                                                     task.client_id)\n </s> add         remote_class = self.lookup_or_register_actor(\n            task.payload_id, task.client_id,\n            decode_options(task.baseline_options)) </s> remove             return_id=actor._actor_id.binary())\n </s> add             return_ids=[actor._actor_id.binary()]) </s> add         opts = decode_options(task.options)\n        if opts is not None:\n            remote_func = remote_func.options(**opts) </s> remove         remote_func = self.lookup_or_register_func(task.payload_id,\n                                                   task.client_id)\n </s> add         remote_func = self.lookup_or_register_func(\n            task.payload_id, task.client_id,\n            decode_options(task.baseline_options)) </s> remove         output = getattr(actor_handle, task.name).remote(*arglist, **kwargs)\n        self.object_refs[task.client_id][output.binary()] = output\n        return ray_client_pb2.ClientTaskTicket(return_id=output.binary())\n </s> add         method = getattr(actor_handle, task.name)\n        opts = decode_options(task.options)\n        if opts is not None:\n            method = method.options(**opts)\n        output = method.remote(*arglist, **kwargs)\n        ids = self.unify_and_track_outputs(output, task.client_id)\n        return ray_client_pb2.ClientTaskTicket(return_ids=ids) </s> remove         if output.binary() in self.object_refs[task.client_id]:\n            raise Exception(\"already found it\")\n        self.object_refs[task.client_id][output.binary()] = output\n        return ray_client_pb2.ClientTaskTicket(return_id=output.binary())\n </s> add         ids = self.unify_and_track_outputs(output, task.client_id)\n        return ray_client_pb2.ClientTaskTicket(return_ids=ids)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server.py"}
{"docstring_tokens": "keep keep replace keep keep keep replace replace", "code_tokens": " <mask>         self.actor_owners[task.client_id].add(actor._actor_id.binary())\n <mask>         return ray_client_pb2.ClientTaskTicket(\n <mask>             return_id=actor._actor_id.binary())\n <mask> \n <mask>     def _schedule_function(self, task: ray_client_pb2.ClientTask,\n <mask>                            context=None) -> ray_client_pb2.ClientTaskTicket:\n <mask>         remote_func = self.lookup_or_register_func(task.payload_id,\n <mask>                                                    task.client_id)\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove         remote_class = self.lookup_or_register_actor(task.payload_id,\n                                                     task.client_id)\n </s> add         remote_class = self.lookup_or_register_actor(\n            task.payload_id, task.client_id,\n            decode_options(task.baseline_options)) </s> remove         output = getattr(actor_handle, task.name).remote(*arglist, **kwargs)\n        self.object_refs[task.client_id][output.binary()] = output\n        return ray_client_pb2.ClientTaskTicket(return_id=output.binary())\n </s> add         method = getattr(actor_handle, task.name)\n        opts = decode_options(task.options)\n        if opts is not None:\n            method = method.options(**opts)\n        output = method.remote(*arglist, **kwargs)\n        ids = self.unify_and_track_outputs(output, task.client_id)\n        return ray_client_pb2.ClientTaskTicket(return_ids=ids) </s> add                 raise e </s> add         opts = decode_options(task.options)\n        if opts is not None:\n            remote_class = remote_class.options(**opts) </s> remove         logger.info(\"schedule: %s %s\" %\n                    (task.name,\n                     ray_client_pb2.ClientTask.RemoteExecType.Name(task.type)))\n </s> add         logger.debug(\n            \"schedule: %s %s\" % (task.name,\n                                 ray_client_pb2.ClientTask.RemoteExecType.Name(\n                                     task.type)))", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask>             decode_options(task.baseline_options))\n <mask>         arglist, kwargs = self._convert_args(task.args, task.kwargs)\n <mask>         with current_remote(remote_func):\n <mask>             output = remote_func.remote(*arglist, **kwargs)\n <mask>         ids = self.unify_and_track_outputs(output, task.client_id)\n <mask>         return ray_client_pb2.ClientTaskTicket(return_ids=ids)\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove         if output.binary() in self.object_refs[task.client_id]:\n            raise Exception(\"already found it\")\n        self.object_refs[task.client_id][output.binary()] = output\n        return ray_client_pb2.ClientTaskTicket(return_id=output.binary())\n </s> add         ids = self.unify_and_track_outputs(output, task.client_id)\n        return ray_client_pb2.ClientTaskTicket(return_ids=ids) </s> remove         remote_func = self.lookup_or_register_func(task.payload_id,\n                                                   task.client_id)\n </s> add         remote_func = self.lookup_or_register_func(\n            task.payload_id, task.client_id,\n            decode_options(task.baseline_options)) </s> remove         output = getattr(actor_handle, task.name).remote(*arglist, **kwargs)\n        self.object_refs[task.client_id][output.binary()] = output\n        return ray_client_pb2.ClientTaskTicket(return_id=output.binary())\n </s> add         method = getattr(actor_handle, task.name)\n        opts = decode_options(task.options)\n        if opts is not None:\n            method = method.options(**opts)\n        output = method.remote(*arglist, **kwargs)\n        ids = self.unify_and_track_outputs(output, task.client_id)\n        return ray_client_pb2.ClientTaskTicket(return_ids=ids) </s> remove         remote_class = self.lookup_or_register_actor(task.payload_id,\n                                                     task.client_id)\n </s> add         remote_class = self.lookup_or_register_actor(\n            task.payload_id, task.client_id,\n            decode_options(task.baseline_options)) </s> add         opts = decode_options(task.options)\n        if opts is not None:\n            remote_class = remote_class.options(**opts) </s> remove             return_id=actor._actor_id.binary())\n </s> add             return_ids=[actor._actor_id.binary()])", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>                                                    task.client_id)\n <mask>         arglist, kwargs = self._convert_args(task.args, task.kwargs)\n <mask>         with current_remote(remote_func):\n <mask>             output = remote_func.remote(*arglist, **kwargs)\n <mask>         if output.binary() in self.object_refs[task.client_id]:\n <mask>             raise Exception(\"already found it\")\n <mask>         self.object_refs[task.client_id][output.binary()] = output\n <mask>         return ray_client_pb2.ClientTaskTicket(return_id=output.binary())\n <mask> \n <mask>     def _convert_args(self, arg_list, kwarg_map):\n <mask>         argout = []\n <mask>         for arg in arg_list:\n <mask>             t = convert_from_arg(arg, self)\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove         remote_func = self.lookup_or_register_func(task.payload_id,\n                                                   task.client_id)\n </s> add         remote_func = self.lookup_or_register_func(\n            task.payload_id, task.client_id,\n            decode_options(task.baseline_options)) </s> add         opts = decode_options(task.options)\n        if opts is not None:\n            remote_func = remote_func.options(**opts) </s> remove         output = getattr(actor_handle, task.name).remote(*arglist, **kwargs)\n        self.object_refs[task.client_id][output.binary()] = output\n        return ray_client_pb2.ClientTaskTicket(return_id=output.binary())\n </s> add         method = getattr(actor_handle, task.name)\n        opts = decode_options(task.options)\n        if opts is not None:\n            method = method.options(**opts)\n        output = method.remote(*arglist, **kwargs)\n        ids = self.unify_and_track_outputs(output, task.client_id)\n        return ray_client_pb2.ClientTaskTicket(return_ids=ids) </s> remove         remote_class = self.lookup_or_register_actor(task.payload_id,\n                                                     task.client_id)\n </s> add         remote_class = self.lookup_or_register_actor(\n            task.payload_id, task.client_id,\n            decode_options(task.baseline_options)) </s> add     def unify_and_track_outputs(self, output, client_id):\n        if output is None:\n            outputs = []\n        elif isinstance(output, list):\n            outputs = output\n        else:\n            outputs = [output]\n        for out in outputs:\n            if out.binary() in self.object_refs[client_id]:\n                logger.warning(f\"Already saw object_ref {out}\")\n            self.object_refs[client_id][out.binary()] = out\n        return [out.binary() for out in outputs]\n </s> add         opts = decode_options(task.options)\n        if opts is not None:\n            remote_class = remote_class.options(**opts)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         for k in kwarg_map:\n <mask>             kwargout[k] = convert_from_arg(kwarg_map[k], self)\n <mask>         return argout, kwargout\n <mask> \n <mask>     def lookup_or_register_func(self, id: bytes, client_id: str\n <mask>                                 ) -> ray.remote_function.RemoteFunction:\n <mask>         if id not in self.function_refs:\n <mask>             funcref = self.object_refs[client_id][id]\n <mask>             func = ray.get(funcref)\n <mask>             if not inspect.isfunction(func):\n <mask>                 raise Exception(\"Attempting to register function that \"\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove             self.function_refs[id] = ray.remote(func)\n </s> add             if options is None or len(options) == 0:\n                self.function_refs[id] = ray.remote(func)\n            else:\n                self.function_refs[id] = ray.remote(**options)(func) </s> remove     def lookup_or_register_actor(self, id: bytes, client_id: str):\n </s> add     def lookup_or_register_actor(self, id: bytes, client_id: str,\n                                 options: Optional[Dict]): </s> remove              timeout: float = None\n </s> add              timeout: float = None,\n             fetch_local: bool = True </s> remove             reg_class = ray.remote(actor_class)\n </s> add             if options is None or len(options) == 0:\n                reg_class = ray.remote(actor_class)\n            else:\n                reg_class = ray.remote(**options)(actor_class) </s> remove     def call_remote(self, instance, *args, **kwargs) -> bytes:\n </s> add     def call_remote(self, instance, *args, **kwargs) -> List[bytes]: </s> remove         if output.binary() in self.object_refs[task.client_id]:\n            raise Exception(\"already found it\")\n        self.object_refs[task.client_id][output.binary()] = output\n        return ray_client_pb2.ClientTaskTicket(return_id=output.binary())\n </s> add         ids = self.unify_and_track_outputs(output, task.client_id)\n        return ray_client_pb2.ClientTaskTicket(return_ids=ids)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server.py"}
{"docstring_tokens": "keep keep replace keep keep replace keep", "code_tokens": " <mask>                 raise Exception(\"Attempting to register function that \"\n <mask>                                 \"isn't a function.\")\n <mask>             self.function_refs[id] = ray.remote(func)\n <mask>         return self.function_refs[id]\n <mask> \n <mask>     def lookup_or_register_actor(self, id: bytes, client_id: str):\n <mask>         if id not in self.registered_actor_classes:\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove     def lookup_or_register_func(self, id: bytes, client_id: str\n                                ) -> ray.remote_function.RemoteFunction:\n </s> add     def lookup_or_register_func(\n            self, id: bytes, client_id: str,\n            options: Optional[Dict]) -> ray.remote_function.RemoteFunction: </s> remove             reg_class = ray.remote(actor_class)\n </s> add             if options is None or len(options) == 0:\n                reg_class = ray.remote(actor_class)\n            else:\n                reg_class = ray.remote(**options)(actor_class) </s> add def remote_decorator(options: Optional[Dict[str, Any]]):\n    def decorator(function_or_class) -> ClientStub:\n        if (inspect.isfunction(function_or_class)\n                or is_cython(function_or_class)):\n            return ClientRemoteFunc(function_or_class, options=options)\n        elif inspect.isclass(function_or_class):\n            return ClientActorClass(function_or_class, options=options)\n        else:\n            raise TypeError(\"The @ray.remote decorator must be applied to \"\n                            \"either a function or to a class.\")\n\n    return decorator\n\n </s> remove     def call_remote(self, instance, *args, **kwargs) -> bytes:\n </s> add     def call_remote(self, instance, *args, **kwargs) -> List[bytes]: </s> add         if isinstance(val, ClientObjectRef):\n            raise TypeError(\n                \"Calling 'put' on an ObjectRef is not allowed \"\n                \"(similarly, returning an ObjectRef from a remote \"\n                \"function is not allowed). If you really want to \"\n                \"do this, you can wrap the ObjectRef in a list and \"\n                \"call 'put' on it (or return it).\")", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             actor_class = ray.get(actor_class_ref)\n <mask>             if not inspect.isclass(actor_class):\n <mask>                 raise Exception(\"Attempting to schedule actor that \"\n <mask>                                 \"isn't a class.\")\n <mask>             reg_class = ray.remote(actor_class)\n <mask>             self.registered_actor_classes[id] = reg_class\n <mask>         return self.registered_actor_classes[id]\n <mask> \n <mask> \n <mask> def return_exception_in_context(err, context):\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove     def lookup_or_register_actor(self, id: bytes, client_id: str):\n </s> add     def lookup_or_register_actor(self, id: bytes, client_id: str,\n                                 options: Optional[Dict]): </s> remove             self.function_refs[id] = ray.remote(func)\n </s> add             if options is None or len(options) == 0:\n                self.function_refs[id] = ray.remote(func)\n            else:\n                self.function_refs[id] = ray.remote(**options)(func) </s> add     def unify_and_track_outputs(self, output, client_id):\n        if output is None:\n            outputs = []\n        elif isinstance(output, list):\n            outputs = output\n        else:\n            outputs = [output]\n        for out in outputs:\n            if out.binary() in self.object_refs[client_id]:\n                logger.warning(f\"Already saw object_ref {out}\")\n            self.object_refs[client_id][out.binary()] = out\n        return [out.binary() for out in outputs]\n </s> remove     def lookup_or_register_func(self, id: bytes, client_id: str\n                                ) -> ray.remote_function.RemoteFunction:\n </s> add     def lookup_or_register_func(\n            self, id: bytes, client_id: str,\n            options: Optional[Dict]) -> ray.remote_function.RemoteFunction: </s> remove     def call_remote(self, instance, *args, **kwargs) -> bytes:\n </s> add     def call_remote(self, instance, *args, **kwargs) -> List[bytes]: </s> add def remote_decorator(options: Optional[Dict[str, Any]]):\n    def decorator(function_or_class) -> ClientStub:\n        if (inspect.isfunction(function_or_class)\n                or is_cython(function_or_class)):\n            return ClientRemoteFunc(function_or_class, options=options)\n        elif inspect.isclass(function_or_class):\n            return ClientActorClass(function_or_class, options=options)\n        else:\n            raise TypeError(\"The @ray.remote decorator must be applied to \"\n                            \"either a function or to a class.\")\n\n    return decorator\n\n", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>         return self.registered_actor_classes[id]\n <mask> \n <mask> \n <mask> def return_exception_in_context(err, context):\n <mask>     if context is not None:\n <mask>         context.set_details(encode_exception(err))\n <mask>         context.set_code(grpc.StatusCode.INTERNAL)\n <mask> \n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove             reg_class = ray.remote(actor_class)\n </s> add             if options is None or len(options) == 0:\n                reg_class = ray.remote(actor_class)\n            else:\n                reg_class = ray.remote(**options)(actor_class) </s> remove         output = getattr(actor_handle, task.name).remote(*arglist, **kwargs)\n        self.object_refs[task.client_id][output.binary()] = output\n        return ray_client_pb2.ClientTaskTicket(return_id=output.binary())\n </s> add         method = getattr(actor_handle, task.name)\n        opts = decode_options(task.options)\n        if opts is not None:\n            method = method.options(**opts)\n        output = method.remote(*arglist, **kwargs)\n        ids = self.unify_and_track_outputs(output, task.client_id)\n        return ray_client_pb2.ClientTaskTicket(return_ids=ids) </s> add         opts = decode_options(task.options)\n        if opts is not None:\n            remote_func = remote_func.options(**opts) </s> add         opts = decode_options(task.options)\n        if opts is not None:\n            remote_class = remote_class.options(**opts) </s> remove         return ticket.return_id\n </s> add         return ticket.return_ids </s> remove             raise e.details()\n </s> add             raise decode_exception(e.details)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server.py"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask> \n <mask> \n <mask> def serve(connection_str, test_mode=False):\n <mask>     server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n <mask>     task_servicer = RayletServicer(test_mode=test_mode)\n <mask>     data_servicer = DataServicer(task_servicer)\n <mask>     _set_server_api(RayServerAPI(task_servicer))\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove         output = getattr(actor_handle, task.name).remote(*arglist, **kwargs)\n        self.object_refs[task.client_id][output.binary()] = output\n        return ray_client_pb2.ClientTaskTicket(return_id=output.binary())\n </s> add         method = getattr(actor_handle, task.name)\n        opts = decode_options(task.options)\n        if opts is not None:\n            method = method.options(**opts)\n        output = method.remote(*arglist, **kwargs)\n        ids = self.unify_and_track_outputs(output, task.client_id)\n        return ray_client_pb2.ClientTaskTicket(return_ids=ids) </s> remove   // A reference to the returned value from the execution.\n  bytes return_id = 2;\n </s> add   // A reference to the returned values from the execution.\n  repeated bytes return_ids = 2; </s> add     def options(self, **kwargs):\n        return OptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs)\n </s> remove         remote_class = self.lookup_or_register_actor(task.payload_id,\n                                                     task.client_id)\n </s> add         remote_class = self.lookup_or_register_actor(\n            task.payload_id, task.client_id,\n            decode_options(task.baseline_options)) </s> add         self._options = validate_options(options) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\")\n </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"interferes with grpc\")", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>                 type=\"Object\",\n <mask>                 client_id=self.client_id,\n <mask>                 ref_id=obj_id,\n <mask>                 name=None,\n <mask>             )\n <mask>         elif isinstance(obj, ray.actor.ActorHandle):\n <mask>             actor_id = obj._actor_id.binary()\n <mask>             if actor_id not in self.server.actor_refs:\n <mask>                 # We're passing back a handle, probably inside a reference.\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove              timeout: float = None\n </s> add              timeout: float = None,\n             fetch_local: bool = True </s> add                 baseline_options=None, </s> remove     def lookup_or_register_actor(self, id: bytes, client_id: str):\n </s> add     def lookup_or_register_actor(self, id: bytes, client_id: str,\n                                 options: Optional[Dict]): </s> remove             self.function_refs[id] = ray.remote(func)\n </s> add             if options is None or len(options) == 0:\n                self.function_refs[id] = ray.remote(func)\n            else:\n                self.function_refs[id] = ray.remote(**options)(func) </s> remove     def lookup_or_register_func(self, id: bytes, client_id: str\n                                ) -> ray.remote_function.RemoteFunction:\n </s> add     def lookup_or_register_func(\n            self, id: bytes, client_id: str,\n            options: Optional[Dict]) -> ray.remote_function.RemoteFunction: </s> add     def unify_and_track_outputs(self, output, client_id):\n        if output is None:\n            outputs = []\n        elif isinstance(output, list):\n            outputs = output\n        else:\n            outputs = [output]\n        for out in outputs:\n            if out.binary() in self.object_refs[client_id]:\n                logger.warning(f\"Already saw object_ref {out}\")\n            self.object_refs[client_id][out.binary()] = out\n        return [out.binary() for out in outputs]\n", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server_pickler.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>                 ref_id=obj._actor_id.binary(),\n <mask>                 name=None,\n <mask>             )\n <mask>         return None\n <mask> \n <mask> \n <mask> class ClientUnpickler(pickle.Unpickler):\n <mask>     def __init__(self, server, *args, **kwargs):\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove     def __init__(self, actor_cls):\n </s> add     def __init__(self, actor_cls, options=None): </s> add         self._options = validate_options(options) </s> remove         return ClientObjectRef(ray.call_remote(self, *args, **kwargs))\n </s> add         return return_refs(ray.call_remote(self, *args, **kwargs))\n\n    def options(self, **kwargs):\n        return OptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs) </s> remove         return ClientObjectRef(ray.call_remote(self, *args, **kwargs))\n </s> add         return return_refs(ray.call_remote(self, *args, **kwargs)) </s> add                 baseline_options=None, </s> remove         ref_id = ray.call_remote(self, *args, **kwargs)\n        return ClientActorHandle(ClientActorRef(ref_id), self)\n </s> add         ref_ids = ray.call_remote(self, *args, **kwargs)\n        assert len(ref_ids) == 1\n        return ClientActorHandle(ClientActorRef(ref_ids[0]), self)\n\n    def options(self, **kwargs):\n        return ActorOptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server_pickler.py"}
{"docstring_tokens": "keep keep keep replace replace keep keep keep keep replace keep keep keep", "code_tokens": " <mask>         elif pid.type == \"RemoteFuncSelfReference\":\n <mask>             return ServerSelfReferenceSentinel()\n <mask>         elif pid.type == \"RemoteFunc\":\n <mask>             return self.server.lookup_or_register_func(pid.ref_id,\n <mask>                                                        pid.client_id)\n <mask>         elif pid.type == \"RemoteActorSelfReference\":\n <mask>             return ServerSelfReferenceSentinel()\n <mask>         elif pid.type == \"RemoteActor\":\n <mask>             return self.server.lookup_or_register_actor(\n <mask>                 pid.ref_id, pid.client_id)\n <mask>         elif pid.type == \"RemoteMethod\":\n <mask>             actor = self.server.actor_refs[pid.ref_id]\n <mask>             return getattr(actor, pid.name)\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove         logger.info(\"schedule: %s %s\" %\n                    (task.name,\n                     ray_client_pb2.ClientTask.RemoteExecType.Name(task.type)))\n </s> add         logger.debug(\n            \"schedule: %s %s\" % (task.name,\n                                 ray_client_pb2.ClientTask.RemoteExecType.Name(\n                                     task.type))) </s> add                 baseline_options=None, </s> remove     def remote(self, function_or_class, *args, **kwargs):\n        # TODO(barakmich): Arguments to ray.remote\n        # get captured here.\n        if (inspect.isfunction(function_or_class)\n                or is_cython(function_or_class)):\n            return ClientRemoteFunc(function_or_class)\n        elif inspect.isclass(function_or_class):\n            return ClientActorClass(function_or_class)\n        else:\n            raise TypeError(\"The @ray.remote decorator must be applied to \"\n                            \"either a function or to a class.\")\n </s> add     def remote(self, *args, **kwargs):\n        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n            # This is the case where the decorator is just @ray.remote.\n            return remote_decorator(options=None)(args[0])\n        error_string = (\"The @ray.remote decorator must be applied either \"\n                        \"with no arguments and no parentheses, for example \"\n                        \"'@ray.remote', or it must be applied using some of \"\n                        \"the arguments 'num_returns', 'num_cpus', 'num_gpus', \"\n                        \"'memory', 'object_store_memory', 'resources', \"\n                        \"'max_calls', or 'max_restarts', like \"\n                        \"'@ray.remote(num_returns=2, \"\n                        \"resources={\\\"CustomResource\\\": 1})'.\")\n        assert len(args) == 0 and len(kwargs) > 0, error_string\n        return remote_decorator(options=kwargs) </s> add def remote_decorator(options: Optional[Dict[str, Any]]):\n    def decorator(function_or_class) -> ClientStub:\n        if (inspect.isfunction(function_or_class)\n                or is_cython(function_or_class)):\n            return ClientRemoteFunc(function_or_class, options=options)\n        elif inspect.isclass(function_or_class):\n            return ClientActorClass(function_or_class, options=options)\n        else:\n            raise TypeError(\"The @ray.remote decorator must be applied to \"\n                            \"either a function or to a class.\")\n\n    return decorator\n\n </s> add     def unify_and_track_outputs(self, output, client_id):\n        if output is None:\n            outputs = []\n        elif isinstance(output, list):\n            outputs = output\n        else:\n            outputs = [output]\n        for out in outputs:\n            if out.binary() in self.object_refs[client_id]:\n                logger.warning(f\"Already saw object_ref {out}\")\n            self.object_refs[client_id][out.binary()] = out\n        return [out.binary() for out in outputs]\n", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/server/server_pickler.py"}
{"docstring_tokens": "keep keep keep keep replace keep replace keep keep keep keep", "code_tokens": " <mask> import ray.cloudpickle as cloudpickle\n <mask> import ray.core.generated.ray_client_pb2 as ray_client_pb2\n <mask> import ray.core.generated.ray_client_pb2_grpc as ray_client_pb2_grpc\n <mask> from ray.experimental.client.client_pickler import convert_to_arg\n <mask> from ray.experimental.client.client_pickler import loads_from_server\n <mask> from ray.experimental.client.client_pickler import dumps_from_client\n <mask> from ray.experimental.client.common import ClientObjectRef\n <mask> from ray.experimental.client.common import ClientActorClass\n <mask> from ray.experimental.client.common import ClientActorHandle\n <mask> from ray.experimental.client.common import ClientRemoteFunc\n <mask> from ray.experimental.client.dataclient import DataClient\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add from ray.experimental.client.common import ClientObjectRef </s> add from ray.experimental.client.common import ClientStub </s> add from typing import Optional </s> add from typing import Any </s> remove import ray\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/worker.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask> from ray.experimental.client.client_pickler import loads_from_server\n <mask> from ray.experimental.client.common import ClientActorClass\n <mask> from ray.experimental.client.common import ClientActorHandle\n <mask> from ray.experimental.client.common import ClientRemoteFunc\n <mask> from ray.experimental.client.common import ClientStub\n <mask> from ray.experimental.client.dataclient import DataClient\n <mask> \n <mask> logger = logging.getLogger(__name__)\n <mask> \n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add from ray.experimental.client.common import ClientStub </s> remove from ray.experimental.client.common import ClientObjectRef\n </s> add from ray.experimental.client.client_pickler import loads_from_server </s> remove from ray.experimental.client.client_pickler import loads_from_server\n </s> add  </s> add if client_test_enabled():\n    from ray.experimental.client import ray\nelse:\n    import ray\n </s> add from typing import Any </s> add from typing import Optional", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/worker.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask> from ray.experimental.client.common import ClientActorClass\n <mask> from ray.experimental.client.common import ClientActorHandle\n <mask> from ray.experimental.client.common import ClientObjectRef\n <mask> from ray.experimental.client.common import ClientRemoteFunc\n <mask> from ray.experimental.client.dataclient import DataClient\n <mask> \n <mask> logger = logging.getLogger(__name__)\n <mask> \n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add from ray.experimental.client.common import ClientObjectRef </s> remove from ray.experimental.client.common import ClientObjectRef\n </s> add from ray.experimental.client.client_pickler import loads_from_server </s> remove from ray.experimental.client.client_pickler import loads_from_server\n </s> add  </s> add if client_test_enabled():\n    from ray.experimental.client import ray\nelse:\n    import ray\n </s> add from typing import Any </s> add from typing import Optional", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             data = self.data_client.GetObject(req)\n <mask>         except grpc.RpcError as e:\n <mask>             raise e.details()\n <mask>         if not data.valid:\n <mask>             raise cloudpickle.loads(data.error)\n <mask>         return loads_from_server(data.data)\n <mask> \n <mask>     def put(self, vals):\n <mask>         to_put = []\n <mask>         single = False\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove             raise e.details()\n </s> add             raise decode_exception(e.details) </s> remove         return ticket.return_id\n </s> add         return ticket.return_ids </s> add                 raise e </s> remove                 timeout=timeout if timeout != -1 else None)\n        except Exception:\n </s> add                 timeout=timeout if timeout != -1 else None,\n            )\n        except Exception as e: </s> add             logger.error(f\"Exception {e}\") </s> remove         if output.binary() in self.object_refs[task.client_id]:\n            raise Exception(\"already found it\")\n        self.object_refs[task.client_id][output.binary()] = output\n        return ray_client_pb2.ClientTaskTicket(return_id=output.binary())\n </s> add         ids = self.unify_and_track_outputs(output, task.client_id)\n        return ray_client_pb2.ClientTaskTicket(return_ids=ids)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/worker.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>         return out\n <mask> \n <mask>     def _put(self, val):\n <mask>         data = dumps_from_client(val, self._client_id)\n <mask>         req = ray_client_pb2.PutRequest(data=data)\n <mask>         resp = self.data_client.PutObject(req)\n <mask>         return ClientObjectRef(resp.id)\n <mask> \n <mask>     def wait(self,\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add     def unify_and_track_outputs(self, output, client_id):\n        if output is None:\n            outputs = []\n        elif isinstance(output, list):\n            outputs = output\n        else:\n            outputs = [output]\n        for out in outputs:\n            if out.binary() in self.object_refs[client_id]:\n                logger.warning(f\"Already saw object_ref {out}\")\n            self.object_refs[client_id][out.binary()] = out\n        return [out.binary() for out in outputs]\n </s> remove             raise cloudpickle.loads(data.error)\n </s> add             err = cloudpickle.loads(data.error)\n            logger.error(err)\n            raise err </s> remove              timeout: float = None\n </s> add              timeout: float = None,\n             fetch_local: bool = True </s> remove     def call_remote(self, instance, *args, **kwargs) -> bytes:\n </s> add     def call_remote(self, instance, *args, **kwargs) -> List[bytes]: </s> add     def options(self, **kwargs):\n        return OptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs)\n </s> add def decode_options(\n        options: ray_client_pb2.TaskOptions) -> Optional[Dict[str, Any]]:\n    if options.json_options == \"\":\n        return None\n    opts = json.loads(options.json_options)\n    assert isinstance(opts, dict)\n    return opts\n\n", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     def wait(self,\n <mask>              object_refs: List[ClientObjectRef],\n <mask>              *,\n <mask>              num_returns: int = 1,\n <mask>              timeout: float = None\n <mask>              ) -> Tuple[List[ClientObjectRef], List[ClientObjectRef]]:\n <mask>         if not isinstance(object_refs, list):\n <mask>             raise TypeError(\"wait() expected a list of ClientObjectRef, \"\n <mask>                             f\"got {type(object_refs)}\")\n <mask>         for ref in object_refs:\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add         if isinstance(val, ClientObjectRef):\n            raise TypeError(\n                \"Calling 'put' on an ObjectRef is not allowed \"\n                \"(similarly, returning an ObjectRef from a remote \"\n                \"function is not allowed). If you really want to \"\n                \"do this, you can wrap the ObjectRef in a list and \"\n                \"call 'put' on it (or return it).\") </s> remove     def lookup_or_register_func(self, id: bytes, client_id: str\n                                ) -> ray.remote_function.RemoteFunction:\n </s> add     def lookup_or_register_func(\n            self, id: bytes, client_id: str,\n            options: Optional[Dict]) -> ray.remote_function.RemoteFunction: </s> remove             self.function_refs[id] = ray.remote(func)\n </s> add             if options is None or len(options) == 0:\n                self.function_refs[id] = ray.remote(func)\n            else:\n                self.function_refs[id] = ray.remote(**options)(func) </s> remove     def call_remote(self, instance, *args, **kwargs) -> bytes:\n </s> add     def call_remote(self, instance, *args, **kwargs) -> List[bytes]: </s> add     def unify_and_track_outputs(self, output, client_id):\n        if output is None:\n            outputs = []\n        elif isinstance(output, list):\n            outputs = output\n        else:\n            outputs = [output]\n        for out in outputs:\n            if out.binary() in self.object_refs[client_id]:\n                logger.warning(f\"Already saw object_ref {out}\")\n            self.object_refs[client_id][out.binary()] = out\n        return [out.binary() for out in outputs]\n </s> remove             reg_class = ray.remote(actor_class)\n </s> add             if options is None or len(options) == 0:\n                reg_class = ray.remote(actor_class)\n            else:\n                reg_class = ray.remote(**options)(actor_class)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/worker.py"}
{"docstring_tokens": "keep keep replace replace replace replace replace replace replace replace replace replace replace keep replace keep keep keep", "code_tokens": " <mask>         return (client_ready_object_ids, client_remaining_object_ids)\n <mask> \n <mask>     def remote(self, function_or_class, *args, **kwargs):\n <mask>         # TODO(barakmich): Arguments to ray.remote\n <mask>         # get captured here.\n <mask>         if (inspect.isfunction(function_or_class)\n <mask>                 or is_cython(function_or_class)):\n <mask>             return ClientRemoteFunc(function_or_class)\n <mask>         elif inspect.isclass(function_or_class):\n <mask>             return ClientActorClass(function_or_class)\n <mask>         else:\n <mask>             raise TypeError(\"The @ray.remote decorator must be applied to \"\n <mask>                             \"either a function or to a class.\")\n <mask> \n <mask>     def call_remote(self, instance, *args, **kwargs) -> bytes:\n <mask>         task = instance._prepare_client_task()\n <mask>         for arg in args:\n <mask>             pb_arg = convert_to_arg(arg, self._client_id)\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add def remote_decorator(options: Optional[Dict[str, Any]]):\n    def decorator(function_or_class) -> ClientStub:\n        if (inspect.isfunction(function_or_class)\n                or is_cython(function_or_class)):\n            return ClientRemoteFunc(function_or_class, options=options)\n        elif inspect.isclass(function_or_class):\n            return ClientActorClass(function_or_class, options=options)\n        else:\n            raise TypeError(\"The @ray.remote decorator must be applied to \"\n                            \"either a function or to a class.\")\n\n    return decorator\n\n </s> remove             reg_class = ray.remote(actor_class)\n </s> add             if options is None or len(options) == 0:\n                reg_class = ray.remote(actor_class)\n            else:\n                reg_class = ray.remote(**options)(actor_class) </s> remove         return ClientObjectRef(ray.call_remote(self, *args, **kwargs))\n </s> add         return return_refs(ray.call_remote(self, *args, **kwargs))\n\n    def options(self, **kwargs):\n        return OptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs) </s> remove             self.function_refs[id] = ray.remote(func)\n </s> add             if options is None or len(options) == 0:\n                self.function_refs[id] = ray.remote(func)\n            else:\n                self.function_refs[id] = ray.remote(**options)(func) </s> remove         ref_id = ray.call_remote(self, *args, **kwargs)\n        return ClientActorHandle(ClientActorRef(ref_id), self)\n </s> add         ref_ids = ray.call_remote(self, *args, **kwargs)\n        assert len(ref_ids) == 1\n        return ClientActorHandle(ClientActorRef(ref_ids[0]), self)\n\n    def options(self, **kwargs):\n        return ActorOptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/worker.py"}
{"docstring_tokens": "keep keep keep replace keep keep replace keep", "code_tokens": " <mask>         try:\n <mask>             ticket = self.server.Schedule(task, metadata=self.metadata)\n <mask>         except grpc.RpcError as e:\n <mask>             raise e.details()\n <mask>         if not ticket.valid:\n <mask>             raise cloudpickle.loads(ticket.error)\n <mask>         return ticket.return_id\n <mask> \n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove             raise cloudpickle.loads(data.error)\n </s> add             err = cloudpickle.loads(data.error)\n            logger.error(err)\n            raise err </s> remove                 timeout=timeout if timeout != -1 else None)\n        except Exception:\n </s> add                 timeout=timeout if timeout != -1 else None,\n            )\n        except Exception as e: </s> add                 raise e </s> add             logger.error(f\"Exception {e}\") </s> remove             self.function_refs[id] = ray.remote(func)\n </s> add             if options is None or len(options) == 0:\n                self.function_refs[id] = ray.remote(func)\n            else:\n                self.function_refs[id] = ray.remote(**options)(func)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/worker.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> \n <mask> \n <mask> def make_client_id() -> str:\n <mask>     id = uuid.uuid4()\n <mask>     return id.hex\n <mask> \n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove     def lookup_or_register_func(self, id: bytes, client_id: str\n                                ) -> ray.remote_function.RemoteFunction:\n </s> add     def lookup_or_register_func(\n            self, id: bytes, client_id: str,\n            options: Optional[Dict]) -> ray.remote_function.RemoteFunction: </s> remove     def lookup_or_register_actor(self, id: bytes, client_id: str):\n </s> add     def lookup_or_register_actor(self, id: bytes, client_id: str,\n                                 options: Optional[Dict]): </s> remove             self.function_refs[id] = ray.remote(func)\n </s> add             if options is None or len(options) == 0:\n                self.function_refs[id] = ray.remote(func)\n            else:\n                self.function_refs[id] = ray.remote(**options)(func) </s> add     def options(self, **kwargs):\n        return OptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs)\n </s> remove         ray.call_release(self.actor_ref.id)\n </s> add         if ray.is_connected():\n            ray.call_release(self.actor_ref.id) </s> add def decode_options(\n        options: ray_client_pb2.TaskOptions) -> Optional[Dict[str, Any]]:\n    if options.json_options == \"\":\n        return None\n    opts = json.loads(options.json_options)\n    assert isinstance(opts, dict)\n    return opts\n\n", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/experimental/client/worker.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask> py_test_module_list(\n <mask>   files = [\n <mask>     \"test_actor.py\",\n <mask>     \"test_basic.py\",\n <mask>     \"test_basic_2.py\",\n <mask>   ],\n <mask>   size = \"medium\",\n <mask>   extra_srcs = SRCS,\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add             logger.error(f\"Exception {e}\") </s> remove                 timeout=timeout if timeout != -1 else None)\n        except Exception:\n </s> add                 timeout=timeout if timeout != -1 else None,\n            )\n        except Exception as e: </s> add         opts = decode_options(task.options)\n        if opts is not None:\n            remote_func = remote_func.options(**opts) </s> add         opts = decode_options(task.options)\n        if opts is not None:\n            remote_class = remote_class.options(**opts) </s> remove         output = getattr(actor_handle, task.name).remote(*arglist, **kwargs)\n        self.object_refs[task.client_id][output.binary()] = output\n        return ray_client_pb2.ClientTaskTicket(return_id=output.binary())\n </s> add         method = getattr(actor_handle, task.name)\n        opts = decode_options(task.options)\n        if opts is not None:\n            method = method.options(**opts)\n        output = method.remote(*arglist, **kwargs)\n        ids = self.unify_and_track_outputs(output, task.client_id)\n        return ray_client_pb2.ClientTaskTicket(return_ids=ids) </s> remove         remote_class = self.lookup_or_register_actor(task.payload_id,\n                                                     task.client_id)\n </s> add         remote_class = self.lookup_or_register_actor(\n            task.payload_id, task.client_id,\n            decode_options(task.baseline_options))", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/BUILD"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> # with ray.\n <mask> import setproctitle  # noqa\n <mask> \n <mask> \n <mask> @pytest.mark.skipif(client_test_enabled(), reason=\"test setup order\")\n <mask> def test_caching_actors(shutdown_only):\n <mask>     # Test defining actors before ray.init() has been called.\n <mask> \n <mask>     @ray.remote\n <mask>     class Foo:\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\") </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"message size\") </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"ray.method unimplemented\") </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_actor.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     t = TestActor.remote()\n <mask>     assert ray.get(t.g.remote()) == 3\n <mask> \n <mask> \n <mask> @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n <mask> def test_decorator_args(ray_start_regular_shared):\n <mask>     # This is an invalid way of using the actor decorator.\n <mask>     with pytest.raises(Exception):\n <mask> \n <mask>         @ray.remote()\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove         if self._ref is None:\n            # While calling ray.put() on our function, if\n            # our function is recursive, it will attempt to\n            # encode the ClientRemoteFunc -- itself -- and\n            # infinitely recurse on _ensure_ref.\n            #\n            # So we set the state of the reference to be an\n            # in-progress self reference value, which\n            # the encoding can detect and handle correctly.\n            self._ref = SelfReferenceSentinel()\n            self._ref = ray.put(self._func)\n </s> add         with self._lock:\n            if self._ref is None:\n                # While calling ray.put() on our function, if\n                # our function is recursive, it will attempt to\n                # encode the ClientRemoteFunc -- itself -- and\n                # infinitely recurse on _ensure_ref.\n                #\n                # So we set the state of the reference to be an\n                # in-progress self reference value, which\n                # the encoding can detect and handle correctly.\n                self._ref = SelfReferenceSentinel()\n                self._ref = ray.put(self._func) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_actor.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             def __init__(self):\n <mask>                 pass\n <mask> \n <mask> \n <mask> @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n <mask> def test_multiple_return_values(ray_start_regular_shared):\n <mask>     @ray.remote\n <mask>     class Foo:\n <mask>         def method0(self):\n <mask>             return 1\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"ray.timeline\") </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_actor.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     id3a, id3b, id3c = f.method3.remote()\n <mask>     assert ray.get([id3a, id3b, id3c]) == [1, 2, 3]\n <mask> \n <mask> \n <mask> @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n <mask> def test_options_num_returns(ray_start_regular_shared):\n <mask>     @ray.remote\n <mask>     class Foo:\n <mask>         def method(self):\n <mask>             return 1, 2\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"ray.method unimplemented\") </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_actor.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> import numpy as np\n <mask> import pytest\n <mask> \n <mask> import ray\n <mask> import ray.cluster_utils\n <mask> import ray.test_utils\n <mask> \n <mask> from ray.test_utils import RayTestTimeoutException\n <mask> \n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add from ray.test_utils import client_test_enabled </s> add if client_test_enabled():\n    from ray.experimental.client import ray\nelse:\n    import ray\n </s> add from typing import Optional </s> remove from ray.experimental.client.client_pickler import loads_from_server\n </s> add  </s> add from typing import Any </s> remove from ray.experimental.client.common import ClientObjectRef\n </s> add from ray.experimental.client.client_pickler import loads_from_server", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_advanced.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask> import ray.cluster_utils\n <mask> import ray.test_utils\n <mask> \n <mask> from ray.test_utils import RayTestTimeoutException\n <mask> \n <mask> if client_test_enabled():\n <mask>     from ray.experimental.client import ray\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add if client_test_enabled():\n    from ray.experimental.client import ray\nelse:\n    import ray\n </s> remove import ray\n </s> add  </s> add from typing import Optional </s> add from typing import Any </s> remove from ray.experimental.client.common import ClientObjectRef\n </s> add from ray.experimental.client.client_pickler import loads_from_server </s> remove from ray.experimental.client.client_pickler import loads_from_server\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_advanced.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask> \n <mask> from ray.test_utils import client_test_enabled\n <mask> from ray.test_utils import RayTestTimeoutException\n <mask> \n <mask> logger = logging.getLogger(__name__)\n <mask> \n <mask> \n <mask> # issue https://github.com/ray-project/ray/issues/7105\n <mask> @pytest.mark.skipif(client_test_enabled(), reason=\"message size\")\n <mask> def test_internal_free(shutdown_only):\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add from ray.test_utils import client_test_enabled </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"message size\") </s> remove import ray\n </s> add  </s> add from ray.experimental.client.common import ClientStub </s> add from ray.experimental.client.common import ClientObjectRef </s> remove from ray.experimental.client.common import ClientObjectRef\n </s> add from ray.experimental.client.client_pickler import loads_from_server", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_advanced.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask> \n <mask> \n <mask> # issue https://github.com/ray-project/ray/issues/7105\n <mask> def test_internal_free(shutdown_only):\n <mask>     ray.init(num_cpus=1)\n <mask> \n <mask>     @ray.remote\n <mask>     class Sampler:\n <mask>         def sample(self):\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add if client_test_enabled():\n    from ray.experimental.client import ray\nelse:\n    import ray\n </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"ray.method unimplemented\") </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\")\n </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"interferes with grpc\") </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"test setup order\")\n </s> add @pytest.mark.skipif(\n    client_test_enabled(),\n    reason=\"defining early, no ray package injection yet\")", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_advanced.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         time.sleep(delay)\n <mask>         return 1\n <mask> \n <mask>     @ray.remote\n <mask>     def g(l):\n <mask>         # The argument l should be a list containing one object ref.\n <mask>         ray.wait([l[0]])\n <mask> \n <mask>     @ray.remote\n <mask>     def h(l):\n <mask>         # The argument l should be a list containing one object ref.\n <mask>         ray.get(l[0])\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove     def h(l):\n        # The argument l should be a list containing one object ref.\n        ray.get(l[0])\n </s> add     def h(input_list):\n        # The argument input_list should be a list containing one object ref.\n        ray.get(input_list[0]) </s> remove   // A name parameter, if the payload can be called in more than one way (like a method on\n  // a payload object).\n </s> add   // A name parameter, if the payload can be called in more than one way\n  // (like a method on a payload object). </s> remove     def remote(self, function_or_class, *args, **kwargs):\n        # TODO(barakmich): Arguments to ray.remote\n        # get captured here.\n        if (inspect.isfunction(function_or_class)\n                or is_cython(function_or_class)):\n            return ClientRemoteFunc(function_or_class)\n        elif inspect.isclass(function_or_class):\n            return ClientActorClass(function_or_class)\n        else:\n            raise TypeError(\"The @ray.remote decorator must be applied to \"\n                            \"either a function or to a class.\")\n </s> add     def remote(self, *args, **kwargs):\n        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n            # This is the case where the decorator is just @ray.remote.\n            return remote_decorator(options=None)(args[0])\n        error_string = (\"The @ray.remote decorator must be applied either \"\n                        \"with no arguments and no parentheses, for example \"\n                        \"'@ray.remote', or it must be applied using some of \"\n                        \"the arguments 'num_returns', 'num_cpus', 'num_gpus', \"\n                        \"'memory', 'object_store_memory', 'resources', \"\n                        \"'max_calls', or 'max_restarts', like \"\n                        \"'@ray.remote(num_returns=2, \"\n                        \"resources={\\\"CustomResource\\\": 1})'.\")\n        assert len(args) == 0 and len(kwargs) > 0, error_string\n        return remote_decorator(options=kwargs) </s> add def remote_decorator(options: Optional[Dict[str, Any]]):\n    def decorator(function_or_class) -> ClientStub:\n        if (inspect.isfunction(function_or_class)\n                or is_cython(function_or_class)):\n            return ClientRemoteFunc(function_or_class, options=options)\n        elif inspect.isclass(function_or_class):\n            return ClientActorClass(function_or_class, options=options)\n        else:\n            raise TypeError(\"The @ray.remote decorator must be applied to \"\n                            \"either a function or to a class.\")\n\n    return decorator\n\n </s> remove     def call_remote(self, instance, *args, **kwargs) -> bytes:\n </s> add     def call_remote(self, instance, *args, **kwargs) -> List[bytes]: </s> remove     def __init__(self, actor_cls):\n </s> add     def __init__(self, actor_cls, options=None):", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_advanced.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         # The argument l should be a list containing one object ref.\n <mask>         ray.wait([l[0]])\n <mask> \n <mask>     @ray.remote\n <mask>     def h(l):\n <mask>         # The argument l should be a list containing one object ref.\n <mask>         ray.get(l[0])\n <mask> \n <mask>     # Make sure that multiple wait requests involving the same object ref\n <mask>     # all return.\n <mask>     x = f.remote(1)\n <mask>     ray.get([g.remote([x]), g.remote([x])])\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove     def g(l):\n        # The argument l should be a list containing one object ref.\n        ray.wait([l[0]])\n </s> add     def g(input_list):\n        # The argument input_list should be a list containing one object ref.\n        ray.wait([input_list[0]]) </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\") </s> remove   // A name parameter, if the payload can be called in more than one way (like a method on\n  // a payload object).\n </s> add   // A name parameter, if the payload can be called in more than one way\n  // (like a method on a payload object). </s> remove         if self._ref is None:\n            # While calling ray.put() on our function, if\n            # our function is recursive, it will attempt to\n            # encode the ClientRemoteFunc -- itself -- and\n            # infinitely recurse on _ensure_ref.\n            #\n            # So we set the state of the reference to be an\n            # in-progress self reference value, which\n            # the encoding can detect and handle correctly.\n            self._ref = SelfReferenceSentinel()\n            self._ref = ray.put(self._func)\n </s> add         with self._lock:\n            if self._ref is None:\n                # While calling ray.put() on our function, if\n                # our function is recursive, it will attempt to\n                # encode the ClientRemoteFunc -- itself -- and\n                # infinitely recurse on _ensure_ref.\n                #\n                # So we set the state of the reference to be an\n                # in-progress self reference value, which\n                # the encoding can detect and handle correctly.\n                self._ref = SelfReferenceSentinel()\n                self._ref = ray.put(self._func) </s> remove     def remote(self, function_or_class, *args, **kwargs):\n        # TODO(barakmich): Arguments to ray.remote\n        # get captured here.\n        if (inspect.isfunction(function_or_class)\n                or is_cython(function_or_class)):\n            return ClientRemoteFunc(function_or_class)\n        elif inspect.isclass(function_or_class):\n            return ClientActorClass(function_or_class)\n        else:\n            raise TypeError(\"The @ray.remote decorator must be applied to \"\n                            \"either a function or to a class.\")\n </s> add     def remote(self, *args, **kwargs):\n        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n            # This is the case where the decorator is just @ray.remote.\n            return remote_decorator(options=None)(args[0])\n        error_string = (\"The @ray.remote decorator must be applied either \"\n                        \"with no arguments and no parentheses, for example \"\n                        \"'@ray.remote', or it must be applied using some of \"\n                        \"the arguments 'num_returns', 'num_cpus', 'num_gpus', \"\n                        \"'memory', 'object_store_memory', 'resources', \"\n                        \"'max_calls', or 'max_restarts', like \"\n                        \"'@ray.remote(num_returns=2, \"\n                        \"resources={\\\"CustomResource\\\": 1})'.\")\n        assert len(args) == 0 and len(kwargs) > 0, error_string\n        return remote_decorator(options=kwargs) </s> remove              timeout: float = None\n </s> add              timeout: float = None,\n             fetch_local: bool = True", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_advanced.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>     ray.get([h.remote([x]), h.remote([x])])\n <mask> \n <mask> \n <mask> def test_caching_functions_to_run(shutdown_only):\n <mask>     # Test that we export functions to run on all workers before the driver\n <mask>     # is connected.\n <mask>     def f(worker_info):\n <mask>         sys.path.append(1)\n <mask> \n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove         if self._ref is None:\n            # While calling ray.put() on our function, if\n            # our function is recursive, it will attempt to\n            # encode the ClientRemoteFunc -- itself -- and\n            # infinitely recurse on _ensure_ref.\n            #\n            # So we set the state of the reference to be an\n            # in-progress self reference value, which\n            # the encoding can detect and handle correctly.\n            self._ref = SelfReferenceSentinel()\n            self._ref = ray.put(self._func)\n </s> add         with self._lock:\n            if self._ref is None:\n                # While calling ray.put() on our function, if\n                # our function is recursive, it will attempt to\n                # encode the ClientRemoteFunc -- itself -- and\n                # infinitely recurse on _ensure_ref.\n                #\n                # So we set the state of the reference to be an\n                # in-progress self reference value, which\n                # the encoding can detect and handle correctly.\n                self._ref = SelfReferenceSentinel()\n                self._ref = ray.put(self._func) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"test setup order\")\n </s> add @pytest.mark.skipif(\n    client_test_enabled(),\n    reason=\"defining early, no ray package injection yet\") </s> remove     def h(l):\n        # The argument l should be a list containing one object ref.\n        ray.get(l[0])\n </s> add     def h(input_list):\n        # The argument input_list should be a list containing one object ref.\n        ray.get(input_list[0]) </s> add         if isinstance(val, ClientObjectRef):\n            raise TypeError(\n                \"Calling 'put' on an ObjectRef is not allowed \"\n                \"(similarly, returning an ObjectRef from a remote \"\n                \"function is not allowed). If you really want to \"\n                \"do this, you can wrap the ObjectRef in a list and \"\n                \"call 'put' on it (or return it).\") </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\") </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_advanced.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     ray.worker.global_worker.run_function_on_all_workers(f)\n <mask> \n <mask> \n <mask> def test_running_function_on_all_workers(ray_start_regular):\n <mask>     def f(worker_info):\n <mask>         sys.path.append(\"fake_directory\")\n <mask> \n <mask>     ray.worker.global_worker.run_function_on_all_workers(f)\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\") </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"ray.method unimplemented\") </s> remove         return ClientObjectRef(ray.call_remote(self, *args, **kwargs))\n </s> add         return return_refs(ray.call_remote(self, *args, **kwargs))\n\n    def options(self, **kwargs):\n        return OptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs) </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"ray.timeline\") </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"message size\") </s> remove         ref_id = ray.call_remote(self, *args, **kwargs)\n        return ClientActorHandle(ClientActorRef(ref_id), self)\n </s> add         ref_ids = ray.call_remote(self, *args, **kwargs)\n        assert len(ref_ids) == 1\n        return ClientActorHandle(ClientActorRef(ref_ids[0]), self)\n\n    def options(self, **kwargs):\n        return ActorOptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_advanced.py"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask> \n <mask> \n <mask> def test_profiling_api(ray_start_2_cpus):\n <mask>     @ray.remote\n <mask>     def f():\n <mask>         with ray.profile(\"custom_event\", extra_data={\"name\": \"custom name\"}):\n <mask>             pass\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"ray.method unimplemented\") </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"message size\") </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"test setup order\")\n </s> add @pytest.mark.skipif(\n    client_test_enabled(),\n    reason=\"defining early, no ray package injection yet\") </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove   // The ID of the client namespace associated with the Datapath stream making this\n  // request.\n </s> add   // The ID of the client namespace associated with the Datapath stream\n  // making this request.", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_advanced.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>     ray.get(actor.join.remote()) == \"ok\"\n <mask> \n <mask> \n <mask> def test_wait_makes_object_local(ray_start_cluster):\n <mask>     cluster = ray_start_cluster\n <mask>     cluster.add_node(num_cpus=0)\n <mask>     cluster.add_node(num_cpus=2)\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add def decode_options(\n        options: ray_client_pb2.TaskOptions) -> Optional[Dict[str, Any]]:\n    if options.json_options == \"\":\n        return None\n    opts = json.loads(options.json_options)\n    assert isinstance(opts, dict)\n    return opts\n\n </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove         ref_id = ray.call_remote(self, *args, **kwargs)\n        return ClientActorHandle(ClientActorRef(ref_id), self)\n </s> add         ref_ids = ray.call_remote(self, *args, **kwargs)\n        assert len(ref_ids) == 1\n        return ClientActorHandle(ClientActorRef(ref_ids[0]), self)\n\n    def options(self, **kwargs):\n        return ActorOptionWrapper(self, kwargs)\n\n    def _remote(self, args=[], kwargs={}, **option_args):\n        return self.options(**option_args).remote(*args, **kwargs) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove             reg_class = ray.remote(actor_class)\n </s> add             if options is None or len(options) == 0:\n                reg_class = ray.remote(actor_class)\n            else:\n                reg_class = ray.remote(**options)(actor_class)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_advanced.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> logger = logging.getLogger(__name__)\n <mask> \n <mask> \n <mask> # https://github.com/ray-project/ray/issues/6662\n <mask> @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\")\n <mask> def test_ignore_http_proxy(shutdown_only):\n <mask>     ray.init(num_cpus=1)\n <mask>     os.environ[\"http_proxy\"] = \"http://example.com\"\n <mask>     os.environ[\"https_proxy\"] = \"http://example.com\"\n <mask> \n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\")\n </s> add  </s> add if client_test_enabled():\n    from ray.experimental.client import ray\nelse:\n    import ray\n </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\") </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\") </s> add from ray.experimental.client.common import ClientStub </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"message size\")", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_basic.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     ray.get(bar.remote(*[f() for _ in range(200)]))\n <mask> \n <mask> \n <mask> # https://github.com/ray-project/ray/issues/7287\n <mask> @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\")\n <mask> def test_omp_threads_set(shutdown_only):\n <mask>     ray.init(num_cpus=1)\n <mask>     # Should have been auto set by ray init.\n <mask>     assert os.environ[\"OMP_NUM_THREADS\"] == \"1\"\n <mask> \n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\")\n </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"interferes with grpc\") </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\") </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\") </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"test setup order\")\n </s> add @pytest.mark.skipif(\n    client_test_enabled(),\n    reason=\"defining early, no ray package injection yet\") </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"message size\")", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_basic.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     # Should have been auto set by ray init.\n <mask>     assert os.environ[\"OMP_NUM_THREADS\"] == \"1\"\n <mask> \n <mask> \n <mask> @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n <mask> def test_submit_api(shutdown_only):\n <mask>     ray.init(num_cpus=2, num_gpus=1, resources={\"Custom\": 1})\n <mask> \n <mask>     @ray.remote\n <mask>     def f(n):\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_basic.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         args=[\"test\"], kwargs={\"b\": 2}, num_returns=4)\n <mask>     assert ray.get([id1, id2, id3, id4]) == [0, 1, \"test\", 2]\n <mask> \n <mask> \n <mask> @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n <mask> def test_invalid_arguments(shutdown_only):\n <mask>     ray.init(num_cpus=2)\n <mask> \n <mask>     for opt in [np.random.randint(-100, -1), np.random.uniform(0, 1)]:\n <mask>         with pytest.raises(\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_basic.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             class A2:\n <mask>                 x = 1\n <mask> \n <mask> \n <mask> @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n <mask> def test_many_fractional_resources(shutdown_only):\n <mask>     ray.init(num_cpus=2, num_gpus=2, resources={\"Custom\": 2})\n <mask> \n <mask>     @ray.remote\n <mask>     def g():\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"ray.method unimplemented\") </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_basic.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     if not correct_available_resources:\n <mask>         assert False, \"Did not get correct available resources.\"\n <mask> \n <mask> \n <mask> @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n <mask> def test_background_tasks_with_max_calls(shutdown_only):\n <mask>     ray.init(num_cpus=2)\n <mask> \n <mask>     @ray.remote\n <mask>     def g():\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"ray.method unimplemented\") </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_basic.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     d = {python_descriptor: 123}\n <mask>     assert d.get(python_descriptor2) == 123\n <mask> \n <mask> \n <mask> @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n <mask> def test_ray_options(shutdown_only):\n <mask>     @ray.remote(\n <mask>         num_cpus=2, num_gpus=3, memory=150 * 2**20, resources={\"custom1\": 1})\n <mask>     def foo():\n <mask>         import time\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add     ray.init(num_cpus=10, num_gpus=10, resources={\"custom1\": 2})\n </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_basic.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>     assert d.get(python_descriptor2) == 123\n <mask> \n <mask> \n <mask> def test_ray_options(shutdown_only):\n <mask>     @ray.remote(\n <mask>         num_cpus=2, num_gpus=3, memory=150 * 2**20, resources={\"custom1\": 1})\n <mask>     def foo():\n <mask>         import time\n <mask>         # Sleep for a heartbeat period to ensure resources changing reported.\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove     ray.init(num_cpus=10, num_gpus=10, resources={\"custom1\": 2})\n\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"internal api\")\n </s> add  </s> remove     def remote(self, function_or_class, *args, **kwargs):\n        # TODO(barakmich): Arguments to ray.remote\n        # get captured here.\n        if (inspect.isfunction(function_or_class)\n                or is_cython(function_or_class)):\n            return ClientRemoteFunc(function_or_class)\n        elif inspect.isclass(function_or_class):\n            return ClientActorClass(function_or_class)\n        else:\n            raise TypeError(\"The @ray.remote decorator must be applied to \"\n                            \"either a function or to a class.\")\n </s> add     def remote(self, *args, **kwargs):\n        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n            # This is the case where the decorator is just @ray.remote.\n            return remote_decorator(options=None)(args[0])\n        error_string = (\"The @ray.remote decorator must be applied either \"\n                        \"with no arguments and no parentheses, for example \"\n                        \"'@ray.remote', or it must be applied using some of \"\n                        \"the arguments 'num_returns', 'num_cpus', 'num_gpus', \"\n                        \"'memory', 'object_store_memory', 'resources', \"\n                        \"'max_calls', or 'max_restarts', like \"\n                        \"'@ray.remote(num_returns=2, \"\n                        \"resources={\\\"CustomResource\\\": 1})'.\")\n        assert len(args) == 0 and len(kwargs) > 0, error_string\n        return remote_decorator(options=kwargs)", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_basic.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         # Sleep for a heartbeat period to ensure resources changing reported.\n <mask>         time.sleep(0.1)\n <mask>         return ray.available_resources()\n <mask> \n <mask>     ray.init(num_cpus=10, num_gpus=10, resources={\"custom1\": 2})\n <mask> \n <mask>     without_options = ray.get(foo.remote())\n <mask>     with_options = ray.get(\n <mask>         foo.options(\n <mask>             num_cpus=3,\n <mask>             num_gpus=4,\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add     ray.init(num_cpus=10, num_gpus=10, resources={\"custom1\": 2})\n </s> remove     def call_remote(self, instance, *args, **kwargs) -> bytes:\n </s> add     def call_remote(self, instance, *args, **kwargs) -> List[bytes]: </s> add // A message representing the valid options to modify a task exectution\n//\n// TODO(barakmich): In the longer term, if everything were a client,\n// this message could be the actual standard for which options are\n// allowed in the API. Today, however, it's a bit flexible and defined in the\n// Python code. So for now, it's a stand-in message with a json field, but\n// this is forwards-compatible with deprecating that field and instituting\n// strongly defined and typed fields, without migrating the original ClientTask.\nmessage TaskOptions {\n  string json_options = 1;\n}\n </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove     def remote(self, function_or_class, *args, **kwargs):\n        # TODO(barakmich): Arguments to ray.remote\n        # get captured here.\n        if (inspect.isfunction(function_or_class)\n                or is_cython(function_or_class)):\n            return ClientRemoteFunc(function_or_class)\n        elif inspect.isclass(function_or_class):\n            return ClientActorClass(function_or_class)\n        else:\n            raise TypeError(\"The @ray.remote decorator must be applied to \"\n                            \"either a function or to a class.\")\n </s> add     def remote(self, *args, **kwargs):\n        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n            # This is the case where the decorator is just @ray.remote.\n            return remote_decorator(options=None)(args[0])\n        error_string = (\"The @ray.remote decorator must be applied either \"\n                        \"with no arguments and no parentheses, for example \"\n                        \"'@ray.remote', or it must be applied using some of \"\n                        \"the arguments 'num_returns', 'num_cpus', 'num_gpus', \"\n                        \"'memory', 'object_store_memory', 'resources', \"\n                        \"'max_calls', or 'max_restarts', like \"\n                        \"'@ray.remote(num_returns=2, \"\n                        \"resources={\\\"CustomResource\\\": 1})'.\")\n        assert len(args) == 0 and len(kwargs) > 0, error_string\n        return remote_decorator(options=kwargs) </s> remove   // A name parameter, if the payload can be called in more than one way (like a method on\n  // a payload object).\n </s> add   // A name parameter, if the payload can be called in more than one way\n  // (like a method on a payload object).", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_basic.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     result = ray.get(result)\n <mask>     assert result == [x * 2 for x in range(100)]\n <mask> \n <mask> \n <mask> @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n <mask> def test_actor_concurrent(ray_start_regular_shared):\n <mask>     @ray.remote\n <mask>     class Batcher:\n <mask>         def __init__(self):\n <mask>             self.batch = []\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add @pytest.mark.skipif(client_test_enabled(), reason=\"ray.method unimplemented\") </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add  </s> remove @pytest.mark.skipif(client_test_enabled(), reason=\"remote args\")\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "python/ray/tests/test_basic_2.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>   // How to decode this data blob.\n <mask>   Type type = 4;\n <mask> }\n <mask> \n <mask> // Represents one unit of work to be executed by the server.\n <mask> message ClientTask {\n <mask>   enum RemoteExecType {\n <mask>     FUNCTION = 0;\n <mask>     ACTOR = 1;\n <mask>     METHOD = 2;\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove   // A name parameter, if the payload can be called in more than one way (like a method on\n  // a payload object).\n </s> add   // A name parameter, if the payload can be called in more than one way\n  // (like a method on a payload object). </s> remove   // A reference to the returned value from the execution.\n  bytes return_id = 2;\n </s> add   // A reference to the returned values from the execution.\n  repeated bytes return_ids = 2; </s> remove   // The ID of the client namespace associated with the Datapath stream making this\n  // request.\n </s> add   // The ID of the client namespace associated with the Datapath stream\n  // making this request. </s> add   // Options for modifying the remote task execution environment.\n  TaskOptions options = 7;\n  // Options passed to create the default remote task excution environment.\n  TaskOptions baseline_options = 8; </s> remove         if self._ref is None:\n            # While calling ray.put() on our function, if\n            # our function is recursive, it will attempt to\n            # encode the ClientRemoteFunc -- itself -- and\n            # infinitely recurse on _ensure_ref.\n            #\n            # So we set the state of the reference to be an\n            # in-progress self reference value, which\n            # the encoding can detect and handle correctly.\n            self._ref = SelfReferenceSentinel()\n            self._ref = ray.put(self._func)\n </s> add         with self._lock:\n            if self._ref is None:\n                # While calling ray.put() on our function, if\n                # our function is recursive, it will attempt to\n                # encode the ClientRemoteFunc -- itself -- and\n                # infinitely recurse on _ensure_ref.\n                #\n                # So we set the state of the reference to be an\n                # in-progress self reference value, which\n                # the encoding can detect and handle correctly.\n                self._ref = SelfReferenceSentinel()\n                self._ref = ray.put(self._func) </s> remove     def __init__(self, actor_cls):\n </s> add     def __init__(self, actor_cls, options=None):", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "src/ray/protobuf/ray_client.proto"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>     STATIC_METHOD = 3;\n <mask>   }\n <mask>   // Which type of work this request represents.\n <mask>   RemoteExecType type = 1;\n <mask>   // A name parameter, if the payload can be called in more than one way (like a method on\n <mask>   // a payload object).\n <mask>   string name = 2;\n <mask>   // A reference to the payload.\n <mask>   bytes payload_id = 3;\n <mask>   // Positional parameters to pass to this call.\n <mask>   repeated Arg args = 4;\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove   // The ID of the client namespace associated with the Datapath stream making this\n  // request.\n </s> add   // The ID of the client namespace associated with the Datapath stream\n  // making this request. </s> remove   // A reference to the returned value from the execution.\n  bytes return_id = 2;\n </s> add   // A reference to the returned values from the execution.\n  repeated bytes return_ids = 2; </s> add // A message representing the valid options to modify a task exectution\n//\n// TODO(barakmich): In the longer term, if everything were a client,\n// this message could be the actual standard for which options are\n// allowed in the API. Today, however, it's a bit flexible and defined in the\n// Python code. So for now, it's a stand-in message with a json field, but\n// this is forwards-compatible with deprecating that field and instituting\n// strongly defined and typed fields, without migrating the original ClientTask.\nmessage TaskOptions {\n  string json_options = 1;\n}\n </s> add   // Options for modifying the remote task execution environment.\n  TaskOptions options = 7;\n  // Options passed to create the default remote task excution environment.\n  TaskOptions baseline_options = 8; </s> remove         if self._ref is None:\n            # While calling ray.put() on our function, if\n            # our function is recursive, it will attempt to\n            # encode the ClientRemoteFunc -- itself -- and\n            # infinitely recurse on _ensure_ref.\n            #\n            # So we set the state of the reference to be an\n            # in-progress self reference value, which\n            # the encoding can detect and handle correctly.\n            self._ref = SelfReferenceSentinel()\n            self._ref = ray.put(self._func)\n </s> add         with self._lock:\n            if self._ref is None:\n                # While calling ray.put() on our function, if\n                # our function is recursive, it will attempt to\n                # encode the ClientRemoteFunc -- itself -- and\n                # infinitely recurse on _ensure_ref.\n                #\n                # So we set the state of the reference to be an\n                # in-progress self reference value, which\n                # the encoding can detect and handle correctly.\n                self._ref = SelfReferenceSentinel()\n                self._ref = ray.put(self._func) </s> add         set_task_options(task, self._options, \"baseline_options\")", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "src/ray/protobuf/ray_client.proto"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>   // Positional parameters to pass to this call.\n <mask>   repeated Arg args = 4;\n <mask>   // Keyword parameters to pass to this call.\n <mask>   map<string, Arg> kwargs = 5;\n <mask>   // The ID of the client namespace associated with the Datapath stream making this\n <mask>   // request.\n <mask>   string client_id = 6;\n <mask> }\n <mask> \n <mask> message ClientTaskTicket {\n <mask>   // Was the task successful?\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add   // Options for modifying the remote task execution environment.\n  TaskOptions options = 7;\n  // Options passed to create the default remote task excution environment.\n  TaskOptions baseline_options = 8; </s> remove   // A name parameter, if the payload can be called in more than one way (like a method on\n  // a payload object).\n </s> add   // A name parameter, if the payload can be called in more than one way\n  // (like a method on a payload object). </s> remove   // A reference to the returned value from the execution.\n  bytes return_id = 2;\n </s> add   // A reference to the returned values from the execution.\n  repeated bytes return_ids = 2; </s> add // A message representing the valid options to modify a task exectution\n//\n// TODO(barakmich): In the longer term, if everything were a client,\n// this message could be the actual standard for which options are\n// allowed in the API. Today, however, it's a bit flexible and defined in the\n// Python code. So for now, it's a stand-in message with a json field, but\n// this is forwards-compatible with deprecating that field and instituting\n// strongly defined and typed fields, without migrating the original ClientTask.\nmessage TaskOptions {\n  string json_options = 1;\n}\n </s> remove         if self._ref is None:\n            # While calling ray.put() on our function, if\n            # our function is recursive, it will attempt to\n            # encode the ClientRemoteFunc -- itself -- and\n            # infinitely recurse on _ensure_ref.\n            #\n            # So we set the state of the reference to be an\n            # in-progress self reference value, which\n            # the encoding can detect and handle correctly.\n            self._ref = SelfReferenceSentinel()\n            self._ref = ray.put(self._func)\n </s> add         with self._lock:\n            if self._ref is None:\n                # While calling ray.put() on our function, if\n                # our function is recursive, it will attempt to\n                # encode the ClientRemoteFunc -- itself -- and\n                # infinitely recurse on _ensure_ref.\n                #\n                # So we set the state of the reference to be an\n                # in-progress self reference value, which\n                # the encoding can detect and handle correctly.\n                self._ref = SelfReferenceSentinel()\n                self._ref = ray.put(self._func) </s> remove     def __init__(self, actor_cls):\n </s> add     def __init__(self, actor_cls, options=None):", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "src/ray/protobuf/ray_client.proto"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>   map<string, Arg> kwargs = 5;\n <mask>   // The ID of the client namespace associated with the Datapath stream\n <mask>   // making this request.\n <mask>   string client_id = 6;\n <mask> }\n <mask> \n <mask> message ClientTaskTicket {\n <mask>   // Was the task successful?\n <mask>   bool valid = 1;\n <mask>   // A reference to the returned values from the execution.\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> remove   // The ID of the client namespace associated with the Datapath stream making this\n  // request.\n </s> add   // The ID of the client namespace associated with the Datapath stream\n  // making this request. </s> remove   // A reference to the returned value from the execution.\n  bytes return_id = 2;\n </s> add   // A reference to the returned values from the execution.\n  repeated bytes return_ids = 2; </s> add // A message representing the valid options to modify a task exectution\n//\n// TODO(barakmich): In the longer term, if everything were a client,\n// this message could be the actual standard for which options are\n// allowed in the API. Today, however, it's a bit flexible and defined in the\n// Python code. So for now, it's a stand-in message with a json field, but\n// this is forwards-compatible with deprecating that field and instituting\n// strongly defined and typed fields, without migrating the original ClientTask.\nmessage TaskOptions {\n  string json_options = 1;\n}\n </s> remove   // A name parameter, if the payload can be called in more than one way (like a method on\n  // a payload object).\n </s> add   // A name parameter, if the payload can be called in more than one way\n  // (like a method on a payload object). </s> remove         if self._ref is None:\n            # While calling ray.put() on our function, if\n            # our function is recursive, it will attempt to\n            # encode the ClientRemoteFunc -- itself -- and\n            # infinitely recurse on _ensure_ref.\n            #\n            # So we set the state of the reference to be an\n            # in-progress self reference value, which\n            # the encoding can detect and handle correctly.\n            self._ref = SelfReferenceSentinel()\n            self._ref = ray.put(self._func)\n </s> add         with self._lock:\n            if self._ref is None:\n                # While calling ray.put() on our function, if\n                # our function is recursive, it will attempt to\n                # encode the ClientRemoteFunc -- itself -- and\n                # infinitely recurse on _ensure_ref.\n                #\n                # So we set the state of the reference to be an\n                # in-progress self reference value, which\n                # the encoding can detect and handle correctly.\n                self._ref = SelfReferenceSentinel()\n                self._ref = ray.put(self._func) </s> remove     def __init__(self, actor_cls):\n </s> add     def __init__(self, actor_cls, options=None):", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "src/ray/protobuf/ray_client.proto"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> message ClientTaskTicket {\n <mask>   // Was the task successful?\n <mask>   bool valid = 1;\n <mask>   // A reference to the returned value from the execution.\n <mask>   bytes return_id = 2;\n <mask>   // If unsuccessful, an encoding of the error.\n <mask>   bytes error = 3;\n <mask> }\n <mask> \n <mask> // Delivers data to the server\n </s> [ray_client] Implement optional arguments to ray.remote() and f.options() (#12985) </s> add   // Options for modifying the remote task execution environment.\n  TaskOptions options = 7;\n  // Options passed to create the default remote task excution environment.\n  TaskOptions baseline_options = 8; </s> remove   // The ID of the client namespace associated with the Datapath stream making this\n  // request.\n </s> add   // The ID of the client namespace associated with the Datapath stream\n  // making this request. </s> remove   // A name parameter, if the payload can be called in more than one way (like a method on\n  // a payload object).\n </s> add   // A name parameter, if the payload can be called in more than one way\n  // (like a method on a payload object). </s> add // A message representing the valid options to modify a task exectution\n//\n// TODO(barakmich): In the longer term, if everything were a client,\n// this message could be the actual standard for which options are\n// allowed in the API. Today, however, it's a bit flexible and defined in the\n// Python code. So for now, it's a stand-in message with a json field, but\n// this is forwards-compatible with deprecating that field and instituting\n// strongly defined and typed fields, without migrating the original ClientTask.\nmessage TaskOptions {\n  string json_options = 1;\n}\n </s> remove         if self._ref is None:\n            # While calling ray.put() on our function, if\n            # our function is recursive, it will attempt to\n            # encode the ClientRemoteFunc -- itself -- and\n            # infinitely recurse on _ensure_ref.\n            #\n            # So we set the state of the reference to be an\n            # in-progress self reference value, which\n            # the encoding can detect and handle correctly.\n            self._ref = SelfReferenceSentinel()\n            self._ref = ray.put(self._func)\n </s> add         with self._lock:\n            if self._ref is None:\n                # While calling ray.put() on our function, if\n                # our function is recursive, it will attempt to\n                # encode the ClientRemoteFunc -- itself -- and\n                # infinitely recurse on _ensure_ref.\n                #\n                # So we set the state of the reference to be an\n                # in-progress self reference value, which\n                # the encoding can detect and handle correctly.\n                self._ref = SelfReferenceSentinel()\n                self._ref = ray.put(self._func) </s> add         set_task_options(task, self._options, \"baseline_options\")", "html_url": "https://github.com/ray-project/ray/commit/80f6dd16b2a7f5e90ff882751c37b7f2e02bd147", "file_name": "src/ray/protobuf/ray_client.proto"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     wait_until_succeeded_without_exception,\n <mask> )\n <mask> from ray.dashboard import dashboard\n <mask> from ray.dashboard.head import DashboardHead\n <mask> from ray.dashboard.modules.dashboard_sdk import DEFAULT_DASHBOARD_ADDRESS\n <mask> from ray.experimental.state.api import StateApiClient\n <mask> from ray.experimental.state.common import ListApiOptions, StateResource\n <mask> from ray.experimental.state.exception import ServerUnavailable\n <mask> from ray.experimental.internal_kv import _initialize_internal_kv\n <mask> from unittest.mock import MagicMock\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     get as state_cli_get,\n    list as state_cli_list,\n    get_api_server_url,\n </s> add     ray_get,\n    ray_list, </s> remove from ray.experimental.state.state_cli import get as cli_get\nfrom ray.experimental.state.state_cli import list as cli_list\n </s> add from ray.experimental.state.state_cli import ray_get\nfrom ray.experimental.state.state_cli import ray_list </s> add     ray_address_to_api_server_url, </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~ </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI </s> remove cli.add_command(state_cli_list)\ncli.add_command(state_cli_get)\n </s> add cli.add_command(ray_list, name=\"list\")\ncli.add_command(ray_get, name=\"get\")", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "dashboard/tests/test_dashboard.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     \"\"\"Check that requests from client fail with minimal installation\"\"\"\n <mask>     response = None\n <mask> \n <mask>     with pytest.raises(ServerUnavailable):\n <mask>         client = StateApiClient(address=DEFAULT_DASHBOARD_ADDRESS)\n <mask>         response = client.list(\n <mask>             StateResource.NODES, options=ListApiOptions(), raise_on_missing_output=False\n <mask>         )\n <mask> \n <mask>     # Response should not be populated\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     client = StateApiClient(\n        address=address if address else get_api_server_url(),\n    )\n </s> add     client = StateApiClient(address=address) </s> remove         # TODO(swang): This command should also support\n        # passing --address or RAY_ADDRESS, like others.\n        address = ray._private.services.canonicalize_bootstrap_address_or_die(None)\n </s> add         address = ray._private.services.canonicalize_bootstrap_address_or_die(address) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\", \"--format=table\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\", \"--format=table\"]) </s> remove         max_concurrent_reqs_error = 0\n        for _ in range(len(procs)):\n            try:\n                res = q.get(timeout=10)\n                if isinstance(res, Exception):\n                    assert False, f\"State API error: {res}\"\n                elif isinstance(res, int):\n                    max_concurrent_reqs_error += res\n                else:\n                    raise ValueError(res)\n            except queue.Empty:\n                assert False, \"Failed to get some results from a subprocess\"\n\n        assert max_concurrent_reqs_error == 0, \"All requests should be successful\"\n        [p.join(5) for p in procs]\n        for proc in procs:\n            assert not proc.is_alive(), \"All processes should exit\"\n </s> add         wait_for_condition(verify) </s> remove     api_server_url = get_api_server_url()\n\n </s> add  </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "dashboard/tests/test_dashboard.py"}
{"docstring_tokens": "keep keep keep replace keep keep keep keep keep keep keep keep replace", "code_tokens": " <mask> .. click:: ray.experimental.state.state_cli:object_summary\n <mask>    :prog: ray summary objects\n <mask> \n <mask> .. click:: ray.experimental.state.state_cli:list\n <mask>    :prog: ray list\n <mask> \n <mask> .. click:: ray.experimental.state.state_cli:get\n <mask>    :prog: ray get\n <mask> \n <mask> .. click:: ray.experimental.state.state_cli:list\n <mask>    :prog: ray list\n <mask> \n <mask> .. click:: ray.experimental.state.state_cli:get\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove .. code-block:: bash\n </s> add     .. code-block:: python </s> remove     # To get callsite info, set env variable `RAY_record_ref_creation_sites=1` when starting the ray cluster\n    # RAY_record_ref_creation_sites=1 ray start --head\n    ray summary objects \n </s> add         from ray.experimental.state.api import summarize_tasks\n        print(summarize_tasks())\n\nE.g., Summarize all objects  \n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. note::\n\n    By default, objects are summarized by callsite. However, callsite is not recorded by Ray by default.\n    To get callsite info, set env variable `RAY_record_ref_creation_sites=1` when starting the ray cluster\n    RAY_record_ref_creation_sites=1 ray start --head\n\n\n.. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray summary objects \n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import summarize_objects\n        print(summarize_objects()) </s> remove .. code-block:: bash\n </s> add .. tabbed:: CLI </s> remove E.g., Summarize all objects (e.g., the total number of objects, size of all objects, etc) \n </s> add .. tabbed:: Python SDK </s> remove     ray list objects -f pid=12345 -f reference_type=LOCAL_REFERENCE\n </s> add .. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list objects -f pid=<PID> -f reference_type=LOCAL_REFERENCE\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_objects \n        list_objects(filters=[(\"pid\", \"=\", <PID>), (\"reference_type\", \"=\", \"LOCAL_REFERENCE\")])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/ray-state-api-reference.rst"}
{"docstring_tokens": "keep replace keep replace keep keep", "code_tokens": " <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     ray summary tasks\n <mask> \n <mask> .. code-block:: text\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove E.g., Summarize all tasks (e.g., task count in different states, type of different tasks, etc)  \n </s> add .. tabbed:: CLI </s> remove     ray summary tasks\n </s> add         ray summary tasks </s> remove .. code-block:: bash\n </s> add     .. code-block:: bash </s> remove .. code-block:: bash\n </s> add .. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list actors </s> remove .. code-block:: bash\n </s> add     .. code-block:: python", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep replace keep replace keep keep keep", "code_tokens": " <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     ray list actors\n <mask> \n <mask> .. code-block:: text\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove .. code-block:: bash\n </s> add .. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list actors\n        # In this case, ACTOR_ID is 31405554844820381c2f0f8501000000\n        ray logs --actor-id <ACTOR_ID> \n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import get_log </s> remove     ray list actors -f state=ALIVE\n </s> add     .. code-block:: bash\n\n        ray list actors -f state=ALIVE\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_actors \n        list_actors(filters=[(\"state\", \"=\", \"ALIVE\")]) </s> remove     ray get actors <ACTOR_ID> # In this case, 31405554844820381c2f0f8501000000\n </s> add  </s> remove .. code-block:: bash\n </s> add .. tabbed:: CLI\n\n    .. code-block:: bash\n\n        # In this case, 31405554844820381c2f0f8501000000\n        ray get actors <ACTOR_ID> \n    \n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import get_actor\n        # In this case, 31405554844820381c2f0f8501000000\n        print(get_actor(id=<ACTOR_ID>)) </s> remove     ray list actors\n    ray logs --actor-id <ACTOR_ID>\n </s> add         # In this case, ACTOR_ID is 31405554844820381c2f0f8501000000\n        for line in get_log(actor_id=<ACTOR_ID>):\n            print(line)", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep keep replace keep replace keep keep keep", "code_tokens": " <mask> \n <mask> You can get the state of a single task using the get API. \n <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     ray get actors <ACTOR_ID> # In this case, 31405554844820381c2f0f8501000000\n <mask> \n <mask> .. code-block:: text\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove .. code-block:: bash\n </s> add .. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list actors\n        # In this case, ACTOR_ID is 31405554844820381c2f0f8501000000\n        ray logs --actor-id <ACTOR_ID> \n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import get_log </s> remove     ray list actors\n    ray logs --actor-id <ACTOR_ID>\n </s> add         # In this case, ACTOR_ID is 31405554844820381c2f0f8501000000\n        for line in get_log(actor_id=<ACTOR_ID>):\n            print(line) </s> remove .. code-block:: bash\n </s> add .. tip:: You can list resources with one or multiple filters: using `--filter` or `-f` </s> remove .. code-block:: bash\n </s> add         from ray.experimental.state.api import summarize_actors\n        print(summarize_actors()) </s> remove E.g., Summarize all objects (e.g., the total number of objects, size of all objects, etc) \n </s> add .. tabbed:: Python SDK", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep replace keep replace replace keep keep keep", "code_tokens": " <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     ray list actors\n <mask>     ray logs --actor-id <ACTOR_ID>\n <mask> \n <mask> .. code-block:: text\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     ray get actors <ACTOR_ID> # In this case, 31405554844820381c2f0f8501000000\n </s> add  </s> remove .. code-block:: bash\n </s> add .. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list actors </s> remove .. code-block:: bash\n </s> add .. tabbed:: CLI\n\n    .. code-block:: bash\n\n        # In this case, 31405554844820381c2f0f8501000000\n        ray get actors <ACTOR_ID> \n    \n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import get_actor\n        # In this case, 31405554844820381c2f0f8501000000\n        print(get_actor(id=<ACTOR_ID>)) </s> remove     ray list actors\n </s> add .. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_actors \n        print(list_actors()) </s> remove     ray list actors -f state=ALIVE\n </s> add     .. code-block:: bash\n\n        ray list actors -f state=ALIVE\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_actors \n        list_actors(filters=[(\"state\", \"=\", \"ALIVE\")])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep keep keep replace keep replace keep keep keep keep", "code_tokens": " <mask> It is recommended to start monitoring states through summary APIs first. When you find anomalies\n <mask> (e.g., actors running for a long time, tasks that are not scheduled for a long time),\n <mask> you can use ``list`` or ``get`` APIs to get more details for an individual abnormal resource.\n <mask> \n <mask> E.g., Summarize all actors (e.g., number of alive actors, different actor classes, etc)\n <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     ray summary actors\n <mask> \n <mask> E.g., Summarize all tasks (e.g., task count in different states, type of different tasks, etc)  \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     ray summary actors\n </s> add E.g., Summarize all tasks  \n~~~~~~~~~~~~~~~~~~~~~~~~~ </s> remove .. code-block:: bash\n </s> add     .. code-block:: bash </s> remove     ray summary tasks\n </s> add         ray summary tasks </s> remove E.g., Summarize all tasks (e.g., task count in different states, type of different tasks, etc)  \n </s> add .. tabbed:: CLI </s> remove .. code-block:: bash\n </s> add     .. code-block:: python", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep keep replace keep keep keep keep keep keep keep replace keep", "code_tokens": " <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     ray summary actors\n <mask> \n <mask> E.g., Summarize all tasks (e.g., task count in different states, type of different tasks, etc)  \n <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     ray summary actors\n <mask> \n <mask> E.g., Summarize all tasks (e.g., task count in different states, type of different tasks, etc)  \n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove .. code-block:: bash\n </s> add     .. code-block:: bash </s> remove     ray summary tasks\n </s> add         ray summary tasks </s> remove .. code-block:: bash\n </s> add         from ray.experimental.state.api import summarize_actors\n        print(summarize_actors()) </s> remove E.g., Summarize all actors (e.g., number of alive actors, different actor classes, etc)\n </s> add E.g., Summarize all actors\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray summary actors\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python </s> remove .. code-block:: bash\n </s> add     .. code-block:: python", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep keep replace keep replace keep", "code_tokens": " <mask> \n <mask> E.g., Summarize all tasks (e.g., task count in different states, type of different tasks, etc)  \n <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     ray summary tasks\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     ray summary actors\n </s> add E.g., Summarize all tasks  \n~~~~~~~~~~~~~~~~~~~~~~~~~ </s> remove E.g., Summarize all tasks (e.g., task count in different states, type of different tasks, etc)  \n </s> add .. tabbed:: CLI </s> remove .. code-block:: bash\n </s> add         from ray.experimental.state.api import summarize_actors\n        print(summarize_actors()) </s> remove E.g., Summarize all actors (e.g., number of alive actors, different actor classes, etc)\n </s> add E.g., Summarize all actors\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray summary actors\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python </s> remove .. code-block:: bash\n </s> add     .. code-block:: python", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep keep keep replace keep replace keep keep keep", "code_tokens": " <mask> .. code-block:: bash\n <mask> \n <mask>     ray summary tasks\n <mask> \n <mask> E.g., Summarize all objects (e.g., the total number of objects, size of all objects, etc) \n <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     # To get callsite info, set env variable `RAY_record_ref_creation_sites=1` when starting the ray cluster\n <mask>     # RAY_record_ref_creation_sites=1 ray start --head\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     # To get callsite info, set env variable `RAY_record_ref_creation_sites=1` when starting the ray cluster\n    # RAY_record_ref_creation_sites=1 ray start --head\n    ray summary objects \n </s> add         from ray.experimental.state.api import summarize_tasks\n        print(summarize_tasks())\n\nE.g., Summarize all objects  \n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. note::\n\n    By default, objects are summarized by callsite. However, callsite is not recorded by Ray by default.\n    To get callsite info, set env variable `RAY_record_ref_creation_sites=1` when starting the ray cluster\n    RAY_record_ref_creation_sites=1 ray start --head\n\n\n.. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray summary objects \n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import summarize_objects\n        print(summarize_objects()) </s> remove     ray summary tasks\n </s> add         ray summary tasks </s> remove .. code-block:: bash\n </s> add     .. code-block:: bash </s> remove     ray summary actors\n </s> add E.g., Summarize all tasks  \n~~~~~~~~~~~~~~~~~~~~~~~~~ </s> remove .. code-block:: bash\n </s> add         from ray.experimental.state.api import summarize_actors\n        print(summarize_actors())", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask> E.g., Summarize all objects (e.g., the total number of objects, size of all objects, etc) \n <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     # To get callsite info, set env variable `RAY_record_ref_creation_sites=1` when starting the ray cluster\n <mask>     # RAY_record_ref_creation_sites=1 ray start --head\n <mask>     ray summary objects \n <mask> \n <mask> List\n <mask> ----\n <mask> \n <mask> Get a list of resources, possible resources include: \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove .. code-block:: bash\n </s> add     .. code-block:: python </s> remove E.g., Summarize all objects (e.g., the total number of objects, size of all objects, etc) \n </s> add .. tabbed:: Python SDK </s> remove .. code-block:: bash\n </s> add     .. code-block:: bash </s> remove     ray summary tasks\n </s> add         ray summary tasks </s> remove     ray summary actors\n </s> add E.g., Summarize all tasks  \n~~~~~~~~~~~~~~~~~~~~~~~~~ </s> remove .. code-block:: bash\n </s> add         from ray.experimental.state.api import summarize_actors\n        print(summarize_actors())", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep replace replace replace replace replace replace replace replace keep replace keep", "code_tokens": " <mask> Get a list of resources, possible resources include: \n <mask> \n <mask> - :ref:`Actors <actor-guide>`\n <mask> - :ref:`Tasks <ray-remote-functions>`\n <mask> - :ref:`Objects <objects-in-ray>`\n <mask> - :ref:`Jobs <jobs-overview>`\n <mask> - :ref:`Placement Groups <ray-placement-group-doc-ref>`\n <mask> - Nodes (Ray worker nodes)\n <mask> - Workers (Ray worker processes)\n <mask> - :ref:`Runtime environments <runtime-environments>`\n <mask> \n <mask> E.g., List all nodes\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove .. code-block:: bash\n </s> add         ray list nodes  </s> remove         lambda: verify_output(cli_get, [\"objects\", obj.hex()], [\"object_id\", obj.hex()])\n </s> add         lambda: verify_output(ray_get, [\"objects\", obj.hex()], [\"object_id\", obj.hex()])\n    )\n\n    # Test address flag auto detection\n    wait_for_condition(\n        lambda: verify_output(\n            ray_get,\n            [\"objects\", obj.hex(), \"--address\", \"auto\"],\n            [\"object_id\", obj.hex()],\n        )\n    )\n    wait_for_condition(\n        lambda: verify_output(\n            ray_list, [\"tasks\", \"--address\", \"auto\"], [\"Stats:\", \"Table:\", \"TASK_ID\"]\n        ) </s> remove     # To get callsite info, set env variable `RAY_record_ref_creation_sites=1` when starting the ray cluster\n    # RAY_record_ref_creation_sites=1 ray start --head\n    ray summary objects \n </s> add         from ray.experimental.state.api import summarize_tasks\n        print(summarize_tasks())\n\nE.g., Summarize all objects  \n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. note::\n\n    By default, objects are summarized by callsite. However, callsite is not recorded by Ray by default.\n    To get callsite info, set env variable `RAY_record_ref_creation_sites=1` when starting the ray cluster\n    RAY_record_ref_creation_sites=1 ray start --head\n\n\n.. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray summary objects \n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import summarize_objects\n        print(summarize_objects()) </s> remove .. code-block:: bash\n </s> add         from ray.experimental.state.api import list_nodes() \n        list_nodes() </s> remove     ray list nodes \n </s> add .. tabbed:: Python SDK", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep keep replace keep replace keep", "code_tokens": " <mask> \n <mask> E.g., List all nodes\n <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     ray list nodes \n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove E.g., List all nodes\n </s> add     .. code-block:: bash </s> remove E.g., List all placement groups\n </s> add     .. code-block:: python </s> remove .. code-block:: bash\n </s> add         from ray.experimental.state.api import list_nodes() \n        list_nodes() </s> remove     ray list placement-groups\n </s> add E.g., List all placement groups \n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list placement-groups\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_placement_groups \n        list_placement_groups() </s> remove - :ref:`Actors <actor-guide>`\n- :ref:`Tasks <ray-remote-functions>`\n- :ref:`Objects <objects-in-ray>`\n- :ref:`Jobs <jobs-overview>`\n- :ref:`Placement Groups <ray-placement-group-doc-ref>`\n- Nodes (Ray worker nodes)\n- Workers (Ray worker processes)\n- :ref:`Runtime environments <runtime-environments>`\n </s> add - :ref:`Actors <actor-guide>`, e.g., actor id, state, pid, death_cause. (:ref:`output schema <state-api-schema-actor>`)\n- :ref:`Tasks <ray-remote-functions>`, e.g., name, scheduling state, type, runtime env info (:ref:`output schema <state-api-schema-task>`)\n- :ref:`Objects <objects-in-ray>`, e.g., object id, callsites, reference types. (:ref:`output schema <state-api-schema-obj>`)\n- :ref:`Jobs <jobs-overview>`, e.g., start/end time, entrypoint, status. (:ref:`output schema <state-api-schema-job>`)\n- :ref:`Placement Groups <ray-placement-group-doc-ref>`, e.g., name, bundles, stats. (:ref:`output schema <state-api-schema-pg>`)\n- Nodes (Ray worker nodes), e.g., node id, node ip, node state. (:ref:`output schema <state-api-schema-node>`)\n- Workers (Ray worker processes), e.g., worker id, type, exit type and details. (:ref:`output schema <state-api-schema-worker>`)\n- :ref:`Runtime environments <runtime-environments>`, e.g., runtime envs, creation time, nodes (:ref:`output schema <state-api-schema-runtime-env>`)\n\nE.g., List all nodes \n~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep replace keep replace", "code_tokens": " <mask>     ray list nodes \n <mask> \n <mask> E.g., List all placement groups\n <mask> \n <mask> .. code-block:: bash\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove .. code-block:: bash\n </s> add         ray list nodes  </s> remove     ray list nodes \n </s> add .. tabbed:: Python SDK </s> remove     ray list placement-groups\n </s> add E.g., List all placement groups \n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list placement-groups\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_placement_groups \n        list_placement_groups() </s> remove E.g., List all nodes\n </s> add     .. code-block:: bash </s> remove             cli_get,\n </s> add             ray_get,", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep replace keep replace keep keep keep keep", "code_tokens": " <mask> .. code-block:: bash\n <mask> \n <mask>     ray list placement-groups\n <mask> \n <mask> You can list resources with one or multiple filters.\n <mask>  \n <mask> E.g., List local referenced objects created by a process\n <mask> \n <mask> .. code-block:: bash\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove .. code-block:: bash\n </s> add .. tip:: You can list resources with one or multiple filters: using `--filter` or `-f` </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ </s> remove .. code-block:: bash\n </s> add         from ray.experimental.state.api import list_nodes() \n        list_nodes() </s> remove     ray list objects -f pid=12345 -f reference_type=LOCAL_REFERENCE\n </s> add .. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list objects -f pid=<PID> -f reference_type=LOCAL_REFERENCE\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_objects \n        list_objects(filters=[(\"pid\", \"=\", <PID>), (\"reference_type\", \"=\", \"LOCAL_REFERENCE\")]) </s> remove E.g., List all placement groups\n </s> add     .. code-block:: python", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask>         list_placement_groups()\n <mask> \n <mask>  \n <mask> E.g., List local referenced objects created by a process\n <mask> \n <mask> .. tip:: You can list resources with one or multiple filters: using `--filter` or `-f`\n <mask> \n <mask> .. tabbed:: CLI\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove .. code-block:: bash\n </s> add .. tip:: You can list resources with one or multiple filters: using `--filter` or `-f` </s> remove You can list resources with one or multiple filters.\n </s> add  </s> remove     ray list placement-groups\n </s> add E.g., List all placement groups \n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list placement-groups\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_placement_groups \n        list_placement_groups() </s> remove .. code-block:: bash\n </s> add         from ray.experimental.state.api import list_nodes() \n        list_nodes() </s> remove     ray list objects -f pid=12345 -f reference_type=LOCAL_REFERENCE\n </s> add .. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list objects -f pid=<PID> -f reference_type=LOCAL_REFERENCE\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_objects \n        list_objects(filters=[(\"pid\", \"=\", <PID>), (\"reference_type\", \"=\", \"LOCAL_REFERENCE\")]) </s> remove     # To get callsite info, set env variable `RAY_record_ref_creation_sites=1` when starting the ray cluster\n    # RAY_record_ref_creation_sites=1 ray start --head\n    ray summary objects \n </s> add         from ray.experimental.state.api import summarize_tasks\n        print(summarize_tasks())\n\nE.g., Summarize all objects  \n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. note::\n\n    By default, objects are summarized by callsite. However, callsite is not recorded by Ray by default.\n    To get callsite info, set env variable `RAY_record_ref_creation_sites=1` when starting the ray cluster\n    RAY_record_ref_creation_sites=1 ray start --head\n\n\n.. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray summary objects \n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import summarize_objects\n        print(summarize_objects())", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep replace keep replace", "code_tokens": " <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     ray list objects -f pid=12345 -f reference_type=LOCAL_REFERENCE\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove .. code-block:: bash\n </s> add .. tabbed:: CLI </s> remove .. code-block:: bash\n </s> add     .. code-block:: bash </s> remove .. code-block:: bash\n </s> add .. tabbed:: Python SDK </s> remove     ray list tasks -f scheduling_state=RUNNING\n </s> add         ray list tasks -f scheduling_state=RUNNING\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"=\", \"RUNNING\")]) </s> remove     ray list actors -f state=ALIVE\n </s> add     .. code-block:: bash\n\n        ray list actors -f state=ALIVE\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_actors \n        list_actors(filters=[(\"state\", \"=\", \"ALIVE\")])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask>         from ray.experimental.state.api import list_objects \n <mask>         list_objects(filters=[(\"pid\", \"=\", <PID>), (\"reference_type\", \"=\", \"LOCAL_REFERENCE\")])\n <mask> \n <mask> E.g., List alive actors\n <mask> \n <mask> .. tabbed:: CLI\n <mask> \n <mask>     .. code-block:: bash\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     ray list objects -f pid=12345 -f reference_type=LOCAL_REFERENCE\n </s> add .. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list objects -f pid=<PID> -f reference_type=LOCAL_REFERENCE\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_objects \n        list_objects(filters=[(\"pid\", \"=\", <PID>), (\"reference_type\", \"=\", \"LOCAL_REFERENCE\")]) </s> remove     ray list actors -f state=ALIVE\n </s> add     .. code-block:: bash\n\n        ray list actors -f state=ALIVE\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_actors \n        list_actors(filters=[(\"state\", \"=\", \"ALIVE\")]) </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list tasks -f scheduling_state=RUNNING -f name=\"task_running_300_seconds()\"\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python </s> remove     ray list tasks -f scheduling_state=RUNNING\n </s> add         ray list tasks -f scheduling_state=RUNNING\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"=\", \"RUNNING\")]) </s> remove .. code-block:: bash\n </s> add .. tabbed:: CLI", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep replace keep replace keep keep", "code_tokens": " <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     ray list actors -f state=ALIVE\n <mask> \n <mask> E.g., List running tasks\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove .. code-block:: bash\n </s> add     .. code-block:: bash </s> remove .. code-block:: bash\n </s> add .. tabbed:: Python SDK </s> remove     ray list tasks -f scheduling_state=RUNNING\n </s> add         ray list tasks -f scheduling_state=RUNNING\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"=\", \"RUNNING\")]) </s> remove     ray list tasks -f shceduling_state!=RUNNING\n </s> add     .. code-block:: python\n\n        from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"!=\", \"RUNNING\")]) </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>         from ray.experimental.state.api import list_actors \n <mask>         list_actors(filters=[(\"state\", \"=\", \"ALIVE\")])\n <mask> \n <mask> E.g., List running tasks\n <mask> \n <mask>     .. code-block:: bash\n <mask> \n <mask>         ray list tasks -f scheduling_state=RUNNING\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     ray list actors -f state=ALIVE\n </s> add     .. code-block:: bash\n\n        ray list actors -f state=ALIVE\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_actors \n        list_actors(filters=[(\"state\", \"=\", \"ALIVE\")]) </s> remove     ray list tasks -f scheduling_state=RUNNING\n </s> add         ray list tasks -f scheduling_state=RUNNING\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"=\", \"RUNNING\")]) </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list tasks -f scheduling_state=RUNNING -f name=\"task_running_300_seconds()\"\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python </s> remove .. code-block:: bash\n </s> add         from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"=\", \"RUNNING\"), (\"name\", \"=\", \"task_running_300_seconds()\")]) </s> remove .. code-block:: bash\n </s> add     .. code-block:: bash </s> remove .. code-block:: bash\n </s> add .. tabbed:: Python SDK", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep keep replace keep replace keep keep keep keep", "code_tokens": " <mask> \n <mask> E.g., List running tasks\n <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     ray list tasks -f scheduling_state=RUNNING\n <mask> \n <mask> E.g., List non-running tasks\n <mask> \n <mask> .. code-block:: bash\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove .. code-block:: bash\n </s> add .. tabbed:: Python SDK </s> remove     ray list tasks -f shceduling_state!=RUNNING\n </s> add     .. code-block:: python\n\n        from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"!=\", \"RUNNING\")]) </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list tasks -f scheduling_state!=RUNNING </s> remove .. code-block:: bash\n </s> add .. tabbed:: CLI", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask> \n <mask> E.g., List non-running tasks\n <mask> \n <mask> .. tabbed:: Python SDK\n <mask> \n <mask>     .. code-block:: python\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     ray list tasks -f scheduling_state=RUNNING\n </s> add         ray list tasks -f scheduling_state=RUNNING\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"=\", \"RUNNING\")]) </s> remove .. code-block:: bash\n </s> add .. tabbed:: Python SDK </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list tasks -f scheduling_state=RUNNING -f name=\"task_running_300_seconds()\"\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python </s> remove     ray list actors -f state=ALIVE\n </s> add     .. code-block:: bash\n\n        ray list actors -f state=ALIVE\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_actors \n        list_actors(filters=[(\"state\", \"=\", \"ALIVE\")]) </s> remove     ray list tasks -f shceduling_state!=RUNNING\n </s> add     .. code-block:: python\n\n        from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"!=\", \"RUNNING\")]) </s> remove     ray list nodes \n </s> add .. tabbed:: Python SDK", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep keep keep replace keep replace keep keep keep", "code_tokens": " <mask>     ray list tasks -f scheduling_state=RUNNING\n <mask> \n <mask> E.g., List non-running tasks\n <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     ray list tasks -f shceduling_state!=RUNNING\n <mask> \n <mask> E.g., List running tasks that have a name func\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove .. code-block:: bash\n </s> add         from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"=\", \"RUNNING\"), (\"name\", \"=\", \"task_running_300_seconds()\")]) </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list tasks -f scheduling_state=RUNNING -f name=\"task_running_300_seconds()\"\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python </s> remove .. code-block:: bash\n </s> add     .. code-block:: bash </s> remove     ray list tasks -f scheduling_state=RUNNING\n </s> add         ray list tasks -f scheduling_state=RUNNING\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"=\", \"RUNNING\")]) </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> \n <mask> E.g., List running tasks that have a name func\n <mask> \n <mask>         from ray.experimental.state.api import list_tasks \n <mask>         list_tasks(filters=[(\"scheduling_state\", \"=\", \"RUNNING\"), (\"name\", \"=\", \"task_running_300_seconds()\")])\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove .. code-block:: bash\n </s> add         from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"=\", \"RUNNING\"), (\"name\", \"=\", \"task_running_300_seconds()\")]) </s> remove     ray list tasks -f shceduling_state!=RUNNING\n </s> add     .. code-block:: python\n\n        from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"!=\", \"RUNNING\")]) </s> remove     ray list tasks -f scheduling_state=RUNNING\n </s> add         ray list tasks -f scheduling_state=RUNNING\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"=\", \"RUNNING\")]) </s> remove .. code-block:: bash\n </s> add .. tabbed:: Python SDK </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     ray list tasks -f shceduling_state!=RUNNING\n <mask> \n <mask> E.g., List running tasks that have a name func\n <mask> \n <mask> .. code-block:: bash\n <mask> \n <mask>     ray list tasks -f scheduling_state=RUNNING -f name=func\n <mask> \n <mask> E.g., List tasks with more details. When ``--detail`` is specified, the API can query more data sources to obtain state information in details.\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove .. code-block:: bash\n </s> add .. tabbed:: Python SDK </s> remove     ray list tasks -f shceduling_state!=RUNNING\n </s> add     .. code-block:: python\n\n        from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"!=\", \"RUNNING\")]) </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list tasks -f scheduling_state=RUNNING -f name=\"task_running_300_seconds()\"\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python </s> remove .. code-block:: bash\n </s> add     .. code-block:: bash </s> remove     ray list tasks -f scheduling_state=RUNNING\n </s> add         ray list tasks -f scheduling_state=RUNNING\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"=\", \"RUNNING\")]) </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "doc/source/ray-observability/state/state-api.rst"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     resource = StateResource(resource.replace(\"-\", \"_\"))\n <mask>     format = AvailableFormat(format)\n <mask> \n <mask>     # Create the State API server and put it into context\n <mask>     client = StateApiClient(\n <mask>         address=address if address else get_api_server_url(),\n <mask>     )\n <mask> \n <mask>     filter = [_parse_filter(f) for f in filter]\n <mask> \n <mask>     options = ListApiOptions(\n <mask>         limit=limit,\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove         client = StateApiClient(address=DEFAULT_DASHBOARD_ADDRESS)\n </s> add         client = StateApiClient() </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\", \"--format=table\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\", \"--format=table\"]) </s> remove         # TODO(swang): This command should also support\n        # passing --address or RAY_ADDRESS, like others.\n        address = ray._private.services.canonicalize_bootstrap_address_or_die(None)\n </s> add         address = ray._private.services.canonicalize_bootstrap_address_or_die(address) </s> remove     api_server_url = get_api_server_url()\n\n </s> add  </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\"]) </s> remove     address = address or ctx.obj[\"api_server_url\"]\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/experimental/state/state_cli.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> @click.group(\"summary\")\n <mask> @click.pass_context\n <mask> def summary_state_cli_group(ctx):\n <mask>     \"\"\"Return the summarized information of a given resource.\"\"\"\n <mask>     ctx.ensure_object(dict)\n <mask>     ctx.obj[\"api_server_url\"] = get_api_server_url()\n <mask> \n <mask> \n <mask> @summary_state_cli_group.command(name=\"tasks\")\n <mask> @timeout_option\n <mask> @address_option\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             result = runner.invoke(cli_list, [\"actors\"])\n </s> add             result = runner.invoke(ray_list, [\"actors\"]) </s> remove .. code-block:: bash\n </s> add .. tabbed:: CLI </s> remove     address = address or ctx.obj[\"api_server_url\"]\n </s> add  </s> remove     address = address or ctx.obj[\"api_server_url\"]\n </s> add  </s> remove     address = address or ctx.obj[\"api_server_url\"]\n </s> add  </s> remove def _try_state_query_expect_rate_limit(api_func, res_q, start_q=None):\n </s> add def _try_state_query_expect_rate_limit(api_func, res_q, start_q=None, **kwargs):", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/experimental/state/state_cli.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     Raises:\n <mask>         :ref:`RayStateApiException <state-api-exceptions>`\n <mask>             if the CLI is failed to query the data.\n <mask>     \"\"\"\n <mask>     address = address or ctx.obj[\"api_server_url\"]\n <mask>     print(\n <mask>         format_summary_output(\n <mask>             summarize_tasks(\n <mask>                 address=address,\n <mask>                 timeout=timeout,\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     address = address or ctx.obj[\"api_server_url\"]\n </s> add  </s> remove     address = address or ctx.obj[\"api_server_url\"]\n </s> add  </s> remove     api_server_url = get_api_server_url()\n\n </s> add  </s> remove         # TODO(swang): This command should also support\n        # passing --address or RAY_ADDRESS, like others.\n        address = ray._private.services.canonicalize_bootstrap_address_or_die(None)\n </s> add         address = ray._private.services.canonicalize_bootstrap_address_or_die(address) </s> remove             api_server_url=api_server_url,\n </s> add             address=address, </s> remove     client = StateApiClient(\n        address=address if address else get_api_server_url(),\n    )\n </s> add     client = StateApiClient(address=address)", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/experimental/state/state_cli.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     Raises:\n <mask>         :ref:`RayStateApiException <state-api-exceptions>`\n <mask>             if the CLI is failed to query the data.\n <mask>     \"\"\"\n <mask>     address = address or ctx.obj[\"api_server_url\"]\n <mask>     print(\n <mask>         format_summary_output(\n <mask>             summarize_actors(\n <mask>                 address=address,\n <mask>                 timeout=timeout,\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     address = address or ctx.obj[\"api_server_url\"]\n </s> add  </s> remove     address = address or ctx.obj[\"api_server_url\"]\n </s> add  </s> remove     api_server_url = get_api_server_url()\n\n </s> add  </s> remove         # TODO(swang): This command should also support\n        # passing --address or RAY_ADDRESS, like others.\n        address = ray._private.services.canonicalize_bootstrap_address_or_die(None)\n </s> add         address = ray._private.services.canonicalize_bootstrap_address_or_die(address) </s> remove             api_server_url=api_server_url,\n </s> add             address=address, </s> remove     client = StateApiClient(\n        address=address if address else get_api_server_url(),\n    )\n </s> add     client = StateApiClient(address=address)", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/experimental/state/state_cli.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     Raises:\n <mask>         :ref:`RayStateApiException <state-api-exceptions>`\n <mask>             if the CLI is failed to query the data.\n <mask>     \"\"\"\n <mask>     address = address or ctx.obj[\"api_server_url\"]\n <mask>     print(\n <mask>         format_object_summary_output(\n <mask>             summarize_objects(\n <mask>                 address=address,\n <mask>                 timeout=timeout,\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     address = address or ctx.obj[\"api_server_url\"]\n </s> add  </s> remove     address = address or ctx.obj[\"api_server_url\"]\n </s> add  </s> remove     api_server_url = get_api_server_url()\n\n </s> add  </s> remove         # TODO(swang): This command should also support\n        # passing --address or RAY_ADDRESS, like others.\n        address = ray._private.services.canonicalize_bootstrap_address_or_die(None)\n </s> add         address = ray._private.services.canonicalize_bootstrap_address_or_die(address) </s> remove             api_server_url=api_server_url,\n </s> add             address=address, </s> remove     client = StateApiClient(\n        address=address if address else get_api_server_url(),\n    )\n </s> add     client = StateApiClient(address=address)", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/experimental/state/state_cli.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask> from ray.experimental.state.common import DEFAULT_RPC_TIMEOUT, DEFAULT_LOG_LIMIT\n <mask> from ray.util.annotations import PublicAPI\n <mask> \n <mask> from ray.experimental.state.state_cli import (\n <mask>     get as state_cli_get,\n <mask>     list as state_cli_list,\n <mask>     get_api_server_url,\n <mask>     output_with_format,\n <mask>     summary_state_cli_group,\n <mask>     AvailableFormat,\n <mask> )\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove from ray.experimental.state.state_cli import get as cli_get\nfrom ray.experimental.state.state_cli import list as cli_list\n </s> add from ray.experimental.state.state_cli import ray_get\nfrom ray.experimental.state.state_cli import ray_list </s> add     ray_address_to_api_server_url, </s> remove from ray.dashboard.modules.dashboard_sdk import DEFAULT_DASHBOARD_ADDRESS\n </s> add  </s> remove     import multiprocessing as mp\n    import os\n    import signal\n </s> add     import threading </s> remove             result = runner.invoke(cli_list, [\"placement-groups\"])\n </s> add             result = runner.invoke(ray_list, [\"placement-groups\"]) </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/scripts/scripts.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>         f\"Default is {DEFAULT_RPC_TIMEOUT}. If --follow is specified, \"\n <mask>         \"this option will be ignored.\"\n <mask>     ),\n <mask> )\n <mask> def ray_logs(\n <mask>     glob_filter,\n <mask>     node_ip: str,\n <mask>     node_id: str,\n <mask>     pid: str,\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> add                 kwargs={\"timeout\": 6}, </s> remove         api_func()\n </s> add         api_func(**kwargs) </s> remove         # Kill the 3 slow running threads\n        [os.kill(p.pid, signal.SIGKILL) for p in procs]\n        [p.join() for p in procs]\n        for p in procs:\n            assert not p.is_alive(), \"Slow queries should be killed\"\n\n        # Running another 3 should return no error\n        q = mp.Queue()\n        procs = [\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_objects,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_runtime_envs,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_placement_groups,\n                    q,\n                ),\n            ),\n        ]\n </s> add         # Consecutive APIs should be successful after the previous delay ones timeout\n        def verify():\n            assert len(list_objects()) > 0, \"non-delay APIs should be successful\"\n            \"after previous ones timeout\" </s> remove     api_server_url = get_api_server_url()\n\n </s> add  </s> remove def _try_state_query_expect_rate_limit(api_func, res_q, start_q=None):\n </s> add def _try_state_query_expect_rate_limit(api_func, res_q, start_q=None, **kwargs): </s> remove         # TODO(swang): This command should also support\n        # passing --address or RAY_ADDRESS, like others.\n        address = ray._private.services.canonicalize_bootstrap_address_or_die(None)\n </s> add         address = ray._private.services.canonicalize_bootstrap_address_or_die(address)", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/scripts/scripts.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>     tail: int,\n <mask>     interval: float,\n <mask>     timeout: int,\n <mask> ):\n <mask>     \"\"\"Print the log file that matches the GLOB_FILTER.\n <mask> \n <mask>     By default, it prints a list of log files that match the filter.\n <mask>     If there's only 1 match, it will print the log file.\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             api_server_url=api_server_url,\n </s> add             address=address, </s> remove .. code-block:: bash\n </s> add .. tabbed:: CLI </s> add @click.option(\n    \"--address\",\n    default=None,\n    help=(\n        \"The address of Ray API server. If not provided, it will be configured \"\n        \"automatically from querying the GCS server.\"\n    ),\n) </s> remove         # TODO(swang): This command should also support\n        # passing --address or RAY_ADDRESS, like others.\n        address = ray._private.services.canonicalize_bootstrap_address_or_die(None)\n </s> add         address = ray._private.services.canonicalize_bootstrap_address_or_die(address) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\", \"--format=table\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\", \"--format=table\"]) </s> remove     ray summary tasks\n </s> add     .. code-block:: bash\n\n        ray summary tasks\n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import summarize_tasks\n        print(summarize_tasks())", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/scripts/scripts.py"}
{"docstring_tokens": "keep replace replace keep keep replace replace replace keep", "code_tokens": " <mask> \n <mask>     api_server_url = get_api_server_url()\n <mask> \n <mask>     # If both id & ip are not provided, choose a head node as a default.\n <mask>     if node_id is None and node_ip is None:\n <mask>         # TODO(swang): This command should also support\n <mask>         # passing --address or RAY_ADDRESS, like others.\n <mask>         address = ray._private.services.canonicalize_bootstrap_address_or_die(None)\n <mask>         node_ip = address.split(\":\")[0]\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             result = runner.invoke(cli_list, [\"tasks\", \"--timeout=3\"])\n </s> add             result = runner.invoke(ray_list, [\"tasks\", \"--timeout=3\"]) </s> remove             result = runner.invoke(cli_list, [\"tasks\", \"--timeout=3\"])\n </s> add             result = runner.invoke(ray_list, [\"tasks\", \"--timeout=3\"]) </s> remove     address = address or ctx.obj[\"api_server_url\"]\n </s> add  </s> remove     address = address or ctx.obj[\"api_server_url\"]\n </s> add  </s> remove     address = address or ctx.obj[\"api_server_url\"]\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/scripts/scripts.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     # If there's no unique match, try listing logs based on the glob filter.\n <mask>     if not match_unique:\n <mask>         logs = list_logs(\n <mask>             api_server_url=api_server_url,\n <mask>             node_id=node_id,\n <mask>             node_ip=node_ip,\n <mask>             glob_filter=glob_filter,\n <mask>             timeout=timeout,\n <mask>         )\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             api_server_url=api_server_url,\n </s> add             address=address, </s> add     address: Optional[str], </s> remove .. code-block:: bash\n </s> add .. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray list actors\n        # In this case, ACTOR_ID is 31405554844820381c2f0f8501000000\n        ray logs --actor-id <ACTOR_ID> \n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import get_log </s> remove     ray list actors\n    ray logs --actor-id <ACTOR_ID>\n </s> add         # In this case, ACTOR_ID is 31405554844820381c2f0f8501000000\n        for line in get_log(actor_id=<ACTOR_ID>):\n            print(line) </s> remove     address = address or ctx.obj[\"api_server_url\"]\n </s> add  </s> remove     address = address or ctx.obj[\"api_server_url\"]\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/scripts/scripts.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                     \" Use `--tail` flag to toggle. ---\\n\"\n <mask>                 )\n <mask> \n <mask>         for chunk in get_log(\n <mask>             api_server_url=api_server_url,\n <mask>             node_id=node_id,\n <mask>             node_ip=node_ip,\n <mask>             filename=filename,\n <mask>             actor_id=actor_id,\n <mask>             task_id=task_id,\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     ray list actors\n    ray logs --actor-id <ACTOR_ID>\n </s> add         # In this case, ACTOR_ID is 31405554844820381c2f0f8501000000\n        for line in get_log(actor_id=<ACTOR_ID>):\n            print(line) </s> remove             api_server_url=api_server_url,\n </s> add             address=address, </s> remove             result = runner.invoke(cli_list, [\"placement-groups\"])\n </s> add             result = runner.invoke(ray_list, [\"placement-groups\"]) </s> add                 kwargs={\"timeout\": 6}, </s> remove         max_concurrent_reqs_error = 0\n        for _ in range(len(procs)):\n            try:\n                res = q.get(timeout=10)\n                if isinstance(res, Exception):\n                    assert False, f\"State API error: {res}\"\n                elif isinstance(res, int):\n                    max_concurrent_reqs_error += res\n                else:\n                    raise ValueError(res)\n            except queue.Empty:\n                assert False, \"Failed to get some results from a subprocess\"\n\n        assert max_concurrent_reqs_error == 0, \"All requests should be successful\"\n        [p.join(5) for p in procs]\n        for proc in procs:\n            assert not proc.is_alive(), \"All processes should exit\"\n </s> add         wait_for_condition(verify) </s> add @click.option(\n    \"--address\",\n    default=None,\n    help=(\n        \"The address of Ray API server. If not provided, it will be configured \"\n        \"automatically from querying the GCS server.\"\n    ),\n)", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/scripts/scripts.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> cli.add_command(cpp)\n <mask> cli.add_command(disable_usage_stats)\n <mask> cli.add_command(enable_usage_stats)\n <mask> add_command_alias(ray_logs, name=\"logs\", hidden=False)\n <mask> cli.add_command(state_cli_list)\n <mask> cli.add_command(state_cli_get)\n <mask> add_command_alias(summary_state_cli_group, name=\"summary\", hidden=False)\n <mask> \n <mask> try:\n <mask>     from ray.dashboard.modules.job.cli import job_cli_group\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove from ray.dashboard.modules.dashboard_sdk import DEFAULT_DASHBOARD_ADDRESS\n </s> add  </s> remove from ray.experimental.state.state_cli import get as cli_get\nfrom ray.experimental.state.state_cli import list as cli_list\n </s> add from ray.experimental.state.state_cli import ray_get\nfrom ray.experimental.state.state_cli import ray_list </s> remove     get as state_cli_get,\n    list as state_cli_list,\n    get_api_server_url,\n </s> add     ray_get,\n    ray_list, </s> add     ray_address_to_api_server_url, </s> remove         [p.start() for p in procs]\n </s> add             return True </s> remove def _try_state_query_expect_rate_limit(api_func, res_q, start_q=None):\n </s> add def _try_state_query_expect_rate_limit(api_func, res_q, start_q=None, **kwargs):", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/scripts/scripts.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>     SupportedFilterType,\n <mask>     TaskState,\n <mask>     WorkerState,\n <mask>     StateSchema,\n <mask>     state_column,\n <mask> )\n <mask> from ray.experimental.state.exception import DataSourceUnavailable, RayStateApiException\n <mask> from ray.experimental.state.state_cli import (\n <mask>     AvailableFormat,\n <mask>     format_list_api_output,\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     get as state_cli_get,\n    list as state_cli_list,\n    get_api_server_url,\n </s> add     ray_get,\n    ray_list, </s> remove from ray.experimental.state.state_cli import get as cli_get\nfrom ray.experimental.state.state_cli import list as cli_list\n </s> add from ray.experimental.state.state_cli import ray_get\nfrom ray.experimental.state.state_cli import ray_list </s> remove from ray.dashboard.modules.dashboard_sdk import DEFAULT_DASHBOARD_ADDRESS\n </s> add  </s> remove     import multiprocessing as mp\n    import os\n    import signal\n </s> add     import threading </s> remove cli.add_command(state_cli_list)\ncli.add_command(state_cli_get)\n </s> add cli.add_command(ray_list, name=\"list\")\ncli.add_command(ray_get, name=\"get\") </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>     format_list_api_output,\n <mask>     _parse_filter,\n <mask>     summary_state_cli_group,\n <mask> )\n <mask> from ray.experimental.state.state_cli import get as cli_get\n <mask> from ray.experimental.state.state_cli import list as cli_list\n <mask> from ray.experimental.state.state_manager import IdToIpMap, StateDataSourceClient\n <mask> from ray.job_submission import JobSubmissionClient\n <mask> from ray.runtime_env import RuntimeEnv\n <mask> \n <mask> if sys.version_info > (3, 7, 0):\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     get as state_cli_get,\n    list as state_cli_list,\n    get_api_server_url,\n </s> add     ray_get,\n    ray_list, </s> add     ray_address_to_api_server_url, </s> remove from ray.dashboard.modules.dashboard_sdk import DEFAULT_DASHBOARD_ADDRESS\n </s> add  </s> remove     import multiprocessing as mp\n    import os\n    import signal\n </s> add     import threading </s> add ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tabbed:: CLI </s> remove cli.add_command(state_cli_list)\ncli.add_command(state_cli_get)\n </s> add cli.add_command(ray_list, name=\"list\")\ncli.add_command(ray_get, name=\"get\")", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep replace keep keep keep keep keep keep keep keep replace keep", "code_tokens": " <mask> \n <mask>     wait_for_condition(\n <mask>         lambda: verify_output(cli_list, [\"actors\"], [\"Stats:\", \"Table:\", \"ACTOR_ID\"])\n <mask>     )\n <mask>     wait_for_condition(\n <mask>         lambda: verify_output(cli_list, [\"workers\"], [\"Stats:\", \"Table:\", \"WORKER_ID\"])\n <mask>     )\n <mask>     wait_for_condition(\n <mask>         lambda: verify_output(cli_list, [\"actors\"], [\"Stats:\", \"Table:\", \"ACTOR_ID\"])\n <mask>     )\n <mask>     wait_for_condition(\n <mask>         lambda: verify_output(cli_list, [\"workers\"], [\"Stats:\", \"Table:\", \"WORKER_ID\"])\n <mask>     )\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove         lambda: verify_output(cli_list, [\"nodes\"], [\"Stats:\", \"Table:\", \"NODE_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"nodes\"], [\"Stats:\", \"Table:\", \"NODE_ID\"]) </s> remove         lambda: verify_output(cli_list, [\"tasks\"], [\"Stats:\", \"Table:\", \"TASK_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"tasks\"], [\"Stats:\", \"Table:\", \"TASK_ID\"]) </s> remove         lambda: verify_output(cli_list, [\"objects\"], [\"Stats:\", \"Table:\", \"OBJECT_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"objects\"], [\"Stats:\", \"Table:\", \"OBJECT_ID\"]) </s> remove             cli_list, [\"placement-groups\"], [\"Stats:\", \"Table:\", \"PLACEMENT_GROUP_ID\"]\n </s> add             ray_list, [\"placement-groups\"], [\"Stats:\", \"Table:\", \"PLACEMENT_GROUP_ID\"] </s> remove     wait_for_condition(lambda: verify_output(cli_list, [\"jobs\"], [\"raysubmit\"]))\n </s> add     wait_for_condition(lambda: verify_output(ray_list, [\"jobs\"], [\"raysubmit\"]))", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep replace", "code_tokens": " <mask>     wait_for_condition(\n <mask>         lambda: verify_output(cli_list, [\"workers\"], [\"Stats:\", \"Table:\", \"WORKER_ID\"])\n <mask>     )\n <mask>     wait_for_condition(\n <mask>         lambda: verify_output(cli_list, [\"nodes\"], [\"Stats:\", \"Table:\", \"NODE_ID\"])\n <mask>     )\n <mask>     wait_for_condition(\n <mask>         lambda: verify_output(\n <mask>             cli_list, [\"placement-groups\"], [\"Stats:\", \"Table:\", \"PLACEMENT_GROUP_ID\"]\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove         lambda: verify_output(cli_list, [\"workers\"], [\"Stats:\", \"Table:\", \"WORKER_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"workers\"], [\"Stats:\", \"Table:\", \"WORKER_ID\"]) </s> remove     wait_for_condition(lambda: verify_output(cli_list, [\"jobs\"], [\"raysubmit\"]))\n </s> add     wait_for_condition(lambda: verify_output(ray_list, [\"jobs\"], [\"raysubmit\"])) </s> remove         lambda: verify_output(cli_list, [\"actors\"], [\"Stats:\", \"Table:\", \"ACTOR_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"actors\"], [\"Stats:\", \"Table:\", \"ACTOR_ID\"]) </s> remove         lambda: verify_output(cli_list, [\"objects\"], [\"Stats:\", \"Table:\", \"OBJECT_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"objects\"], [\"Stats:\", \"Table:\", \"OBJECT_ID\"]) </s> remove         lambda: verify_output(cli_list, [\"tasks\"], [\"Stats:\", \"Table:\", \"TASK_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"tasks\"], [\"Stats:\", \"Table:\", \"TASK_ID\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep replace keep replace keep keep keep keep", "code_tokens": " <mask>     )\n <mask>     wait_for_condition(lambda: verify_output(cli_list, [\"jobs\"], [\"raysubmit\"]))\n <mask>     wait_for_condition(\n <mask>         lambda: verify_output(cli_list, [\"tasks\"], [\"Stats:\", \"Table:\", \"TASK_ID\"])\n <mask>     )\n <mask>     wait_for_condition(\n <mask>         lambda: verify_output(cli_list, [\"objects\"], [\"Stats:\", \"Table:\", \"OBJECT_ID\"])\n <mask>     )\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             cli_list, [\"placement-groups\"], [\"Stats:\", \"Table:\", \"PLACEMENT_GROUP_ID\"]\n </s> add             ray_list, [\"placement-groups\"], [\"Stats:\", \"Table:\", \"PLACEMENT_GROUP_ID\"] </s> remove         lambda: verify_output(cli_list, [\"objects\"], [\"Stats:\", \"Table:\", \"OBJECT_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"objects\"], [\"Stats:\", \"Table:\", \"OBJECT_ID\"]) </s> remove         lambda: verify_output(cli_list, [\"workers\"], [\"Stats:\", \"Table:\", \"WORKER_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"workers\"], [\"Stats:\", \"Table:\", \"WORKER_ID\"]) </s> remove         lambda: verify_output(cli_list, [\"nodes\"], [\"Stats:\", \"Table:\", \"NODE_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"nodes\"], [\"Stats:\", \"Table:\", \"NODE_ID\"]) </s> remove         lambda: verify_output(cli_list, [\"actors\"], [\"Stats:\", \"Table:\", \"ACTOR_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"actors\"], [\"Stats:\", \"Table:\", \"ACTOR_ID\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep replace keep keep keep replace keep keep keep", "code_tokens": " <mask>     )\n <mask>     wait_for_condition(\n <mask>         lambda: verify_output(cli_list, [\"objects\"], [\"Stats:\", \"Table:\", \"OBJECT_ID\"])\n <mask>     )\n <mask>     wait_for_condition(\n <mask>         lambda: verify_output(\n <mask>             cli_list, [\"runtime-envs\"], [\"Stats:\", \"Table:\", \"RUNTIME_ENV\"]\n <mask>         )\n <mask>     )\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     wait_for_condition(lambda: verify_output(cli_list, [\"jobs\"], [\"raysubmit\"]))\n </s> add     wait_for_condition(lambda: verify_output(ray_list, [\"jobs\"], [\"raysubmit\"])) </s> remove         lambda: verify_output(cli_list, [\"tasks\"], [\"Stats:\", \"Table:\", \"TASK_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"tasks\"], [\"Stats:\", \"Table:\", \"TASK_ID\"]) </s> remove         lambda: verify_output(cli_list, [\"nodes\"], [\"Stats:\", \"Table:\", \"NODE_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"nodes\"], [\"Stats:\", \"Table:\", \"NODE_ID\"]) </s> remove             cli_list, [\"placement-groups\"], [\"Stats:\", \"Table:\", \"PLACEMENT_GROUP_ID\"]\n </s> add             ray_list, [\"placement-groups\"], [\"Stats:\", \"Table:\", \"PLACEMENT_GROUP_ID\"] </s> remove         lambda: verify_output(cli_list, [\"workers\"], [\"Stats:\", \"Table:\", \"WORKER_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"workers\"], [\"Stats:\", \"Table:\", \"WORKER_ID\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     # Test get node by id\n <mask>     nodes = ray.nodes()\n <mask>     wait_for_condition(\n <mask>         lambda: verify_output(\n <mask>             cli_get, [\"nodes\", nodes[0][\"NodeID\"]], [\"node_id\", nodes[0][\"NodeID\"]]\n <mask>         )\n <mask>     )\n <mask>     # Test get workers by id\n <mask>     workers = global_state.workers()\n <mask>     assert len(workers) > 0\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove         lambda: verify_output(cli_get, [\"workers\", worker_id], [\"worker_id\", worker_id])\n </s> add         lambda: verify_output(ray_get, [\"workers\", worker_id], [\"worker_id\", worker_id]) </s> remove             cli_list, [\"runtime-envs\"], [\"Stats:\", \"Table:\", \"RUNTIME_ENV\"]\n </s> add             ray_list, [\"runtime-envs\"], [\"Stats:\", \"Table:\", \"RUNTIME_ENV\"] </s> remove             cli_get,\n </s> add             ray_get, </s> remove             cli_get,\n </s> add             ray_get, </s> remove         lambda: verify_output(cli_get, [\"objects\", obj.hex()], [\"object_id\", obj.hex()])\n </s> add         lambda: verify_output(ray_get, [\"objects\", obj.hex()], [\"object_id\", obj.hex()])\n    )\n\n    # Test address flag auto detection\n    wait_for_condition(\n        lambda: verify_output(\n            ray_get,\n            [\"objects\", obj.hex(), \"--address\", \"auto\"],\n            [\"object_id\", obj.hex()],\n        )\n    )\n    wait_for_condition(\n        lambda: verify_output(\n            ray_list, [\"tasks\", \"--address\", \"auto\"], [\"Stats:\", \"Table:\", \"TASK_ID\"]\n        ) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     workers = global_state.workers()\n <mask>     assert len(workers) > 0\n <mask>     worker_id = list(workers.keys())[0]\n <mask>     wait_for_condition(\n <mask>         lambda: verify_output(cli_get, [\"workers\", worker_id], [\"worker_id\", worker_id])\n <mask>     )\n <mask> \n <mask>     # Test get actors by id\n <mask>     wait_for_condition(\n <mask>         lambda: verify_output(\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             cli_get, [\"nodes\", nodes[0][\"NodeID\"]], [\"node_id\", nodes[0][\"NodeID\"]]\n </s> add             ray_get, [\"nodes\", nodes[0][\"NodeID\"]], [\"node_id\", nodes[0][\"NodeID\"]] </s> remove             cli_get,\n </s> add             ray_get, </s> remove         lambda: verify_output(cli_get, [\"objects\", obj.hex()], [\"object_id\", obj.hex()])\n </s> add         lambda: verify_output(ray_get, [\"objects\", obj.hex()], [\"object_id\", obj.hex()])\n    )\n\n    # Test address flag auto detection\n    wait_for_condition(\n        lambda: verify_output(\n            ray_get,\n            [\"objects\", obj.hex(), \"--address\", \"auto\"],\n            [\"object_id\", obj.hex()],\n        )\n    )\n    wait_for_condition(\n        lambda: verify_output(\n            ray_list, [\"tasks\", \"--address\", \"auto\"], [\"Stats:\", \"Table:\", \"TASK_ID\"]\n        ) </s> remove             cli_get,\n </s> add             ray_get, </s> remove             cli_list, [\"runtime-envs\"], [\"Stats:\", \"Table:\", \"RUNTIME_ENV\"]\n </s> add             ray_list, [\"runtime-envs\"], [\"Stats:\", \"Table:\", \"RUNTIME_ENV\"] </s> remove         lambda: verify_output(cli_list, [\"objects\"], [\"Stats:\", \"Table:\", \"OBJECT_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"objects\"], [\"Stats:\", \"Table:\", \"OBJECT_ID\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     # Test get actors by id\n <mask>     wait_for_condition(\n <mask>         lambda: verify_output(\n <mask>             cli_get,\n <mask>             [\"actors\", actor._actor_id.hex()],\n <mask>             [\"actor_id\", actor._actor_id.hex()],\n <mask>         )\n <mask>     )\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             cli_get,\n </s> add             ray_get, </s> remove             cli_get, [\"nodes\", nodes[0][\"NodeID\"]], [\"node_id\", nodes[0][\"NodeID\"]]\n </s> add             ray_get, [\"nodes\", nodes[0][\"NodeID\"]], [\"node_id\", nodes[0][\"NodeID\"]] </s> remove         lambda: verify_output(cli_get, [\"objects\", obj.hex()], [\"object_id\", obj.hex()])\n </s> add         lambda: verify_output(ray_get, [\"objects\", obj.hex()], [\"object_id\", obj.hex()])\n    )\n\n    # Test address flag auto detection\n    wait_for_condition(\n        lambda: verify_output(\n            ray_get,\n            [\"objects\", obj.hex(), \"--address\", \"auto\"],\n            [\"object_id\", obj.hex()],\n        )\n    )\n    wait_for_condition(\n        lambda: verify_output(\n            ray_list, [\"tasks\", \"--address\", \"auto\"], [\"Stats:\", \"Table:\", \"TASK_ID\"]\n        ) </s> remove         lambda: verify_output(cli_get, [\"workers\", worker_id], [\"worker_id\", worker_id])\n </s> add         lambda: verify_output(ray_get, [\"workers\", worker_id], [\"worker_id\", worker_id]) </s> remove             cli_list, [\"runtime-envs\"], [\"Stats:\", \"Table:\", \"RUNTIME_ENV\"]\n </s> add             ray_list, [\"runtime-envs\"], [\"Stats:\", \"Table:\", \"RUNTIME_ENV\"] </s> remove         lambda: verify_output(cli_list, [\"nodes\"], [\"Stats:\", \"Table:\", \"NODE_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"nodes\"], [\"Stats:\", \"Table:\", \"NODE_ID\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     # Test get placement groups by id\n <mask>     wait_for_condition(\n <mask>         lambda: verify_output(\n <mask>             cli_get,\n <mask>             [\"placement-groups\", pg.id.hex()],\n <mask>             [\"placement_group_id\", pg.id.hex()],\n <mask>         )\n <mask>     )\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             cli_get,\n </s> add             ray_get, </s> remove             cli_get, [\"nodes\", nodes[0][\"NodeID\"]], [\"node_id\", nodes[0][\"NodeID\"]]\n </s> add             ray_get, [\"nodes\", nodes[0][\"NodeID\"]], [\"node_id\", nodes[0][\"NodeID\"]] </s> remove         lambda: verify_output(cli_get, [\"objects\", obj.hex()], [\"object_id\", obj.hex()])\n </s> add         lambda: verify_output(ray_get, [\"objects\", obj.hex()], [\"object_id\", obj.hex()])\n    )\n\n    # Test address flag auto detection\n    wait_for_condition(\n        lambda: verify_output(\n            ray_get,\n            [\"objects\", obj.hex(), \"--address\", \"auto\"],\n            [\"object_id\", obj.hex()],\n        )\n    )\n    wait_for_condition(\n        lambda: verify_output(\n            ray_list, [\"tasks\", \"--address\", \"auto\"], [\"Stats:\", \"Table:\", \"TASK_ID\"]\n        ) </s> remove             cli_list, [\"runtime-envs\"], [\"Stats:\", \"Table:\", \"RUNTIME_ENV\"]\n </s> add             ray_list, [\"runtime-envs\"], [\"Stats:\", \"Table:\", \"RUNTIME_ENV\"] </s> remove         lambda: verify_output(cli_get, [\"workers\", worker_id], [\"worker_id\", worker_id])\n </s> add         lambda: verify_output(ray_get, [\"workers\", worker_id], [\"worker_id\", worker_id]) </s> remove         lambda: verify_output(cli_list, [\"nodes\"], [\"Stats:\", \"Table:\", \"NODE_ID\"])\n </s> add         lambda: verify_output(ray_list, [\"nodes\"], [\"Stats:\", \"Table:\", \"NODE_ID\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     )\n <mask> \n <mask>     # Test get objects by id\n <mask>     wait_for_condition(\n <mask>         lambda: verify_output(cli_get, [\"objects\", obj.hex()], [\"object_id\", obj.hex()])\n <mask>     )\n <mask> \n <mask>     # TODO(rickyyx:alpha-obs):\n <mask>     # - get job by id: jobs is not currently filterable by id\n <mask>     # - get task by id: no easy access to tasks yet\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             cli_get, [\"nodes\", nodes[0][\"NodeID\"]], [\"node_id\", nodes[0][\"NodeID\"]]\n </s> add             ray_get, [\"nodes\", nodes[0][\"NodeID\"]], [\"node_id\", nodes[0][\"NodeID\"]] </s> remove             cli_get,\n </s> add             ray_get, </s> remove             cli_get,\n </s> add             ray_get, </s> remove         lambda: verify_output(cli_get, [\"workers\", worker_id], [\"worker_id\", worker_id])\n </s> add         lambda: verify_output(ray_get, [\"workers\", worker_id], [\"worker_id\", worker_id]) </s> remove             cli_list, [\"runtime-envs\"], [\"Stats:\", \"Table:\", \"RUNTIME_ENV\"]\n </s> add             ray_list, [\"runtime-envs\"], [\"Stats:\", \"Table:\", \"RUNTIME_ENV\"] </s> remove     # To get callsite info, set env variable `RAY_record_ref_creation_sites=1` when starting the ray cluster\n    # RAY_record_ref_creation_sites=1 ray start --head\n    ray summary objects \n </s> add         from ray.experimental.state.api import summarize_tasks\n        print(summarize_tasks())\n\nE.g., Summarize all objects  \n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. note::\n\n    By default, objects are summarized by callsite. However, callsite is not recorded by Ray by default.\n    To get callsite info, set env variable `RAY_record_ref_creation_sites=1` when starting the ray cluster\n    RAY_record_ref_creation_sites=1 ray start --head\n\n\n.. tabbed:: CLI\n\n    .. code-block:: bash\n\n        ray summary objects \n\n.. tabbed:: Python SDK\n\n    .. code-block:: python\n\n        from ray.experimental.state.api import summarize_objects\n        print(summarize_objects())", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     # Kill raylet so that list_tasks will have network error on querying raylets.\n <mask>     ray._private.worker._global_node.kill_raylet()\n <mask> \n <mask>     with pytest.raises(RayStateApiException):\n <mask>         list_tasks(_explain=True)\n <mask> \n <mask> \n <mask> def test_network_partial_failures(monkeypatch, ray_start_cluster):\n <mask>     \"\"\"When the request fails due to network failure,\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove         # Kill the 3 slow running threads\n        [os.kill(p.pid, signal.SIGKILL) for p in procs]\n        [p.join() for p in procs]\n        for p in procs:\n            assert not p.is_alive(), \"Slow queries should be killed\"\n\n        # Running another 3 should return no error\n        q = mp.Queue()\n        procs = [\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_objects,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_runtime_envs,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_placement_groups,\n                    q,\n                ),\n            ),\n        ]\n </s> add         # Consecutive APIs should be successful after the previous delay ones timeout\n        def verify():\n            assert len(list_objects()) > 0, \"non-delay APIs should be successful\"\n            \"after previous ones timeout\" </s> add                 kwargs={\"timeout\": 6}, </s> add @click.option(\n    \"--address\",\n    default=None,\n    help=(\n        \"The address of Ray API server. If not provided, it will be configured \"\n        \"automatically from querying the GCS server.\"\n    ),\n) </s> remove         res_q = mp.Queue()\n        start_q = mp.Queue()  # not used\n </s> add         res_q = queue.Queue()\n        start_q = queue.Queue()  # used for sync </s> remove .. code-block:: bash\n </s> add         from ray.experimental.state.api import list_tasks \n        list_tasks(filters=[(\"scheduling_state\", \"=\", \"RUNNING\"), (\"name\", \"=\", \"task_running_300_seconds()\")]) </s> remove     import multiprocessing as mp\n    import os\n    import signal\n </s> add     import threading", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep replace keep keep keep keep", "code_tokens": " <mask>     \"\"\"\n <mask>     dead_actor_id = list_actors(filters=[(\"state\", \"=\", \"DEAD\")])[0][\"actor_id\"]\n <mask>     alive_actor_id = list_actors(filters=[(\"state\", \"=\", \"ALIVE\")])[0][\"actor_id\"]\n <mask>     runner = CliRunner()\n <mask>     result = runner.invoke(cli_list, [\"actors\", \"--filter\", \"state=DEAD\"])\n <mask>     assert result.exit_code == 0\n <mask>     assert dead_actor_id in result.output\n <mask>     assert alive_actor_id not in result.output\n <mask> \n <mask>     result = runner.invoke(cli_list, [\"actors\", \"--filter\", \"state!=DEAD\"])\n <mask>     assert result.exit_code == 0\n <mask>     assert dead_actor_id not in result.output\n <mask>     assert alive_actor_id in result.output\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\"]) </s> remove     result = runner.invoke(cli_get, [\"actors\", \"1234\"])\n </s> add     result = runner.invoke(ray_get, [\"actors\", \"1234\"]) </s> remove             result = runner.invoke(cli_list, [\"objects\"])\n </s> add             result = runner.invoke(ray_list, [\"objects\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\", \"--format=table\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\", \"--format=table\"]) </s> remove             result = runner.invoke(cli_list, [\"tasks\", \"--timeout=3\"])\n </s> add             result = runner.invoke(ray_list, [\"tasks\", \"--timeout=3\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             for _ in range(max_limit_data_source + 1)\n <mask>         ]\n <mask>         runner = CliRunner()\n <mask>         with pytest.warns(UserWarning) as record:\n <mask>             result = runner.invoke(cli_list, [\"placement-groups\"])\n <mask>         assert (\n <mask>             f\"{max_limit_data_source} ({max_limit_data_source + 1} total \"\n <mask>             \"from the cluster) placement_groups are retrieved from the \"\n <mask>             \"data source. 1 entries have been truncated.\" in record[0].message.args[0]\n <mask>         )\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             result = runner.invoke(cli_list, [\"objects\"])\n </s> add             result = runner.invoke(ray_list, [\"objects\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\"]) </s> remove             result = runner.invoke(cli_list, [\"tasks\", \"--timeout=3\"])\n </s> add             result = runner.invoke(ray_list, [\"tasks\", \"--timeout=3\"]) </s> remove             result = runner.invoke(cli_list, [\"tasks\", \"--timeout=3\"])\n </s> add             result = runner.invoke(ray_list, [\"tasks\", \"--timeout=3\"]) </s> remove     result = runner.invoke(cli_get, [\"actors\", \"1234\"])\n </s> add     result = runner.invoke(ray_get, [\"actors\", \"1234\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--filter\", \"state=DEAD\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--filter\", \"state=DEAD\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         a = A.remote()\n <mask>         ray.get(a.ready.remote())\n <mask> \n <mask>         with pytest.warns(None) as record:\n <mask>             result = runner.invoke(cli_list, [\"actors\"])\n <mask>         assert len(record) == 0\n <mask> \n <mask> \n <mask> def test_detail(shutdown_only):\n <mask>     ray.init(num_cpus=1)\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             result = runner.invoke(cli_list, [\"objects\"])\n </s> add             result = runner.invoke(ray_list, [\"objects\"]) </s> remove             result = runner.invoke(cli_list, [\"tasks\", \"--timeout=3\"])\n </s> add             result = runner.invoke(ray_list, [\"tasks\", \"--timeout=3\"]) </s> remove             result = runner.invoke(cli_list, [\"tasks\", \"--timeout=3\"])\n </s> add             result = runner.invoke(ray_list, [\"tasks\", \"--timeout=3\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--filter\", \"state!=DEAD\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--filter\", \"state!=DEAD\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--filter\", \"state=DEAD\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--filter\", \"state=DEAD\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\", \"--format=table\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\", \"--format=table\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     \"\"\"\n <mask>     Test CLI\n <mask>     \"\"\"\n <mask>     runner = CliRunner()\n <mask>     result = runner.invoke(cli_list, [\"actors\", \"--detail\"])\n <mask>     print(result.output)\n <mask>     assert result.exit_code == 0\n <mask>     # The column for --detail should be in the output.\n <mask>     assert \"test_detail\" in result.output\n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--filter\", \"state=DEAD\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--filter\", \"state=DEAD\"]) </s> remove     result = runner.invoke(cli_get, [\"actors\", \"1234\"])\n </s> add     result = runner.invoke(ray_get, [\"actors\", \"1234\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--filter\", \"state!=DEAD\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--filter\", \"state!=DEAD\"]) </s> remove             result = runner.invoke(cli_list, [\"objects\"])\n </s> add             result = runner.invoke(ray_list, [\"objects\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\", \"--format=table\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\", \"--format=table\"]) </s> remove             result = runner.invoke(cli_list, [\"placement-groups\"])\n </s> add             result = runner.invoke(ray_list, [\"placement-groups\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         )\n <mask>     )\n <mask> \n <mask>     # When the format is given, it should respect that formatting.\n <mask>     result = runner.invoke(cli_list, [\"actors\", \"--detail\", \"--format=table\"])\n <mask>     assert result.exit_code == 0\n <mask>     with pytest.raises(yaml.YAMLError):\n <mask>         yaml.load(result.output, Loader=yaml.FullLoader)\n <mask> \n <mask> \n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--filter\", \"state!=DEAD\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--filter\", \"state!=DEAD\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--filter\", \"state=DEAD\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--filter\", \"state=DEAD\"]) </s> remove     result = runner.invoke(cli_get, [\"actors\", \"1234\"])\n </s> add     result = runner.invoke(ray_get, [\"actors\", \"1234\"]) </s> remove             result = runner.invoke(cli_list, [\"objects\"])\n </s> add             result = runner.invoke(ray_list, [\"objects\"]) </s> remove             result = runner.invoke(cli_list, [\"tasks\", \"--timeout=3\"])\n </s> add             result = runner.invoke(ray_list, [\"tasks\", \"--timeout=3\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     with pytest.raises(yaml.YAMLError):\n <mask>         yaml.load(result.output, Loader=yaml.FullLoader)\n <mask> \n <mask> \n <mask> def _try_state_query_expect_rate_limit(api_func, res_q, start_q=None):\n <mask>     \"\"\"Utility functions for rate limit related e2e tests below\"\"\"\n <mask>     try:\n <mask>         # Indicate start of the process\n <mask>         if start_q is not None:\n <mask>             start_q.put(1)\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove         api_func()\n </s> add         api_func(**kwargs) </s> add                 kwargs={\"timeout\": 6}, </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\", \"--format=table\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\", \"--format=table\"]) </s> remove     api_server_url = get_api_server_url()\n\n </s> add  </s> remove             mp.Process(\n </s> add             threading.Thread( </s> remove     import multiprocessing as mp\n    import os\n    import signal\n </s> add     import threading", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     try:\n <mask>         # Indicate start of the process\n <mask>         if start_q is not None:\n <mask>             start_q.put(1)\n <mask>         api_func()\n <mask>     except RayStateApiException as e:\n <mask>         # Other exceptions will be thrown\n <mask>         if \"Max number of in-progress requests\" in str(e):\n <mask>             res_q.put(1)\n <mask>         else:\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove def _try_state_query_expect_rate_limit(api_func, res_q, start_q=None):\n </s> add def _try_state_query_expect_rate_limit(api_func, res_q, start_q=None, **kwargs): </s> remove     api_server_url = get_api_server_url()\n\n </s> add  </s> remove         max_concurrent_reqs_error = 0\n        for _ in range(len(procs)):\n            try:\n                res = q.get(timeout=10)\n                if isinstance(res, Exception):\n                    assert False, f\"State API error: {res}\"\n                elif isinstance(res, int):\n                    max_concurrent_reqs_error += res\n                else:\n                    raise ValueError(res)\n            except queue.Empty:\n                assert False, \"Failed to get some results from a subprocess\"\n\n        assert max_concurrent_reqs_error == 0, \"All requests should be successful\"\n        [p.join(5) for p in procs]\n        for proc in procs:\n            assert not proc.is_alive(), \"All processes should exit\"\n </s> add         wait_for_condition(verify) </s> add @click.option(\n    \"--address\",\n    default=None,\n    help=(\n        \"The address of Ray API server. If not provided, it will be configured \"\n        \"automatically from querying the GCS server.\"\n    ),\n) </s> remove         # TODO(swang): This command should also support\n        # passing --address or RAY_ADDRESS, like others.\n        address = ray._private.services.canonicalize_bootstrap_address_or_die(None)\n </s> add         address = ray._private.services.canonicalize_bootstrap_address_or_die(address) </s> add                 kwargs={\"timeout\": 6},", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     reason=\"Lambda test functions could not be pickled on Windows\",\n <mask> )\n <mask> def test_state_api_rate_limit_with_failure(monkeypatch, shutdown_only):\n <mask>     import queue\n <mask>     import multiprocessing as mp\n <mask>     import os\n <mask>     import signal\n <mask> \n <mask>     # Set environment\n <mask>     with monkeypatch.context() as m:\n <mask>         m.setenv(\"RAY_STATE_SERVER_MAX_HTTP_REQUEST\", \"3\")\n <mask>         m.setenv(\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> add         # These make list_nodes, list_workers, list_actors never return in 20secs </s> remove                 \"NodeManagerService.grpc_server.GetTasksInfo=10000000:10000000,\"\n                \"WorkerInfoGcsService.grpc_server.GetAllWorkerInfo=10000000:10000000,\"\n                \"ActorInfoGcsService.grpc_server.GetAllActorInfo=10000000:10000000\"\n </s> add                 \"NodeManagerService.grpc_server.GetTasksInfo=20000000:20000000,\"\n                \"WorkerInfoGcsService.grpc_server.GetAllWorkerInfo=20000000:20000000,\"\n                \"ActorInfoGcsService.grpc_server.GetAllActorInfo=20000000:20000000\" </s> remove         max_concurrent_reqs_error = 0\n        for _ in range(len(procs)):\n            try:\n                res = q.get(timeout=10)\n                if isinstance(res, Exception):\n                    assert False, f\"State API error: {res}\"\n                elif isinstance(res, int):\n                    max_concurrent_reqs_error += res\n                else:\n                    raise ValueError(res)\n            except queue.Empty:\n                assert False, \"Failed to get some results from a subprocess\"\n\n        assert max_concurrent_reqs_error == 0, \"All requests should be successful\"\n        [p.join(5) for p in procs]\n        for proc in procs:\n            assert not proc.is_alive(), \"All processes should exit\"\n </s> add         wait_for_condition(verify) </s> remove     get as state_cli_get,\n    list as state_cli_list,\n    get_api_server_url,\n </s> add     ray_get,\n    ray_list, </s> remove from ray.experimental.state.state_cli import get as cli_get\nfrom ray.experimental.state.state_cli import list as cli_list\n </s> add from ray.experimental.state.state_cli import ray_get\nfrom ray.experimental.state.state_cli import ray_list </s> remove from ray.dashboard.modules.dashboard_sdk import DEFAULT_DASHBOARD_ADDRESS\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>     # Set environment\n <mask>     with monkeypatch.context() as m:\n <mask>         m.setenv(\"RAY_STATE_SERVER_MAX_HTTP_REQUEST\", \"3\")\n <mask>         m.setenv(\n <mask>             \"RAY_testing_asio_delay_us\",\n <mask>             (\n <mask>                 \"NodeManagerService.grpc_server.GetTasksInfo=20000000:20000000,\"\n <mask>                 \"WorkerInfoGcsService.grpc_server.GetAllWorkerInfo=20000000:20000000,\"\n <mask>                 \"ActorInfoGcsService.grpc_server.GetAllActorInfo=20000000:20000000\"\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove                 \"NodeManagerService.grpc_server.GetTasksInfo=10000000:10000000,\"\n                \"WorkerInfoGcsService.grpc_server.GetAllWorkerInfo=10000000:10000000,\"\n                \"ActorInfoGcsService.grpc_server.GetAllActorInfo=10000000:10000000\"\n </s> add                 \"NodeManagerService.grpc_server.GetTasksInfo=20000000:20000000,\"\n                \"WorkerInfoGcsService.grpc_server.GetAllWorkerInfo=20000000:20000000,\"\n                \"ActorInfoGcsService.grpc_server.GetAllActorInfo=20000000:20000000\" </s> remove     import multiprocessing as mp\n    import os\n    import signal\n </s> add     import threading </s> remove     get as state_cli_get,\n    list as state_cli_list,\n    get_api_server_url,\n </s> add     ray_get,\n    ray_list, </s> remove             result = runner.invoke(cli_list, [\"placement-groups\"])\n </s> add             result = runner.invoke(ray_list, [\"placement-groups\"]) </s> remove             result = runner.invoke(cli_list, [\"tasks\", \"--timeout=3\"])\n </s> add             result = runner.invoke(ray_list, [\"tasks\", \"--timeout=3\"]) </s> remove             result = runner.invoke(cli_list, [\"tasks\", \"--timeout=3\"])\n </s> add             result = runner.invoke(ray_list, [\"tasks\", \"--timeout=3\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         m.setenv(\"RAY_STATE_SERVER_MAX_HTTP_REQUEST\", \"3\")\n <mask>         m.setenv(\n <mask>             \"RAY_testing_asio_delay_us\",\n <mask>             (\n <mask>                 \"NodeManagerService.grpc_server.GetTasksInfo=10000000:10000000,\"\n <mask>                 \"WorkerInfoGcsService.grpc_server.GetAllWorkerInfo=10000000:10000000,\"\n <mask>                 \"ActorInfoGcsService.grpc_server.GetAllActorInfo=10000000:10000000\"\n <mask>             ),\n <mask>         )\n <mask> \n <mask>         # Set up scripts\n <mask>         ray.init()\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> add         # These make list_nodes, list_workers, list_actors never return in 20secs </s> remove     import multiprocessing as mp\n    import os\n    import signal\n </s> add     import threading </s> add     ray_address_to_api_server_url, </s> remove     get as state_cli_get,\n    list as state_cli_list,\n    get_api_server_url,\n </s> add     ray_get,\n    ray_list, </s> add                 kwargs={\"timeout\": 6}, </s> remove         # Kill the 3 slow running threads\n        [os.kill(p.pid, signal.SIGKILL) for p in procs]\n        [p.join() for p in procs]\n        for p in procs:\n            assert not p.is_alive(), \"Slow queries should be killed\"\n\n        # Running another 3 should return no error\n        q = mp.Queue()\n        procs = [\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_objects,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_runtime_envs,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_placement_groups,\n                    q,\n                ),\n            ),\n        ]\n </s> add         # Consecutive APIs should be successful after the previous delay ones timeout\n        def verify():\n            assert len(list_objects()) > 0, \"non-delay APIs should be successful\"\n            \"after previous ones timeout\"", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep replace keep keep keep", "code_tokens": " <mask>         # means `list_tasks` will also see the registered raylets.\n <mask>         wait_for_condition(lambda: len(list_objects()) > 0)\n <mask> \n <mask>         # Running 3 slow apis to exhaust the limits\n <mask>         res_q = mp.Queue()\n <mask>         start_q = mp.Queue()  # not used\n <mask>         procs = [\n <mask>             mp.Process(\n <mask>                 target=_try_state_query_expect_rate_limit,\n <mask>                 args=(\n <mask>                     list_workers,\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove         # Kill the 3 slow running threads\n        [os.kill(p.pid, signal.SIGKILL) for p in procs]\n        [p.join() for p in procs]\n        for p in procs:\n            assert not p.is_alive(), \"Slow queries should be killed\"\n\n        # Running another 3 should return no error\n        q = mp.Queue()\n        procs = [\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_objects,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_runtime_envs,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_placement_groups,\n                    q,\n                ),\n            ),\n        ]\n </s> add         # Consecutive APIs should be successful after the previous delay ones timeout\n        def verify():\n            assert len(list_objects()) > 0, \"non-delay APIs should be successful\"\n            \"after previous ones timeout\" </s> remove             result = runner.invoke(cli_list, [\"objects\"])\n </s> add             result = runner.invoke(ray_list, [\"objects\"]) </s> remove     with pytest.raises(RayStateApiException):\n </s> add     with pytest.raises(ConnectionError): </s> remove         api_func()\n </s> add         api_func(**kwargs) </s> remove         # TODO(swang): This command should also support\n        # passing --address or RAY_ADDRESS, like others.\n        address = ray._private.services.canonicalize_bootstrap_address_or_die(None)\n </s> add         address = ray._private.services.canonicalize_bootstrap_address_or_die(address)", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>                     res_q,\n <mask>                     start_q,\n <mask>                 ),\n <mask>             ),\n <mask>             threading.Thread(\n <mask>                 target=_try_state_query_expect_rate_limit,\n <mask>                 args=(\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             mp.Process(\n </s> add             threading.Thread( </s> remove             mp.Process(\n </s> add             threading.Thread( </s> add                 kwargs={\"timeout\": 6}, </s> remove             mp.Process(\n </s> add             threading.Thread( </s> add                 kwargs={\"timeout\": 6}, </s> remove         # Kill the 3 slow running threads\n        [os.kill(p.pid, signal.SIGKILL) for p in procs]\n        [p.join() for p in procs]\n        for p in procs:\n            assert not p.is_alive(), \"Slow queries should be killed\"\n\n        # Running another 3 should return no error\n        q = mp.Queue()\n        procs = [\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_objects,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_runtime_envs,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_placement_groups,\n                    q,\n                ),\n            ),\n        ]\n </s> add         # Consecutive APIs should be successful after the previous delay ones timeout\n        def verify():\n            assert len(list_objects()) > 0, \"non-delay APIs should be successful\"\n            \"after previous ones timeout\"", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                     res_q,\n <mask>                     start_q,\n <mask>                 ),\n <mask>             ),\n <mask>             mp.Process(\n <mask>                 target=_try_state_query_expect_rate_limit,\n <mask>                 args=(\n <mask>                     list_tasks,\n <mask>                     res_q,\n <mask>                     start_q,\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             mp.Process(\n </s> add             threading.Thread( </s> add                 kwargs={\"timeout\": 6}, </s> add                 kwargs={\"timeout\": 6}, </s> remove             mp.Process(\n </s> add             threading.Thread( </s> add                 kwargs={\"timeout\": 6}, </s> remove         # Kill the 3 slow running threads\n        [os.kill(p.pid, signal.SIGKILL) for p in procs]\n        [p.join() for p in procs]\n        for p in procs:\n            assert not p.is_alive(), \"Slow queries should be killed\"\n\n        # Running another 3 should return no error\n        q = mp.Queue()\n        procs = [\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_objects,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_runtime_envs,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_placement_groups,\n                    q,\n                ),\n            ),\n        ]\n </s> add         # Consecutive APIs should be successful after the previous delay ones timeout\n        def verify():\n            assert len(list_objects()) > 0, \"non-delay APIs should be successful\"\n            \"after previous ones timeout\"", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>                     list_tasks,\n <mask>                     res_q,\n <mask>                     start_q,\n <mask>                 ),\n <mask>             ),\n <mask>             threading.Thread(\n <mask>                 target=_try_state_query_expect_rate_limit,\n <mask>                 args=(\n <mask>                     list_actors,\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             mp.Process(\n </s> add             threading.Thread( </s> remove             mp.Process(\n </s> add             threading.Thread( </s> add                 kwargs={\"timeout\": 6}, </s> remove             mp.Process(\n </s> add             threading.Thread( </s> add                 kwargs={\"timeout\": 6}, </s> remove         # Kill the 3 slow running threads\n        [os.kill(p.pid, signal.SIGKILL) for p in procs]\n        [p.join() for p in procs]\n        for p in procs:\n            assert not p.is_alive(), \"Slow queries should be killed\"\n\n        # Running another 3 should return no error\n        q = mp.Queue()\n        procs = [\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_objects,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_runtime_envs,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_placement_groups,\n                    q,\n                ),\n            ),\n        ]\n </s> add         # Consecutive APIs should be successful after the previous delay ones timeout\n        def verify():\n            assert len(list_objects()) > 0, \"non-delay APIs should be successful\"\n            \"after previous ones timeout\"", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                     res_q,\n <mask>                     start_q,\n <mask>                 ),\n <mask>             ),\n <mask>             mp.Process(\n <mask>                 target=_try_state_query_expect_rate_limit,\n <mask>                 args=(\n <mask>                     list_actors,\n <mask>                     res_q,\n <mask>                     start_q,\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             mp.Process(\n </s> add             threading.Thread( </s> add                 kwargs={\"timeout\": 6}, </s> add                 kwargs={\"timeout\": 6}, </s> remove             mp.Process(\n </s> add             threading.Thread( </s> add                 kwargs={\"timeout\": 6}, </s> remove         # Kill the 3 slow running threads\n        [os.kill(p.pid, signal.SIGKILL) for p in procs]\n        [p.join() for p in procs]\n        for p in procs:\n            assert not p.is_alive(), \"Slow queries should be killed\"\n\n        # Running another 3 should return no error\n        q = mp.Queue()\n        procs = [\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_objects,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_runtime_envs,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_placement_groups,\n                    q,\n                ),\n            ),\n        ]\n </s> add         # Consecutive APIs should be successful after the previous delay ones timeout\n        def verify():\n            assert len(list_objects()) > 0, \"non-delay APIs should be successful\"\n            \"after previous ones timeout\"", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>                     res_q,\n <mask>                     start_q,\n <mask>                 ),\n <mask>             ),\n <mask>         ]\n <mask> \n <mask>         [p.start() for p in procs]\n <mask> \n <mask>         # Wait for other processes to start so rate limit will be reached\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove         [p.start() for p in procs]\n </s> add             return True </s> remove         # Kill the 3 slow running threads\n        [os.kill(p.pid, signal.SIGKILL) for p in procs]\n        [p.join() for p in procs]\n        for p in procs:\n            assert not p.is_alive(), \"Slow queries should be killed\"\n\n        # Running another 3 should return no error\n        q = mp.Queue()\n        procs = [\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_objects,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_runtime_envs,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_placement_groups,\n                    q,\n                ),\n            ),\n        ]\n </s> add         # Consecutive APIs should be successful after the previous delay ones timeout\n        def verify():\n            assert len(list_objects()) > 0, \"non-delay APIs should be successful\"\n            \"after previous ones timeout\" </s> remove         max_concurrent_reqs_error = 0\n        for _ in range(len(procs)):\n            try:\n                res = q.get(timeout=10)\n                if isinstance(res, Exception):\n                    assert False, f\"State API error: {res}\"\n                elif isinstance(res, int):\n                    max_concurrent_reqs_error += res\n                else:\n                    raise ValueError(res)\n            except queue.Empty:\n                assert False, \"Failed to get some results from a subprocess\"\n\n        assert max_concurrent_reqs_error == 0, \"All requests should be successful\"\n        [p.join(5) for p in procs]\n        for proc in procs:\n            assert not proc.is_alive(), \"All processes should exit\"\n </s> add         wait_for_condition(verify) </s> remove def _try_state_query_expect_rate_limit(api_func, res_q, start_q=None):\n </s> add def _try_state_query_expect_rate_limit(api_func, res_q, start_q=None, **kwargs): </s> remove             mp.Process(\n </s> add             threading.Thread( </s> remove             mp.Process(\n </s> add             threading.Thread(", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep replace keep keep keep", "code_tokens": " <mask> \n <mask>         # Kill the 3 slow running threads\n <mask>         [os.kill(p.pid, signal.SIGKILL) for p in procs]\n <mask>         [p.join() for p in procs]\n <mask>         for p in procs:\n <mask>             assert not p.is_alive(), \"Slow queries should be killed\"\n <mask> \n <mask>         # Running another 3 should return no error\n <mask>         q = mp.Queue()\n <mask>         procs = [\n <mask>             mp.Process(\n <mask>                 target=_try_state_query_expect_rate_limit,\n <mask>                 args=(\n <mask>                     list_objects,\n <mask>                     q,\n <mask>                 ),\n <mask>             ),\n <mask>             mp.Process(\n <mask>                 target=_try_state_query_expect_rate_limit,\n <mask>                 args=(\n <mask>                     list_runtime_envs,\n <mask>                     q,\n <mask>                 ),\n <mask>             ),\n <mask>             mp.Process(\n <mask>                 target=_try_state_query_expect_rate_limit,\n <mask>                 args=(\n <mask>                     list_placement_groups,\n <mask>                     q,\n <mask>                 ),\n <mask>             ),\n <mask>         ]\n <mask> \n <mask>         [p.start() for p in procs]\n <mask> \n <mask>         max_concurrent_reqs_error = 0\n <mask>         for _ in range(len(procs)):\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> add                 kwargs={\"timeout\": 6}, </s> remove         max_concurrent_reqs_error = 0\n        for _ in range(len(procs)):\n            try:\n                res = q.get(timeout=10)\n                if isinstance(res, Exception):\n                    assert False, f\"State API error: {res}\"\n                elif isinstance(res, int):\n                    max_concurrent_reqs_error += res\n                else:\n                    raise ValueError(res)\n            except queue.Empty:\n                assert False, \"Failed to get some results from a subprocess\"\n\n        assert max_concurrent_reqs_error == 0, \"All requests should be successful\"\n        [p.join(5) for p in procs]\n        for proc in procs:\n            assert not proc.is_alive(), \"All processes should exit\"\n </s> add         wait_for_condition(verify) </s> remove             mp.Process(\n </s> add             threading.Thread( </s> remove         res_q = mp.Queue()\n        start_q = mp.Queue()  # not used\n </s> add         res_q = queue.Queue()\n        start_q = queue.Queue()  # used for sync </s> remove             mp.Process(\n </s> add             threading.Thread(", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         ]\n <mask> \n <mask>         [p.start() for p in procs]\n <mask> \n <mask>         max_concurrent_reqs_error = 0\n <mask>         for _ in range(len(procs)):\n <mask>             try:\n <mask>                 res = q.get(timeout=10)\n <mask>                 if isinstance(res, Exception):\n <mask>                     assert False, f\"State API error: {res}\"\n <mask>                 elif isinstance(res, int):\n <mask>                     max_concurrent_reqs_error += res\n <mask>                 else:\n <mask>                     raise ValueError(res)\n <mask>             except queue.Empty:\n <mask>                 assert False, \"Failed to get some results from a subprocess\"\n <mask> \n <mask>         assert max_concurrent_reqs_error == 0, \"All requests should be successful\"\n <mask>         [p.join(5) for p in procs]\n <mask>         for proc in procs:\n <mask>             assert not proc.is_alive(), \"All processes should exit\"\n <mask> \n <mask> \n <mask> @pytest.mark.skipif(\n <mask>     sys.platform == \"win32\",\n <mask>     reason=\"Lambda test functions could not be pickled on Windows\",\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove         [p.start() for p in procs]\n </s> add             return True </s> remove         # Kill the 3 slow running threads\n        [os.kill(p.pid, signal.SIGKILL) for p in procs]\n        [p.join() for p in procs]\n        for p in procs:\n            assert not p.is_alive(), \"Slow queries should be killed\"\n\n        # Running another 3 should return no error\n        q = mp.Queue()\n        procs = [\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_objects,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_runtime_envs,\n                    q,\n                ),\n            ),\n            mp.Process(\n                target=_try_state_query_expect_rate_limit,\n                args=(\n                    list_placement_groups,\n                    q,\n                ),\n            ),\n        ]\n </s> add         # Consecutive APIs should be successful after the previous delay ones timeout\n        def verify():\n            assert len(list_objects()) > 0, \"non-delay APIs should be successful\"\n            \"after previous ones timeout\" </s> add                 kwargs={\"timeout\": 6}, </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--filter\", \"state!=DEAD\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--filter\", \"state!=DEAD\"]) </s> remove     result = runner.invoke(cli_get, [\"actors\", \"1234\"])\n </s> add     result = runner.invoke(ray_get, [\"actors\", \"1234\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         runner = CliRunner()\n <mask>         wait_for_condition(lambda: len(list_objects()) > 0)\n <mask> \n <mask>         with pytest.warns(None) as record:\n <mask>             result = runner.invoke(cli_list, [\"objects\"])\n <mask>             assert result.exit_code == 0\n <mask> \n <mask>         if callsite_enabled:\n <mask>             assert len(record) == 0\n <mask>         else:\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             result = runner.invoke(cli_list, [\"tasks\", \"--timeout=3\"])\n </s> add             result = runner.invoke(ray_list, [\"tasks\", \"--timeout=3\"]) </s> remove             result = runner.invoke(cli_list, [\"tasks\", \"--timeout=3\"])\n </s> add             result = runner.invoke(ray_list, [\"tasks\", \"--timeout=3\"]) </s> remove             result = runner.invoke(cli_list, [\"actors\"])\n </s> add             result = runner.invoke(ray_list, [\"actors\"]) </s> remove     result = runner.invoke(cli_get, [\"actors\", \"1234\"])\n </s> add     result = runner.invoke(ray_get, [\"actors\", \"1234\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--filter\", \"state=DEAD\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--filter\", \"state=DEAD\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         assert len(record) == 1\n <mask> \n <mask>         # Verify when CLI is used, exceptions are not raised.\n <mask>         with pytest.warns(None) as record:\n <mask>             result = runner.invoke(cli_list, [\"tasks\", \"--timeout=3\"])\n <mask>         assert len(record) == 1\n <mask>         assert result.exit_code == 0\n <mask> \n <mask>         # Verify summary CLI also doesn't raise an exception.\n <mask>         with pytest.warns(None) as record:\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove             result = runner.invoke(cli_list, [\"tasks\", \"--timeout=3\"])\n </s> add             result = runner.invoke(ray_list, [\"tasks\", \"--timeout=3\"]) </s> remove             result = runner.invoke(cli_list, [\"objects\"])\n </s> add             result = runner.invoke(ray_list, [\"objects\"]) </s> remove             result = runner.invoke(cli_list, [\"actors\"])\n </s> add             result = runner.invoke(ray_list, [\"actors\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--filter\", \"state!=DEAD\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--filter\", \"state!=DEAD\"]) </s> remove     result = runner.invoke(cli_get, [\"actors\", \"1234\"])\n </s> add     result = runner.invoke(ray_get, [\"actors\", \"1234\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\", \"--format=table\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\", \"--format=table\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         assert len(record) == 1\n <mask> \n <mask>         # Verify when CLI is used, exceptions are not raised.\n <mask>         with pytest.warns(None) as record:\n <mask>             result = runner.invoke(cli_list, [\"tasks\", \"--timeout=3\"])\n <mask>         assert len(record) == 1\n <mask>         assert result.exit_code == 0\n <mask> \n <mask>         # Verify summary CLI also doesn't raise an exception.\n <mask>         with pytest.warns(None) as record:\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com>", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     Related: https://github.com/ray-project/ray/issues/26808\n <mask>     \"\"\"\n <mask>     ray.init()\n <mask>     runner = CliRunner()\n <mask>     result = runner.invoke(cli_get, [\"actors\", \"1234\"])\n <mask>     assert result.exit_code == 0\n <mask>     assert \"Resource with id=1234 not found in the cluster.\" in result.output\n <mask> \n <mask> \n <mask> if __name__ == \"__main__\":\n </s> [Doc][Core][State Observability] Adding Python SDK doc and docstring (#26997)\n\n1. Add doc for python SDK and docstrings on public SDK\r\n2. Rename list -> ray_list and get -> ray_get for better naming \r\n3. Fix some typos \r\n4. Auto translate address to api server url.\r\n\r\nCo-authored-by: SangBin Cho <rkooo567@gmail.com> </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--filter\", \"state=DEAD\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--filter\", \"state=DEAD\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--filter\", \"state!=DEAD\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--filter\", \"state!=DEAD\"]) </s> remove             result = runner.invoke(cli_list, [\"objects\"])\n </s> add             result = runner.invoke(ray_list, [\"objects\"]) </s> remove     result = runner.invoke(cli_list, [\"actors\", \"--detail\", \"--format=table\"])\n </s> add     result = runner.invoke(ray_list, [\"actors\", \"--detail\", \"--format=table\"]) </s> remove             result = runner.invoke(cli_list, [\"tasks\", \"--timeout=3\"])\n </s> add             result = runner.invoke(ray_list, [\"tasks\", \"--timeout=3\"])", "html_url": "https://github.com/ray-project/ray/commit/82a24f931944929d1aa1942d5e8bb0f0349bc8a8", "file_name": "python/ray/tests/test_state_api.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     visibility = [\"//visibility:private\"],\n <mask> )\n <mask> \n <mask> compile_pip_requirements(\n <mask>     name = \"requirements_byod\",\n <mask>     requirements_in = \"ray_release/byod/requirements_byod.in\",\n <mask>     requirements_txt = \"ray_release/byod/requirements_byod.txt\",\n <mask>     tags = [\n <mask>         \"team:ci\",\n <mask>     ],\n <mask>     visibility = [\"//visibility:private\"],\n <mask> )\n </s> [ci][byod] support python 3.9 (#36672) </s> remove     name = \"requirements_ml_byod\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod.txt\",\n </s> add     name = \"requirements_byod_3.9\",\n    requirements_in = \"ray_release/byod/requirements_byod_3.9.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod_3.9.txt\",\n    tags = [\n        \"team:ci\",\n    ],\n    visibility = [\"//visibility:private\"],\n)\n\ncompile_pip_requirements(\n    name = \"requirements_ml_byod_3.8\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod_3.8.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod_3.8.txt\",\n    tags = [\n        \"team:ci\",\n    ],\n    visibility = [\"//visibility:private\"],\n)\n\ncompile_pip_requirements(\n    name = \"requirements_ml_byod_3.9\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod_3.9.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod_3.9.txt\", </s> remove         image_suffix = \"-gpu\" if self.get_byod_type() == \"gpu\" else \"\"\n </s> add         byod_type = self.get_byod_type()\n        image_suffix = f\"-{byod_type}\" if byod_type != \"cpu\" else \"\" </s> remove REQUIREMENTS_BYOD = \"requirements_byod.txt\"\nREQUIREMENTS_ML_BYOD = \"requirements_ml_byod.txt\"\n </s> add REQUIREMENTS_BYOD = \"requirements_byod\"\nREQUIREMENTS_ML_BYOD = \"requirements_ml_byod\"\nPYTHON_VERSION = \"3.8\" </s> remove     assert _stub_test({}).get_ray_image() == \"rayproject/ray:1.0.0.123456-py37\"\n </s> add     assert (\n        _stub_test({\"cluster\": {\"byod\": {}}}).get_ray_image()\n        == \"rayproject/ray:1.0.0.123456-py37\"\n    ) </s> remove         _stub_test({}).get_anyscale_byod_image()\n </s> add         _stub_test({\"python\": \"3.7\", \"cluster\": {\"byod\": {}}}).get_anyscale_byod_image() </s> remove                 else REQUIREMENTS_ML_BYOD\n </s> add                 else f\"{REQUIREMENTS_ML_BYOD}_{test.get('python', PYTHON_VERSION)}.txt\"", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/BUILD"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     visibility = [\"//visibility:private\"],\n <mask> )\n <mask> \n <mask> compile_pip_requirements(\n <mask>     name = \"requirements_ml_byod\",\n <mask>     requirements_in = \"ray_release/byod/requirements_ml_byod.in\",\n <mask>     requirements_txt = \"ray_release/byod/requirements_ml_byod.txt\",\n <mask>     tags = [\n <mask>         \"team:ci\",\n <mask>     ],\n <mask>     visibility = [\"//visibility:private\"],\n <mask> )\n </s> [ci][byod] support python 3.9 (#36672) </s> remove     name = \"requirements_byod\",\n    requirements_in = \"ray_release/byod/requirements_byod.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod.txt\",\n </s> add     name = \"requirements_byod_3.8\",\n    requirements_in = \"ray_release/byod/requirements_byod_3.8.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod_3.8.txt\", </s> remove         image_suffix = \"-gpu\" if self.get_byod_type() == \"gpu\" else \"\"\n </s> add         byod_type = self.get_byod_type()\n        image_suffix = f\"-{byod_type}\" if byod_type != \"cpu\" else \"\" </s> remove REQUIREMENTS_BYOD = \"requirements_byod.txt\"\nREQUIREMENTS_ML_BYOD = \"requirements_ml_byod.txt\"\n </s> add REQUIREMENTS_BYOD = \"requirements_byod\"\nREQUIREMENTS_ML_BYOD = \"requirements_ml_byod\"\nPYTHON_VERSION = \"3.8\" </s> remove     assert _stub_test({}).get_ray_image() == \"rayproject/ray:1.0.0.123456-py37\"\n </s> add     assert (\n        _stub_test({\"cluster\": {\"byod\": {}}}).get_ray_image()\n        == \"rayproject/ray:1.0.0.123456-py37\"\n    ) </s> remove         _stub_test({}).get_anyscale_byod_image()\n </s> add         _stub_test({\"python\": \"3.7\", \"cluster\": {\"byod\": {}}}).get_anyscale_byod_image() </s> remove                 else REQUIREMENTS_ML_BYOD\n </s> add                 else f\"{REQUIREMENTS_ML_BYOD}_{test.get('python', PYTHON_VERSION)}.txt\"", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/BUILD"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> DATAPLANE_DIGEST = \"f9b0055085690ddad2faa804bb6b38addbcf345b9166f2204928a7ece1c8a39b\"\n <mask> BASE_IMAGE_WAIT_TIMEOUT = 7200\n <mask> BASE_IMAGE_WAIT_DURATION = 30\n <mask> RELEASE_BYOD_DIR = os.path.join(RELEASE_PACKAGE_DIR, \"ray_release/byod\")\n <mask> REQUIREMENTS_BYOD = \"requirements_byod.txt\"\n <mask> REQUIREMENTS_ML_BYOD = \"requirements_ml_byod.txt\"\n <mask> \n <mask> \n <mask> def build_anyscale_custom_byod_image(test: Test) -> None:\n <mask>     if not test.require_custom_byod_image():\n <mask>         logger.info(f\"Test {test.get_name()} does not require a custom byod image\")\n </s> [ci][byod] support python 3.9 (#36672) </s> remove                 else REQUIREMENTS_ML_BYOD\n </s> add                 else f\"{REQUIREMENTS_ML_BYOD}_{test.get('python', PYTHON_VERSION)}.txt\" </s> remove                 REQUIREMENTS_BYOD\n </s> add                 f\"{REQUIREMENTS_BYOD}_{test.get('python', PYTHON_VERSION)}.txt\" </s> remove         image_suffix = \"-gpu\" if self.get_byod_type() == \"gpu\" else \"\"\n </s> add         byod_type = self.get_byod_type()\n        image_suffix = f\"-{byod_type}\" if byod_type != \"cpu\" else \"\" </s> remove         ray_project = \"ray-ml\" if self.get_byod_type() == \"gpu\" else \"ray\"\n </s> add         ray_project = \"ray\" if self.get_byod_type() == \"cpu\" else \"ray-ml\" </s> remove     name = \"requirements_ml_byod\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod.txt\",\n </s> add     name = \"requirements_byod_3.9\",\n    requirements_in = \"ray_release/byod/requirements_byod_3.9.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod_3.9.txt\",\n    tags = [\n        \"team:ci\",\n    ],\n    visibility = [\"//visibility:private\"],\n)\n\ncompile_pip_requirements(\n    name = \"requirements_ml_byod_3.8\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod_3.8.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod_3.8.txt\",\n    tags = [\n        \"team:ci\",\n    ],\n    visibility = [\"//visibility:private\"],\n)\n\ncompile_pip_requirements(\n    name = \"requirements_ml_byod_3.9\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod_3.9.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod_3.9.txt\", </s> remove     name = \"requirements_byod\",\n    requirements_in = \"ray_release/byod/requirements_byod.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod.txt\",\n </s> add     name = \"requirements_byod_3.8\",\n    requirements_in = \"ray_release/byod/requirements_byod_3.8.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod_3.8.txt\",", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/ray_release/byod/build.py"}
{"docstring_tokens": "keep keep keep keep replace keep replace keep keep keep", "code_tokens": " <mask>     ):\n <mask>         for ray_image, test in to_be_built.items():\n <mask>             byod_image = test.get_anyscale_base_byod_image()\n <mask>             byod_requirements = (\n <mask>                 REQUIREMENTS_BYOD\n <mask>                 if test.get_byod_type() == \"cpu\"\n <mask>                 else REQUIREMENTS_ML_BYOD\n <mask>             )\n <mask>             if _byod_image_exist(test):\n <mask>                 logger.info(f\"Image {byod_image} already exists\")\n </s> [ci][byod] support python 3.9 (#36672) </s> remove REQUIREMENTS_BYOD = \"requirements_byod.txt\"\nREQUIREMENTS_ML_BYOD = \"requirements_ml_byod.txt\"\n </s> add REQUIREMENTS_BYOD = \"requirements_byod\"\nREQUIREMENTS_ML_BYOD = \"requirements_ml_byod\"\nPYTHON_VERSION = \"3.8\" </s> remove         ray_project = \"ray-ml\" if self.get_byod_type() == \"gpu\" else \"ray\"\n </s> add         ray_project = \"ray\" if self.get_byod_type() == \"cpu\" else \"ray-ml\" </s> remove             DATAPLANE_ECR_ML_REPO\n            if self.get_byod_type() == \"gpu\"\n            else DATAPLANE_ECR_REPO\n </s> add             DATAPLANE_ECR_REPO\n            if self.get_byod_type() == \"cpu\"\n            else DATAPLANE_ECR_ML_REPO </s> remove         image_suffix = \"-gpu\" if self.get_byod_type() == \"gpu\" else \"\"\n </s> add         byod_type = self.get_byod_type()\n        image_suffix = f\"-{byod_type}\" if byod_type != \"cpu\" else \"\" </s> remove     assert _stub_test({}).get_ray_image() == \"rayproject/ray:1.0.0.123456-py37\"\n </s> add     assert (\n        _stub_test({\"cluster\": {\"byod\": {}}}).get_ray_image()\n        == \"rayproject/ray:1.0.0.123456-py37\"\n    )", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/ray_release/byod/build.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \t\t\t\t\"type\": {\n <mask> \t\t\t\t\t\"type\": \"string\",\n <mask> \t\t\t\t\t\"enum\": [\n <mask> \t\t\t\t\t\t\"cpu\",\n <mask> \t\t\t\t\t\t\"gpu\"\n <mask> \t\t\t\t\t]\n <mask> \t\t\t\t},\n <mask> \t\t\t\t\"post_build_script\":{\n <mask> \t\t\t\t\t\"type\": \"string\"\n <mask> \t\t\t\t},\n </s> [ci][byod] support python 3.9 (#36672) </s> remove     assert _stub_test({\"python\": \"3.8\"}).get_ray_image() == \"rayproject/ray:123456-py38\"\n </s> add     assert (\n        _stub_test(\n            {\n                \"python\": \"3.8\",\n                \"cluster\": {\"byod\": {}},\n            }\n        ).get_ray_image()\n        == \"rayproject/ray:123456-py38\"\n    ) </s> remove     name = \"requirements_ml_byod\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod.txt\",\n </s> add     name = \"requirements_byod_3.9\",\n    requirements_in = \"ray_release/byod/requirements_byod_3.9.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod_3.9.txt\",\n    tags = [\n        \"team:ci\",\n    ],\n    visibility = [\"//visibility:private\"],\n)\n\ncompile_pip_requirements(\n    name = \"requirements_ml_byod_3.8\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod_3.8.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod_3.8.txt\",\n    tags = [\n        \"team:ci\",\n    ],\n    visibility = [\"//visibility:private\"],\n)\n\ncompile_pip_requirements(\n    name = \"requirements_ml_byod_3.9\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod_3.9.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod_3.9.txt\", </s> remove         _stub_test({}).get_anyscale_byod_image()\n </s> add         _stub_test({\"python\": \"3.7\", \"cluster\": {\"byod\": {}}}).get_anyscale_byod_image() </s> remove     name = \"requirements_byod\",\n    requirements_in = \"ray_release/byod/requirements_byod.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod.txt\",\n </s> add     name = \"requirements_byod_3.8\",\n    requirements_in = \"ray_release/byod/requirements_byod_3.8.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod_3.8.txt\", </s> remove             DATAPLANE_ECR_ML_REPO\n            if self.get_byod_type() == \"gpu\"\n            else DATAPLANE_ECR_REPO\n </s> add             DATAPLANE_ECR_REPO\n            if self.get_byod_type() == \"cpu\"\n            else DATAPLANE_ECR_ML_REPO </s> remove         image_suffix = \"-gpu\" if self.get_byod_type() == \"gpu\" else \"\"\n </s> add         byod_type = self.get_byod_type()\n        image_suffix = f\"-{byod_type}\" if byod_type != \"cpu\" else \"\"", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/ray_release/schema.json"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         ), f\"Invalid branch name {branch}\"\n <mask>         if branch.startswith(\"releases/\"):\n <mask>             release_name = branch[len(\"releases/\") :]\n <mask>             ray_version = f\"{release_name}.{ray_version}\"\n <mask>         image_suffix = \"-gpu\" if self.get_byod_type() == \"gpu\" else \"\"\n <mask>         python_version = f\"py{self.get_python_version().replace('.',   '')}\"\n <mask>         return f\"{ray_version}-{python_version}{image_suffix}\"\n <mask> \n <mask>     def get_byod_image_tag(self) -> str:\n <mask>         \"\"\"\n </s> [ci][byod] support python 3.9 (#36672) </s> remove         ray_project = \"ray-ml\" if self.get_byod_type() == \"gpu\" else \"ray\"\n </s> add         ray_project = \"ray\" if self.get_byod_type() == \"cpu\" else \"ray-ml\" </s> remove             DATAPLANE_ECR_ML_REPO\n            if self.get_byod_type() == \"gpu\"\n            else DATAPLANE_ECR_REPO\n </s> add             DATAPLANE_ECR_REPO\n            if self.get_byod_type() == \"cpu\"\n            else DATAPLANE_ECR_ML_REPO </s> remove                 else REQUIREMENTS_ML_BYOD\n </s> add                 else f\"{REQUIREMENTS_ML_BYOD}_{test.get('python', PYTHON_VERSION)}.txt\" </s> remove     name = \"requirements_ml_byod\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod.txt\",\n </s> add     name = \"requirements_byod_3.9\",\n    requirements_in = \"ray_release/byod/requirements_byod_3.9.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod_3.9.txt\",\n    tags = [\n        \"team:ci\",\n    ],\n    visibility = [\"//visibility:private\"],\n)\n\ncompile_pip_requirements(\n    name = \"requirements_ml_byod_3.8\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod_3.8.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod_3.8.txt\",\n    tags = [\n        \"team:ci\",\n    ],\n    visibility = [\"//visibility:private\"],\n)\n\ncompile_pip_requirements(\n    name = \"requirements_ml_byod_3.9\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod_3.9.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod_3.9.txt\", </s> remove REQUIREMENTS_BYOD = \"requirements_byod.txt\"\nREQUIREMENTS_ML_BYOD = \"requirements_ml_byod.txt\"\n </s> add REQUIREMENTS_BYOD = \"requirements_byod\"\nREQUIREMENTS_ML_BYOD = \"requirements_ml_byod\"\nPYTHON_VERSION = \"3.8\" </s> remove     name = \"requirements_byod\",\n    requirements_in = \"ray_release/byod/requirements_byod.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod.txt\",\n </s> add     name = \"requirements_byod_3.8\",\n    requirements_in = \"ray_release/byod/requirements_byod_3.8.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod_3.8.txt\",", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/ray_release/test.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         \"\"\"\n <mask>         Returns the byod repo to use for this test.\n <mask>         \"\"\"\n <mask>         return (\n <mask>             DATAPLANE_ECR_ML_REPO\n <mask>             if self.get_byod_type() == \"gpu\"\n <mask>             else DATAPLANE_ECR_REPO\n <mask>         )\n <mask> \n <mask>     def get_ray_image(self) -> str:\n <mask>         \"\"\"\n <mask>         Returns the ray docker image to use for this test.\n </s> [ci][byod] support python 3.9 (#36672) </s> remove         ray_project = \"ray-ml\" if self.get_byod_type() == \"gpu\" else \"ray\"\n </s> add         ray_project = \"ray\" if self.get_byod_type() == \"cpu\" else \"ray-ml\" </s> remove         image_suffix = \"-gpu\" if self.get_byod_type() == \"gpu\" else \"\"\n </s> add         byod_type = self.get_byod_type()\n        image_suffix = f\"-{byod_type}\" if byod_type != \"cpu\" else \"\" </s> remove                 REQUIREMENTS_BYOD\n </s> add                 f\"{REQUIREMENTS_BYOD}_{test.get('python', PYTHON_VERSION)}.txt\" </s> remove                 else REQUIREMENTS_ML_BYOD\n </s> add                 else f\"{REQUIREMENTS_ML_BYOD}_{test.get('python', PYTHON_VERSION)}.txt\" </s> remove REQUIREMENTS_BYOD = \"requirements_byod.txt\"\nREQUIREMENTS_ML_BYOD = \"requirements_ml_byod.txt\"\n </s> add REQUIREMENTS_BYOD = \"requirements_byod\"\nREQUIREMENTS_ML_BYOD = \"requirements_ml_byod\"\nPYTHON_VERSION = \"3.8\" </s> remove         _stub_test({}).get_anyscale_byod_image()\n </s> add         _stub_test({\"python\": \"3.7\", \"cluster\": {\"byod\": {}}}).get_anyscale_byod_image()", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/ray_release/test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     def get_ray_image(self) -> str:\n <mask>         \"\"\"\n <mask>         Returns the ray docker image to use for this test.\n <mask>         \"\"\"\n <mask>         ray_project = \"ray-ml\" if self.get_byod_type() == \"gpu\" else \"ray\"\n <mask>         return f\"rayproject/{ray_project}:{self.get_byod_base_image_tag()}\"\n <mask> \n <mask>     def get_anyscale_base_byod_image(self) -> str:\n <mask>         \"\"\"\n <mask>         Returns the anyscale byod image to use for this test.\n </s> [ci][byod] support python 3.9 (#36672) </s> remove             DATAPLANE_ECR_ML_REPO\n            if self.get_byod_type() == \"gpu\"\n            else DATAPLANE_ECR_REPO\n </s> add             DATAPLANE_ECR_REPO\n            if self.get_byod_type() == \"cpu\"\n            else DATAPLANE_ECR_ML_REPO </s> remove         image_suffix = \"-gpu\" if self.get_byod_type() == \"gpu\" else \"\"\n </s> add         byod_type = self.get_byod_type()\n        image_suffix = f\"-{byod_type}\" if byod_type != \"cpu\" else \"\" </s> remove                 REQUIREMENTS_BYOD\n </s> add                 f\"{REQUIREMENTS_BYOD}_{test.get('python', PYTHON_VERSION)}.txt\" </s> remove REQUIREMENTS_BYOD = \"requirements_byod.txt\"\nREQUIREMENTS_ML_BYOD = \"requirements_ml_byod.txt\"\n </s> add REQUIREMENTS_BYOD = \"requirements_byod\"\nREQUIREMENTS_ML_BYOD = \"requirements_ml_byod\"\nPYTHON_VERSION = \"3.8\" </s> remove                 else REQUIREMENTS_ML_BYOD\n </s> add                 else f\"{REQUIREMENTS_ML_BYOD}_{test.get('python', PYTHON_VERSION)}.txt\" </s> remove     assert _stub_test({}).get_ray_image() == \"rayproject/ray:1.0.0.123456-py37\"\n </s> add     assert (\n        _stub_test({\"cluster\": {\"byod\": {}}}).get_ray_image()\n        == \"rayproject/ray:1.0.0.123456-py37\"\n    )", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/ray_release/test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> def test_get_ray_image():\n <mask>     os.environ[\"BUILDKITE_BRANCH\"] = \"master\"\n <mask>     os.environ[\"BUILDKITE_COMMIT\"] = \"1234567890\"\n <mask>     assert _stub_test({\"python\": \"3.8\"}).get_ray_image() == \"rayproject/ray:123456-py38\"\n <mask>     assert (\n <mask>         _stub_test(\n <mask>             {\n <mask>                 \"python\": \"3.8\",\n <mask>                 \"cluster\": {\n </s> [ci][byod] support python 3.9 (#36672) </s> remove         _stub_test({}).get_anyscale_byod_image()\n </s> add         _stub_test({\"python\": \"3.7\", \"cluster\": {\"byod\": {}}}).get_anyscale_byod_image() </s> remove     assert _stub_test({}).get_ray_image() == \"rayproject/ray:1.0.0.123456-py37\"\n </s> add     assert (\n        _stub_test({\"cluster\": {\"byod\": {}}}).get_ray_image()\n        == \"rayproject/ray:1.0.0.123456-py37\"\n    ) </s> remove \t\t\t\t\t\t\"gpu\"\n </s> add \t\t\t\t\t\t\"gpu\",\n\t\t\t\t\t\t\"cu118\" </s> remove                 else REQUIREMENTS_ML_BYOD\n </s> add                 else f\"{REQUIREMENTS_ML_BYOD}_{test.get('python', PYTHON_VERSION)}.txt\" </s> remove REQUIREMENTS_BYOD = \"requirements_byod.txt\"\nREQUIREMENTS_ML_BYOD = \"requirements_ml_byod.txt\"\n </s> add REQUIREMENTS_BYOD = \"requirements_byod\"\nREQUIREMENTS_ML_BYOD = \"requirements_ml_byod\"\nPYTHON_VERSION = \"3.8\" </s> remove         image_suffix = \"-gpu\" if self.get_byod_type() == \"gpu\" else \"\"\n </s> add         byod_type = self.get_byod_type()\n        image_suffix = f\"-{byod_type}\" if byod_type != \"cpu\" else \"\"", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/ray_release/tests/test_test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         ).get_ray_image()\n <mask>         == \"rayproject/ray-ml:123456-py38-gpu\"\n <mask>     )\n <mask>     os.environ[\"BUILDKITE_BRANCH\"] = \"releases/1.0.0\"\n <mask>     assert _stub_test({}).get_ray_image() == \"rayproject/ray:1.0.0.123456-py37\"\n <mask> \n <mask> \n <mask> def test_get_anyscale_byod_image():\n <mask>     os.environ[\"BUILDKITE_BRANCH\"] = \"master\"\n <mask>     os.environ[\"BUILDKITE_COMMIT\"] = \"1234567890\"\n </s> [ci][byod] support python 3.9 (#36672) </s> remove         _stub_test({}).get_anyscale_byod_image()\n </s> add         _stub_test({\"python\": \"3.7\", \"cluster\": {\"byod\": {}}}).get_anyscale_byod_image() </s> remove     assert _stub_test({\"python\": \"3.8\"}).get_ray_image() == \"rayproject/ray:123456-py38\"\n </s> add     assert (\n        _stub_test(\n            {\n                \"python\": \"3.8\",\n                \"cluster\": {\"byod\": {}},\n            }\n        ).get_ray_image()\n        == \"rayproject/ray:123456-py38\"\n    ) </s> remove     name = \"requirements_ml_byod\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod.txt\",\n </s> add     name = \"requirements_byod_3.9\",\n    requirements_in = \"ray_release/byod/requirements_byod_3.9.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod_3.9.txt\",\n    tags = [\n        \"team:ci\",\n    ],\n    visibility = [\"//visibility:private\"],\n)\n\ncompile_pip_requirements(\n    name = \"requirements_ml_byod_3.8\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod_3.8.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod_3.8.txt\",\n    tags = [\n        \"team:ci\",\n    ],\n    visibility = [\"//visibility:private\"],\n)\n\ncompile_pip_requirements(\n    name = \"requirements_ml_byod_3.9\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod_3.9.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod_3.9.txt\", </s> remove     name = \"requirements_byod\",\n    requirements_in = \"ray_release/byod/requirements_byod.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod.txt\",\n </s> add     name = \"requirements_byod_3.8\",\n    requirements_in = \"ray_release/byod/requirements_byod_3.8.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod_3.8.txt\", </s> remove         image_suffix = \"-gpu\" if self.get_byod_type() == \"gpu\" else \"\"\n </s> add         byod_type = self.get_byod_type()\n        image_suffix = f\"-{byod_type}\" if byod_type != \"cpu\" else \"\" </s> remove REQUIREMENTS_BYOD = \"requirements_byod.txt\"\nREQUIREMENTS_ML_BYOD = \"requirements_ml_byod.txt\"\n </s> add REQUIREMENTS_BYOD = \"requirements_byod\"\nREQUIREMENTS_ML_BYOD = \"requirements_ml_byod\"\nPYTHON_VERSION = \"3.8\"", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/ray_release/tests/test_test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> def test_get_anyscale_byod_image():\n <mask>     os.environ[\"BUILDKITE_BRANCH\"] = \"master\"\n <mask>     os.environ[\"BUILDKITE_COMMIT\"] = \"1234567890\"\n <mask>     assert (\n <mask>         _stub_test({}).get_anyscale_byod_image()\n <mask>         == f\"{DATAPLANE_ECR}/{DATAPLANE_ECR_REPO}:123456-py37\"\n <mask>     )\n <mask>     assert (\n <mask>         _stub_test(\n <mask>             {\n </s> [ci][byod] support python 3.9 (#36672) </s> remove     assert _stub_test({\"python\": \"3.8\"}).get_ray_image() == \"rayproject/ray:123456-py38\"\n </s> add     assert (\n        _stub_test(\n            {\n                \"python\": \"3.8\",\n                \"cluster\": {\"byod\": {}},\n            }\n        ).get_ray_image()\n        == \"rayproject/ray:123456-py38\"\n    ) </s> remove     assert _stub_test({}).get_ray_image() == \"rayproject/ray:1.0.0.123456-py37\"\n </s> add     assert (\n        _stub_test({\"cluster\": {\"byod\": {}}}).get_ray_image()\n        == \"rayproject/ray:1.0.0.123456-py37\"\n    ) </s> remove                 else REQUIREMENTS_ML_BYOD\n </s> add                 else f\"{REQUIREMENTS_ML_BYOD}_{test.get('python', PYTHON_VERSION)}.txt\" </s> remove                 REQUIREMENTS_BYOD\n </s> add                 f\"{REQUIREMENTS_BYOD}_{test.get('python', PYTHON_VERSION)}.txt\" </s> remove     name = \"requirements_ml_byod\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod.txt\",\n </s> add     name = \"requirements_byod_3.9\",\n    requirements_in = \"ray_release/byod/requirements_byod_3.9.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod_3.9.txt\",\n    tags = [\n        \"team:ci\",\n    ],\n    visibility = [\"//visibility:private\"],\n)\n\ncompile_pip_requirements(\n    name = \"requirements_ml_byod_3.8\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod_3.8.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod_3.8.txt\",\n    tags = [\n        \"team:ci\",\n    ],\n    visibility = [\"//visibility:private\"],\n)\n\ncompile_pip_requirements(\n    name = \"requirements_ml_byod_3.9\",\n    requirements_in = \"ray_release/byod/requirements_ml_byod_3.9.in\",\n    requirements_txt = \"ray_release/byod/requirements_ml_byod_3.9.txt\", </s> remove     name = \"requirements_byod\",\n    requirements_in = \"ray_release/byod/requirements_byod.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod.txt\",\n </s> add     name = \"requirements_byod_3.8\",\n    requirements_in = \"ray_release/byod/requirements_byod_3.8.in\",\n    requirements_txt = \"ray_release/byod/requirements_byod_3.8.txt\",", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/ray_release/tests/test_test.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask> \n <mask>   frequency: weekly\n <mask>   team: ml\n <mask>   cluster:\n <mask>     cluster_env: gptj_deepspeed_env.yaml\n <mask>     cluster_compute: gptj_deepspeed_compute_aws.yaml\n <mask> \n <mask>   run:\n <mask>     timeout: 4500\n </s> [ci][byod] support python 3.9 (#36672) </s> add     byod:\n      type: gpu </s> add     byod: \n      type: gpu </s> add     byod:\n      type: cu118 </s> add     byod:\n      type: gpu\n      pip:\n        - myst-parser==0.15.2\n        - myst-nb==0.13.1\n        - jupytext==1.13.6 </s> add     byod:\n      type: gpu </s> remove \t\t\t\t\t\t\"gpu\"\n </s> add \t\t\t\t\t\t\"gpu\",\n\t\t\t\t\t\t\"cu118\"", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/release_tests.yaml"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask>   team: ml\n <mask>   cluster:\n <mask>     cluster_env: 30b_deepspeed_env.yaml\n <mask>     cluster_compute: 30b_deepspeed_compute.yaml\n <mask> \n <mask>   run:\n </s> [ci][byod] support python 3.9 (#36672) </s> add     byod:\n      type: gpu </s> add     byod:\n      type: gpu\n      pip:\n        - myst-parser==0.15.2\n        - myst-nb==0.13.1\n        - jupytext==1.13.6 </s> add     byod: \n      type: gpu </s> add     byod:\n      type: cu118 </s> add     byod:\n      type: gpu </s> remove \t\t\t\t\t\t\"gpu\"\n </s> add \t\t\t\t\t\t\"gpu\",\n\t\t\t\t\t\t\"cu118\"", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/release_tests.yaml"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask>   team: data\n <mask>   cluster:\n <mask>     cluster_env: ../testing/cluster_envs/default_cluster_env_nightly_ml_py39.yaml\n <mask>     cluster_compute: ../testing/compute_configs/gpu/aws.yaml\n <mask> \n <mask>   run:\n <mask>     timeout: 600\n </s> [ci][byod] support python 3.9 (#36672) </s> add     byod:\n      type: gpu </s> add     byod:\n      type: cu118 </s> add     byod:\n      type: gpu\n      pip:\n        - myst-parser==0.15.2\n        - myst-nb==0.13.1\n        - jupytext==1.13.6 </s> add     byod:\n      type: gpu\n      pip:\n        - myst-parser==0.15.2\n        - myst-nb==0.13.1\n        - jupytext==1.13.6 </s> add     byod:\n      type: gpu </s> remove \t\t\t\t\t\t\"gpu\"\n </s> add \t\t\t\t\t\t\"gpu\",\n\t\t\t\t\t\t\"cu118\"", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/release_tests.yaml"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>   frequency: nightly-3x\n <mask>   team: ml\n <mask>   cluster:\n <mask>     cluster_env: ../testing/cluster_envs/02_many_model_training.yaml\n <mask>     cluster_compute: ../testing/compute_configs/cpu/aws.yaml\n <mask> \n <mask>   run:\n <mask>     timeout: 600\n </s> [ci][byod] support python 3.9 (#36672) </s> add     byod: \n      type: gpu </s> add     byod:\n      type: cu118 </s> add     byod:\n      type: gpu\n      pip:\n        - myst-parser==0.15.2\n        - myst-nb==0.13.1\n        - jupytext==1.13.6 </s> add     byod:\n      type: gpu\n      pip:\n        - myst-parser==0.15.2\n        - myst-nb==0.13.1\n        - jupytext==1.13.6 </s> add     byod:\n      type: gpu </s> remove \t\t\t\t\t\t\"gpu\"\n </s> add \t\t\t\t\t\t\"gpu\",\n\t\t\t\t\t\t\"cu118\"", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/release_tests.yaml"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>   frequency: nightly-3x\n <mask>   team: serve\n <mask>   cluster:\n <mask>     cluster_env: ../testing/cluster_envs/03_serving_stable_diffusion.yaml\n <mask>     cluster_compute: ../testing/compute_configs/gpu/aws.yaml\n <mask> \n <mask>   run:\n </s> [ci][byod] support python 3.9 (#36672) </s> add     byod:\n      type: gpu </s> add     byod: \n      type: gpu </s> add     byod:\n      type: gpu\n      pip:\n        - myst-parser==0.15.2\n        - myst-nb==0.13.1\n        - jupytext==1.13.6 </s> add     byod:\n      type: gpu\n      pip:\n        - myst-parser==0.15.2\n        - myst-nb==0.13.1\n        - jupytext==1.13.6 </s> add     byod:\n      type: gpu </s> remove \t\t\t\t\t\t\"gpu\"\n </s> add \t\t\t\t\t\t\"gpu\",\n\t\t\t\t\t\t\"cu118\"", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/release_tests.yaml"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask> \n <mask>   python: \"3.9\"\n <mask> \n <mask>   cluster:\n <mask>     cluster_env: app_config_gpu.yaml\n <mask>     cluster_compute: only_head_node_1gpu_64cpu.yaml\n <mask> \n <mask>   run:\n </s> [ci][byod] support python 3.9 (#36672) </s> add     byod: \n      type: gpu </s> add     byod:\n      type: cu118 </s> add     byod:\n      type: gpu </s> add     byod:\n      type: gpu\n      pip:\n        - myst-parser==0.15.2\n        - myst-nb==0.13.1\n        - jupytext==1.13.6 </s> add     byod:\n      type: gpu\n      pip:\n        - myst-parser==0.15.2\n        - myst-nb==0.13.1\n        - jupytext==1.13.6 </s> remove \t\t\t\t\t\t\"gpu\"\n </s> add \t\t\t\t\t\t\"gpu\",\n\t\t\t\t\t\t\"cu118\"", "html_url": "https://github.com/ray-project/ray/commit/8480709aa03590fff91101d4e7f512d9329f4451", "file_name": "release/release_tests.yaml"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   Map<UniqueId, Object> localActors = new HashMap<>();\n <mask> \n <mask>   public AbstractRayRuntime(RayConfig rayConfig) {\n <mask>     this.rayConfig = rayConfig;\n <mask>     functionManager = new FunctionManager();\n <mask>     worker = new Worker(this);\n <mask>     workerContext = new WorkerContext(rayConfig.workerMode, rayConfig.driverId);\n <mask>   }\n <mask> \n <mask>   /**\n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(FunctionManagerTest.resourcePath); </s> add   /**\n   * The resource path which we can load the driver's jar resources.\n   */\n  private String driverResourcePath;\n\n  /**\n   * Construct a FunctionManager with the specified driver resource path.\n   *\n   * @param driverResourcePath The specified driver resource that\n   *     can store the driver's resources.\n   */\n  public FunctionManager(String driverResourcePath) {\n    this.driverResourcePath = driverResourcePath;\n  }\n </s> remove         RayLog.core.info(\"load jar \" + appJar.getAbsolutePath());\n </s> add         LOGGER.info(\"succeeded to load jar {}.\", appJar.getAbsolutePath()); </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(JarLoader.class);\n </s> remove       //TODO(hchen): distinguish class loader by driver id.\n      ClassLoader classLoader = getClass().getClassLoader();\n </s> add       String resourcePath = driverResourcePath + \"/\" + driverId.toString() + \"/\";\n      ClassLoader classLoader;\n\n      try {\n        classLoader = JarLoader.loadJars(resourcePath, false);\n        LOGGER.info(\"Succeeded to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n      } catch (Exception e) {\n        LOGGER.error(\"Failed to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n        classLoader = getClass().getClassLoader();\n      }\n </s> add     // driver resource path\n    String localDriverResourcePath;\n    if (config.hasPath(\"ray.driver.resource-path\")) {\n      localDriverResourcePath = config.getString(\"ray.driver.resource-path\");\n    } else {\n      localDriverResourcePath = rayHome + \"/driver/resource\";\n      LOGGER.warn(\"Didn't configure ray.driver.resource-path, set it to default value: {}\",\n          localDriverResourcePath);\n    }\n\n    driverResourcePath = localDriverResourcePath;\n", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/runtime/src/main/java/org/ray/runtime/AbstractRayRuntime.java"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>   public final String plasmaStoreExecutablePath;\n <mask>   public final String rayletExecutablePath;\n <mask> \n <mask>   private void validate() {\n <mask>     if (workerMode == WorkerMode.WORKER) {\n <mask>       Preconditions.checkArgument(redisAddress != null,\n <mask>           \"Redis address must be set in worker mode.\");\n <mask>     } else {\n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> add   private final static String resourcePath = \"/tmp/ray/test/resource\";\n </s> add     // driver resource path\n    String localDriverResourcePath;\n    if (config.hasPath(\"ray.driver.resource-path\")) {\n      localDriverResourcePath = config.getString(\"ray.driver.resource-path\");\n    } else {\n      localDriverResourcePath = rayHome + \"/driver/resource\";\n      LOGGER.warn(\"Didn't configure ray.driver.resource-path, set it to default value: {}\",\n          localDriverResourcePath);\n    }\n\n    driverResourcePath = localDriverResourcePath;\n </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(FunctionManager.class);\n </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(JarLoader.class);\n </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(FunctionManagerTest.resourcePath); </s> remove       //TODO(hchen): distinguish class loader by driver id.\n      ClassLoader classLoader = getClass().getClassLoader();\n </s> add       String resourcePath = driverResourcePath + \"/\" + driverId.toString() + \"/\";\n      ClassLoader classLoader;\n\n      try {\n        classLoader = JarLoader.loadJars(resourcePath, false);\n        LOGGER.info(\"Succeeded to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n      } catch (Exception e) {\n        LOGGER.error(\"Failed to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n        classLoader = getClass().getClassLoader();\n      }\n", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/runtime/src/main/java/org/ray/runtime/config/RayConfig.java"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask>     rayletExecutablePath = rayHome + \"/build/src/ray/raylet/raylet\";\n <mask> \n <mask>     // validate config\n <mask>     validate();\n <mask>     LOGGER.debug(\"Created config: {}\", this);\n <mask>   }\n <mask> \n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> remove       //TODO(hchen): distinguish class loader by driver id.\n      ClassLoader classLoader = getClass().getClassLoader();\n </s> add       String resourcePath = driverResourcePath + \"/\" + driverId.toString() + \"/\";\n      ClassLoader classLoader;\n\n      try {\n        classLoader = JarLoader.loadJars(resourcePath, false);\n        LOGGER.info(\"Succeeded to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n      } catch (Exception e) {\n        LOGGER.error(\"Failed to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n        classLoader = getClass().getClassLoader();\n      }\n </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(FunctionManagerTest.resourcePath); </s> add     Assert.assertEquals(\"path/to/ray/driver/resource/path\", rayConfig.driverResourcePath);\n </s> remove         RayLog.core.info(\"load jar \" + appJar.getAbsolutePath());\n </s> add         LOGGER.info(\"succeeded to load jar {}.\", appJar.getAbsolutePath()); </s> remove   // If worker.mode is DRIVER, specify the driver id.\n  // If not provided, a random id will be used.\n  driver.id: \"\"\n </s> add     // Configuration items about driver.\n    driver {\n      // If worker.mode is DRIVER, specify the driver id.\n      // If not provided, a random id will be used.\n      id: \"\"\n      // If worker.mode is WORKER, it means that worker will load\n      // the resources from this path to execute tasks.\n      resource-path: /tmp/ray/driver/resource\n    } </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(JarLoader.class);\n", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/runtime/src/main/java/org/ray/runtime/config/RayConfig.java"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> import org.ray.api.function.RayFunc;\n <mask> import org.ray.api.id.UniqueId;\n <mask> import org.ray.runtime.util.LambdaUtils;\n <mask> import org.slf4j.Logger;\n <mask> import org.slf4j.LoggerFactory;\n <mask> \n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> add import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory; </s> remove import org.ray.runtime.util.logger.RayLog;\n </s> add import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory; </s> add import java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\nimport java.nio.file.StandardCopyOption; </s> remove       //TODO(hchen): distinguish class loader by driver id.\n      ClassLoader classLoader = getClass().getClassLoader();\n </s> add       String resourcePath = driverResourcePath + \"/\" + driverId.toString() + \"/\";\n      ClassLoader classLoader;\n\n      try {\n        classLoader = JarLoader.loadJars(resourcePath, false);\n        LOGGER.info(\"Succeeded to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n      } catch (Exception e) {\n        LOGGER.error(\"Failed to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n        classLoader = getClass().getClassLoader();\n      }\n </s> add   public final String driverResourcePath; </s> add     // driver resource path\n    String localDriverResourcePath;\n    if (config.hasPath(\"ray.driver.resource-path\")) {\n      localDriverResourcePath = config.getString(\"ray.driver.resource-path\");\n    } else {\n      localDriverResourcePath = rayHome + \"/driver/resource\";\n      LOGGER.warn(\"Didn't configure ray.driver.resource-path, set it to default value: {}\",\n          localDriverResourcePath);\n    }\n\n    driverResourcePath = localDriverResourcePath;\n", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/runtime/src/main/java/org/ray/runtime/functionmanager/FunctionManager.java"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask> import org.ray.runtime.util.JarLoader;\n <mask> import org.ray.runtime.util.LambdaUtils;\n <mask> \n <mask> /**\n <mask>  * Manages functions by driver id.\n <mask>  */\n <mask> public class FunctionManager {\n <mask> \n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(FunctionManager.class);\n </s> add import org.ray.runtime.util.JarLoader; </s> remove import org.ray.runtime.util.logger.RayLog;\n </s> add import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory; </s> add   /**\n   * The resource path which we can load the driver's jar resources.\n   */\n  private String driverResourcePath;\n\n  /**\n   * Construct a FunctionManager with the specified driver resource path.\n   *\n   * @param driverResourcePath The specified driver resource that\n   *     can store the driver's resources.\n   */\n  public FunctionManager(String driverResourcePath) {\n    this.driverResourcePath = driverResourcePath;\n  }\n </s> add import java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\nimport java.nio.file.StandardCopyOption; </s> remove       //TODO(hchen): distinguish class loader by driver id.\n      ClassLoader classLoader = getClass().getClassLoader();\n </s> add       String resourcePath = driverResourcePath + \"/\" + driverId.toString() + \"/\";\n      ClassLoader classLoader;\n\n      try {\n        classLoader = JarLoader.loadJars(resourcePath, false);\n        LOGGER.info(\"Succeeded to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n      } catch (Exception e) {\n        LOGGER.error(\"Failed to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n        classLoader = getClass().getClassLoader();\n      }\n", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/runtime/src/main/java/org/ray/runtime/functionmanager/FunctionManager.java"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>  * Manages functions by driver id.\n <mask>  */\n <mask> public class FunctionManager {\n <mask> \n <mask>   static final String CONSTRUCTOR_NAME = \"<init>\";\n <mask> \n <mask>   /**\n <mask>    * Cache from a RayFunc object to its corresponding FunctionDescriptor. Because\n <mask>    * `LambdaUtils.getSerializedLambda` is expensive.\n <mask>    */\n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> add import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory; </s> add   /**\n   * The resource path which we can load the driver's jar resources.\n   */\n  private String driverResourcePath;\n\n  /**\n   * Construct a FunctionManager with the specified driver resource path.\n   *\n   * @param driverResourcePath The specified driver resource that\n   *     can store the driver's resources.\n   */\n  public FunctionManager(String driverResourcePath) {\n    this.driverResourcePath = driverResourcePath;\n  }\n </s> remove import org.ray.runtime.util.logger.RayLog;\n </s> add import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory; </s> add   private final static String resourcePath = \"/tmp/ray/test/resource\";\n </s> remove       //TODO(hchen): distinguish class loader by driver id.\n      ClassLoader classLoader = getClass().getClassLoader();\n </s> add       String resourcePath = driverResourcePath + \"/\" + driverId.toString() + \"/\";\n      ClassLoader classLoader;\n\n      try {\n        classLoader = JarLoader.loadJars(resourcePath, false);\n        LOGGER.info(\"Succeeded to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n      } catch (Exception e) {\n        LOGGER.error(\"Failed to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n        classLoader = getClass().getClassLoader();\n      }\n </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(JarLoader.class);\n", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/runtime/src/main/java/org/ray/runtime/functionmanager/FunctionManager.java"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>    */\n <mask>   private Map<UniqueId, DriverFunctionTable> driverFunctionTables = new HashMap<>();\n <mask> \n <mask>   /**\n <mask>    * Get the RayFunction from a RayFunc instance (a lambda).\n <mask>    *\n <mask>    * @param driverId current driver id.\n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(FunctionManager.class);\n </s> remove import org.ray.runtime.util.logger.RayLog;\n </s> add import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory; </s> add import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory; </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(rayConfig.driverResourcePath); </s> remove   // If worker.mode is DRIVER, specify the driver id.\n  // If not provided, a random id will be used.\n  driver.id: \"\"\n </s> add     // Configuration items about driver.\n    driver {\n      // If worker.mode is DRIVER, specify the driver id.\n      // If not provided, a random id will be used.\n      id: \"\"\n      // If worker.mode is WORKER, it means that worker will load\n      // the resources from this path to execute tasks.\n      resource-path: /tmp/ray/driver/resource\n    } </s> remove       //TODO(hchen): distinguish class loader by driver id.\n      ClassLoader classLoader = getClass().getClassLoader();\n </s> add       String resourcePath = driverResourcePath + \"/\" + driverId.toString() + \"/\";\n      ClassLoader classLoader;\n\n      try {\n        classLoader = JarLoader.loadJars(resourcePath, false);\n        LOGGER.info(\"Succeeded to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n      } catch (Exception e) {\n        LOGGER.error(\"Failed to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n        classLoader = getClass().getClassLoader();\n      }\n", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/runtime/src/main/java/org/ray/runtime/functionmanager/FunctionManager.java"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>    */\n <mask>   public RayFunction getFunction(UniqueId driverId, FunctionDescriptor functionDescriptor) {\n <mask>     DriverFunctionTable driverFunctionTable = driverFunctionTables.get(driverId);\n <mask>     if (driverFunctionTable == null) {\n <mask>       //TODO(hchen): distinguish class loader by driver id.\n <mask>       ClassLoader classLoader = getClass().getClassLoader();\n <mask>       driverFunctionTable = new DriverFunctionTable(classLoader);\n <mask>       driverFunctionTables.put(driverId, driverFunctionTable);\n <mask>     }\n <mask>     return driverFunctionTable.getFunction(functionDescriptor);\n <mask>   }\n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(FunctionManager.class);\n </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(FunctionManagerTest.resourcePath); </s> add import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory; </s> add     // driver resource path\n    String localDriverResourcePath;\n    if (config.hasPath(\"ray.driver.resource-path\")) {\n      localDriverResourcePath = config.getString(\"ray.driver.resource-path\");\n    } else {\n      localDriverResourcePath = rayHome + \"/driver/resource\";\n      LOGGER.warn(\"Didn't configure ray.driver.resource-path, set it to default value: {}\",\n          localDriverResourcePath);\n    }\n\n    driverResourcePath = localDriverResourcePath;\n </s> add   /**\n   * The resource path which we can load the driver's jar resources.\n   */\n  private String driverResourcePath;\n\n  /**\n   * Construct a FunctionManager with the specified driver resource path.\n   *\n   * @param driverResourcePath The specified driver resource that\n   *     can store the driver's resources.\n   */\n  public FunctionManager(String driverResourcePath) {\n    this.driverResourcePath = driverResourcePath;\n  }\n </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(rayConfig.driverResourcePath);", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/runtime/src/main/java/org/ray/runtime/functionmanager/FunctionManager.java"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> import org.apache.commons.io.FileUtils;\n <mask> import org.apache.commons.io.IOUtils;\n <mask> import org.apache.commons.io.filefilter.DirectoryFileFilter;\n <mask> import org.apache.commons.io.filefilter.RegexFileFilter;\n <mask> import org.ray.runtime.util.logger.RayLog;\n <mask> \n <mask> /**\n <mask>  * load and unload jars from a dir.\n <mask>  */\n <mask> public class JarLoader {\n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> add import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory; </s> add import java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\nimport java.nio.file.StandardCopyOption; </s> add import org.ray.runtime.util.JarLoader; </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(FunctionManager.class);\n </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(JarLoader.class);\n </s> add   /**\n   * The resource path which we can load the driver's jar resources.\n   */\n  private String driverResourcePath;\n\n  /**\n   * Construct a FunctionManager with the specified driver resource path.\n   *\n   * @param driverResourcePath The specified driver resource that\n   *     can store the driver's resources.\n   */\n  public FunctionManager(String driverResourcePath) {\n    this.driverResourcePath = driverResourcePath;\n  }\n", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/runtime/src/main/java/org/ray/runtime/util/JarLoader.java"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> public class JarLoader {\n <mask> \n <mask>   public static URLClassLoader loadJars(String dir, boolean explicitLoad) {\n <mask>     // get all jars\n <mask>     Collection<File> jars = FileUtils.listFiles(\n <mask>         new File(dir),\n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> remove import org.ray.runtime.util.logger.RayLog;\n </s> add import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory; </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(FunctionManagerTest.resourcePath); </s> add   private final static String resourcePath = \"/tmp/ray/test/resource\";\n </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(FunctionManager.class);\n </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(rayConfig.driverResourcePath); </s> remove       //TODO(hchen): distinguish class loader by driver id.\n      ClassLoader classLoader = getClass().getClassLoader();\n </s> add       String resourcePath = driverResourcePath + \"/\" + driverId.toString() + \"/\";\n      ClassLoader classLoader;\n\n      try {\n        classLoader = JarLoader.loadJars(resourcePath, false);\n        LOGGER.info(\"Succeeded to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n      } catch (Exception e) {\n        LOGGER.error(\"Failed to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n        classLoader = getClass().getClassLoader();\n      }\n", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/runtime/src/main/java/org/ray/runtime/util/JarLoader.java"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     List<URL> urls = new ArrayList<>();\n <mask> \n <mask>     for (File appJar : appJars) {\n <mask>       try {\n <mask>         RayLog.core.info(\"load jar \" + appJar.getAbsolutePath());\n <mask>         JarFile jar = new JarFile(appJar.getAbsolutePath());\n <mask>         jars.add(jar);\n <mask>         urls.add(appJar.toURI().toURL());\n <mask>       } catch (IOException e) {\n <mask>         throw new RuntimeException(\n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> remove       //TODO(hchen): distinguish class loader by driver id.\n      ClassLoader classLoader = getClass().getClassLoader();\n </s> add       String resourcePath = driverResourcePath + \"/\" + driverId.toString() + \"/\";\n      ClassLoader classLoader;\n\n      try {\n        classLoader = JarLoader.loadJars(resourcePath, false);\n        LOGGER.info(\"Succeeded to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n      } catch (Exception e) {\n        LOGGER.error(\"Failed to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n        classLoader = getClass().getClassLoader();\n      }\n </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(FunctionManagerTest.resourcePath); </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(rayConfig.driverResourcePath); </s> add     // driver resource path\n    String localDriverResourcePath;\n    if (config.hasPath(\"ray.driver.resource-path\")) {\n      localDriverResourcePath = config.getString(\"ray.driver.resource-path\");\n    } else {\n      localDriverResourcePath = rayHome + \"/driver/resource\";\n      LOGGER.warn(\"Didn't configure ray.driver.resource-path, set it to default value: {}\",\n          localDriverResourcePath);\n    }\n\n    driverResourcePath = localDriverResourcePath;\n </s> add   /**\n   * The resource path which we can load the driver's jar resources.\n   */\n  private String driverResourcePath;\n\n  /**\n   * Construct a FunctionManager with the specified driver resource path.\n   *\n   * @param driverResourcePath The specified driver resource that\n   *     can store the driver's resources.\n   */\n  public FunctionManager(String driverResourcePath) {\n    this.driverResourcePath = driverResourcePath;\n  }\n </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(JarLoader.class);\n", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/runtime/src/main/java/org/ray/runtime/util/JarLoader.java"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>   // Available resources on this node, for example \"CPU:4,GPU:0\".\n <mask>   resources: \"\"\n <mask> \n <mask>   // If worker.mode is DRIVER, specify the driver id.\n <mask>   // If not provided, a random id will be used.\n <mask>   driver.id: \"\"\n <mask> \n <mask>   // Root dir of log files.\n <mask>   log-dir: /tmp/ray/logs\n <mask> \n <mask>   // If true, output of worker processes will be redirected to log files.\n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> add     // driver resource path\n    String localDriverResourcePath;\n    if (config.hasPath(\"ray.driver.resource-path\")) {\n      localDriverResourcePath = config.getString(\"ray.driver.resource-path\");\n    } else {\n      localDriverResourcePath = rayHome + \"/driver/resource\";\n      LOGGER.warn(\"Didn't configure ray.driver.resource-path, set it to default value: {}\",\n          localDriverResourcePath);\n    }\n\n    driverResourcePath = localDriverResourcePath;\n </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(FunctionManagerTest.resourcePath); </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(JarLoader.class);\n </s> add   public final String driverResourcePath; </s> add   /**\n   * The resource path which we can load the driver's jar resources.\n   */\n  private String driverResourcePath;\n\n  /**\n   * Construct a FunctionManager with the specified driver resource path.\n   *\n   * @param driverResourcePath The specified driver resource that\n   *     can store the driver's resources.\n   */\n  public FunctionManager(String driverResourcePath) {\n    this.driverResourcePath = driverResourcePath;\n  }\n </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(FunctionManager.class);\n", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/runtime/src/main/resources/ray.default.conf"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> package org.ray.runtime.functionmanager;\n <mask> \n <mask> import java.util.Map;\n <mask> import org.apache.commons.lang3.tuple.ImmutablePair;\n <mask> import org.apache.commons.lang3.tuple.Pair;\n <mask> import org.junit.Assert;\n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> add import org.ray.runtime.util.JarLoader; </s> remove import org.ray.runtime.util.logger.RayLog;\n </s> add import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory; </s> add import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory; </s> remove       //TODO(hchen): distinguish class loader by driver id.\n      ClassLoader classLoader = getClass().getClassLoader();\n </s> add       String resourcePath = driverResourcePath + \"/\" + driverId.toString() + \"/\";\n      ClassLoader classLoader;\n\n      try {\n        classLoader = JarLoader.loadJars(resourcePath, false);\n        LOGGER.info(\"Succeeded to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n      } catch (Exception e) {\n        LOGGER.error(\"Failed to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n        classLoader = getClass().getClassLoader();\n      }\n </s> add   public final String driverResourcePath; </s> add     // driver resource path\n    String localDriverResourcePath;\n    if (config.hasPath(\"ray.driver.resource-path\")) {\n      localDriverResourcePath = config.getString(\"ray.driver.resource-path\");\n    } else {\n      localDriverResourcePath = rayHome + \"/driver/resource\";\n      LOGGER.warn(\"Didn't configure ray.driver.resource-path, set it to default value: {}\",\n          localDriverResourcePath);\n    }\n\n    driverResourcePath = localDriverResourcePath;\n", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/runtime/src/test/java/org/ray/runtime/functionmanager/FunctionManagerTest.java"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>   private static FunctionDescriptor barConstructorDescriptor;\n <mask> \n <mask>   private FunctionManager functionManager;\n <mask> \n <mask>   @BeforeClass\n <mask>   public static void beforeClass() {\n <mask>     fooFunc = FunctionManagerTest::foo;\n <mask>     barConstructor = Bar::new;\n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(JarLoader.class);\n </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(FunctionManager.class);\n </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(FunctionManagerTest.resourcePath); </s> add   public final String driverResourcePath; </s> add   /**\n   * The resource path which we can load the driver's jar resources.\n   */\n  private String driverResourcePath;\n\n  /**\n   * Construct a FunctionManager with the specified driver resource path.\n   *\n   * @param driverResourcePath The specified driver resource that\n   *     can store the driver's resources.\n   */\n  public FunctionManager(String driverResourcePath) {\n    this.driverResourcePath = driverResourcePath;\n  }\n </s> add     System.setProperty(\"ray.driver.resource-path\", \"path/to/ray/driver/resource/path\");", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/runtime/src/test/java/org/ray/runtime/functionmanager/FunctionManagerTest.java"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   }\n <mask> \n <mask>   @Before\n <mask>   public void before() {\n <mask>     functionManager = new FunctionManager();\n <mask>   }\n <mask> \n <mask>   @Test\n <mask>   public void testGetFunctionFromRayFunc() {\n <mask>     // Test normal function.\n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(rayConfig.driverResourcePath); </s> add     System.setProperty(\"ray.driver.resource-path\", \"path/to/ray/driver/resource/path\"); </s> add   public final String driverResourcePath; </s> add   private final static String resourcePath = \"/tmp/ray/test/resource\";\n </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(JarLoader.class);\n </s> add     // driver resource path\n    String localDriverResourcePath;\n    if (config.hasPath(\"ray.driver.resource-path\")) {\n      localDriverResourcePath = config.getString(\"ray.driver.resource-path\");\n    } else {\n      localDriverResourcePath = rayHome + \"/driver/resource\";\n      LOGGER.warn(\"Didn't configure ray.driver.resource-path, set it to default value: {}\",\n          localDriverResourcePath);\n    }\n\n    driverResourcePath = localDriverResourcePath;\n", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/runtime/src/test/java/org/ray/runtime/functionmanager/FunctionManagerTest.java"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>   @Test\n <mask>   public void testCreateRayConfig() {\n <mask>     System.setProperty(\"ray.home\", \"/path/to/ray\");\n <mask>     RayConfig rayConfig = RayConfig.create();\n <mask> \n <mask>     Assert.assertEquals(\"/path/to/ray\", rayConfig.rayHome);\n <mask>     Assert.assertEquals(WorkerMode.DRIVER, rayConfig.workerMode);\n <mask>     Assert.assertEquals(RunMode.CLUSTER, rayConfig.runMode);\n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(FunctionManagerTest.resourcePath); </s> add   private final static String resourcePath = \"/tmp/ray/test/resource\";\n </s> add   public final String driverResourcePath; </s> add   private static final Logger LOGGER = LoggerFactory.getLogger(JarLoader.class);\n </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(rayConfig.driverResourcePath); </s> remove       //TODO(hchen): distinguish class loader by driver id.\n      ClassLoader classLoader = getClass().getClassLoader();\n </s> add       String resourcePath = driverResourcePath + \"/\" + driverId.toString() + \"/\";\n      ClassLoader classLoader;\n\n      try {\n        classLoader = JarLoader.loadJars(resourcePath, false);\n        LOGGER.info(\"Succeeded to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n      } catch (Exception e) {\n        LOGGER.error(\"Failed to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n        classLoader = getClass().getClassLoader();\n      }\n", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/test/src/main/java/org/ray/api/test/RayConfigTest.java"}
{"docstring_tokens": "keep add keep keep", "code_tokens": " <mask>     Assert.assertEquals(System.getProperty(\"user.dir\") +\n <mask>         \"/build/src/common/thirdparty/redis/src/redis-server\", rayConfig.redisServerExecutablePath);\n <mask>   }\n <mask> }\n </s> [Java] Load driver resources from local path. (#3001)\n\n## What do these changes do?\r\n1. Add a configuration item `driver.resource-path`.\r\n2. Load driver resources from the local path which is specified in the `ray.conf`.\r\n\r\nBefore this change, we should add all driver resources(like user's jar package, dependencies package and config files) into `classpath`.\r\n\r\nAfter this change, we should add the driver resources into the mount path which we can configure it in `ray.conf`, and we shouldn't configure `classpath` for driver resources any more.\r\n\r\n## Related issue number\r\nN/A </s> add     // driver resource path\n    String localDriverResourcePath;\n    if (config.hasPath(\"ray.driver.resource-path\")) {\n      localDriverResourcePath = config.getString(\"ray.driver.resource-path\");\n    } else {\n      localDriverResourcePath = rayHome + \"/driver/resource\";\n      LOGGER.warn(\"Didn't configure ray.driver.resource-path, set it to default value: {}\",\n          localDriverResourcePath);\n    }\n\n    driverResourcePath = localDriverResourcePath;\n </s> remove       //TODO(hchen): distinguish class loader by driver id.\n      ClassLoader classLoader = getClass().getClassLoader();\n </s> add       String resourcePath = driverResourcePath + \"/\" + driverId.toString() + \"/\";\n      ClassLoader classLoader;\n\n      try {\n        classLoader = JarLoader.loadJars(resourcePath, false);\n        LOGGER.info(\"Succeeded to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n      } catch (Exception e) {\n        LOGGER.error(\"Failed to load driver({}) resource. Resource path is {}\",\n            driverId, resourcePath);\n        classLoader = getClass().getClassLoader();\n      }\n </s> remove         RayLog.core.info(\"load jar \" + appJar.getAbsolutePath());\n </s> add         LOGGER.info(\"succeeded to load jar {}.\", appJar.getAbsolutePath()); </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(FunctionManagerTest.resourcePath); </s> remove     functionManager = new FunctionManager();\n </s> add     functionManager = new FunctionManager(rayConfig.driverResourcePath); </s> add   public final String driverResourcePath;", "html_url": "https://github.com/ray-project/ray/commit/84bf5fc8f35c7ed46283d8c9b11e6b85df41d677", "file_name": "java/test/src/main/java/org/ray/api/test/RayConfigTest.java"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask> from filelock import FileLock\n <mask> from torch.nn import functional as F\n <mask> import pytorch_lightning as pl\n <mask> from pl_bolts.datamodules.mnist_datamodule import MNISTDataModule\n <mask> import os\n <mask> from ray.tune.integration.pytorch_lightning import TuneReportCallback\n <mask> \n <mask> from ray import tune\n </s> [tune] Update Lightning examples to support PTL 1.5 (#20562)\n\nTo helps resolve the issues users are facing with running Lightning examples with Ray Tune PyTorchLightning/pytorch-lightning#10407\r\n\r\nCo-authored-by: Amog Kamsetty <amogkamsetty@yahoo.com> </s> remove from ray.util.ray_lightning.tune import TuneReportCallback, get_tune_ddp_resources\n </s> add from ray.util.ray_lightning.tune import TuneReportCallback, get_tune_resources </s> remove get_tune_ddp_resources = None\n </s> add get_tune_resources = None </s> remove         get_tune_ddp_resources,\n </s> add         get_tune_resources, </s> remove ray_lightning==0.1.1\n </s> add ray_lightning==0.2.0 </s> remove pytorch-lightning==1.4.9\n </s> add pytorch-lightning==1.5.10 </s> remove     \"get_tune_ddp_resources\",\n </s> add     \"get_tune_resources\",", "html_url": "https://github.com/ray-project/ray/commit/8515fdd6db604ab484a22481bc74d81476b5c65f", "file_name": "python/ray/tune/examples/mnist_ptl_mini.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         # mnist images are (1, 28, 28) (channels, width, height)\n <mask>         self.layer_1 = torch.nn.Linear(28 * 28, layer_1)\n <mask>         self.layer_2 = torch.nn.Linear(layer_1, layer_2)\n <mask>         self.layer_3 = torch.nn.Linear(layer_2, 10)\n <mask>         self.accuracy = pl.metrics.Accuracy()\n <mask> \n <mask>     def forward(self, x):\n <mask>         batch_size, channels, width, height = x.size()\n <mask>         x = x.view(batch_size, -1)\n <mask>         x = self.layer_1(x)\n </s> [tune] Update Lightning examples to support PTL 1.5 (#20562)\n\nTo helps resolve the issues users are facing with running Lightning examples with Ray Tune PyTorchLightning/pytorch-lightning#10407\r\n\r\nCo-authored-by: Amog Kamsetty <amogkamsetty@yahoo.com> </s> remove     trainer = pl.Trainer(max_epochs=10, show_progress_bar=False)\n </s> add     trainer = pl.Trainer(max_epochs=10, enable_progress_bar=False) </s> remove get_tune_ddp_resources = None\n </s> add get_tune_resources = None </s> remove     \"get_tune_ddp_resources\",\n </s> add     \"get_tune_resources\", </s> remove from ray.util.ray_lightning.tune import TuneReportCallback, get_tune_ddp_resources\n </s> add from ray.util.ray_lightning.tune import TuneReportCallback, get_tune_resources </s> remove         progress_bar_refresh_rate=0,\n </s> add         enable_progress_bar=False, </s> remove ray_lightning==0.1.1\n </s> add ray_lightning==0.2.0", "html_url": "https://github.com/ray-project/ray/commit/8515fdd6db604ab484a22481bc74d81476b5c65f", "file_name": "python/ray/tune/examples/mnist_ptl_mini.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     trainer = pl.Trainer(\n <mask>         max_epochs=num_epochs,\n <mask>         # If fractional GPUs passed in, convert to int.\n <mask>         gpus=math.ceil(num_gpus),\n <mask>         progress_bar_refresh_rate=0,\n <mask>         callbacks=[TuneReportCallback(metrics, on=\"validation_end\")],\n <mask>     )\n <mask>     trainer.fit(model, dm)\n <mask> \n <mask> \n </s> [tune] Update Lightning examples to support PTL 1.5 (#20562)\n\nTo helps resolve the issues users are facing with running Lightning examples with Ray Tune PyTorchLightning/pytorch-lightning#10407\r\n\r\nCo-authored-by: Amog Kamsetty <amogkamsetty@yahoo.com> </s> remove         progress_bar_refresh_rate=0,\n </s> add         enable_progress_bar=False, </s> remove         \"progress_bar_refresh_rate\": 0,\n </s> add         \"enable_progress_bar\": False, </s> remove     trainer = pl.Trainer(max_epochs=10, show_progress_bar=False)\n </s> add     trainer = pl.Trainer(max_epochs=10, enable_progress_bar=False) </s> remove         resources_per_trial=get_tune_ddp_resources(\n </s> add         resources_per_trial=get_tune_resources( </s> remove         get_tune_ddp_resources,\n </s> add         get_tune_resources, </s> remove         self.accuracy = pl.metrics.Accuracy()\n </s> add         self.accuracy = Accuracy()", "html_url": "https://github.com/ray-project/ray/commit/8515fdd6db604ab484a22481bc74d81476b5c65f", "file_name": "python/ray/tune/examples/mnist_ptl_mini.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> \n <mask> def train_mnist(config):\n <mask>     model = LightningMNISTClassifier(config)\n <mask>     trainer = pl.Trainer(max_epochs=10, show_progress_bar=False)\n <mask> \n <mask>     trainer.fit(model)\n <mask> # __lightning_end__\n <mask> \n <mask> # __no_tune_train_begin__\n </s> [tune] Update Lightning examples to support PTL 1.5 (#20562)\n\nTo helps resolve the issues users are facing with running Lightning examples with Ray Tune PyTorchLightning/pytorch-lightning#10407\r\n\r\nCo-authored-by: Amog Kamsetty <amogkamsetty@yahoo.com> </s> remove         progress_bar_refresh_rate=0,\n </s> add         enable_progress_bar=False, </s> remove         self.accuracy = pl.metrics.Accuracy()\n </s> add         self.accuracy = Accuracy() </s> remove get_tune_ddp_resources = None\n </s> add get_tune_resources = None </s> remove ray_lightning==0.1.1\n </s> add ray_lightning==0.2.0 </s> remove     \"get_tune_ddp_resources\",\n </s> add     \"get_tune_resources\", </s> remove from ray.util.ray_lightning.tune import TuneReportCallback, get_tune_ddp_resources\n </s> add from ray.util.ray_lightning.tune import TuneReportCallback, get_tune_resources", "html_url": "https://github.com/ray-project/ray/commit/8515fdd6db604ab484a22481bc74d81476b5c65f", "file_name": "python/ray/tune/examples/mnist_pytorch_lightning.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         # If fractional GPUs passed in, convert to int.\n <mask>         gpus=math.ceil(num_gpus),\n <mask>         logger=TensorBoardLogger(\n <mask>             save_dir=tune.get_trial_dir(), name=\"\", version=\".\"),\n <mask>         progress_bar_refresh_rate=0,\n <mask>         callbacks=[\n <mask>             TuneReportCallback(\n <mask>                 {\n <mask>                     \"loss\": \"ptl/val_loss\",\n <mask>                     \"mean_accuracy\": \"ptl/val_accuracy\"\n </s> [tune] Update Lightning examples to support PTL 1.5 (#20562)\n\nTo helps resolve the issues users are facing with running Lightning examples with Ray Tune PyTorchLightning/pytorch-lightning#10407\r\n\r\nCo-authored-by: Amog Kamsetty <amogkamsetty@yahoo.com> </s> remove         \"progress_bar_refresh_rate\": 0,\n </s> add         \"enable_progress_bar\": False, </s> remove         progress_bar_refresh_rate=0,\n </s> add         enable_progress_bar=False, </s> remove     trainer = pl.Trainer(max_epochs=10, show_progress_bar=False)\n </s> add     trainer = pl.Trainer(max_epochs=10, enable_progress_bar=False) </s> remove ray_lightning==0.1.1\n </s> add ray_lightning==0.2.0 </s> remove         self.accuracy = pl.metrics.Accuracy()\n </s> add         self.accuracy = Accuracy() </s> remove pytorch-lightning==1.4.9\n </s> add pytorch-lightning==1.5.10", "html_url": "https://github.com/ray-project/ray/commit/8515fdd6db604ab484a22481bc74d81476b5c65f", "file_name": "python/ray/tune/examples/mnist_pytorch_lightning.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         # If fractional GPUs passed in, convert to int.\n <mask>         \"gpus\": math.ceil(num_gpus),\n <mask>         \"logger\": TensorBoardLogger(\n <mask>             save_dir=tune.get_trial_dir(), name=\"\", version=\".\"),\n <mask>         \"progress_bar_refresh_rate\": 0,\n <mask>         \"callbacks\": [\n <mask>             TuneReportCheckpointCallback(\n <mask>                 metrics={\n <mask>                     \"loss\": \"ptl/val_loss\",\n <mask>                     \"mean_accuracy\": \"ptl/val_accuracy\"\n </s> [tune] Update Lightning examples to support PTL 1.5 (#20562)\n\nTo helps resolve the issues users are facing with running Lightning examples with Ray Tune PyTorchLightning/pytorch-lightning#10407\r\n\r\nCo-authored-by: Amog Kamsetty <amogkamsetty@yahoo.com> </s> remove         progress_bar_refresh_rate=0,\n </s> add         enable_progress_bar=False, </s> remove         progress_bar_refresh_rate=0,\n </s> add         enable_progress_bar=False, </s> remove     \"get_tune_ddp_resources\",\n </s> add     \"get_tune_resources\", </s> remove     trainer = pl.Trainer(max_epochs=10, show_progress_bar=False)\n </s> add     trainer = pl.Trainer(max_epochs=10, enable_progress_bar=False) </s> remove ray_lightning==0.1.1\n </s> add ray_lightning==0.2.0 </s> remove         self.accuracy = pl.metrics.Accuracy()\n </s> add         self.accuracy = Accuracy()", "html_url": "https://github.com/ray-project/ray/commit/8515fdd6db604ab484a22481bc74d81476b5c65f", "file_name": "python/ray/tune/examples/mnist_pytorch_lightning.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> from torchvision import transforms\n <mask> import pytorch_lightning as pl\n <mask> \n <mask> from ray.util.ray_lightning import RayPlugin\n <mask> from ray.util.ray_lightning.tune import TuneReportCallback, get_tune_ddp_resources\n <mask> \n <mask> num_cpus_per_actor = 1\n <mask> num_workers = 1\n <mask> \n <mask> \n </s> [tune] Update Lightning examples to support PTL 1.5 (#20562)\n\nTo helps resolve the issues users are facing with running Lightning examples with Ray Tune PyTorchLightning/pytorch-lightning#10407\r\n\r\nCo-authored-by: Amog Kamsetty <amogkamsetty@yahoo.com> </s> add from torchmetrics import Accuracy </s> remove get_tune_ddp_resources = None\n </s> add get_tune_resources = None </s> remove         get_tune_ddp_resources,\n </s> add         get_tune_resources, </s> remove         self.accuracy = pl.metrics.Accuracy()\n </s> add         self.accuracy = Accuracy() </s> remove     trainer = pl.Trainer(max_epochs=10, show_progress_bar=False)\n </s> add     trainer = pl.Trainer(max_epochs=10, enable_progress_bar=False) </s> remove     \"get_tune_ddp_resources\",\n </s> add     \"get_tune_resources\",", "html_url": "https://github.com/ray-project/ray/commit/8515fdd6db604ab484a22481bc74d81476b5c65f", "file_name": "python/ray/util/ray_lightning/simple_tune.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         config=config,\n <mask>         num_samples=1,\n <mask>         metric=\"loss\",\n <mask>         mode=\"min\",\n <mask>         resources_per_trial=get_tune_ddp_resources(\n <mask>             num_workers=num_workers, cpus_per_worker=num_cpus_per_actor\n <mask>         ),\n <mask>     )\n <mask> \n <mask>     print(\"Best hyperparameters: \", analysis.best_config)\n </s> [tune] Update Lightning examples to support PTL 1.5 (#20562)\n\nTo helps resolve the issues users are facing with running Lightning examples with Ray Tune PyTorchLightning/pytorch-lightning#10407\r\n\r\nCo-authored-by: Amog Kamsetty <amogkamsetty@yahoo.com> </s> remove         progress_bar_refresh_rate=0,\n </s> add         enable_progress_bar=False, </s> remove         get_tune_ddp_resources,\n </s> add         get_tune_resources, </s> remove ray_lightning==0.1.1\n </s> add ray_lightning==0.2.0 </s> remove pytorch-lightning==1.4.9\n </s> add pytorch-lightning==1.5.10 </s> remove     \"get_tune_ddp_resources\",\n </s> add     \"get_tune_resources\", </s> remove get_tune_ddp_resources = None\n </s> add get_tune_resources = None", "html_url": "https://github.com/ray-project/ray/commit/8515fdd6db604ab484a22481bc74d81476b5c65f", "file_name": "python/ray/util/ray_lightning/simple_tune.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> logger = logging.getLogger(__name__)\n <mask> \n <mask> TuneReportCallback = None\n <mask> TuneReportCheckpointCallback = None\n <mask> get_tune_ddp_resources = None\n <mask> \n <mask> try:\n <mask>     from ray_lightning.tune import (\n <mask>         TuneReportCallback,\n <mask>         TuneReportCheckpointCallback,\n </s> [tune] Update Lightning examples to support PTL 1.5 (#20562)\n\nTo helps resolve the issues users are facing with running Lightning examples with Ray Tune PyTorchLightning/pytorch-lightning#10407\r\n\r\nCo-authored-by: Amog Kamsetty <amogkamsetty@yahoo.com> </s> remove         get_tune_ddp_resources,\n </s> add         get_tune_resources, </s> remove from ray.util.ray_lightning.tune import TuneReportCallback, get_tune_ddp_resources\n </s> add from ray.util.ray_lightning.tune import TuneReportCallback, get_tune_resources </s> add from torchmetrics import Accuracy </s> remove         self.accuracy = pl.metrics.Accuracy()\n </s> add         self.accuracy = Accuracy() </s> remove     trainer = pl.Trainer(max_epochs=10, show_progress_bar=False)\n </s> add     trainer = pl.Trainer(max_epochs=10, enable_progress_bar=False) </s> remove     \"get_tune_ddp_resources\",\n </s> add     \"get_tune_resources\",", "html_url": "https://github.com/ray-project/ray/commit/8515fdd6db604ab484a22481bc74d81476b5c65f", "file_name": "python/ray/util/ray_lightning/tune/__init__.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> try:\n <mask>     from ray_lightning.tune import (\n <mask>         TuneReportCallback,\n <mask>         TuneReportCheckpointCallback,\n <mask>         get_tune_ddp_resources,\n <mask>     )\n <mask> except ImportError:\n <mask>     logger.info(\n <mask>         \"ray_lightning is not installed. Please run \"\n <mask>         \"`pip install git+https://github.com/ray-project/\"\n </s> [tune] Update Lightning examples to support PTL 1.5 (#20562)\n\nTo helps resolve the issues users are facing with running Lightning examples with Ray Tune PyTorchLightning/pytorch-lightning#10407\r\n\r\nCo-authored-by: Amog Kamsetty <amogkamsetty@yahoo.com> </s> remove get_tune_ddp_resources = None\n </s> add get_tune_resources = None </s> remove from ray.util.ray_lightning.tune import TuneReportCallback, get_tune_ddp_resources\n </s> add from ray.util.ray_lightning.tune import TuneReportCallback, get_tune_resources </s> add from torchmetrics import Accuracy </s> remove         resources_per_trial=get_tune_ddp_resources(\n </s> add         resources_per_trial=get_tune_resources( </s> remove         progress_bar_refresh_rate=0,\n </s> add         enable_progress_bar=False, </s> remove ray_lightning==0.1.1\n </s> add ray_lightning==0.2.0", "html_url": "https://github.com/ray-project/ray/commit/8515fdd6db604ab484a22481bc74d81476b5c65f", "file_name": "python/ray/util/ray_lightning/tune/__init__.py"}
{"docstring_tokens": "keep keep keep keep replace keep", "code_tokens": " <mask> \n <mask> __all__ = [\n <mask>     \"TuneReportCallback\",\n <mask>     \"TuneReportCheckpointCallback\",\n <mask>     \"get_tune_ddp_resources\",\n <mask> ]\n </s> [tune] Update Lightning examples to support PTL 1.5 (#20562)\n\nTo helps resolve the issues users are facing with running Lightning examples with Ray Tune PyTorchLightning/pytorch-lightning#10407\r\n\r\nCo-authored-by: Amog Kamsetty <amogkamsetty@yahoo.com> </s> remove         \"progress_bar_refresh_rate\": 0,\n </s> add         \"enable_progress_bar\": False, </s> remove get_tune_ddp_resources = None\n </s> add get_tune_resources = None </s> remove         self.accuracy = pl.metrics.Accuracy()\n </s> add         self.accuracy = Accuracy() </s> remove     trainer = pl.Trainer(max_epochs=10, show_progress_bar=False)\n </s> add     trainer = pl.Trainer(max_epochs=10, enable_progress_bar=False) </s> remove from ray.util.ray_lightning.tune import TuneReportCallback, get_tune_ddp_resources\n </s> add from ray.util.ray_lightning.tune import TuneReportCallback, get_tune_resources </s> remove         progress_bar_refresh_rate=0,\n </s> add         enable_progress_bar=False,", "html_url": "https://github.com/ray-project/ray/commit/8515fdd6db604ab484a22481bc74d81476b5c65f", "file_name": "python/ray/util/ray_lightning/tune/__init__.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> nevergrad==0.4.3.post7\n <mask> optuna==2.9.1\n <mask> pytest-remotedata==0.3.2\n <mask> lightning-bolts==0.4.0\n <mask> pytorch-lightning==1.4.9\n <mask> shortuuid==1.0.1\n <mask> scikit-learn==0.24.2\n <mask> scikit-optimize==0.8.1\n <mask> sigopt==7.5.0\n <mask> smart_open==5.1.0\n </s> [tune] Update Lightning examples to support PTL 1.5 (#20562)\n\nTo helps resolve the issues users are facing with running Lightning examples with Ray Tune PyTorchLightning/pytorch-lightning#10407\r\n\r\nCo-authored-by: Amog Kamsetty <amogkamsetty@yahoo.com> </s> remove ray_lightning==0.1.1\n </s> add ray_lightning==0.2.0 </s> remove     \"get_tune_ddp_resources\",\n </s> add     \"get_tune_resources\", </s> remove         get_tune_ddp_resources,\n </s> add         get_tune_resources, </s> remove get_tune_ddp_resources = None\n </s> add get_tune_resources = None </s> remove         resources_per_trial=get_tune_ddp_resources(\n </s> add         resources_per_trial=get_tune_resources( </s> remove from ray.util.ray_lightning.tune import TuneReportCallback, get_tune_ddp_resources\n </s> add from ray.util.ray_lightning.tune import TuneReportCallback, get_tune_resources", "html_url": "https://github.com/ray-project/ray/commit/8515fdd6db604ab484a22481bc74d81476b5c65f", "file_name": "python/requirements/ml/requirements_tune.txt"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep", "code_tokens": " <mask> # Upstream dependencies that depend on Ray.\n <mask> # Because they depend on Ray, we can't pin the subdependencies.\n <mask> # So we separate its own requirements file.\n <mask> \n <mask> ray_lightning==0.1.1\n <mask> tune-sklearn==0.4.1\n <mask> xgboost_ray==0.1.4\n <mask> lightgbm_ray==0.0.2\n <mask> modin>=0.11.0; python_version >= '3.7'\n </s> [tune] Update Lightning examples to support PTL 1.5 (#20562)\n\nTo helps resolve the issues users are facing with running Lightning examples with Ray Tune PyTorchLightning/pytorch-lightning#10407\r\n\r\nCo-authored-by: Amog Kamsetty <amogkamsetty@yahoo.com> </s> remove     trainer = pl.Trainer(max_epochs=10, show_progress_bar=False)\n </s> add     trainer = pl.Trainer(max_epochs=10, enable_progress_bar=False) </s> remove         progress_bar_refresh_rate=0,\n </s> add         enable_progress_bar=False, </s> remove         progress_bar_refresh_rate=0,\n </s> add         enable_progress_bar=False, </s> remove         \"progress_bar_refresh_rate\": 0,\n </s> add         \"enable_progress_bar\": False, </s> remove         self.accuracy = pl.metrics.Accuracy()\n </s> add         self.accuracy = Accuracy() </s> remove pytorch-lightning==1.4.9\n </s> add pytorch-lightning==1.5.10", "html_url": "https://github.com/ray-project/ray/commit/8515fdd6db604ab484a22481bc74d81476b5c65f", "file_name": "python/requirements/ml/requirements_upstream.txt"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> \n <mask> import ray\n <mask> from ray.experimental.data.datasource import Datasource, WriteTask\n <mask> from ray.experimental.data.impl.batcher import Batcher\n <mask> from ray.experimental.data.impl.compute import get_compute\n <mask> from ray.experimental.data.impl.progress_bar import ProgressBar\n </s> [data] Move Block to public API so that datasource API doesn't reference a private interface (#17098) </s> remove from ray.experimental.data.impl.block import ObjectRef, Block, SimpleBlock, \\\n    BlockMetadata\n </s> add from ray.experimental.data.impl.block_builder import SimpleBlock </s> add from ray.experimental.data.block import ObjectRef, Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata\n </s> add from ray.experimental.data.block import Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef </s> remove from ray.experimental.data.impl.block import ObjectRef\n </s> add from ray.experimental.data.block import ObjectRef </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T\nfrom ray.experimental.data.impl.block_builder import SimpleBlock", "html_url": "https://github.com/ray-project/ray/commit/85725f2018285244efc71366d6a18586d3bbbc9e", "file_name": "python/ray/experimental/data/dataset.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> from ray.experimental.data.impl.batcher import Batcher\n <mask> from ray.experimental.data.impl.compute import get_compute\n <mask> from ray.experimental.data.impl.progress_bar import ProgressBar\n <mask> from ray.experimental.data.impl.shuffle import simple_shuffle\n <mask> from ray.experimental.data.impl.block import ObjectRef, Block, SimpleBlock, \\\n <mask>     BlockMetadata\n <mask> from ray.experimental.data.impl.block_list import BlockList\n <mask> from ray.experimental.data.impl.arrow_block import (\n <mask>     DelegatingArrowBlockBuilder, ArrowBlock)\n <mask> \n <mask> T = TypeVar(\"T\")\n </s> [data] Move Block to public API so that datasource API doesn't reference a private interface (#17098) </s> add from ray.experimental.data.block import ObjectRef, Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata\n </s> add from ray.experimental.data.block import Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef </s> remove from ray.experimental.data.impl.block import Block, SimpleBlock\nfrom ray.experimental.data.impl.block_list import BlockList, BlockMetadata\n </s> add  </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef, T\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T </s> add from ray.experimental.data.block import ObjectRef, Block, BlockMetadata", "html_url": "https://github.com/ray-project/ray/commit/85725f2018285244efc71366d6a18586d3bbbc9e", "file_name": "python/ray/experimental/data/dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             datasource: The datasource to write to.\n <mask>             write_args: Additional write args to pass to the datasource.\n <mask>         \"\"\"\n <mask> \n <mask>         write_tasks = datasource.prepare_write(self._blocks, **write_args)\n <mask>         progress = ProgressBar(\"Write Progress\", len(write_tasks))\n <mask> \n <mask>         @ray.remote\n <mask>         def remote_write(task: WriteTask) -> Any:\n <mask>             return task()\n </s> [data] Move Block to public API so that datasource API doesn't reference a private interface (#17098) </s> remove             blocks: List of data block references and block metadata. It is\n                recommended that one write task be generated per block.\n </s> add             blocks: List of data block references. It is recommended that one\n                write task be generated per block.\n            metadata: List of block metadata. </s> remove     def prepare_write(self, blocks: BlockList,\n </s> add     def prepare_write(self, blocks: List[ObjectRef[Block[T]]],\n                      metadata: List[BlockMetadata], </s> remove     def prepare_write(self, blocks: BlockList,\n </s> add     def prepare_write(self, blocks: List[ObjectRef[Block[T]]],\n                      metadata: List[BlockMetadata], </s> remove T = TypeVar(\"T\")\n </s> add  </s> remove from ray.experimental.data.impl.block import Block, SimpleBlock\nfrom ray.experimental.data.impl.block_list import BlockList, BlockMetadata\n </s> add  </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef", "html_url": "https://github.com/ray-project/ray/commit/85725f2018285244efc71366d6a18586d3bbbc9e", "file_name": "python/ray/experimental/data/dataset.py"}
{"docstring_tokens": "keep replace keep keep keep keep keep", "code_tokens": " <mask> import builtins\n <mask> from typing import Any, Generic, List, Callable, Union, TypeVar\n <mask> \n <mask> import numpy as np\n <mask> \n <mask> import ray\n <mask> from ray.experimental.data.impl.arrow_block import ArrowRow, ArrowBlock\n </s> [data] Move Block to public API so that datasource API doesn't reference a private interface (#17098) </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T\nfrom ray.experimental.data.impl.block_builder import SimpleBlock </s> remove from ray.experimental.data.impl.block import Block, SimpleBlock\nfrom ray.experimental.data.impl.block_list import BlockList, BlockMetadata\n </s> add  </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef, T\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T </s> remove T = TypeVar(\"T\")\n </s> add  </s> add from ray.experimental.data.block import ObjectRef, Block, BlockMetadata", "html_url": "https://github.com/ray-project/ray/commit/85725f2018285244efc71366d6a18586d3bbbc9e", "file_name": "python/ray/experimental/data/datasource/datasource.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask> \n <mask> import numpy as np\n <mask> \n <mask> import ray\n <mask> from ray.experimental.data.impl.arrow_block import ArrowRow, ArrowBlock\n <mask> \n <mask> WriteResult = Any\n <mask> \n </s> [data] Move Block to public API so that datasource API doesn't reference a private interface (#17098) </s> remove from ray.experimental.data.impl.block import Block, SimpleBlock\nfrom ray.experimental.data.impl.block_list import BlockList, BlockMetadata\n </s> add  </s> remove from typing import Any, Generic, List, Callable, Union, TypeVar\n </s> add from typing import Any, Generic, List, Callable, Union </s> remove T = TypeVar(\"T\")\n </s> add  </s> add from ray.experimental.data.block import ObjectRef, Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata\n </s> add from ray.experimental.data.block import Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef", "html_url": "https://github.com/ray-project/ray/commit/85725f2018285244efc71366d6a18586d3bbbc9e", "file_name": "python/ray/experimental/data/datasource/datasource.py"}
{"docstring_tokens": "keep keep replace replace keep replace keep keep keep keep", "code_tokens": " <mask> import ray\n <mask> from ray.experimental.data.impl.arrow_block import ArrowRow, ArrowBlock\n <mask> from ray.experimental.data.impl.block import Block, SimpleBlock\n <mask> from ray.experimental.data.impl.block_list import BlockList, BlockMetadata\n <mask> \n <mask> T = TypeVar(\"T\")\n <mask> WriteResult = Any\n <mask> \n <mask> \n <mask> class Datasource(Generic[T]):\n </s> [data] Move Block to public API so that datasource API doesn't reference a private interface (#17098) </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T\nfrom ray.experimental.data.impl.block_builder import SimpleBlock </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata\n </s> add from ray.experimental.data.block import Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import ObjectRef, Block, SimpleBlock, \\\n    BlockMetadata\n </s> add from ray.experimental.data.impl.block_builder import SimpleBlock </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef, T\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T", "html_url": "https://github.com/ray-project/ray/commit/85725f2018285244efc71366d6a18586d3bbbc9e", "file_name": "python/ray/experimental/data/datasource/datasource.py"}
{"docstring_tokens": "keep replace keep keep keep keep replace replace keep", "code_tokens": " <mask> \n <mask>     def prepare_write(self, blocks: BlockList,\n <mask>                       **write_args) -> List[\"WriteTask[T]\"]:\n <mask>         \"\"\"Return the list of tasks needed to perform a write.\n <mask> \n <mask>         Args:\n <mask>             blocks: List of data block references and block metadata. It is\n <mask>                 recommended that one write task be generated per block.\n <mask>             write_args: Additional kwargs to pass to the datasource impl.\n </s> [data] Move Block to public API so that datasource API doesn't reference a private interface (#17098) </s> remove         write_tasks = datasource.prepare_write(self._blocks, **write_args)\n </s> add         write_tasks = datasource.prepare_write(self._blocks,\n                                               self._blocks.get_metadata(),\n                                               **write_args) </s> remove     def prepare_write(self, blocks: BlockList,\n </s> add     def prepare_write(self, blocks: List[ObjectRef[Block[T]]],\n                      metadata: List[BlockMetadata], </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef, T\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T </s> remove T = TypeVar(\"T\")\n </s> add  </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef, T\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T", "html_url": "https://github.com/ray-project/ray/commit/85725f2018285244efc71366d6a18586d3bbbc9e", "file_name": "python/ray/experimental/data/datasource/datasource.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         self.data_sink = DataSink.remote()\n <mask>         self.num_ok = 0\n <mask>         self.num_failed = 0\n <mask> \n <mask>     def prepare_write(self, blocks: BlockList,\n <mask>                       **write_args) -> List[\"WriteTask[T]\"]:\n <mask>         tasks = []\n <mask>         for b in blocks:\n <mask>             tasks.append(\n <mask>                 WriteTask(lambda b=b: ray.get(self.data_sink.write.remote(b))))\n </s> [data] Move Block to public API so that datasource API doesn't reference a private interface (#17098) </s> remove     def prepare_write(self, blocks: BlockList,\n </s> add     def prepare_write(self, blocks: List[ObjectRef[Block[T]]],\n                      metadata: List[BlockMetadata], </s> remove             blocks: List of data block references and block metadata. It is\n                recommended that one write task be generated per block.\n </s> add             blocks: List of data block references. It is recommended that one\n                write task be generated per block.\n            metadata: List of block metadata. </s> remove         write_tasks = datasource.prepare_write(self._blocks, **write_args)\n </s> add         write_tasks = datasource.prepare_write(self._blocks,\n                                               self._blocks.get_metadata(),\n                                               **write_args) </s> remove T = TypeVar(\"T\")\n </s> add  </s> remove from ray.experimental.data.impl.block import Block, SimpleBlock\nfrom ray.experimental.data.impl.block_list import BlockList, BlockMetadata\n </s> add  </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef, T\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T", "html_url": "https://github.com/ray-project/ray/commit/85725f2018285244efc71366d6a18586d3bbbc9e", "file_name": "python/ray/experimental/data/datasource/datasource.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     import pyarrow\n <mask> except ImportError:\n <mask>     pyarrow = None\n <mask> \n <mask> from ray.experimental.data.impl.block import Block, BlockBuilder, \\\n <mask>     SimpleBlockBuilder\n <mask> \n <mask> if TYPE_CHECKING:\n <mask>     import pandas\n <mask> \n </s> [data] Move Block to public API so that datasource API doesn't reference a private interface (#17098) </s> remove from ray.experimental.data.impl.block import ObjectRef\n </s> add from ray.experimental.data.block import ObjectRef </s> remove from ray.experimental.data.impl.block import ObjectRef, Block, SimpleBlock, \\\n    BlockMetadata\n </s> add from ray.experimental.data.impl.block_builder import SimpleBlock </s> add from ray.experimental.data.block import ObjectRef, Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata\n </s> add from ray.experimental.data.block import Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef </s> remove from ray.experimental.data.impl.block import Block, SimpleBlock\nfrom ray.experimental.data.impl.block_list import BlockList, BlockMetadata\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/85725f2018285244efc71366d6a18586d3bbbc9e", "file_name": "python/ray/experimental/data/impl/arrow_block.py"}
{"docstring_tokens": "keep keep replace keep keep keep keep keep", "code_tokens": " <mask> from typing import Optional\n <mask> \n <mask> from ray.experimental.data.impl.block import Block\n <mask> from ray.experimental.data.impl.arrow_block import DelegatingArrowBlockBuilder\n <mask> \n <mask> \n <mask> class Batcher:\n <mask>     \"\"\"Chunks blocks into batches.\n </s> [data] Move Block to public API so that datasource API doesn't reference a private interface (#17098) </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata\n </s> add from ray.experimental.data.block import Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import Block, BlockBuilder, \\\n </s> add from ray.experimental.data.block import Block\nfrom ray.experimental.data.impl.block_builder import BlockBuilder, \\ </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef, T\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef, T\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T </s> remove T = TypeVar(\"T\")\n </s> add  </s> remove from ray.experimental.data.impl.block import ObjectRef\n </s> add from ray.experimental.data.block import ObjectRef", "html_url": "https://github.com/ray-project/ray/commit/85725f2018285244efc71366d6a18586d3bbbc9e", "file_name": "python/ray/experimental/data/impl/batcher.py"}
{"docstring_tokens": "keep keep replace keep keep keep keep keep", "code_tokens": " <mask> from typing import Iterable, List\n <mask> \n <mask> from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef, T\n <mask> \n <mask> \n <mask> class BlockList(Iterable[ObjectRef[Block[T]]]):\n <mask>     def __init__(self, blocks: List[ObjectRef[Block[T]]],\n <mask>                  metadata: List[BlockMetadata]):\n </s> [data] Move Block to public API so that datasource API doesn't reference a private interface (#17098) </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef, T\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T\nfrom ray.experimental.data.impl.block_builder import SimpleBlock </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata\n </s> add from ray.experimental.data.block import Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import Block\n </s> add from ray.experimental.data.block import Block </s> remove     def prepare_write(self, blocks: BlockList,\n </s> add     def prepare_write(self, blocks: List[ObjectRef[Block[T]]],\n                      metadata: List[BlockMetadata],", "html_url": "https://github.com/ray-project/ray/commit/85725f2018285244efc71366d6a18586d3bbbc9e", "file_name": "python/ray/experimental/data/impl/block_list.py"}
{"docstring_tokens": "keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> from typing import TypeVar, Iterable, Any, Union\n <mask> \n <mask> import ray\n <mask> from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\n <mask> from ray.experimental.data.impl.block_list import BlockList\n <mask> from ray.experimental.data.impl.progress_bar import ProgressBar\n <mask> \n <mask> T = TypeVar(\"T\")\n <mask> U = TypeVar(\"U\")\n </s> [data] Move Block to public API so that datasource API doesn't reference a private interface (#17098) </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata\n </s> add from ray.experimental.data.block import Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import ObjectRef, Block, SimpleBlock, \\\n    BlockMetadata\n </s> add from ray.experimental.data.impl.block_builder import SimpleBlock </s> remove from ray.experimental.data.impl.block import ObjectRef\n </s> add from ray.experimental.data.block import ObjectRef </s> remove from typing import Any, Generic, List, Callable, Union, TypeVar\n </s> add from typing import Any, Generic, List, Callable, Union </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef, T\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef, T\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T", "html_url": "https://github.com/ray-project/ray/commit/85725f2018285244efc71366d6a18586d3bbbc9e", "file_name": "python/ray/experimental/data/impl/compute.py"}
{"docstring_tokens": "keep keep replace keep keep keep keep keep", "code_tokens": " <mask> from typing import Callable, List\n <mask> \n <mask> from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef, T\n <mask> from ray.experimental.data.impl.block_list import BlockList\n <mask> \n <mask> \n <mask> class LazyBlockList(BlockList[T]):\n <mask>     def __init__(self, calls: Callable[[], ObjectRef[Block]],\n </s> [data] Move Block to public API so that datasource API doesn't reference a private interface (#17098) </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef, T\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata\n </s> add from ray.experimental.data.block import Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import ObjectRef, Block, SimpleBlock, \\\n    BlockMetadata\n </s> add from ray.experimental.data.impl.block_builder import SimpleBlock </s> remove from typing import Any, Generic, List, Callable, Union, TypeVar\n </s> add from typing import Any, Generic, List, Callable, Union </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T\nfrom ray.experimental.data.impl.block_builder import SimpleBlock", "html_url": "https://github.com/ray-project/ray/commit/85725f2018285244efc71366d6a18586d3bbbc9e", "file_name": "python/ray/experimental/data/impl/lazy_block_list.py"}
{"docstring_tokens": "keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> from typing import List\n <mask> \n <mask> import ray\n <mask> from ray.experimental.data.impl.block import ObjectRef\n <mask> \n <mask> try:\n <mask>     import tqdm\n <mask>     needs_warning = False\n <mask> except ImportError:\n </s> [data] Move Block to public API so that datasource API doesn't reference a private interface (#17098) </s> remove from ray.experimental.data.impl.block import Block, BlockBuilder, \\\n </s> add from ray.experimental.data.block import Block\nfrom ray.experimental.data.impl.block_builder import BlockBuilder, \\ </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata\n </s> add from ray.experimental.data.block import Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import Block, SimpleBlock\nfrom ray.experimental.data.impl.block_list import BlockList, BlockMetadata\n </s> add  </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef, T\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T </s> remove from ray.experimental.data.impl.block import ObjectRef, Block, SimpleBlock, \\\n    BlockMetadata\n </s> add from ray.experimental.data.impl.block_builder import SimpleBlock", "html_url": "https://github.com/ray-project/ray/commit/85725f2018285244efc71366d6a18586d3bbbc9e", "file_name": "python/ray/experimental/data/impl/progress_bar.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> import math\n <mask> from typing import TypeVar, List\n <mask> \n <mask> import ray\n <mask> from ray.experimental.data.impl.block import Block, BlockMetadata\n <mask> from ray.experimental.data.impl.progress_bar import ProgressBar\n <mask> from ray.experimental.data.impl.block_list import BlockList\n <mask> from ray.experimental.data.impl.arrow_block import DelegatingArrowBlockBuilder\n <mask> \n <mask> T = TypeVar(\"T\")\n </s> [data] Move Block to public API so that datasource API doesn't reference a private interface (#17098) </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef </s> remove from ray.experimental.data.impl.block import ObjectRef, Block, SimpleBlock, \\\n    BlockMetadata\n </s> add from ray.experimental.data.impl.block_builder import SimpleBlock </s> add from ray.experimental.data.block import ObjectRef, Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import Block, SimpleBlock\nfrom ray.experimental.data.impl.block_list import BlockList, BlockMetadata\n </s> add  </s> remove from ray.experimental.data.impl.block import Block, BlockMetadata, ObjectRef, T\n </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T </s> remove from ray.experimental.data.impl.block import Block\n </s> add from ray.experimental.data.block import Block", "html_url": "https://github.com/ray-project/ray/commit/85725f2018285244efc71366d6a18586d3bbbc9e", "file_name": "python/ray/experimental/data/impl/shuffle.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask> \n <mask> import ray\n <mask> from ray.experimental.data.dataset import Dataset\n <mask> from ray.experimental.data.datasource import Datasource, RangeDatasource, \\\n <mask>     JSONDatasource, CSVDatasource, ReadTask\n <mask> from ray.experimental.data.impl import reader as _reader\n <mask> from ray.experimental.data.impl.arrow_block import ArrowBlock, ArrowRow\n <mask> from ray.experimental.data.impl.block_builder import SimpleBlock\n </s> [data] Move Block to public API so that datasource API doesn't reference a private interface (#17098) </s> add from ray.experimental.data.block import ObjectRef, Block, BlockMetadata </s> remove from ray.experimental.data.impl.block import ObjectRef, Block, SimpleBlock, \\\n    BlockMetadata\n </s> add from ray.experimental.data.impl.block_builder import SimpleBlock </s> add from ray.experimental.data.block import Block, BlockMetadata, ObjectRef, T\nfrom ray.experimental.data.impl.block_builder import SimpleBlock </s> remove from ray.experimental.data.impl.block import Block, BlockBuilder, \\\n </s> add from ray.experimental.data.block import Block\nfrom ray.experimental.data.impl.block_builder import BlockBuilder, \\ </s> remove from ray.experimental.data.impl.block import Block, SimpleBlock\nfrom ray.experimental.data.impl.block_list import BlockList, BlockMetadata\n </s> add  </s> remove from typing import Any, Generic, List, Callable, Union, TypeVar\n </s> add from typing import Any, Generic, List, Callable, Union", "html_url": "https://github.com/ray-project/ray/commit/85725f2018285244efc71366d6a18586d3bbbc9e", "file_name": "python/ray/experimental/data/read_api.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> We use Github to track issues, feature requests, and bugs. Take a look at the\n <mask> ones labeled `\"good first issue\" <https://github.com/ray-project/ray/issues?utf8=%E2%9C%93&q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22>`__ and `\"help wanted\" <https://github.com/ray-project/ray/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22>`__ for a place to start.\n <mask> \n <mask> \n <mask> Examples of features to contribute\n <mask> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n <mask> \n <mask> Tune\n <mask> ####\n <mask> \n <mask> We encourage both the developers and users of optimization libraries to contribute\n <mask> :ref:`tune-search-alg` to Tune wrapping around them. Search Algorithms allow Tune's users\n <mask> to take advantage of algorithms contained in external libraries while benefitting from \n <mask> a unified API and other Tune features.\n <mask> \n <mask> For implementation details, please refer to :ref:`byo-algo`.\n <mask> \n <mask> \n <mask> Setting up your development environment\n <mask> ---------------------------------------\n <mask> \n <mask> To edit the Ray source code, you'll want to checkout the repository and also build Ray from source. Follow :ref:`these instructions for building <building-ray>` a local copy of Ray to easily make changes.\n <mask> \n </s> Update the contribution guide / style guide (#18753) </s> remove 5. Request code reviews from other contributors and address their comments. One fast way to get reviews is\n   to help review others' code so that they return the favor. You should aim to improve the code as much as\n   possible before the review. We highly value patches that can get in without extensive reviews.\n </s> add 5. Request code reviews from other contributors and address their comments. During the review\n   process you may need to address merge conflicts with other changes. To resolve merge conflicts,\n   run ``git pull . upstream/master`` on your branch (please do not use rebase, as it is less\n   friendly to the GitHub review tool. All commits will be squashed on merge.) </s> remove      git pull upstream master\n </s> add      git pull . upstream/master </s> remove Suppose that one of the tests in a file of tests, e.g.,\n``python/ray/tests/test_basic.py``, is failing. You can run just that\ntest file locally as follows:\n </s> add The full suite of tests is too large to run on a single machine. However, you can run individual relevant Python test files. Suppose that one of the tests in a file of tests, e.g., ``python/ray/tests/test_basic.py``, is failing. You can run just that test file locally as follows: </s> remove To run all Python tests:\n\n.. code-block:: shell\n\n    pytest python/ray/tests/\n\nDocumentation should be documented in `Google style <https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html>`__ format.\n\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/85aaca8d4504968eb853f2e3dba05ff4914b0602", "file_name": "doc/source/getting-involved.rst"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>    .. code:: bash\n <mask> \n <mask>      git remote add upstream https://github.com/ray-project/ray.git\n <mask>      git pull upstream master\n <mask> \n <mask> 2. Make sure all existing tests `pass <getting-involved.html#testing>`__.\n <mask> 3. If introducing a new feature or patching a bug, be sure to add new test cases\n <mask>    in the relevant file in `ray/python/ray/tests/`.\n <mask> 4. Document the code. Public functions need to be documented, and remember to provide an usage\n </s> Update the contribution guide / style guide (#18753) </s> remove 5. Request code reviews from other contributors and address their comments. One fast way to get reviews is\n   to help review others' code so that they return the favor. You should aim to improve the code as much as\n   possible before the review. We highly value patches that can get in without extensive reviews.\n </s> add 5. Request code reviews from other contributors and address their comments. During the review\n   process you may need to address merge conflicts with other changes. To resolve merge conflicts,\n   run ``git pull . upstream/master`` on your branch (please do not use rebase, as it is less\n   friendly to the GitHub review tool. All commits will be squashed on merge.) </s> remove Suppose that one of the tests in a file of tests, e.g.,\n``python/ray/tests/test_basic.py``, is failing. You can run just that\ntest file locally as follows:\n </s> add The full suite of tests is too large to run on a single machine. However, you can run individual relevant Python test files. Suppose that one of the tests in a file of tests, e.g., ``python/ray/tests/test_basic.py``, is failing. You can run just that test file locally as follows: </s> remove \nExamples of features to contribute\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTune\n####\n\nWe encourage both the developers and users of optimization libraries to contribute\n:ref:`tune-search-alg` to Tune wrapping around them. Search Algorithms allow Tune's users\nto take advantage of algorithms contained in external libraries while benefitting from \na unified API and other Tune features.\n\nFor implementation details, please refer to :ref:`byo-algo`.\n\n\n </s> add  </s> remove To run all Python tests:\n\n.. code-block:: shell\n\n    pytest python/ray/tests/\n\nDocumentation should be documented in `Google style <https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html>`__ format.\n\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/85aaca8d4504968eb853f2e3dba05ff4914b0602", "file_name": "doc/source/getting-involved.rst"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask> 3. If introducing a new feature or patching a bug, be sure to add new test cases\n <mask>    in the relevant file in `ray/python/ray/tests/`.\n <mask> 4. Document the code. Public functions need to be documented, and remember to provide an usage\n <mask>    example if applicable.\n <mask> 5. Request code reviews from other contributors and address their comments. One fast way to get reviews is\n <mask>    to help review others' code so that they return the favor. You should aim to improve the code as much as\n <mask>    possible before the review. We highly value patches that can get in without extensive reviews.\n <mask> 6. Reviewers will merge and approve the pull request; be sure to ping them if\n <mask>    the pull request is getting stale.\n <mask> \n <mask> Testing\n <mask> -------\n </s> Update the contribution guide / style guide (#18753) </s> remove      git pull upstream master\n </s> add      git pull . upstream/master </s> remove Suppose that one of the tests in a file of tests, e.g.,\n``python/ray/tests/test_basic.py``, is failing. You can run just that\ntest file locally as follows:\n </s> add The full suite of tests is too large to run on a single machine. However, you can run individual relevant Python test files. Suppose that one of the tests in a file of tests, e.g., ``python/ray/tests/test_basic.py``, is failing. You can run just that test file locally as follows: </s> remove \nExamples of features to contribute\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTune\n####\n\nWe encourage both the developers and users of optimization libraries to contribute\n:ref:`tune-search-alg` to Tune wrapping around them. Search Algorithms allow Tune's users\nto take advantage of algorithms contained in external libraries while benefitting from \na unified API and other Tune features.\n\nFor implementation details, please refer to :ref:`byo-algo`.\n\n\n </s> add  </s> remove To run all Python tests:\n\n.. code-block:: shell\n\n    pytest python/ray/tests/\n\nDocumentation should be documented in `Google style <https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html>`__ format.\n\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/85aaca8d4504968eb853f2e3dba05ff4914b0602", "file_name": "doc/source/getting-involved.rst"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask> .. code-block:: shell\n <mask> \n <mask>     pip install -r python/requirements.txt\n <mask> \n <mask> To run all Python tests:\n <mask> \n <mask> .. code-block:: shell\n <mask> \n <mask>     pytest python/ray/tests/\n <mask> \n <mask> Documentation should be documented in `Google style <https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html>`__ format.\n <mask> \n <mask> \n <mask> Testing for Python development\n <mask> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n <mask> \n <mask> Suppose that one of the tests in a file of tests, e.g.,\n <mask> ``python/ray/tests/test_basic.py``, is failing. You can run just that\n </s> Update the contribution guide / style guide (#18753) </s> remove Suppose that one of the tests in a file of tests, e.g.,\n``python/ray/tests/test_basic.py``, is failing. You can run just that\ntest file locally as follows:\n </s> add The full suite of tests is too large to run on a single machine. However, you can run individual relevant Python test files. Suppose that one of the tests in a file of tests, e.g., ``python/ray/tests/test_basic.py``, is failing. You can run just that test file locally as follows: </s> remove 5. Request code reviews from other contributors and address their comments. One fast way to get reviews is\n   to help review others' code so that they return the favor. You should aim to improve the code as much as\n   possible before the review. We highly value patches that can get in without extensive reviews.\n </s> add 5. Request code reviews from other contributors and address their comments. During the review\n   process you may need to address merge conflicts with other changes. To resolve merge conflicts,\n   run ``git pull . upstream/master`` on your branch (please do not use rebase, as it is less\n   friendly to the GitHub review tool. All commits will be squashed on merge.) </s> remove      git pull upstream master\n </s> add      git pull . upstream/master </s> remove \nExamples of features to contribute\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTune\n####\n\nWe encourage both the developers and users of optimization libraries to contribute\n:ref:`tune-search-alg` to Tune wrapping around them. Search Algorithms allow Tune's users\nto take advantage of algorithms contained in external libraries while benefitting from \na unified API and other Tune features.\n\nFor implementation details, please refer to :ref:`byo-algo`.\n\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/85aaca8d4504968eb853f2e3dba05ff4914b0602", "file_name": "doc/source/getting-involved.rst"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> Testing for Python development\n <mask> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n <mask> \n <mask> Suppose that one of the tests in a file of tests, e.g.,\n <mask> ``python/ray/tests/test_basic.py``, is failing. You can run just that\n <mask> test file locally as follows:\n <mask> \n <mask> .. code-block:: shell\n <mask> \n <mask>  python -m pytest -v python/ray/tests/test_basic.py\n <mask> \n </s> Update the contribution guide / style guide (#18753) </s> remove To run all Python tests:\n\n.. code-block:: shell\n\n    pytest python/ray/tests/\n\nDocumentation should be documented in `Google style <https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html>`__ format.\n\n\n </s> add  </s> remove 5. Request code reviews from other contributors and address their comments. One fast way to get reviews is\n   to help review others' code so that they return the favor. You should aim to improve the code as much as\n   possible before the review. We highly value patches that can get in without extensive reviews.\n </s> add 5. Request code reviews from other contributors and address their comments. During the review\n   process you may need to address merge conflicts with other changes. To resolve merge conflicts,\n   run ``git pull . upstream/master`` on your branch (please do not use rebase, as it is less\n   friendly to the GitHub review tool. All commits will be squashed on merge.) </s> remove      git pull upstream master\n </s> add      git pull . upstream/master </s> remove \nExamples of features to contribute\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTune\n####\n\nWe encourage both the developers and users of optimization libraries to contribute\n:ref:`tune-search-alg` to Tune wrapping around them. Search Algorithms allow Tune's users\nto take advantage of algorithms contained in external libraries while benefitting from \na unified API and other Tune features.\n\nFor implementation details, please refer to :ref:`byo-algo`.\n\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/85aaca8d4504968eb853f2e3dba05ff4914b0602", "file_name": "doc/source/getting-involved.rst"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep replace replace keep keep keep", "code_tokens": " <mask> For even finer-grained control over training, you can use RLlib's lower-level `building blocks <rllib-concepts.html>`__ directly to implement `fully customized training workflows <https://github.com/ray-project/ray/blob/master/rllib/examples/rollout_worker_custom_workflow.py>`__.\n <mask> \n <mask> Global Coordination\n <mask> ~~~~~~~~~~~~~~~~~~~\n <mask> Sometimes, it is necessary to coordinate between pieces of code that live in different processes managed by RLlib. For example, it can be useful to maintain a global average of a certain variable, or centrally control a hyperparameter used by policies. Ray provides a general way to achieve this through *named actors* (learn more about Ray actors `here <actors.html>`__). As an example, consider maintaining a shared global counter that is incremented by environments and read periodically from your driver program:\n <mask> \n <mask> .. code-block:: python\n <mask> \n <mask>     from ray.util import named_actors\n <mask> \n <mask>     @ray.remote\n <mask>     class Counter:\n <mask>        def __init__(self):\n </s> Update named actor API (#8559) </s> remove     counter = named_actors.get_actor(\"global_counter\")\n </s> add     counter = ray.get_actor(\"global_counter\") </s> remove                 detached=True,\n </s> add  </s> remove     counter = Counter.remote()\n    named_actors.register_actor(\"global_counter\", counter)\n </s> add     counter = Counter.options(name=\"global_counter\").remote() </s> remove         if name and not detached:\n            raise ValueError(\"Only detached actors can be named. \"\n                             \"Please use Actor._remote(detached=True, \"\n                             \"name='some_name').\")\n\n        if name == \"\":\n            raise ValueError(\"Actor name cannot be an empty string.\")\n </s> add         if name is not None:\n            if not isinstance(name, str):\n                raise TypeError(\"name must be None or a string, \"\n                                \"got: '{}'.\".format(type(name)))\n            if name == \"\":\n                raise ValueError(\"Actor name cannot be an empty string.\") </s> remove                     \"a different name or get existing actor using \"\n                    \"ray.util.get_actor('{name}')\".format(name=name))\n </s> add                     \"a different name or get the existing actor using \"\n                    \"ray.get_actor('{name}')\".format(name=name))\n            detached = True\n        else:\n            detached = False", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "doc/source/rllib-training.rst"}
{"docstring_tokens": "keep replace replace keep keep keep replace keep keep keep keep", "code_tokens": " <mask>     # on the driver\n <mask>     counter = Counter.remote()\n <mask>     named_actors.register_actor(\"global_counter\", counter)\n <mask>     print(ray.get(counter.get.remote()))  # get the latest count\n <mask> \n <mask>     # in your envs\n <mask>     counter = named_actors.get_actor(\"global_counter\")\n <mask>     counter.inc.remote(1)  # async call to increment the global count\n <mask> \n <mask> Ray actors provide high levels of performance, so in more complex cases they can be used implement communication patterns such as parameter servers and allreduce.\n <mask> \n </s> Update named actor API (#8559) </s> remove Sometimes, it is necessary to coordinate between pieces of code that live in different processes managed by RLlib. For example, it can be useful to maintain a global average of a certain variable, or centrally control a hyperparameter used by policies. Ray provides a general way to achieve this through *named actors* (learn more about Ray actors `here <actors.html>`__). As an example, consider maintaining a shared global counter that is incremented by environments and read periodically from your driver program:\n </s> add Sometimes, it is necessary to coordinate between pieces of code that live in different processes managed by RLlib. For example, it can be useful to maintain a global average of a certain variable, or centrally control a hyperparameter used by policies. Ray provides a general way to achieve this through *detached actors* (learn more about Ray actors `here <actors.html>`__). These actors are assigned a global name and handles to them can be retrieved using these names. As an example, consider maintaining a shared global counter that is incremented by environments and read periodically from your driver program: </s> remove     from ray.util import named_actors\n\n </s> add  </s> remove         master_actor = ray.util.get_actor(master_actor_name)\n </s> add         master_actor = ray.get_actor(master_actor_name) </s> remove                 worker = ray.worker.global_worker\n                # Kill the actor with no_restart=True.\n                worker.core_worker.kill_actor(replica._ray_actor_id, True)\n </s> add                 ray.kill(replica, no_restart=True) </s> remove                     replica = ray.util.get_actor(replica_tag)\n </s> add                     replica = ray.get_actor(replica_tag)", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "doc/source/rllib-training.rst"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>     connect,\n <mask>     disconnect,\n <mask>     get,\n <mask>     get_gpu_ids,\n <mask>     get_resource_ids,\n <mask>     get_webui_url,\n <mask>     init,\n </s> Update named actor API (#8559) </s> remove def test_register_and_get_named_actors(ray_start_regular):\n    # TODO(heyucongtom): We should test this from another driver.\n\n    @ray.remote\n    class Foo:\n        def __init__(self):\n            self.x = 0\n\n        def method(self):\n            self.x += 1\n            return self.x\n\n    f1 = Foo.remote()\n    # Test saving f.\n    ray.util.register_actor(\"f1\", f1)\n    # Test getting f.\n    f2 = ray.util.get_actor(\"f1\")\n    assert f1._actor_id == f2._actor_id\n\n    # Test same name register shall raise error.\n    with pytest.raises(ValueError):\n        ray.util.register_actor(\"f1\", f2)\n\n    # Test register with wrong object type.\n    with pytest.raises(TypeError):\n        ray.util.register_actor(\"f3\", 1)\n\n    # Test getting a nonexistent actor.\n    with pytest.raises(ValueError):\n        ray.util.get_actor(\"nonexistent\")\n\n    # Test method\n    assert ray.get(f1.method.remote()) == 1\n    assert ray.get(f2.method.remote()) == 2\n    assert ray.get(f1.method.remote()) == 3\n    assert ray.get(f2.method.remote()) == 4\n\n\n </s> add  </s> remove                 ray.util.get_actor(name)\n </s> add                 ray.get_actor(name) </s> remove             self.router = ray.util.get_actor(router_name)\n </s> add             self.router = ray.get_actor(router_name) </s> remove         detached=True,\n </s> add  </s> remove         master_actor = ray.util.get_actor(master_actor_name)\n </s> add         master_actor = ray.get_actor(master_actor_name) </s> remove     def __ray_kill__(self):\n        \"\"\"Deprecated - use ray.kill() instead.\"\"\"\n        logger.warning(\"actor.__ray_kill__() is deprecated and will be removed\"\n                       \" in the near future. Use ray.kill(actor) instead.\")\n        ray.kill(self)\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/__init__.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>     \"connect\",\n <mask>     \"disconnect\",\n <mask>     \"get\",\n <mask>     \"get_gpu_ids\",\n <mask>     \"get_resource_ids\",\n <mask>     \"get_webui_url\",\n <mask>     \"init\",\n <mask>     \"internal\",\n </s> Update named actor API (#8559) </s> remove def test_register_and_get_named_actors(ray_start_regular):\n    # TODO(heyucongtom): We should test this from another driver.\n\n    @ray.remote\n    class Foo:\n        def __init__(self):\n            self.x = 0\n\n        def method(self):\n            self.x += 1\n            return self.x\n\n    f1 = Foo.remote()\n    # Test saving f.\n    ray.util.register_actor(\"f1\", f1)\n    # Test getting f.\n    f2 = ray.util.get_actor(\"f1\")\n    assert f1._actor_id == f2._actor_id\n\n    # Test same name register shall raise error.\n    with pytest.raises(ValueError):\n        ray.util.register_actor(\"f1\", f2)\n\n    # Test register with wrong object type.\n    with pytest.raises(TypeError):\n        ray.util.register_actor(\"f3\", 1)\n\n    # Test getting a nonexistent actor.\n    with pytest.raises(ValueError):\n        ray.util.get_actor(\"nonexistent\")\n\n    # Test method\n    assert ray.get(f1.method.remote()) == 1\n    assert ray.get(f2.method.remote()) == 2\n    assert ray.get(f1.method.remote()) == 3\n    assert ray.get(f2.method.remote()) == 4\n\n\n </s> add  </s> remove                 ray.util.get_actor(name)\n </s> add                 ray.get_actor(name) </s> remove             self.router = ray.util.get_actor(router_name)\n </s> add             self.router = ray.get_actor(router_name) </s> remove         detached=True,\n </s> add  </s> remove         master_actor = ray.util.get_actor(master_actor_name)\n </s> add         master_actor = ray.get_actor(master_actor_name) </s> remove     def __ray_kill__(self):\n        \"\"\"Deprecated - use ray.kill() instead.\"\"\"\n        logger.warning(\"actor.__ray_kill__() is deprecated and will be removed\"\n                       \" in the near future. Use ray.kill(actor) instead.\")\n        ray.kill(self)\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/__init__.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>                 concurrency defaults to 1 for threaded execution, and 1000 for\n <mask>                 asyncio execution. Note that the execution order is not\n <mask>                 guaranteed when max_concurrency > 1.\n <mask>             name: The globally unique name for the actor.\n <mask>             detached: Whether the actor should be kept alive after driver\n <mask>                 exits.\n <mask> \n <mask>         Returns:\n <mask>             A handle to the newly created actor.\n <mask>         \"\"\"\n <mask>         if args is None:\n </s> Update named actor API (#8559) </s> remove         if detached and name is None:\n            raise ValueError(\"Detached actors must be named. \"\n                             \"Please use Actor._remote(name='some_name') \"\n                             \"to associate the name.\")\n </s> add         if detached:\n            logger.warning(\"The detached flag is deprecated. To create a \"\n                           \"detached actor, use the name parameter.\") </s> remove         if name and not detached:\n            raise ValueError(\"Only detached actors can be named. \"\n                             \"Please use Actor._remote(detached=True, \"\n                             \"name='some_name').\")\n\n        if name == \"\":\n            raise ValueError(\"Actor name cannot be an empty string.\")\n </s> add         if name is not None:\n            if not isinstance(name, str):\n                raise TypeError(\"name must be None or a string, \"\n                                \"got: '{}'.\".format(type(name)))\n            if name == \"\":\n                raise ValueError(\"Actor name cannot be an empty string.\") </s> remove                     \"a different name or get existing actor using \"\n                    \"ray.util.get_actor('{name}')\".format(name=name))\n </s> add                     \"a different name or get the existing actor using \"\n                    \"ray.get_actor('{name}')\".format(name=name))\n            detached = True\n        else:\n            detached = False </s> remove                 self.workers[backend_tag][replica_tag] = ray.util.get_actor(\n </s> add                 self.workers[backend_tag][replica_tag] = ray.get_actor( </s> remove                 ray.util.get_actor(name)\n </s> add                 ray.get_actor(name) </s> remove Sometimes, it is necessary to coordinate between pieces of code that live in different processes managed by RLlib. For example, it can be useful to maintain a global average of a certain variable, or centrally control a hyperparameter used by policies. Ray provides a general way to achieve this through *named actors* (learn more about Ray actors `here <actors.html>`__). As an example, consider maintaining a shared global counter that is incremented by environments and read periodically from your driver program:\n </s> add Sometimes, it is necessary to coordinate between pieces of code that live in different processes managed by RLlib. For example, it can be useful to maintain a global average of a certain variable, or centrally control a hyperparameter used by policies. Ray provides a general way to achieve this through *detached actors* (learn more about Ray actors `here <actors.html>`__). These actors are assigned a global name and handles to them can be retrieved using these names. As an example, consider maintaining a shared global counter that is incremented by environments and read periodically from your driver program:", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/actor.py"}
{"docstring_tokens": "keep keep replace replace replace replace keep replace replace replace replace replace replace replace keep", "code_tokens": " <mask>                                \"has been called.\")\n <mask> \n <mask>         if detached and name is None:\n <mask>             raise ValueError(\"Detached actors must be named. \"\n <mask>                              \"Please use Actor._remote(name='some_name') \"\n <mask>                              \"to associate the name.\")\n <mask> \n <mask>         if name and not detached:\n <mask>             raise ValueError(\"Only detached actors can be named. \"\n <mask>                              \"Please use Actor._remote(detached=True, \"\n <mask>                              \"name='some_name').\")\n <mask> \n <mask>         if name == \"\":\n <mask>             raise ValueError(\"Actor name cannot be an empty string.\")\n <mask> \n </s> Update named actor API (#8559) </s> remove                 ray.util.get_actor(name)\n </s> add                 ray.get_actor(name) </s> remove                     \"a different name or get existing actor using \"\n                    \"ray.util.get_actor('{name}')\".format(name=name))\n </s> add                     \"a different name or get the existing actor using \"\n                    \"ray.get_actor('{name}')\".format(name=name))\n            detached = True\n        else:\n            detached = False </s> remove     def __ray_kill__(self):\n        \"\"\"Deprecated - use ray.kill() instead.\"\"\"\n        logger.warning(\"actor.__ray_kill__() is deprecated and will be removed\"\n                       \" in the near future. Use ray.kill(actor) instead.\")\n        ray.kill(self)\n\n </s> add  </s> remove             detached: Whether the actor should be kept alive after driver\n                exits.\n </s> add             detached: DEPRECATED. </s> remove Sometimes, it is necessary to coordinate between pieces of code that live in different processes managed by RLlib. For example, it can be useful to maintain a global average of a certain variable, or centrally control a hyperparameter used by policies. Ray provides a general way to achieve this through *named actors* (learn more about Ray actors `here <actors.html>`__). As an example, consider maintaining a shared global counter that is incremented by environments and read periodically from your driver program:\n </s> add Sometimes, it is necessary to coordinate between pieces of code that live in different processes managed by RLlib. For example, it can be useful to maintain a global average of a certain variable, or centrally control a hyperparameter used by policies. Ray provides a general way to achieve this through *detached actors* (learn more about Ray actors `here <actors.html>`__). These actors are assigned a global name and handles to them can be retrieved using these names. As an example, consider maintaining a shared global counter that is incremented by environments and read periodically from your driver program:", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/actor.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         # instead check this when we create the actor, but that's currently an\n <mask>         # async call.\n <mask>         if name is not None:\n <mask>             try:\n <mask>                 ray.util.get_actor(name)\n <mask>             except ValueError:  # Name is not taken.\n <mask>                 pass\n <mask>             else:\n <mask>                 raise ValueError(\n <mask>                     \"The name {name} is already taken. Please use \"\n </s> Update named actor API (#8559) </s> remove         if name and not detached:\n            raise ValueError(\"Only detached actors can be named. \"\n                             \"Please use Actor._remote(detached=True, \"\n                             \"name='some_name').\")\n\n        if name == \"\":\n            raise ValueError(\"Actor name cannot be an empty string.\")\n </s> add         if name is not None:\n            if not isinstance(name, str):\n                raise TypeError(\"name must be None or a string, \"\n                                \"got: '{}'.\".format(type(name)))\n            if name == \"\":\n                raise ValueError(\"Actor name cannot be an empty string.\") </s> remove                     \"a different name or get existing actor using \"\n                    \"ray.util.get_actor('{name}')\".format(name=name))\n </s> add                     \"a different name or get the existing actor using \"\n                    \"ray.get_actor('{name}')\".format(name=name))\n            detached = True\n        else:\n            detached = False </s> remove         if detached and name is None:\n            raise ValueError(\"Detached actors must be named. \"\n                             \"Please use Actor._remote(name='some_name') \"\n                             \"to associate the name.\")\n </s> add         if detached:\n            logger.warning(\"The detached flag is deprecated. To create a \"\n                           \"detached actor, use the name parameter.\") </s> remove                     replica = ray.util.get_actor(replica_tag)\n </s> add                     replica = ray.get_actor(replica_tag) </s> remove             worker_handle = ray.util.get_actor(replica_tag)\n </s> add             worker_handle = ray.get_actor(replica_tag) </s> remove                 worker = ray.worker.global_worker\n                # Kill the actor with no_restart=True.\n                worker.core_worker.kill_actor(replica._ray_actor_id, True)\n </s> add                 ray.kill(replica, no_restart=True)", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/actor.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>                 pass\n <mask>             else:\n <mask>                 raise ValueError(\n <mask>                     \"The name {name} is already taken. Please use \"\n <mask>                     \"a different name or get existing actor using \"\n <mask>                     \"ray.util.get_actor('{name}')\".format(name=name))\n <mask> \n <mask>         # Set the actor's default resources if not already set. First three\n <mask>         # conditions are to check that no resources were specified in the\n <mask>         # decorator. Last three conditions are to check that no resources were\n <mask>         # specified when _remote() was called.\n </s> Update named actor API (#8559) </s> remove                 ray.util.get_actor(name)\n </s> add                 ray.get_actor(name) </s> remove         if name and not detached:\n            raise ValueError(\"Only detached actors can be named. \"\n                             \"Please use Actor._remote(detached=True, \"\n                             \"name='some_name').\")\n\n        if name == \"\":\n            raise ValueError(\"Actor name cannot be an empty string.\")\n </s> add         if name is not None:\n            if not isinstance(name, str):\n                raise TypeError(\"name must be None or a string, \"\n                                \"got: '{}'.\".format(type(name)))\n            if name == \"\":\n                raise ValueError(\"Actor name cannot be an empty string.\") </s> remove         if detached and name is None:\n            raise ValueError(\"Detached actors must be named. \"\n                             \"Please use Actor._remote(name='some_name') \"\n                             \"to associate the name.\")\n </s> add         if detached:\n            logger.warning(\"The detached flag is deprecated. To create a \"\n                           \"detached actor, use the name parameter.\") </s> remove                 worker = ray.worker.global_worker\n                # Kill the actor with no_restart=True.\n                worker.core_worker.kill_actor(replica._ray_actor_id, True)\n </s> add                 ray.kill(replica, no_restart=True) </s> remove Sometimes, it is necessary to coordinate between pieces of code that live in different processes managed by RLlib. For example, it can be useful to maintain a global average of a certain variable, or centrally control a hyperparameter used by policies. Ray provides a general way to achieve this through *named actors* (learn more about Ray actors `here <actors.html>`__). As an example, consider maintaining a shared global counter that is incremented by environments and read periodically from your driver program:\n </s> add Sometimes, it is necessary to coordinate between pieces of code that live in different processes managed by RLlib. For example, it can be useful to maintain a global average of a certain variable, or centrally control a hyperparameter used by policies. Ray provides a general way to achieve this through *detached actors* (learn more about Ray actors `here <actors.html>`__). These actors are assigned a global name and handles to them can be retrieved using these names. As an example, consider maintaining a shared global counter that is incremented by environments and read periodically from your driver program: </s> remove         master_actor = ray.util.get_actor(master_actor_name)\n </s> add         master_actor = ray.get_actor(master_actor_name)", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/actor.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             worker.current_session_and_job,\n <mask>             original_handle=True)\n <mask> \n <mask>         if name is not None and not gcs_actor_service_enabled():\n <mask>             ray.util.register_actor(name, actor_handle)\n <mask> \n <mask>         return actor_handle\n <mask> \n <mask> \n <mask> class ActorHandle:\n </s> Update named actor API (#8559) </s> remove         if name and not detached:\n            raise ValueError(\"Only detached actors can be named. \"\n                             \"Please use Actor._remote(detached=True, \"\n                             \"name='some_name').\")\n\n        if name == \"\":\n            raise ValueError(\"Actor name cannot be an empty string.\")\n </s> add         if name is not None:\n            if not isinstance(name, str):\n                raise TypeError(\"name must be None or a string, \"\n                                \"got: '{}'.\".format(type(name)))\n            if name == \"\":\n                raise ValueError(\"Actor name cannot be an empty string.\") </s> remove                 ray.util.get_actor(name)\n </s> add                 ray.get_actor(name) </s> remove         if detached and name is None:\n            raise ValueError(\"Detached actors must be named. \"\n                             \"Please use Actor._remote(name='some_name') \"\n                             \"to associate the name.\")\n </s> add         if detached:\n            logger.warning(\"The detached flag is deprecated. To create a \"\n                           \"detached actor, use the name parameter.\") </s> remove             detached: Whether the actor should be kept alive after driver\n                exits.\n </s> add             detached: DEPRECATED. </s> remove def test_register_and_get_named_actors(ray_start_regular):\n    # TODO(heyucongtom): We should test this from another driver.\n\n    @ray.remote\n    class Foo:\n        def __init__(self):\n            self.x = 0\n\n        def method(self):\n            self.x += 1\n            return self.x\n\n    f1 = Foo.remote()\n    # Test saving f.\n    ray.util.register_actor(\"f1\", f1)\n    # Test getting f.\n    f2 = ray.util.get_actor(\"f1\")\n    assert f1._actor_id == f2._actor_id\n\n    # Test same name register shall raise error.\n    with pytest.raises(ValueError):\n        ray.util.register_actor(\"f1\", f2)\n\n    # Test register with wrong object type.\n    with pytest.raises(TypeError):\n        ray.util.register_actor(\"f3\", 1)\n\n    # Test getting a nonexistent actor.\n    with pytest.raises(ValueError):\n        ray.util.get_actor(\"nonexistent\")\n\n    # Test method\n    assert ray.get(f1.method.remote()) == 1\n    assert ray.get(f2.method.remote()) == 2\n    assert ray.get(f1.method.remote()) == 3\n    assert ray.get(f2.method.remote()) == 4\n\n\n </s> add  </s> remove                     \"a different name or get existing actor using \"\n                    \"ray.util.get_actor('{name}')\".format(name=name))\n </s> add                     \"a different name or get the existing actor using \"\n                    \"ray.get_actor('{name}')\".format(name=name))\n            detached = True\n        else:\n            detached = False", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/actor.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         return \"Actor({}, {})\".format(\n <mask>             self._ray_actor_creation_function_descriptor.class_name,\n <mask>             self._actor_id.hex())\n <mask> \n <mask>     def __ray_kill__(self):\n <mask>         \"\"\"Deprecated - use ray.kill() instead.\"\"\"\n <mask>         logger.warning(\"actor.__ray_kill__() is deprecated and will be removed\"\n <mask>                        \" in the near future. Use ray.kill(actor) instead.\")\n <mask>         ray.kill(self)\n <mask> \n <mask>     @property\n <mask>     def _actor_id(self):\n <mask>         return self._ray_actor_id\n <mask> \n <mask>     def _serialization_helper(self):\n </s> Update named actor API (#8559) </s> remove         if detached and name is None:\n            raise ValueError(\"Detached actors must be named. \"\n                             \"Please use Actor._remote(name='some_name') \"\n                             \"to associate the name.\")\n </s> add         if detached:\n            logger.warning(\"The detached flag is deprecated. To create a \"\n                           \"detached actor, use the name parameter.\") </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove         if name and not detached:\n            raise ValueError(\"Only detached actors can be named. \"\n                             \"Please use Actor._remote(detached=True, \"\n                             \"name='some_name').\")\n\n        if name == \"\":\n            raise ValueError(\"Actor name cannot be an empty string.\")\n </s> add         if name is not None:\n            if not isinstance(name, str):\n                raise TypeError(\"name must be None or a string, \"\n                                \"got: '{}'.\".format(type(name)))\n            if name == \"\":\n                raise ValueError(\"Actor name cannot be an empty string.\") </s> remove def test_register_and_get_named_actors(ray_start_regular):\n    # TODO(heyucongtom): We should test this from another driver.\n\n    @ray.remote\n    class Foo:\n        def __init__(self):\n            self.x = 0\n\n        def method(self):\n            self.x += 1\n            return self.x\n\n    f1 = Foo.remote()\n    # Test saving f.\n    ray.util.register_actor(\"f1\", f1)\n    # Test getting f.\n    f2 = ray.util.get_actor(\"f1\")\n    assert f1._actor_id == f2._actor_id\n\n    # Test same name register shall raise error.\n    with pytest.raises(ValueError):\n        ray.util.register_actor(\"f1\", f2)\n\n    # Test register with wrong object type.\n    with pytest.raises(TypeError):\n        ray.util.register_actor(\"f3\", 1)\n\n    # Test getting a nonexistent actor.\n    with pytest.raises(ValueError):\n        ray.util.get_actor(\"nonexistent\")\n\n    # Test method\n    assert ray.get(f1.method.remote()) == 1\n    assert ray.get(f2.method.remote()) == 2\n    assert ray.get(f1.method.remote()) == 3\n    assert ray.get(f2.method.remote()) == 4\n\n\n </s> add  </s> remove                 detached=True,\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/actor.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     # Try to get serve master actor if it exists\n <mask>     global master_actor\n <mask>     master_actor_name = format_actor_name(SERVE_MASTER_NAME, cluster_name)\n <mask>     try:\n <mask>         master_actor = ray.util.get_actor(master_actor_name)\n <mask>         return\n <mask>     except ValueError:\n <mask>         pass\n <mask> \n <mask>     # Register serialization context once\n </s> Update named actor API (#8559) </s> remove                     \"a different name or get existing actor using \"\n                    \"ray.util.get_actor('{name}')\".format(name=name))\n </s> add                     \"a different name or get the existing actor using \"\n                    \"ray.get_actor('{name}')\".format(name=name))\n            detached = True\n        else:\n            detached = False </s> remove     counter = Counter.remote()\n    named_actors.register_actor(\"global_counter\", counter)\n </s> add     counter = Counter.options(name=\"global_counter\").remote() </s> remove         detached=True,\n </s> add  </s> remove             worker_handle = ray.util.get_actor(replica_tag)\n </s> add             worker_handle = ray.get_actor(replica_tag) </s> remove                 worker = ray.worker.global_worker\n                # Kill the actor with no_restart=True.\n                worker.core_worker.kill_actor(replica._ray_actor_id, True)\n </s> add                 ray.kill(replica, no_restart=True) </s> remove             self.router = ray.util.get_actor(router_name)\n </s> add             self.router = ray.get_actor(router_name)", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     # serve.init() was run on. We should consider making this configurable\n <mask>     # in the future.\n <mask>     http_node_id = ray.state.current_node_id()\n <mask>     master_actor = ServeMaster.options(\n <mask>         detached=True,\n <mask>         name=master_actor_name,\n <mask>         max_restarts=-1,\n <mask>     ).remote(cluster_name, start_server, http_node_id, http_host, http_port,\n <mask>              metric_exporter)\n <mask> \n </s> Update named actor API (#8559) </s> remove         master_actor = ray.util.get_actor(master_actor_name)\n </s> add         master_actor = ray.get_actor(master_actor_name) </s> remove             detached=True,\n </s> add  </s> remove                 self.workers[backend_tag][replica_tag] = ray.util.get_actor(\n </s> add                 self.workers[backend_tag][replica_tag] = ray.get_actor( </s> remove                 detached=True,\n </s> add  </s> remove def test_register_and_get_named_actors(ray_start_regular):\n    # TODO(heyucongtom): We should test this from another driver.\n\n    @ray.remote\n    class Foo:\n        def __init__(self):\n            self.x = 0\n\n        def method(self):\n            self.x += 1\n            return self.x\n\n    f1 = Foo.remote()\n    # Test saving f.\n    ray.util.register_actor(\"f1\", f1)\n    # Test getting f.\n    f2 = ray.util.get_actor(\"f1\")\n    assert f1._actor_id == f2._actor_id\n\n    # Test same name register shall raise error.\n    with pytest.raises(ValueError):\n        ray.util.register_actor(\"f1\", f2)\n\n    # Test register with wrong object type.\n    with pytest.raises(TypeError):\n        ray.util.register_actor(\"f3\", 1)\n\n    # Test getting a nonexistent actor.\n    with pytest.raises(ValueError):\n        ray.util.get_actor(\"nonexistent\")\n\n    # Test method\n    assert ray.get(f1.method.remote()) == 1\n    assert ray.get(f2.method.remote()) == 2\n    assert ray.get(f1.method.remote()) == 3\n    assert ray.get(f2.method.remote()) == 4\n\n\n </s> add  </s> remove                 detached=True,\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/api.py"}
{"docstring_tokens": "keep keep keep replace keep keep keep replace keep keep keep", "code_tokens": " <mask>         \"\"\"\n <mask>         router_name = format_actor_name(SERVE_ROUTER_NAME, self.cluster_name)\n <mask>         try:\n <mask>             self.router = ray.util.get_actor(router_name)\n <mask>         except ValueError:\n <mask>             logger.info(\"Starting router with name '{}'\".format(router_name))\n <mask>             self.router = async_retryable(ray.remote(Router)).options(\n <mask>                 detached=True,\n <mask>                 name=router_name,\n <mask>                 max_concurrency=ASYNC_CONCURRENCY,\n <mask>                 max_restarts=-1,\n </s> Update named actor API (#8559) </s> remove             self.metric_exporter = ray.util.get_actor(metric_sink_name)\n </s> add             self.metric_exporter = ray.get_actor(metric_sink_name) </s> remove             self.http_proxy = ray.util.get_actor(proxy_name)\n </s> add             self.http_proxy = ray.get_actor(proxy_name) </s> remove                 detached=True,\n </s> add  </s> remove             detached=True,\n </s> add  </s> remove                 detached=True,\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/master.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         If the HTTP proxy does not already exist, it will be started.\n <mask>         \"\"\"\n <mask>         proxy_name = format_actor_name(SERVE_PROXY_NAME, self.cluster_name)\n <mask>         try:\n <mask>             self.http_proxy = ray.util.get_actor(proxy_name)\n <mask>         except ValueError:\n <mask>             logger.info(\n <mask>                 \"Starting HTTP proxy with name '{}' on node '{}'\".format(\n <mask>                     proxy_name, node_id))\n <mask>             self.http_proxy = async_retryable(HTTPProxyActor).options(\n </s> Update named actor API (#8559) </s> remove                 detached=True,\n </s> add  </s> remove             self.router = ray.util.get_actor(router_name)\n </s> add             self.router = ray.get_actor(router_name) </s> remove             self.metric_exporter = ray.util.get_actor(metric_sink_name)\n </s> add             self.metric_exporter = ray.get_actor(metric_sink_name) </s> remove             worker_handle = ray.util.get_actor(replica_tag)\n </s> add             worker_handle = ray.get_actor(replica_tag) </s> remove         master_actor = ray.util.get_actor(master_actor_name)\n </s> add         master_actor = ray.get_actor(master_actor_name) </s> remove                 detached=True,\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/master.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             logger.info(\n <mask>                 \"Starting HTTP proxy with name '{}' on node '{}'\".format(\n <mask>                     proxy_name, node_id))\n <mask>             self.http_proxy = async_retryable(HTTPProxyActor).options(\n <mask>                 detached=True,\n <mask>                 name=proxy_name,\n <mask>                 max_concurrency=ASYNC_CONCURRENCY,\n <mask>                 max_restarts=-1,\n <mask>                 resources={\n <mask>                     node_id: 0.01\n </s> Update named actor API (#8559) </s> remove             self.http_proxy = ray.util.get_actor(proxy_name)\n </s> add             self.http_proxy = ray.get_actor(proxy_name) </s> remove                 detached=True,\n </s> add  </s> remove             self.metric_exporter = ray.util.get_actor(metric_sink_name)\n </s> add             self.metric_exporter = ray.get_actor(metric_sink_name) </s> remove                 detached=True,\n </s> add  </s> remove             detached=True,\n </s> add  </s> remove         detached=True,\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/master.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep replace keep keep", "code_tokens": " <mask>         \"\"\"\n <mask>         metric_sink_name = format_actor_name(SERVE_METRIC_SINK_NAME,\n <mask>                                              self.cluster_name)\n <mask>         try:\n <mask>             self.metric_exporter = ray.util.get_actor(metric_sink_name)\n <mask>         except ValueError:\n <mask>             logger.info(\"Starting metric exporter with name '{}'\".format(\n <mask>                 metric_sink_name))\n <mask>             self.metric_exporter = MetricExporterActor.options(\n <mask>                 detached=True,\n <mask>                 name=metric_sink_name).remote(metric_exporter_class)\n <mask> \n </s> Update named actor API (#8559) </s> remove             self.router = ray.util.get_actor(router_name)\n </s> add             self.router = ray.get_actor(router_name) </s> remove             self.http_proxy = ray.util.get_actor(proxy_name)\n </s> add             self.http_proxy = ray.get_actor(proxy_name) </s> remove                 detached=True,\n </s> add  </s> remove                 detached=True,\n </s> add  </s> remove             detached=True,\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/master.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         for backend_tag, replica_tags in self.replicas.items():\n <mask>             for replica_tag in replica_tags:\n <mask>                 replica_name = format_actor_name(replica_tag,\n <mask>                                                  self.cluster_name)\n <mask>                 self.workers[backend_tag][replica_tag] = ray.util.get_actor(\n <mask>                     replica_name)\n <mask> \n <mask>         # Push configuration state to the router.\n <mask>         # TODO(edoakes): should we make this a pull-only model for simplicity?\n <mask>         for endpoint, traffic_policy in self.traffic_policies.items():\n </s> Update named actor API (#8559) </s> remove                     replica = ray.util.get_actor(replica_tag)\n </s> add                     replica = ray.get_actor(replica_tag) </s> remove             detached=True,\n </s> add  </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove             detached: Whether the actor should be kept alive after driver\n                exits.\n </s> add             detached: DEPRECATED. </s> remove     ray.kill(handles[0])\n </s> add     ray.kill(handles[0], no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False)", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/master.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>          replica_config) = self.backends[backend_tag]\n <mask> \n <mask>         replica_name = format_actor_name(replica_tag, self.cluster_name)\n <mask>         worker_handle = async_retryable(ray.remote(backend_worker)).options(\n <mask>             detached=True,\n <mask>             name=replica_name,\n <mask>             max_restarts=-1,\n <mask>             **replica_config.ray_actor_options).remote(\n <mask>                 backend_tag,\n <mask>                 replica_tag,\n </s> Update named actor API (#8559) </s> remove                 self.workers[backend_tag][replica_tag] = ray.util.get_actor(\n </s> add                 self.workers[backend_tag][replica_tag] = ray.get_actor( </s> remove             worker_handle = ray.util.get_actor(replica_tag)\n </s> add             worker_handle = ray.get_actor(replica_tag) </s> remove                 detached=True,\n </s> add  </s> remove             self.metric_exporter = ray.util.get_actor(metric_sink_name)\n </s> add             self.metric_exporter = ray.get_actor(metric_sink_name) </s> remove             self.router = ray.util.get_actor(router_name)\n </s> add             self.router = ray.get_actor(router_name) </s> remove         detached=True,\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/master.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         # NOTE(edoakes): the replicas may already be created if we\n <mask>         # failed after creating them but before writing a\n <mask>         # checkpoint.\n <mask>         try:\n <mask>             worker_handle = ray.util.get_actor(replica_tag)\n <mask>         except ValueError:\n <mask>             worker_handle = await self._start_backend_worker(\n <mask>                 backend_tag, replica_tag)\n <mask> \n <mask>         self.replicas[backend_tag].append(replica_tag)\n </s> Update named actor API (#8559) </s> remove                     replica = ray.util.get_actor(replica_tag)\n </s> add                     replica = ray.get_actor(replica_tag) </s> remove             detached=True,\n </s> add  </s> remove                 ray.util.get_actor(name)\n </s> add                 ray.get_actor(name) </s> remove         master_actor = ray.util.get_actor(master_actor_name)\n </s> add         master_actor = ray.get_actor(master_actor_name) </s> remove                 self.workers[backend_tag][replica_tag] = ray.util.get_actor(\n </s> add                 self.workers[backend_tag][replica_tag] = ray.get_actor( </s> remove             self.router = ray.util.get_actor(router_name)\n </s> add             self.router = ray.get_actor(router_name)", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/master.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             for replica_tag in replicas_to_stop:\n <mask>                 # NOTE(edoakes): the replicas may already be stopped if we\n <mask>                 # failed after stopping them but before writing a checkpoint.\n <mask>                 try:\n <mask>                     replica = ray.util.get_actor(replica_tag)\n <mask>                 except ValueError:\n <mask>                     continue\n <mask> \n <mask>                 # Remove the replica from router. This call is idempotent.\n <mask>                 await self.router.remove_worker.remote(backend_tag,\n </s> Update named actor API (#8559) </s> remove             worker_handle = ray.util.get_actor(replica_tag)\n </s> add             worker_handle = ray.get_actor(replica_tag) </s> remove                 self.workers[backend_tag][replica_tag] = ray.util.get_actor(\n </s> add                 self.workers[backend_tag][replica_tag] = ray.get_actor( </s> remove                 worker = ray.worker.global_worker\n                # Kill the actor with no_restart=True.\n                worker.core_worker.kill_actor(replica._ray_actor_id, True)\n </s> add                 ray.kill(replica, no_restart=True) </s> remove                 ray.util.get_actor(name)\n </s> add                 ray.get_actor(name) </s> remove     ray.kill(handles[0])\n </s> add     ray.kill(handles[0], no_restart=False) </s> remove         if name and not detached:\n            raise ValueError(\"Only detached actors can be named. \"\n                             \"Please use Actor._remote(detached=True, \"\n                             \"name='some_name').\")\n\n        if name == \"\":\n            raise ValueError(\"Actor name cannot be an empty string.\")\n </s> add         if name is not None:\n            if not isinstance(name, str):\n                raise TypeError(\"name must be None or a string, \"\n                                \"got: '{}'.\".format(type(name)))\n            if name == \"\":\n                raise ValueError(\"Actor name cannot be an empty string.\")", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/master.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>                 # pending tasks still executing on the replica. However, if we\n <mask>                 # use replica.__ray_terminate__, we may send it while the\n <mask>                 # replica is being restarted and there's no way to tell if it\n <mask>                 # successfully killed the worker or not.\n <mask>                 worker = ray.worker.global_worker\n <mask>                 # Kill the actor with no_restart=True.\n <mask>                 worker.core_worker.kill_actor(replica._ray_actor_id, True)\n <mask> \n <mask>         self.replicas_to_stop.clear()\n <mask> \n <mask>     async def _remove_pending_backends(self):\n <mask>         \"\"\"Removes the pending backends in self.backends_to_remove.\n </s> Update named actor API (#8559) </s> remove     ray.kill(handles[0])\n </s> add     ray.kill(handles[0], no_restart=False) </s> remove                     replica = ray.util.get_actor(replica_tag)\n </s> add                     replica = ray.get_actor(replica_tag) </s> remove     ray.kill(handles[0])\n </s> add     ray.kill(handles[0], no_restart=False) </s> remove                     \"a different name or get existing actor using \"\n                    \"ray.util.get_actor('{name}')\".format(name=name))\n </s> add                     \"a different name or get the existing actor using \"\n                    \"ray.get_actor('{name}')\".format(name=name))\n            detached = True\n        else:\n            detached = False </s> remove         master_actor = ray.util.get_actor(master_actor_name)\n </s> add         master_actor = ray.get_actor(master_actor_name) </s> remove                 ray.util.get_actor(name)\n </s> add                 ray.get_actor(name)", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/master.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     for _ in range(10):\n <mask>         response = request_with_retries(\"/master_failure\", timeout=30)\n <mask>         assert response.text == \"hello1\"\n <mask> \n <mask>     ray.kill(serve.api._get_master_actor())\n <mask> \n <mask>     for _ in range(10):\n <mask>         response = request_with_retries(\"/master_failure\", timeout=30)\n <mask>         assert response.text == \"hello1\"\n <mask> \n </s> Update named actor API (#8559) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove     ray.kill(handles[0])\n </s> add     ray.kill(handles[0], no_restart=False) </s> remove     ray.kill(handles[0])\n </s> add     ray.kill(handles[0], no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove def test_register_and_get_named_actors(ray_start_regular):\n    # TODO(heyucongtom): We should test this from another driver.\n\n    @ray.remote\n    class Foo:\n        def __init__(self):\n            self.x = 0\n\n        def method(self):\n            self.x += 1\n            return self.x\n\n    f1 = Foo.remote()\n    # Test saving f.\n    ray.util.register_actor(\"f1\", f1)\n    # Test getting f.\n    f2 = ray.util.get_actor(\"f1\")\n    assert f1._actor_id == f2._actor_id\n\n    # Test same name register shall raise error.\n    with pytest.raises(ValueError):\n        ray.util.register_actor(\"f1\", f2)\n\n    # Test register with wrong object type.\n    with pytest.raises(TypeError):\n        ray.util.register_actor(\"f3\", 1)\n\n    # Test getting a nonexistent actor.\n    with pytest.raises(ValueError):\n        ray.util.get_actor(\"nonexistent\")\n\n    # Test method\n    assert ray.get(f1.method.remote()) == 1\n    assert ray.get(f2.method.remote()) == 2\n    assert ray.get(f1.method.remote()) == 3\n    assert ray.get(f2.method.remote()) == 4\n\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/tests/test_failure.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     def function():\n <mask>         return \"hello2\"\n <mask> \n <mask>     ray.kill(serve.api._get_master_actor())\n <mask> \n <mask>     serve.create_backend(\"master_failure:v2\", function)\n <mask>     serve.set_traffic(\"master_failure\", {\"master_failure:v2\": 1.0})\n <mask> \n <mask>     for _ in range(10):\n </s> Update named actor API (#8559) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove     ray.kill(handles[0])\n </s> add     ray.kill(handles[0], no_restart=False) </s> remove     def __ray_kill__(self):\n        \"\"\"Deprecated - use ray.kill() instead.\"\"\"\n        logger.warning(\"actor.__ray_kill__() is deprecated and will be removed\"\n                       \" in the near future. Use ray.kill(actor) instead.\")\n        ray.kill(self)\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/tests/test_failure.py"}
{"docstring_tokens": "keep keep keep replace keep replace keep keep", "code_tokens": " <mask>     def function():\n <mask>         return \"hello3\"\n <mask> \n <mask>     ray.kill(serve.api._get_master_actor())\n <mask>     serve.create_endpoint(\"master_failure_2\", \"/master_failure_2\")\n <mask>     ray.kill(serve.api._get_master_actor())\n <mask>     serve.create_backend(\"master_failure_2\", function)\n <mask>     ray.kill(serve.api._get_master_actor())\n </s> Update named actor API (#8559) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove     def __ray_kill__(self):\n        \"\"\"Deprecated - use ray.kill() instead.\"\"\"\n        logger.warning(\"actor.__ray_kill__() is deprecated and will be removed\"\n                       \" in the near future. Use ray.kill(actor) instead.\")\n        ray.kill(self)\n\n </s> add  </s> remove def test_register_and_get_named_actors(ray_start_regular):\n    # TODO(heyucongtom): We should test this from another driver.\n\n    @ray.remote\n    class Foo:\n        def __init__(self):\n            self.x = 0\n\n        def method(self):\n            self.x += 1\n            return self.x\n\n    f1 = Foo.remote()\n    # Test saving f.\n    ray.util.register_actor(\"f1\", f1)\n    # Test getting f.\n    f2 = ray.util.get_actor(\"f1\")\n    assert f1._actor_id == f2._actor_id\n\n    # Test same name register shall raise error.\n    with pytest.raises(ValueError):\n        ray.util.register_actor(\"f1\", f2)\n\n    # Test register with wrong object type.\n    with pytest.raises(TypeError):\n        ray.util.register_actor(\"f3\", 1)\n\n    # Test getting a nonexistent actor.\n    with pytest.raises(ValueError):\n        ray.util.get_actor(\"nonexistent\")\n\n    # Test method\n    assert ray.get(f1.method.remote()) == 1\n    assert ray.get(f2.method.remote()) == 2\n    assert ray.get(f1.method.remote()) == 3\n    assert ray.get(f2.method.remote()) == 4\n\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/tests/test_failure.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     ray.kill(serve.api._get_master_actor())\n <mask>     serve.create_endpoint(\"master_failure_2\", \"/master_failure_2\")\n <mask>     ray.kill(serve.api._get_master_actor())\n <mask>     serve.create_backend(\"master_failure_2\", function)\n <mask>     ray.kill(serve.api._get_master_actor())\n <mask>     serve.set_traffic(\"master_failure_2\", {\"master_failure_2\": 1.0})\n <mask> \n <mask>     for _ in range(10):\n <mask>         response = request_with_retries(\"/master_failure\", timeout=30)\n <mask>         assert response.text == \"hello2\"\n </s> Update named actor API (#8559) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove     ray.kill(handles[0])\n </s> add     ray.kill(handles[0], no_restart=False) </s> remove     ray.kill(handles[0])\n </s> add     ray.kill(handles[0], no_restart=False)", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/tests/test_failure.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> def _kill_http_proxy():\n <mask>     [http_proxy] = ray.get(\n <mask>         serve.api._get_master_actor().get_http_proxy.remote())\n <mask>     ray.kill(http_proxy)\n <mask> \n <mask> \n <mask> def test_http_proxy_failure(serve_instance):\n <mask>     serve.init()\n <mask>     serve.create_endpoint(\"proxy_failure\", \"/proxy_failure\")\n </s> Update named actor API (#8559) </s> remove     ray.kill(router)\n </s> add     ray.kill(router, no_restart=False) </s> remove         detached=True,\n </s> add  </s> remove def test_register_and_get_named_actors(ray_start_regular):\n    # TODO(heyucongtom): We should test this from another driver.\n\n    @ray.remote\n    class Foo:\n        def __init__(self):\n            self.x = 0\n\n        def method(self):\n            self.x += 1\n            return self.x\n\n    f1 = Foo.remote()\n    # Test saving f.\n    ray.util.register_actor(\"f1\", f1)\n    # Test getting f.\n    f2 = ray.util.get_actor(\"f1\")\n    assert f1._actor_id == f2._actor_id\n\n    # Test same name register shall raise error.\n    with pytest.raises(ValueError):\n        ray.util.register_actor(\"f1\", f2)\n\n    # Test register with wrong object type.\n    with pytest.raises(TypeError):\n        ray.util.register_actor(\"f3\", 1)\n\n    # Test getting a nonexistent actor.\n    with pytest.raises(ValueError):\n        ray.util.get_actor(\"nonexistent\")\n\n    # Test method\n    assert ray.get(f1.method.remote()) == 1\n    assert ray.get(f2.method.remote()) == 2\n    assert ray.get(f1.method.remote()) == 3\n    assert ray.get(f2.method.remote()) == 4\n\n\n </s> add  </s> remove     def __ray_kill__(self):\n        \"\"\"Deprecated - use ray.kill() instead.\"\"\"\n        logger.warning(\"actor.__ray_kill__() is deprecated and will be removed\"\n                       \" in the near future. Use ray.kill(actor) instead.\")\n        ray.kill(self)\n\n </s> add  </s> remove     counter = Counter.remote()\n    named_actors.register_actor(\"global_counter\", counter)\n </s> add     counter = Counter.options(name=\"global_counter\").remote() </s> remove                 detached=True,\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/tests/test_failure.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> \n <mask> def _kill_router():\n <mask>     [router] = ray.get(serve.api._get_master_actor().get_router.remote())\n <mask>     ray.kill(router)\n <mask> \n <mask> \n <mask> def test_router_failure(serve_instance):\n <mask>     serve.init()\n <mask>     serve.create_endpoint(\"router_failure\", \"/router_failure\")\n </s> Update named actor API (#8559) </s> remove     ray.kill(http_proxy)\n </s> add     ray.kill(http_proxy, no_restart=False) </s> remove         detached=True,\n </s> add  </s> remove def test_register_and_get_named_actors(ray_start_regular):\n    # TODO(heyucongtom): We should test this from another driver.\n\n    @ray.remote\n    class Foo:\n        def __init__(self):\n            self.x = 0\n\n        def method(self):\n            self.x += 1\n            return self.x\n\n    f1 = Foo.remote()\n    # Test saving f.\n    ray.util.register_actor(\"f1\", f1)\n    # Test getting f.\n    f2 = ray.util.get_actor(\"f1\")\n    assert f1._actor_id == f2._actor_id\n\n    # Test same name register shall raise error.\n    with pytest.raises(ValueError):\n        ray.util.register_actor(\"f1\", f2)\n\n    # Test register with wrong object type.\n    with pytest.raises(TypeError):\n        ray.util.register_actor(\"f3\", 1)\n\n    # Test getting a nonexistent actor.\n    with pytest.raises(ValueError):\n        ray.util.get_actor(\"nonexistent\")\n\n    # Test method\n    assert ray.get(f1.method.remote()) == 1\n    assert ray.get(f2.method.remote()) == 2\n    assert ray.get(f1.method.remote()) == 3\n    assert ray.get(f2.method.remote()) == 4\n\n\n </s> add  </s> remove     def __ray_kill__(self):\n        \"\"\"Deprecated - use ray.kill() instead.\"\"\"\n        logger.warning(\"actor.__ray_kill__() is deprecated and will be removed\"\n                       \" in the near future. Use ray.kill(actor) instead.\")\n        ray.kill(self)\n\n </s> add  </s> remove     counter = Counter.remote()\n    named_actors.register_actor(\"global_counter\", counter)\n </s> add     counter = Counter.options(name=\"global_counter\").remote() </s> remove                 detached=True,\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/tests/test_failure.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     # Kill the worker.\n <mask>     handles = _get_worker_handles(\"worker_failure:v1\")\n <mask>     assert len(handles) == 1\n <mask>     ray.kill(handles[0])\n <mask> \n <mask>     # Wait until the worker is killed and a one is started.\n <mask>     start = time.time()\n <mask>     while time.time() - start < 30:\n <mask>         response = request_with_retries(\"/worker_failure\", timeout=30)\n </s> Update named actor API (#8559) </s> remove     ray.kill(handles[0])\n </s> add     ray.kill(handles[0], no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove                 worker = ray.worker.global_worker\n                # Kill the actor with no_restart=True.\n                worker.core_worker.kill_actor(replica._ray_actor_id, True)\n </s> add                 ray.kill(replica, no_restart=True) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove def test_register_and_get_named_actors(ray_start_regular):\n    # TODO(heyucongtom): We should test this from another driver.\n\n    @ray.remote\n    class Foo:\n        def __init__(self):\n            self.x = 0\n\n        def method(self):\n            self.x += 1\n            return self.x\n\n    f1 = Foo.remote()\n    # Test saving f.\n    ray.util.register_actor(\"f1\", f1)\n    # Test getting f.\n    f2 = ray.util.get_actor(\"f1\")\n    assert f1._actor_id == f2._actor_id\n\n    # Test same name register shall raise error.\n    with pytest.raises(ValueError):\n        ray.util.register_actor(\"f1\", f2)\n\n    # Test register with wrong object type.\n    with pytest.raises(TypeError):\n        ray.util.register_actor(\"f3\", 1)\n\n    # Test getting a nonexistent actor.\n    with pytest.raises(ValueError):\n        ray.util.get_actor(\"nonexistent\")\n\n    # Test method\n    assert ray.get(f1.method.remote()) == 1\n    assert ray.get(f2.method.remote()) == 2\n    assert ray.get(f1.method.remote()) == 3\n    assert ray.get(f2.method.remote()) == 4\n\n\n </s> add  </s> remove                     replica = ray.util.get_actor(replica_tag)\n </s> add                     replica = ray.get_actor(replica_tag)", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/tests/test_failure.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     # Kill one of the replicas.\n <mask>     handles = _get_worker_handles(\"replica_failure\")\n <mask>     assert len(handles) == 2\n <mask>     ray.kill(handles[0])\n <mask> \n <mask>     # Check that the other replica still serves requests.\n <mask>     for _ in range(10):\n <mask>         while True:\n <mask>             try:\n </s> Update named actor API (#8559) </s> remove     ray.kill(handles[0])\n </s> add     ray.kill(handles[0], no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove                 worker = ray.worker.global_worker\n                # Kill the actor with no_restart=True.\n                worker.core_worker.kill_actor(replica._ray_actor_id, True)\n </s> add                 ray.kill(replica, no_restart=True) </s> remove def test_register_and_get_named_actors(ray_start_regular):\n    # TODO(heyucongtom): We should test this from another driver.\n\n    @ray.remote\n    class Foo:\n        def __init__(self):\n            self.x = 0\n\n        def method(self):\n            self.x += 1\n            return self.x\n\n    f1 = Foo.remote()\n    # Test saving f.\n    ray.util.register_actor(\"f1\", f1)\n    # Test getting f.\n    f2 = ray.util.get_actor(\"f1\")\n    assert f1._actor_id == f2._actor_id\n\n    # Test same name register shall raise error.\n    with pytest.raises(ValueError):\n        ray.util.register_actor(\"f1\", f2)\n\n    # Test register with wrong object type.\n    with pytest.raises(TypeError):\n        ray.util.register_actor(\"f3\", 1)\n\n    # Test getting a nonexistent actor.\n    with pytest.raises(ValueError):\n        ray.util.get_actor(\"nonexistent\")\n\n    # Test method\n    assert ray.get(f1.method.remote()) == 1\n    assert ray.get(f2.method.remote()) == 2\n    assert ray.get(f1.method.remote()) == 3\n    assert ray.get(f2.method.remote()) == 4\n\n\n </s> add  </s> remove                     replica = ray.util.get_actor(replica_tag)\n </s> add                     replica = ray.get_actor(replica_tag)", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/serve/tests/test_failure.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     ray.get(b.step.remote())\n <mask>     ray.get(b.step.remote())\n <mask> \n <mask> \n <mask> def test_register_and_get_named_actors(ray_start_regular):\n <mask>     # TODO(heyucongtom): We should test this from another driver.\n <mask> \n <mask>     @ray.remote\n <mask>     class Foo:\n <mask>         def __init__(self):\n <mask>             self.x = 0\n <mask> \n <mask>         def method(self):\n <mask>             self.x += 1\n <mask>             return self.x\n <mask> \n <mask>     f1 = Foo.remote()\n <mask>     # Test saving f.\n <mask>     ray.util.register_actor(\"f1\", f1)\n <mask>     # Test getting f.\n <mask>     f2 = ray.util.get_actor(\"f1\")\n <mask>     assert f1._actor_id == f2._actor_id\n <mask> \n <mask>     # Test same name register shall raise error.\n <mask>     with pytest.raises(ValueError):\n <mask>         ray.util.register_actor(\"f1\", f2)\n <mask> \n <mask>     # Test register with wrong object type.\n <mask>     with pytest.raises(TypeError):\n <mask>         ray.util.register_actor(\"f3\", 1)\n <mask> \n <mask>     # Test getting a nonexistent actor.\n <mask>     with pytest.raises(ValueError):\n <mask>         ray.util.get_actor(\"nonexistent\")\n <mask> \n <mask>     # Test method\n <mask>     assert ray.get(f1.method.remote()) == 1\n <mask>     assert ray.get(f2.method.remote()) == 2\n <mask>     assert ray.get(f1.method.remote()) == 3\n <mask>     assert ray.get(f2.method.remote()) == 4\n <mask> \n <mask> \n <mask> def test_detached_actor(ray_start_regular):\n <mask>     @ray.remote\n <mask>     class DetachedActor:\n <mask>         def ping(self):\n <mask>             return \"pong\"\n </s> Update named actor API (#8559) </s> remove     ray.kill(handles[0])\n </s> add     ray.kill(handles[0], no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove     ray.kill(handles[0])\n </s> add     ray.kill(handles[0], no_restart=False) </s> remove     ray.kill(serve.api._get_master_actor())\n </s> add     ray.kill(serve.api._get_master_actor(), no_restart=False) </s> remove         if name and not detached:\n            raise ValueError(\"Only detached actors can be named. \"\n                             \"Please use Actor._remote(detached=True, \"\n                             \"name='some_name').\")\n\n        if name == \"\":\n            raise ValueError(\"Actor name cannot be an empty string.\")\n </s> add         if name is not None:\n            if not isinstance(name, str):\n                raise TypeError(\"name must be None or a string, \"\n                                \"got: '{}'.\".format(type(name)))\n            if name == \"\":\n                raise ValueError(\"Actor name cannot be an empty string.\") </s> remove     from ray.util import named_actors\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/860eb6f13a0e570b95bd251eb53105473850cbdc", "file_name": "python/ray/tests/test_actor_advanced.py"}
{"docstring_tokens": "keep keep keep replace replace replace replace replace replace replace replace replace replace replace keep replace replace replace replace replace replace keep", "code_tokens": " <mask>     return;\n <mask>   }\n <mask> \n <mask>   auto worker_id = worker->WorkerId();\n <mask>   const auto pid = worker->GetProcess().GetId();\n <mask>   if (worker_state.idle.count(worker) == 0) {\n <mask>     return;\n <mask>   }\n <mask>   if (worker_state.starting_worker_processes.count(worker->GetProcess()) > 0) {\n <mask>     // A Java worker process may hold multiple workers.\n <mask>     RAY_LOG(DEBUG) << \"Some workers of pid \" << pid\n <mask>                    << \" are pending registration. Skip killing worker \" << worker_id;\n <mask>     return;\n <mask>   }\n <mask> \n <mask>   // Make sure all workers in this worker process are idle.\n <mask>   // This block of code is needed by Java workers.\n <mask>   std::unordered_set<std::shared_ptr<WorkerInterface>> workers_in_the_same_process;\n <mask>   for (const auto &worker_in_the_same_process : worker_state.registered_workers) {\n <mask>     if (worker_in_the_same_process->GetProcess().GetId() == pid) {\n <mask>       if (worker_state.idle.count(worker_in_the_same_process) == 0) {\n <mask>         // Another worker in this process isn't idle, so this process can't be killed.\n </s> [Core] Multi-tenancy: Kill idle workers in FIFO order (#10597)\n\n* Kill idle workers in FIFO order\r\n\r\n* Update test\r\n\r\n* minor update\r\n\r\n* Address comments\r\n\r\n* fix after merge\r\n\r\n* fix worker_pool_test </s> remove         return;\n      } else {\n        workers_in_the_same_process.insert(worker_in_the_same_process);\n </s> add         can_be_killed = false;\n        break;\n      }\n    }\n    if (!can_be_killed) {\n      continue;\n    }\n\n    for (auto worker_it = workers_in_the_same_process.begin();\n         worker_it != workers_in_the_same_process.end(); worker_it++) {\n      RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                    << \" registered workers which exceeds the soft limit of \"\n                    << num_workers_soft_limit_ << \", and worker \"\n                    << (*worker_it)->WorkerId() << \" with pid \" << process.GetId()\n                    << \" is idle. Kill it.\";\n      // Remove the worker from the idle pool so it can't be popped anymore.\n      RemoveWorker(worker_state.idle, *worker_it);\n      if (!(*worker_it)->IsDead()) {\n        (*worker_it)->MarkDead();\n        running_size--; </s> remove   for (auto worker_it = workers_in_the_same_process.begin();\n       worker_it != workers_in_the_same_process.end(); worker_it++) {\n    RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                  << \" registered workers which exceeds the soft limit of \"\n                  << num_workers_soft_limit_ << \", and worker \"\n                  << (*worker_it)->WorkerId() << \" with pid \" << pid\n                  << \" is idle. Kill it.\";\n    // Remove the worker from the idle pool so it can't be popped anymore. However, we\n    // don't remove it from the registered pool because we want the worker to go through\n    // the normal disconnection logic in Node Manager.\n    RemoveWorker(worker_state.idle, *worker_it);\n    worker_state.pending_unregistration_workers.insert(*worker_it);\n </s> add   std::list<std::shared_ptr<WorkerInterface>> new_idle_of_all_languages;\n  for (auto it = idle_of_all_languages.begin(); it != idle_of_all_languages.end(); it++) {\n    if (!(*it)->IsDead()) {\n      new_idle_of_all_languages.push_back(*it);\n    } </s> remove         worker = std::move(*it);\n        state.idle.erase(it);\n </s> add         state.idle.erase(*it);\n        // We can't erase a reverse_iterator.\n        auto lit = it.base();\n        lit--;\n        worker = std::move(*lit);\n        idle_of_all_languages.erase(lit); </s> add std::unordered_set<std::shared_ptr<WorkerInterface>> WorkerPool::GetWorkersByProcess(\n    const Process &process) {\n  std::unordered_set<std::shared_ptr<WorkerInterface>> workers_of_process;\n  for (auto &entry : states_by_lang_) {\n    auto &worker_state = entry.second;\n    for (const auto &worker : worker_state.registered_workers) {\n      if (worker->GetProcess().GetId() == process.GetId()) {\n        workers_of_process.insert(worker);\n      }\n    }\n  }\n  return workers_of_process;\n}\n </s> add   result << \"- num idle workers: \" << idle_of_all_languages.size();", "html_url": "https://github.com/ray-project/ray/commit/864d1d2b59102d5b07aafd871a8a9d4b177def57", "file_name": "src/ray/raylet/worker_pool.cc"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>   for (const auto &worker_in_the_same_process : worker_state.registered_workers) {\n <mask>     if (worker_in_the_same_process->GetProcess().GetId() == pid) {\n <mask>       if (worker_state.idle.count(worker_in_the_same_process) == 0) {\n <mask>         // Another worker in this process isn't idle, so this process can't be killed.\n <mask>         return;\n <mask>       } else {\n <mask>         workers_in_the_same_process.insert(worker_in_the_same_process);\n <mask>       }\n <mask>     }\n <mask>   }\n <mask> \n <mask>   for (auto worker_it = workers_in_the_same_process.begin();\n </s> [Core] Multi-tenancy: Kill idle workers in FIFO order (#10597)\n\n* Kill idle workers in FIFO order\r\n\r\n* Update test\r\n\r\n* minor update\r\n\r\n* Address comments\r\n\r\n* fix after merge\r\n\r\n* fix worker_pool_test </s> remove   // Make sure all workers in this worker process are idle.\n  // This block of code is needed by Java workers.\n  std::unordered_set<std::shared_ptr<WorkerInterface>> workers_in_the_same_process;\n  for (const auto &worker_in_the_same_process : worker_state.registered_workers) {\n    if (worker_in_the_same_process->GetProcess().GetId() == pid) {\n      if (worker_state.idle.count(worker_in_the_same_process) == 0) {\n </s> add     // Make sure all workers in this worker process are idle.\n    // This block of code is needed by Java workers.\n    auto workers_in_the_same_process = GetWorkersByProcess(process);\n    bool can_be_killed = true;\n    for (const auto &worker : workers_in_the_same_process) {\n      if (worker_state.idle.count(worker) == 0) { </s> remove   auto worker_id = worker->WorkerId();\n  const auto pid = worker->GetProcess().GetId();\n  if (worker_state.idle.count(worker) == 0) {\n    return;\n  }\n  if (worker_state.starting_worker_processes.count(worker->GetProcess()) > 0) {\n    // A Java worker process may hold multiple workers.\n    RAY_LOG(DEBUG) << \"Some workers of pid \" << pid\n                   << \" are pending registration. Skip killing worker \" << worker_id;\n    return;\n  }\n </s> add     auto &worker_state = GetStateForLanguage((*it)->GetLanguage());\n\n    if (worker_state.starting_worker_processes.count(process) > 0) {\n      // A Java worker process may hold multiple workers.\n      // Some workers of this process are pending registration. Skip killing this worker.\n      continue;\n    } </s> add std::unordered_set<std::shared_ptr<WorkerInterface>> WorkerPool::GetWorkersByProcess(\n    const Process &process) {\n  std::unordered_set<std::shared_ptr<WorkerInterface>> workers_of_process;\n  for (auto &entry : states_by_lang_) {\n    auto &worker_state = entry.second;\n    for (const auto &worker : worker_state.registered_workers) {\n      if (worker->GetProcess().GetId() == process.GetId()) {\n        workers_of_process.insert(worker);\n      }\n    }\n  }\n  return workers_of_process;\n}\n </s> remove   for (auto worker_it = workers_in_the_same_process.begin();\n       worker_it != workers_in_the_same_process.end(); worker_it++) {\n    RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                  << \" registered workers which exceeds the soft limit of \"\n                  << num_workers_soft_limit_ << \", and worker \"\n                  << (*worker_it)->WorkerId() << \" with pid \" << pid\n                  << \" is idle. Kill it.\";\n    // Remove the worker from the idle pool so it can't be popped anymore. However, we\n    // don't remove it from the registered pool because we want the worker to go through\n    // the normal disconnection logic in Node Manager.\n    RemoveWorker(worker_state.idle, *worker_it);\n    worker_state.pending_unregistration_workers.insert(*worker_it);\n </s> add   std::list<std::shared_ptr<WorkerInterface>> new_idle_of_all_languages;\n  for (auto it = idle_of_all_languages.begin(); it != idle_of_all_languages.end(); it++) {\n    if (!(*it)->IsDead()) {\n      new_idle_of_all_languages.push_back(*it);\n    } </s> remove         worker = std::move(*it);\n        state.idle.erase(it);\n </s> add         state.idle.erase(*it);\n        // We can't erase a reverse_iterator.\n        auto lit = it.base();\n        lit--;\n        worker = std::move(*lit);\n        idle_of_all_languages.erase(lit); </s> remove       for (auto it = state.idle.begin(); it != state.idle.end(); it++) {\n        if ((*it)->GetAssignedJobId() != task_spec.JobId()) {\n </s> add       // Try to pop the most recently pushed worker.\n      for (auto it = idle_of_all_languages.rbegin(); it != idle_of_all_languages.rend();\n           it++) {\n        if (task_spec.GetLanguage() != (*it)->GetLanguage() ||\n            (*it)->GetAssignedJobId() != task_spec.JobId()) {", "html_url": "https://github.com/ray-project/ray/commit/864d1d2b59102d5b07aafd871a8a9d4b177def57", "file_name": "src/ray/raylet/worker_pool.cc"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>         running_size--;\n <mask>       }\n <mask>     }\n <mask>   }\n <mask> \n <mask>   std::list<std::shared_ptr<WorkerInterface>> new_idle_of_all_languages;\n <mask>   for (auto it = idle_of_all_languages.begin(); it != idle_of_all_languages.end(); it++) {\n <mask>     if (!(*it)->IsDead()) {\n </s> [Core] Multi-tenancy: Kill idle workers in FIFO order (#10597)\n\n* Kill idle workers in FIFO order\r\n\r\n* Update test\r\n\r\n* minor update\r\n\r\n* Address comments\r\n\r\n* fix after merge\r\n\r\n* fix worker_pool_test </s> remove   for (auto worker_it = workers_in_the_same_process.begin();\n       worker_it != workers_in_the_same_process.end(); worker_it++) {\n    RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                  << \" registered workers which exceeds the soft limit of \"\n                  << num_workers_soft_limit_ << \", and worker \"\n                  << (*worker_it)->WorkerId() << \" with pid \" << pid\n                  << \" is idle. Kill it.\";\n    // Remove the worker from the idle pool so it can't be popped anymore. However, we\n    // don't remove it from the registered pool because we want the worker to go through\n    // the normal disconnection logic in Node Manager.\n    RemoveWorker(worker_state.idle, *worker_it);\n    worker_state.pending_unregistration_workers.insert(*worker_it);\n </s> add   std::list<std::shared_ptr<WorkerInterface>> new_idle_of_all_languages;\n  for (auto it = idle_of_all_languages.begin(); it != idle_of_all_languages.end(); it++) {\n    if (!(*it)->IsDead()) {\n      new_idle_of_all_languages.push_back(*it);\n    } </s> remove       for (auto it = state.idle.begin(); it != state.idle.end(); it++) {\n        if ((*it)->GetAssignedJobId() != task_spec.JobId()) {\n </s> add       // Try to pop the most recently pushed worker.\n      for (auto it = idle_of_all_languages.rbegin(); it != idle_of_all_languages.rend();\n           it++) {\n        if (task_spec.GetLanguage() != (*it)->GetLanguage() ||\n            (*it)->GetAssignedJobId() != task_spec.JobId()) { </s> remove         worker = std::move(*it);\n        state.idle.erase(it);\n </s> add         state.idle.erase(*it);\n        // We can't erase a reverse_iterator.\n        auto lit = it.base();\n        lit--;\n        worker = std::move(*lit);\n        idle_of_all_languages.erase(lit); </s> remove         return;\n      } else {\n        workers_in_the_same_process.insert(worker_in_the_same_process);\n </s> add         can_be_killed = false;\n        break;\n      }\n    }\n    if (!can_be_killed) {\n      continue;\n    }\n\n    for (auto worker_it = workers_in_the_same_process.begin();\n         worker_it != workers_in_the_same_process.end(); worker_it++) {\n      RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                    << \" registered workers which exceeds the soft limit of \"\n                    << num_workers_soft_limit_ << \", and worker \"\n                    << (*worker_it)->WorkerId() << \" with pid \" << process.GetId()\n                    << \" is idle. Kill it.\";\n      // Remove the worker from the idle pool so it can't be popped anymore.\n      RemoveWorker(worker_state.idle, *worker_it);\n      if (!(*worker_it)->IsDead()) {\n        (*worker_it)->MarkDead();\n        running_size--; </s> add std::unordered_set<std::shared_ptr<WorkerInterface>> WorkerPool::GetWorkersByProcess(\n    const Process &process) {\n  std::unordered_set<std::shared_ptr<WorkerInterface>> workers_of_process;\n  for (auto &entry : states_by_lang_) {\n    auto &worker_state = entry.second;\n    for (const auto &worker : worker_state.registered_workers) {\n      if (worker->GetProcess().GetId() == process.GetId()) {\n        workers_of_process.insert(worker);\n      }\n    }\n  }\n  return workers_of_process;\n}\n </s> remove   auto worker_id = worker->WorkerId();\n  const auto pid = worker->GetProcess().GetId();\n  if (worker_state.idle.count(worker) == 0) {\n    return;\n  }\n  if (worker_state.starting_worker_processes.count(worker->GetProcess()) > 0) {\n    // A Java worker process may hold multiple workers.\n    RAY_LOG(DEBUG) << \"Some workers of pid \" << pid\n                   << \" are pending registration. Skip killing worker \" << worker_id;\n    return;\n  }\n </s> add     auto &worker_state = GetStateForLanguage((*it)->GetLanguage());\n\n    if (worker_state.starting_worker_processes.count(process) > 0) {\n      // A Java worker process may hold multiple workers.\n      // Some workers of this process are pending registration. Skip killing this worker.\n      continue;\n    }", "html_url": "https://github.com/ray-project/ray/commit/864d1d2b59102d5b07aafd871a8a9d4b177def57", "file_name": "src/ray/raylet/worker_pool.cc"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace keep replace keep", "code_tokens": " <mask>       }\n <mask>     }\n <mask>   }\n <mask> \n <mask>   for (auto worker_it = workers_in_the_same_process.begin();\n <mask>        worker_it != workers_in_the_same_process.end(); worker_it++) {\n <mask>     RAY_LOG(INFO) << \"The worker pool has \" << running_size\n <mask>                   << \" registered workers which exceeds the soft limit of \"\n <mask>                   << num_workers_soft_limit_ << \", and worker \"\n <mask>                   << (*worker_it)->WorkerId() << \" with pid \" << pid\n <mask>                   << \" is idle. Kill it.\";\n <mask>     // Remove the worker from the idle pool so it can't be popped anymore. However, we\n <mask>     // don't remove it from the registered pool because we want the worker to go through\n <mask>     // the normal disconnection logic in Node Manager.\n <mask>     RemoveWorker(worker_state.idle, *worker_it);\n <mask>     worker_state.pending_unregistration_workers.insert(*worker_it);\n <mask>   }\n <mask>   worker->GetProcess().Kill();\n <mask> }\n </s> [Core] Multi-tenancy: Kill idle workers in FIFO order (#10597)\n\n* Kill idle workers in FIFO order\r\n\r\n* Update test\r\n\r\n* minor update\r\n\r\n* Address comments\r\n\r\n* fix after merge\r\n\r\n* fix worker_pool_test </s> remove         return;\n      } else {\n        workers_in_the_same_process.insert(worker_in_the_same_process);\n </s> add         can_be_killed = false;\n        break;\n      }\n    }\n    if (!can_be_killed) {\n      continue;\n    }\n\n    for (auto worker_it = workers_in_the_same_process.begin();\n         worker_it != workers_in_the_same_process.end(); worker_it++) {\n      RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                    << \" registered workers which exceeds the soft limit of \"\n                    << num_workers_soft_limit_ << \", and worker \"\n                    << (*worker_it)->WorkerId() << \" with pid \" << process.GetId()\n                    << \" is idle. Kill it.\";\n      // Remove the worker from the idle pool so it can't be popped anymore.\n      RemoveWorker(worker_state.idle, *worker_it);\n      if (!(*worker_it)->IsDead()) {\n        (*worker_it)->MarkDead();\n        running_size--; </s> remove   auto worker_id = worker->WorkerId();\n  const auto pid = worker->GetProcess().GetId();\n  if (worker_state.idle.count(worker) == 0) {\n    return;\n  }\n  if (worker_state.starting_worker_processes.count(worker->GetProcess()) > 0) {\n    // A Java worker process may hold multiple workers.\n    RAY_LOG(DEBUG) << \"Some workers of pid \" << pid\n                   << \" are pending registration. Skip killing worker \" << worker_id;\n    return;\n  }\n </s> add     auto &worker_state = GetStateForLanguage((*it)->GetLanguage());\n\n    if (worker_state.starting_worker_processes.count(process) > 0) {\n      // A Java worker process may hold multiple workers.\n      // Some workers of this process are pending registration. Skip killing this worker.\n      continue;\n    } </s> remove   // Make sure all workers in this worker process are idle.\n  // This block of code is needed by Java workers.\n  std::unordered_set<std::shared_ptr<WorkerInterface>> workers_in_the_same_process;\n  for (const auto &worker_in_the_same_process : worker_state.registered_workers) {\n    if (worker_in_the_same_process->GetProcess().GetId() == pid) {\n      if (worker_state.idle.count(worker_in_the_same_process) == 0) {\n </s> add     // Make sure all workers in this worker process are idle.\n    // This block of code is needed by Java workers.\n    auto workers_in_the_same_process = GetWorkersByProcess(process);\n    bool can_be_killed = true;\n    for (const auto &worker : workers_in_the_same_process) {\n      if (worker_state.idle.count(worker) == 0) { </s> add   result << \"- num idle workers: \" << idle_of_all_languages.size(); </s> remove       for (auto it = state.idle.begin(); it != state.idle.end(); it++) {\n        if ((*it)->GetAssignedJobId() != task_spec.JobId()) {\n </s> add       // Try to pop the most recently pushed worker.\n      for (auto it = idle_of_all_languages.rbegin(); it != idle_of_all_languages.rend();\n           it++) {\n        if (task_spec.GetLanguage() != (*it)->GetLanguage() ||\n            (*it)->GetAssignedJobId() != task_spec.JobId()) {", "html_url": "https://github.com/ray-project/ray/commit/864d1d2b59102d5b07aafd871a8a9d4b177def57", "file_name": "src/ray/raylet/worker_pool.cc"}
{"docstring_tokens": "keep keep keep replace replace keep keep replace replace keep keep keep", "code_tokens": " <mask>       }\n <mask>     } else {\n <mask>       // Find an available worker which is already assigned to this job.\n <mask>       for (auto it = state.idle.begin(); it != state.idle.end(); it++) {\n <mask>         if ((*it)->GetAssignedJobId() != task_spec.JobId()) {\n <mask>           continue;\n <mask>         }\n <mask>         worker = std::move(*it);\n <mask>         state.idle.erase(it);\n <mask>         break;\n <mask>       }\n <mask>       if (worker == nullptr) {\n </s> [Core] Multi-tenancy: Kill idle workers in FIFO order (#10597)\n\n* Kill idle workers in FIFO order\r\n\r\n* Update test\r\n\r\n* minor update\r\n\r\n* Address comments\r\n\r\n* fix after merge\r\n\r\n* fix worker_pool_test </s> remove         return;\n      } else {\n        workers_in_the_same_process.insert(worker_in_the_same_process);\n </s> add         can_be_killed = false;\n        break;\n      }\n    }\n    if (!can_be_killed) {\n      continue;\n    }\n\n    for (auto worker_it = workers_in_the_same_process.begin();\n         worker_it != workers_in_the_same_process.end(); worker_it++) {\n      RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                    << \" registered workers which exceeds the soft limit of \"\n                    << num_workers_soft_limit_ << \", and worker \"\n                    << (*worker_it)->WorkerId() << \" with pid \" << process.GetId()\n                    << \" is idle. Kill it.\";\n      // Remove the worker from the idle pool so it can't be popped anymore.\n      RemoveWorker(worker_state.idle, *worker_it);\n      if (!(*worker_it)->IsDead()) {\n        (*worker_it)->MarkDead();\n        running_size--; </s> add     process.Kill(); </s> remove   for (auto worker_it = workers_in_the_same_process.begin();\n       worker_it != workers_in_the_same_process.end(); worker_it++) {\n    RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                  << \" registered workers which exceeds the soft limit of \"\n                  << num_workers_soft_limit_ << \", and worker \"\n                  << (*worker_it)->WorkerId() << \" with pid \" << pid\n                  << \" is idle. Kill it.\";\n    // Remove the worker from the idle pool so it can't be popped anymore. However, we\n    // don't remove it from the registered pool because we want the worker to go through\n    // the normal disconnection logic in Node Manager.\n    RemoveWorker(worker_state.idle, *worker_it);\n    worker_state.pending_unregistration_workers.insert(*worker_it);\n </s> add   std::list<std::shared_ptr<WorkerInterface>> new_idle_of_all_languages;\n  for (auto it = idle_of_all_languages.begin(); it != idle_of_all_languages.end(); it++) {\n    if (!(*it)->IsDead()) {\n      new_idle_of_all_languages.push_back(*it);\n    } </s> remove   // Make sure all workers in this worker process are idle.\n  // This block of code is needed by Java workers.\n  std::unordered_set<std::shared_ptr<WorkerInterface>> workers_in_the_same_process;\n  for (const auto &worker_in_the_same_process : worker_state.registered_workers) {\n    if (worker_in_the_same_process->GetProcess().GetId() == pid) {\n      if (worker_state.idle.count(worker_in_the_same_process) == 0) {\n </s> add     // Make sure all workers in this worker process are idle.\n    // This block of code is needed by Java workers.\n    auto workers_in_the_same_process = GetWorkersByProcess(process);\n    bool can_be_killed = true;\n    for (const auto &worker : workers_in_the_same_process) {\n      if (worker_state.idle.count(worker) == 0) { </s> remove   auto worker_id = worker->WorkerId();\n  const auto pid = worker->GetProcess().GetId();\n  if (worker_state.idle.count(worker) == 0) {\n    return;\n  }\n  if (worker_state.starting_worker_processes.count(worker->GetProcess()) > 0) {\n    // A Java worker process may hold multiple workers.\n    RAY_LOG(DEBUG) << \"Some workers of pid \" << pid\n                   << \" are pending registration. Skip killing worker \" << worker_id;\n    return;\n  }\n </s> add     auto &worker_state = GetStateForLanguage((*it)->GetLanguage());\n\n    if (worker_state.starting_worker_processes.count(process) > 0) {\n      // A Java worker process may hold multiple workers.\n      // Some workers of this process are pending registration. Skip killing this worker.\n      continue;\n    }", "html_url": "https://github.com/ray-project/ray/commit/864d1d2b59102d5b07aafd871a8a9d4b177def57", "file_name": "src/ray/raylet/worker_pool.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> bool WorkerPool::DisconnectWorker(const std::shared_ptr<WorkerInterface> &worker) {\n <mask>   auto &state = GetStateForLanguage(worker->GetLanguage());\n <mask>   RAY_CHECK(RemoveWorker(state.registered_workers, worker));\n <mask>   RemoveWorker(state.pending_unregistration_workers, worker);\n <mask> \n <mask>   stats::CurrentWorker().Record(\n <mask>       0, {{stats::LanguageKey, Language_Name(worker->GetLanguage())},\n <mask>           {stats::WorkerPidKey, std::to_string(worker->GetProcess().GetId())}});\n <mask> \n </s> [Core] Multi-tenancy: Kill idle workers in FIFO order (#10597)\n\n* Kill idle workers in FIFO order\r\n\r\n* Update test\r\n\r\n* minor update\r\n\r\n* Address comments\r\n\r\n* fix after merge\r\n\r\n* fix worker_pool_test </s> remove   worker->GetProcess().Kill();\n </s> add   idle_of_all_languages = std::move(new_idle_of_all_languages); </s> remove   // Make sure all workers in this worker process are idle.\n  // This block of code is needed by Java workers.\n  std::unordered_set<std::shared_ptr<WorkerInterface>> workers_in_the_same_process;\n  for (const auto &worker_in_the_same_process : worker_state.registered_workers) {\n    if (worker_in_the_same_process->GetProcess().GetId() == pid) {\n      if (worker_state.idle.count(worker_in_the_same_process) == 0) {\n </s> add     // Make sure all workers in this worker process are idle.\n    // This block of code is needed by Java workers.\n    auto workers_in_the_same_process = GetWorkersByProcess(process);\n    bool can_be_killed = true;\n    for (const auto &worker : workers_in_the_same_process) {\n      if (worker_state.idle.count(worker) == 0) { </s> remove   /// Try to kill the worker if it's idle.\n  ///\n  /// \\param worker The worker to be killed.\n  void TryKillingIdleWorker(std::shared_ptr<WorkerInterface> worker);\n </s> add   /// Try killing idle workers to ensure the running workers are in a\n  /// reasonable size.\n  void TryKillingIdleWorkers(); </s> add std::unordered_set<std::shared_ptr<WorkerInterface>> WorkerPool::GetWorkersByProcess(\n    const Process &process) {\n  std::unordered_set<std::shared_ptr<WorkerInterface>> workers_of_process;\n  for (auto &entry : states_by_lang_) {\n    auto &worker_state = entry.second;\n    for (const auto &worker : worker_state.registered_workers) {\n      if (worker->GetProcess().GetId() == process.GetId()) {\n        workers_of_process.insert(worker);\n      }\n    }\n  }\n  return workers_of_process;\n}\n </s> remove   auto worker_id = worker->WorkerId();\n  const auto pid = worker->GetProcess().GetId();\n  if (worker_state.idle.count(worker) == 0) {\n    return;\n  }\n  if (worker_state.starting_worker_processes.count(worker->GetProcess()) > 0) {\n    // A Java worker process may hold multiple workers.\n    RAY_LOG(DEBUG) << \"Some workers of pid \" << pid\n                   << \" are pending registration. Skip killing worker \" << worker_id;\n    return;\n  }\n </s> add     auto &worker_state = GetStateForLanguage((*it)->GetLanguage());\n\n    if (worker_state.starting_worker_processes.count(process) > 0) {\n      // A Java worker process may hold multiple workers.\n      // Some workers of this process are pending registration. Skip killing this worker.\n      continue;\n    } </s> remove         worker = std::move(*it);\n        state.idle.erase(it);\n </s> add         state.idle.erase(*it);\n        // We can't erase a reverse_iterator.\n        auto lit = it.base();\n        lit--;\n        worker = std::move(*lit);\n        idle_of_all_languages.erase(lit);", "html_url": "https://github.com/ray-project/ray/commit/864d1d2b59102d5b07aafd871a8a9d4b177def57", "file_name": "src/ray/raylet/worker_pool.cc"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> }\n <mask> \n <mask> std::string WorkerPool::DebugString() const {\n <mask>   std::stringstream result;\n <mask>   result << \"WorkerPool:\";\n <mask>   for (const auto &entry : states_by_lang_) {\n </s> [Core] Multi-tenancy: Kill idle workers in FIFO order (#10597)\n\n* Kill idle workers in FIFO order\r\n\r\n* Update test\r\n\r\n* minor update\r\n\r\n* Address comments\r\n\r\n* fix after merge\r\n\r\n* fix worker_pool_test </s> add   result << \"- num idle workers: \" << idle_of_all_languages.size(); </s> remove   // Make sure all workers in this worker process are idle.\n  // This block of code is needed by Java workers.\n  std::unordered_set<std::shared_ptr<WorkerInterface>> workers_in_the_same_process;\n  for (const auto &worker_in_the_same_process : worker_state.registered_workers) {\n    if (worker_in_the_same_process->GetProcess().GetId() == pid) {\n      if (worker_state.idle.count(worker_in_the_same_process) == 0) {\n </s> add     // Make sure all workers in this worker process are idle.\n    // This block of code is needed by Java workers.\n    auto workers_in_the_same_process = GetWorkersByProcess(process);\n    bool can_be_killed = true;\n    for (const auto &worker : workers_in_the_same_process) {\n      if (worker_state.idle.count(worker) == 0) { </s> remove   auto worker_id = worker->WorkerId();\n  const auto pid = worker->GetProcess().GetId();\n  if (worker_state.idle.count(worker) == 0) {\n    return;\n  }\n  if (worker_state.starting_worker_processes.count(worker->GetProcess()) > 0) {\n    // A Java worker process may hold multiple workers.\n    RAY_LOG(DEBUG) << \"Some workers of pid \" << pid\n                   << \" are pending registration. Skip killing worker \" << worker_id;\n    return;\n  }\n </s> add     auto &worker_state = GetStateForLanguage((*it)->GetLanguage());\n\n    if (worker_state.starting_worker_processes.count(process) > 0) {\n      // A Java worker process may hold multiple workers.\n      // Some workers of this process are pending registration. Skip killing this worker.\n      continue;\n    } </s> remove         return;\n      } else {\n        workers_in_the_same_process.insert(worker_in_the_same_process);\n </s> add         can_be_killed = false;\n        break;\n      }\n    }\n    if (!can_be_killed) {\n      continue;\n    }\n\n    for (auto worker_it = workers_in_the_same_process.begin();\n         worker_it != workers_in_the_same_process.end(); worker_it++) {\n      RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                    << \" registered workers which exceeds the soft limit of \"\n                    << num_workers_soft_limit_ << \", and worker \"\n                    << (*worker_it)->WorkerId() << \" with pid \" << process.GetId()\n                    << \" is idle. Kill it.\";\n      // Remove the worker from the idle pool so it can't be popped anymore.\n      RemoveWorker(worker_state.idle, *worker_it);\n      if (!(*worker_it)->IsDead()) {\n        (*worker_it)->MarkDead();\n        running_size--; </s> remove   worker->GetProcess().Kill();\n </s> add   idle_of_all_languages = std::move(new_idle_of_all_languages); </s> remove   for (auto worker_it = workers_in_the_same_process.begin();\n       worker_it != workers_in_the_same_process.end(); worker_it++) {\n    RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                  << \" registered workers which exceeds the soft limit of \"\n                  << num_workers_soft_limit_ << \", and worker \"\n                  << (*worker_it)->WorkerId() << \" with pid \" << pid\n                  << \" is idle. Kill it.\";\n    // Remove the worker from the idle pool so it can't be popped anymore. However, we\n    // don't remove it from the registered pool because we want the worker to go through\n    // the normal disconnection logic in Node Manager.\n    RemoveWorker(worker_state.idle, *worker_it);\n    worker_state.pending_unregistration_workers.insert(*worker_it);\n </s> add   std::list<std::shared_ptr<WorkerInterface>> new_idle_of_all_languages;\n  for (auto it = idle_of_all_languages.begin(); it != idle_of_all_languages.end(); it++) {\n    if (!(*it)->IsDead()) {\n      new_idle_of_all_languages.push_back(*it);\n    }", "html_url": "https://github.com/ray-project/ray/commit/864d1d2b59102d5b07aafd871a8a9d4b177def57", "file_name": "src/ray/raylet/worker_pool.cc"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>            << \" workers: \" << entry.second.registered_workers.size();\n <mask>     result << \"\\n- num \" << Language_Name(entry.first)\n <mask>            << \" drivers: \" << entry.second.registered_drivers.size();\n <mask>   }\n <mask>   return result.str();\n <mask> }\n <mask> \n <mask> void WorkerPool::RecordMetrics() const {\n <mask>   for (const auto &entry : states_by_lang_) {\n </s> [Core] Multi-tenancy: Kill idle workers in FIFO order (#10597)\n\n* Kill idle workers in FIFO order\r\n\r\n* Update test\r\n\r\n* minor update\r\n\r\n* Address comments\r\n\r\n* fix after merge\r\n\r\n* fix worker_pool_test </s> add std::unordered_set<std::shared_ptr<WorkerInterface>> WorkerPool::GetWorkersByProcess(\n    const Process &process) {\n  std::unordered_set<std::shared_ptr<WorkerInterface>> workers_of_process;\n  for (auto &entry : states_by_lang_) {\n    auto &worker_state = entry.second;\n    for (const auto &worker : worker_state.registered_workers) {\n      if (worker->GetProcess().GetId() == process.GetId()) {\n        workers_of_process.insert(worker);\n      }\n    }\n  }\n  return workers_of_process;\n}\n </s> remove         return;\n      } else {\n        workers_in_the_same_process.insert(worker_in_the_same_process);\n </s> add         can_be_killed = false;\n        break;\n      }\n    }\n    if (!can_be_killed) {\n      continue;\n    }\n\n    for (auto worker_it = workers_in_the_same_process.begin();\n         worker_it != workers_in_the_same_process.end(); worker_it++) {\n      RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                    << \" registered workers which exceeds the soft limit of \"\n                    << num_workers_soft_limit_ << \", and worker \"\n                    << (*worker_it)->WorkerId() << \" with pid \" << process.GetId()\n                    << \" is idle. Kill it.\";\n      // Remove the worker from the idle pool so it can't be popped anymore.\n      RemoveWorker(worker_state.idle, *worker_it);\n      if (!(*worker_it)->IsDead()) {\n        (*worker_it)->MarkDead();\n        running_size--; </s> remove   for (auto worker_it = workers_in_the_same_process.begin();\n       worker_it != workers_in_the_same_process.end(); worker_it++) {\n    RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                  << \" registered workers which exceeds the soft limit of \"\n                  << num_workers_soft_limit_ << \", and worker \"\n                  << (*worker_it)->WorkerId() << \" with pid \" << pid\n                  << \" is idle. Kill it.\";\n    // Remove the worker from the idle pool so it can't be popped anymore. However, we\n    // don't remove it from the registered pool because we want the worker to go through\n    // the normal disconnection logic in Node Manager.\n    RemoveWorker(worker_state.idle, *worker_it);\n    worker_state.pending_unregistration_workers.insert(*worker_it);\n </s> add   std::list<std::shared_ptr<WorkerInterface>> new_idle_of_all_languages;\n  for (auto it = idle_of_all_languages.begin(); it != idle_of_all_languages.end(); it++) {\n    if (!(*it)->IsDead()) {\n      new_idle_of_all_languages.push_back(*it);\n    } </s> remove   auto worker_id = worker->WorkerId();\n  const auto pid = worker->GetProcess().GetId();\n  if (worker_state.idle.count(worker) == 0) {\n    return;\n  }\n  if (worker_state.starting_worker_processes.count(worker->GetProcess()) > 0) {\n    // A Java worker process may hold multiple workers.\n    RAY_LOG(DEBUG) << \"Some workers of pid \" << pid\n                   << \" are pending registration. Skip killing worker \" << worker_id;\n    return;\n  }\n </s> add     auto &worker_state = GetStateForLanguage((*it)->GetLanguage());\n\n    if (worker_state.starting_worker_processes.count(process) > 0) {\n      // A Java worker process may hold multiple workers.\n      // Some workers of this process are pending registration. Skip killing this worker.\n      continue;\n    } </s> remove   // Make sure all workers in this worker process are idle.\n  // This block of code is needed by Java workers.\n  std::unordered_set<std::shared_ptr<WorkerInterface>> workers_in_the_same_process;\n  for (const auto &worker_in_the_same_process : worker_state.registered_workers) {\n    if (worker_in_the_same_process->GetProcess().GetId() == pid) {\n      if (worker_state.idle.count(worker_in_the_same_process) == 0) {\n </s> add     // Make sure all workers in this worker process are idle.\n    // This block of code is needed by Java workers.\n    auto workers_in_the_same_process = GetWorkersByProcess(process);\n    bool can_be_killed = true;\n    for (const auto &worker : workers_in_the_same_process) {\n      if (worker_state.idle.count(worker) == 0) { </s> remove   worker->GetProcess().Kill();\n </s> add   idle_of_all_languages = std::move(new_idle_of_all_languages);", "html_url": "https://github.com/ray-project/ray/commit/864d1d2b59102d5b07aafd871a8a9d4b177def57", "file_name": "src/ray/raylet/worker_pool.cc"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>   ///\n <mask>   /// \\param The idle worker to add.\n <mask>   void PushWorker(const std::shared_ptr<WorkerInterface> &worker);\n <mask> \n <mask>   /// Try to kill the worker if it's idle.\n <mask>   ///\n <mask>   /// \\param worker The worker to be killed.\n <mask>   void TryKillingIdleWorker(std::shared_ptr<WorkerInterface> worker);\n <mask> \n <mask>   /// Pop an idle worker from the pool. The caller is responsible for pushing\n <mask>   /// the worker back onto the pool once the worker has completed its work.\n <mask>   ///\n <mask>   /// \\param task_spec The returned worker must be able to execute this task.\n </s> [Core] Multi-tenancy: Kill idle workers in FIFO order (#10597)\n\n* Kill idle workers in FIFO order\r\n\r\n* Update test\r\n\r\n* minor update\r\n\r\n* Address comments\r\n\r\n* fix after merge\r\n\r\n* fix worker_pool_test </s> add   /// Get all workers of the given process.\n  ///\n  /// \\param process The process of workers.\n  /// \\return The workers of the given process.\n  std::unordered_set<std::shared_ptr<WorkerInterface>> GetWorkersByProcess(\n      const Process &process);\n </s> remove     /// All workers that have been killed but been unregistered yet.\n    /// This field is used to calculate the size of running workers when trying to kill an\n    /// idle worker.\n    std::unordered_set<std::shared_ptr<WorkerInterface>> pending_unregistration_workers;\n </s> add  </s> add   /// The pool of idle non-actor workers of all languages. This is used to kill idle\n  /// workers in FIFO order.\n  std::list<std::shared_ptr<WorkerInterface>> idle_of_all_languages; </s> remove   for (auto worker_it = workers_in_the_same_process.begin();\n       worker_it != workers_in_the_same_process.end(); worker_it++) {\n    RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                  << \" registered workers which exceeds the soft limit of \"\n                  << num_workers_soft_limit_ << \", and worker \"\n                  << (*worker_it)->WorkerId() << \" with pid \" << pid\n                  << \" is idle. Kill it.\";\n    // Remove the worker from the idle pool so it can't be popped anymore. However, we\n    // don't remove it from the registered pool because we want the worker to go through\n    // the normal disconnection logic in Node Manager.\n    RemoveWorker(worker_state.idle, *worker_it);\n    worker_state.pending_unregistration_workers.insert(*worker_it);\n </s> add   std::list<std::shared_ptr<WorkerInterface>> new_idle_of_all_languages;\n  for (auto it = idle_of_all_languages.begin(); it != idle_of_all_languages.end(); it++) {\n    if (!(*it)->IsDead()) {\n      new_idle_of_all_languages.push_back(*it);\n    } </s> remove         return;\n      } else {\n        workers_in_the_same_process.insert(worker_in_the_same_process);\n </s> add         can_be_killed = false;\n        break;\n      }\n    }\n    if (!can_be_killed) {\n      continue;\n    }\n\n    for (auto worker_it = workers_in_the_same_process.begin();\n         worker_it != workers_in_the_same_process.end(); worker_it++) {\n      RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                    << \" registered workers which exceeds the soft limit of \"\n                    << num_workers_soft_limit_ << \", and worker \"\n                    << (*worker_it)->WorkerId() << \" with pid \" << process.GetId()\n                    << \" is idle. Kill it.\";\n      // Remove the worker from the idle pool so it can't be popped anymore.\n      RemoveWorker(worker_state.idle, *worker_it);\n      if (!(*worker_it)->IsDead()) {\n        (*worker_it)->MarkDead();\n        running_size--; </s> remove       for (auto it = state.idle.begin(); it != state.idle.end(); it++) {\n        if ((*it)->GetAssignedJobId() != task_spec.JobId()) {\n </s> add       // Try to pop the most recently pushed worker.\n      for (auto it = idle_of_all_languages.rbegin(); it != idle_of_all_languages.rend();\n           it++) {\n        if (task_spec.GetLanguage() != (*it)->GetLanguage() ||\n            (*it)->GetAssignedJobId() != task_spec.JobId()) {", "html_url": "https://github.com/ray-project/ray/commit/864d1d2b59102d5b07aafd871a8a9d4b177def57", "file_name": "src/ray/raylet/worker_pool.h"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     /// idle and executing.\n <mask>     std::unordered_set<std::shared_ptr<WorkerInterface>> registered_workers;\n <mask>     /// All drivers that have registered and are still connected.\n <mask>     std::unordered_set<std::shared_ptr<WorkerInterface>> registered_drivers;\n <mask>     /// All workers that have been killed but been unregistered yet.\n <mask>     /// This field is used to calculate the size of running workers when trying to kill an\n <mask>     /// idle worker.\n <mask>     std::unordered_set<std::shared_ptr<WorkerInterface>> pending_unregistration_workers;\n <mask>     /// A map from the pids of starting worker processes\n <mask>     /// to the number of their unregistered workers.\n <mask>     std::unordered_map<Process, int> starting_worker_processes;\n <mask>     /// A map for looking up the task with dynamic options by the pid of\n <mask>     /// worker. Note that this is used for the dedicated worker processes.\n </s> [Core] Multi-tenancy: Kill idle workers in FIFO order (#10597)\n\n* Kill idle workers in FIFO order\r\n\r\n* Update test\r\n\r\n* minor update\r\n\r\n* Address comments\r\n\r\n* fix after merge\r\n\r\n* fix worker_pool_test </s> add   /// Get all workers of the given process.\n  ///\n  /// \\param process The process of workers.\n  /// \\return The workers of the given process.\n  std::unordered_set<std::shared_ptr<WorkerInterface>> GetWorkersByProcess(\n      const Process &process);\n </s> remove   /// Try to kill the worker if it's idle.\n  ///\n  /// \\param worker The worker to be killed.\n  void TryKillingIdleWorker(std::shared_ptr<WorkerInterface> worker);\n </s> add   /// Try killing idle workers to ensure the running workers are in a\n  /// reasonable size.\n  void TryKillingIdleWorkers(); </s> add   /// The pool of idle non-actor workers of all languages. This is used to kill idle\n  /// workers in FIFO order.\n  std::list<std::shared_ptr<WorkerInterface>> idle_of_all_languages; </s> remove   auto worker_id = worker->WorkerId();\n  const auto pid = worker->GetProcess().GetId();\n  if (worker_state.idle.count(worker) == 0) {\n    return;\n  }\n  if (worker_state.starting_worker_processes.count(worker->GetProcess()) > 0) {\n    // A Java worker process may hold multiple workers.\n    RAY_LOG(DEBUG) << \"Some workers of pid \" << pid\n                   << \" are pending registration. Skip killing worker \" << worker_id;\n    return;\n  }\n </s> add     auto &worker_state = GetStateForLanguage((*it)->GetLanguage());\n\n    if (worker_state.starting_worker_processes.count(process) > 0) {\n      // A Java worker process may hold multiple workers.\n      // Some workers of this process are pending registration. Skip killing this worker.\n      continue;\n    } </s> remove   for (auto worker_it = workers_in_the_same_process.begin();\n       worker_it != workers_in_the_same_process.end(); worker_it++) {\n    RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                  << \" registered workers which exceeds the soft limit of \"\n                  << num_workers_soft_limit_ << \", and worker \"\n                  << (*worker_it)->WorkerId() << \" with pid \" << pid\n                  << \" is idle. Kill it.\";\n    // Remove the worker from the idle pool so it can't be popped anymore. However, we\n    // don't remove it from the registered pool because we want the worker to go through\n    // the normal disconnection logic in Node Manager.\n    RemoveWorker(worker_state.idle, *worker_it);\n    worker_state.pending_unregistration_workers.insert(*worker_it);\n </s> add   std::list<std::shared_ptr<WorkerInterface>> new_idle_of_all_languages;\n  for (auto it = idle_of_all_languages.begin(); it != idle_of_all_languages.end(); it++) {\n    if (!(*it)->IsDead()) {\n      new_idle_of_all_languages.push_back(*it);\n    } </s> remove         return;\n      } else {\n        workers_in_the_same_process.insert(worker_in_the_same_process);\n </s> add         can_be_killed = false;\n        break;\n      }\n    }\n    if (!can_be_killed) {\n      continue;\n    }\n\n    for (auto worker_it = workers_in_the_same_process.begin();\n         worker_it != workers_in_the_same_process.end(); worker_it++) {\n      RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                    << \" registered workers which exceeds the soft limit of \"\n                    << num_workers_soft_limit_ << \", and worker \"\n                    << (*worker_it)->WorkerId() << \" with pid \" << process.GetId()\n                    << \" is idle. Kill it.\";\n      // Remove the worker from the idle pool so it can't be popped anymore.\n      RemoveWorker(worker_state.idle, *worker_it);\n      if (!(*worker_it)->IsDead()) {\n        (*worker_it)->MarkDead();\n        running_size--;", "html_url": "https://github.com/ray-project/ray/commit/864d1d2b59102d5b07aafd871a8a9d4b177def57", "file_name": "src/ray/raylet/worker_pool.h"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask>   void TryStartIOWorkers(const Language &language, State &state);\n <mask> \n <mask>   /// For Process class for managing subprocesses (e.g. reaping zombies).\n <mask>   boost::asio::io_service *io_service_;\n <mask>   /// The soft limit of the number of registered workers.\n <mask>   int num_workers_soft_limit_;\n <mask>   /// The maximum number of worker processes that can be started concurrently.\n </s> [Core] Multi-tenancy: Kill idle workers in FIFO order (#10597)\n\n* Kill idle workers in FIFO order\r\n\r\n* Update test\r\n\r\n* minor update\r\n\r\n* Address comments\r\n\r\n* fix after merge\r\n\r\n* fix worker_pool_test </s> remove     /// All workers that have been killed but been unregistered yet.\n    /// This field is used to calculate the size of running workers when trying to kill an\n    /// idle worker.\n    std::unordered_set<std::shared_ptr<WorkerInterface>> pending_unregistration_workers;\n </s> add  </s> remove   /// Try to kill the worker if it's idle.\n  ///\n  /// \\param worker The worker to be killed.\n  void TryKillingIdleWorker(std::shared_ptr<WorkerInterface> worker);\n </s> add   /// Try killing idle workers to ensure the running workers are in a\n  /// reasonable size.\n  void TryKillingIdleWorkers(); </s> add   /// The pool of idle non-actor workers of all languages. This is used to kill idle\n  /// workers in FIFO order.\n  std::list<std::shared_ptr<WorkerInterface>> idle_of_all_languages; </s> remove   for (auto worker_it = workers_in_the_same_process.begin();\n       worker_it != workers_in_the_same_process.end(); worker_it++) {\n    RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                  << \" registered workers which exceeds the soft limit of \"\n                  << num_workers_soft_limit_ << \", and worker \"\n                  << (*worker_it)->WorkerId() << \" with pid \" << pid\n                  << \" is idle. Kill it.\";\n    // Remove the worker from the idle pool so it can't be popped anymore. However, we\n    // don't remove it from the registered pool because we want the worker to go through\n    // the normal disconnection logic in Node Manager.\n    RemoveWorker(worker_state.idle, *worker_it);\n    worker_state.pending_unregistration_workers.insert(*worker_it);\n </s> add   std::list<std::shared_ptr<WorkerInterface>> new_idle_of_all_languages;\n  for (auto it = idle_of_all_languages.begin(); it != idle_of_all_languages.end(); it++) {\n    if (!(*it)->IsDead()) {\n      new_idle_of_all_languages.push_back(*it);\n    } </s> remove         return;\n      } else {\n        workers_in_the_same_process.insert(worker_in_the_same_process);\n </s> add         can_be_killed = false;\n        break;\n      }\n    }\n    if (!can_be_killed) {\n      continue;\n    }\n\n    for (auto worker_it = workers_in_the_same_process.begin();\n         worker_it != workers_in_the_same_process.end(); worker_it++) {\n      RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                    << \" registered workers which exceeds the soft limit of \"\n                    << num_workers_soft_limit_ << \", and worker \"\n                    << (*worker_it)->WorkerId() << \" with pid \" << process.GetId()\n                    << \" is idle. Kill it.\";\n      // Remove the worker from the idle pool so it can't be popped anymore.\n      RemoveWorker(worker_state.idle, *worker_it);\n      if (!(*worker_it)->IsDead()) {\n        (*worker_it)->MarkDead();\n        running_size--; </s> remove   // Make sure all workers in this worker process are idle.\n  // This block of code is needed by Java workers.\n  std::unordered_set<std::shared_ptr<WorkerInterface>> workers_in_the_same_process;\n  for (const auto &worker_in_the_same_process : worker_state.registered_workers) {\n    if (worker_in_the_same_process->GetProcess().GetId() == pid) {\n      if (worker_state.idle.count(worker_in_the_same_process) == 0) {\n </s> add     // Make sure all workers in this worker process are idle.\n    // This block of code is needed by Java workers.\n    auto workers_in_the_same_process = GetWorkersByProcess(process);\n    bool can_be_killed = true;\n    for (const auto &worker : workers_in_the_same_process) {\n      if (worker_state.idle.count(worker) == 0) {", "html_url": "https://github.com/ray-project/ray/commit/864d1d2b59102d5b07aafd871a8a9d4b177def57", "file_name": "src/ray/raylet/worker_pool.h"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask> \n <mask>   /// This map tracks the latest infos of unfinished jobs.\n <mask>   absl::flat_hash_map<JobID, rpc::JobConfig> unfinished_jobs_;\n <mask> };\n <mask> \n <mask> }  // namespace raylet\n <mask> \n <mask> }  // namespace ray\n </s> [Core] Multi-tenancy: Kill idle workers in FIFO order (#10597)\n\n* Kill idle workers in FIFO order\r\n\r\n* Update test\r\n\r\n* minor update\r\n\r\n* Address comments\r\n\r\n* fix after merge\r\n\r\n* fix worker_pool_test </s> remove     /// All workers that have been killed but been unregistered yet.\n    /// This field is used to calculate the size of running workers when trying to kill an\n    /// idle worker.\n    std::unordered_set<std::shared_ptr<WorkerInterface>> pending_unregistration_workers;\n </s> add  </s> remove   // Make sure all workers in this worker process are idle.\n  // This block of code is needed by Java workers.\n  std::unordered_set<std::shared_ptr<WorkerInterface>> workers_in_the_same_process;\n  for (const auto &worker_in_the_same_process : worker_state.registered_workers) {\n    if (worker_in_the_same_process->GetProcess().GetId() == pid) {\n      if (worker_state.idle.count(worker_in_the_same_process) == 0) {\n </s> add     // Make sure all workers in this worker process are idle.\n    // This block of code is needed by Java workers.\n    auto workers_in_the_same_process = GetWorkersByProcess(process);\n    bool can_be_killed = true;\n    for (const auto &worker : workers_in_the_same_process) {\n      if (worker_state.idle.count(worker) == 0) { </s> remove   auto worker_id = worker->WorkerId();\n  const auto pid = worker->GetProcess().GetId();\n  if (worker_state.idle.count(worker) == 0) {\n    return;\n  }\n  if (worker_state.starting_worker_processes.count(worker->GetProcess()) > 0) {\n    // A Java worker process may hold multiple workers.\n    RAY_LOG(DEBUG) << \"Some workers of pid \" << pid\n                   << \" are pending registration. Skip killing worker \" << worker_id;\n    return;\n  }\n </s> add     auto &worker_state = GetStateForLanguage((*it)->GetLanguage());\n\n    if (worker_state.starting_worker_processes.count(process) > 0) {\n      // A Java worker process may hold multiple workers.\n      // Some workers of this process are pending registration. Skip killing this worker.\n      continue;\n    } </s> remove   for (auto worker_it = workers_in_the_same_process.begin();\n       worker_it != workers_in_the_same_process.end(); worker_it++) {\n    RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                  << \" registered workers which exceeds the soft limit of \"\n                  << num_workers_soft_limit_ << \", and worker \"\n                  << (*worker_it)->WorkerId() << \" with pid \" << pid\n                  << \" is idle. Kill it.\";\n    // Remove the worker from the idle pool so it can't be popped anymore. However, we\n    // don't remove it from the registered pool because we want the worker to go through\n    // the normal disconnection logic in Node Manager.\n    RemoveWorker(worker_state.idle, *worker_it);\n    worker_state.pending_unregistration_workers.insert(*worker_it);\n </s> add   std::list<std::shared_ptr<WorkerInterface>> new_idle_of_all_languages;\n  for (auto it = idle_of_all_languages.begin(); it != idle_of_all_languages.end(); it++) {\n    if (!(*it)->IsDead()) {\n      new_idle_of_all_languages.push_back(*it);\n    } </s> remove   worker->GetProcess().Kill();\n </s> add   idle_of_all_languages = std::move(new_idle_of_all_languages); </s> remove         return;\n      } else {\n        workers_in_the_same_process.insert(worker_in_the_same_process);\n </s> add         can_be_killed = false;\n        break;\n      }\n    }\n    if (!can_be_killed) {\n      continue;\n    }\n\n    for (auto worker_it = workers_in_the_same_process.begin();\n         worker_it != workers_in_the_same_process.end(); worker_it++) {\n      RAY_LOG(INFO) << \"The worker pool has \" << running_size\n                    << \" registered workers which exceeds the soft limit of \"\n                    << num_workers_soft_limit_ << \", and worker \"\n                    << (*worker_it)->WorkerId() << \" with pid \" << process.GetId()\n                    << \" is idle. Kill it.\";\n      // Remove the worker from the idle pool so it can't be popped anymore.\n      RemoveWorker(worker_state.idle, *worker_it);\n      if (!(*worker_it)->IsDead()) {\n        (*worker_it)->MarkDead();\n        running_size--;", "html_url": "https://github.com/ray-project/ray/commit/864d1d2b59102d5b07aafd871a8a9d4b177def57", "file_name": "src/ray/raylet/worker_pool.h"}
{"docstring_tokens": "keep keep keep replace replace replace replace replace replace replace replace replace keep replace replace replace replace replace replace replace replace replace", "code_tokens": " <mask> )\n <mask> \n <mask> # DDPG\n <mask> # py_test(\n <mask> #    name = \"learning_tests_pendulum_ddpg\",\n <mask> #    main = \"tests/run_regression_tests.py\",\n <mask> #    tags = [\"team:ml\", \"learning_tests\", \"learning_tests_pendulum\", \"learning_tests_continuous\"],\n <mask> #    size = \"large\",\n <mask> #    srcs = [\"tests/run_regression_tests.py\"],\n <mask> #    data = glob([\"tuned_examples/ddpg/pendulum-ddpg.yaml\"]),\n <mask> #    args = [\"--yaml-dir=tuned_examples/ddpg\"]\n <mask> # )\n <mask> \n <mask> # py_test(\n <mask> #    name = \"learning_tests_pendulum_ddpg_fake_gpus\",\n <mask> #    main = \"tests/run_regression_tests.py\",\n <mask> #    tags = [\"team:ml\", \"learning_tests\", \"learning_tests_pendulum\", \"learning_tests_continuous\", \"fake_gpus\"],\n <mask> #    size = \"large\",\n <mask> #    srcs = [\"tests/run_regression_tests.py\"],\n <mask> #    data = [\"tuned_examples/ddpg/pendulum-ddpg-fake-gpus.yaml\"],\n <mask> #    args = [\"--yaml-dir=tuned_examples/ddpg\"]\n <mask> # )\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove                     buf = MultiAgentReplayBuffer.get_instance_for_testing()\n                    buf._fake_batch = in_\n </s> add                     buf = trainer.local_replay_buffer\n                    patch_buffer_with_fake_sampling_method(buf, in_) </s> remove         cur_ts = self._counters[NUM_ENV_STEPS_SAMPLED]\n </s> add         cur_ts = self._counters[NUM_ENV_STEPS_TRAINED] </s> remove                     buf = MultiAgentReplayBuffer.get_instance_for_testing()\n                    buf._fake_batch = in_\n </s> add                     buf = trainer.local_replay_buffer\n                    patch_buffer_with_fake_sampling_method(buf, in_) </s> remove         config[\"prioritized_replay\"] = False\n </s> add         config[\"replay_buffer_config\"] = {\n            \"_enable_replay_buffer_api\": True,\n            \"type\": \"MultiAgentReplayBuffer\",\n            \"capacity\": 50000,\n        } </s> remove         # Sample one training MultiAgentBatch from replay buffer.\n        train_batch = self.local_replay_buffer.sample(batch_size)\n </s> add         # Use deprecated replay() to support old replay buffers for now\n        train_batch = self.local_replay_buffer.replay(batch_size)", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/BUILD"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>         \"min_sample_timesteps_per_reporting\": 25000,\n <mask>         \"worker_side_prioritization\": True,\n <mask>         \"min_time_s_per_reporting\": 30,\n <mask>     },\n <mask>     _allow_unknown_configs=True,\n <mask> )\n <mask> \n <mask> \n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove         \"buffer_size\": 1000000,\n        \"prioritized_replay\": False,\n </s> add  </s> add         # Size of the replay buffer (in time steps).\n        \"buffer_size\": DEPRECATED_VALUE,\n        \"replay_buffer_config\": {\n            \"_enable_replay_buffer_api\": True,\n            \"type\": \"MultiAgentReplayBuffer\",\n            \"capacity\": 1000000,\n        },\n        \"_disable_execution_plan_api\": True, </s> remove     # If True, the execution plan API will not be used. Instead,\n    # a Trainer's `training_iteration` method will be called as-is each\n    # training iteration.\n    \"_disable_execution_plan_api\": False,\n </s> add     \"_disable_execution_plan_api\": True, </s> remove         \"type\": \"MultiAgentReplayBuffer\",\n </s> add         \"_enable_replay_buffer_api\": True,\n        \"type\": \"MultiAgentPrioritizedReplayBuffer\", </s> add         # Alpha parameter for prioritized replay buffer.\n        \"prioritized_replay_alpha\": 0.6,\n        # Beta parameter for sampling from prioritized replay buffer.\n        \"prioritized_replay_beta\": 0.4,\n        # Epsilon to add to the TD errors when updating priorities.\n        \"prioritized_replay_eps\": 1e-6, </s> add     NUM_ENV_STEPS_TRAINED,", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/ddpg/apex.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     # Size of the replay buffer. Note that if async_updates is set, then\n <mask>     # each worker will have a replay buffer of this size.\n <mask>     \"buffer_size\": DEPRECATED_VALUE,\n <mask>     \"replay_buffer_config\": {\n <mask>         \"type\": \"MultiAgentReplayBuffer\",\n <mask>         \"capacity\": 50000,\n <mask>     },\n <mask>     # Set this to True, if you want the contents of your buffer(s) to be\n <mask>     # stored in any saved checkpoints as well.\n <mask>     # Warnings will be created if:\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> add         # Alpha parameter for prioritized replay buffer.\n        \"prioritized_replay_alpha\": 0.6,\n        # Beta parameter for sampling from prioritized replay buffer.\n        \"prioritized_replay_beta\": 0.4,\n        # Epsilon to add to the TD errors when updating priorities.\n        \"prioritized_replay_eps\": 1e-6, </s> add         # Size of the replay buffer (in time steps).\n        \"buffer_size\": DEPRECATED_VALUE,\n        \"replay_buffer_config\": {\n            \"_enable_replay_buffer_api\": True,\n            \"type\": \"MultiAgentReplayBuffer\",\n            \"capacity\": 1000000,\n        },\n        \"_disable_execution_plan_api\": True, </s> remove     # If True, the execution plan API will not be used. Instead,\n    # a Trainer's `training_iteration` method will be called as-is each\n    # training iteration.\n    \"_disable_execution_plan_api\": False,\n </s> add     \"_disable_execution_plan_api\": True, </s> remove     # If True prioritized replay buffer will be used.\n    \"prioritized_replay\": True,\n    # Alpha parameter for prioritized replay buffer.\n    \"prioritized_replay_alpha\": 0.6,\n    # Beta parameter for sampling from prioritized replay buffer.\n    \"prioritized_replay_beta\": 0.4,\n    # Epsilon to add to the TD errors when updating priorities.\n    \"prioritized_replay_eps\": 1e-6,\n </s> add  </s> add         # Experimental flag.\n        # If True, the execution plan API will not be used. Instead,\n        # a Trainer's `training_iteration` method will be called as-is each\n        # training iteration.\n        \"_disable_execution_plan_api\": False, </s> remove                     buf = MultiAgentReplayBuffer.get_instance_for_testing()\n                    buf._fake_batch = in_\n </s> add                     buf = trainer.local_replay_buffer\n                    patch_buffer_with_fake_sampling_method(buf, in_)", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/ddpg/ddpg.py"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask>         \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n <mask>         \"capacity\": 50000,\n <mask>     },\n <mask>     # Set this to True, if you want the contents of your buffer(s) to be\n <mask>     # stored in any saved checkpoints as well.\n <mask>     # Warnings will be created if:\n <mask>     # - This is True AND restoring from a checkpoint that contains no buffer\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove         \"type\": \"MultiAgentReplayBuffer\",\n </s> add         \"_enable_replay_buffer_api\": True,\n        \"type\": \"MultiAgentPrioritizedReplayBuffer\", </s> remove     # If True prioritized replay buffer will be used.\n    \"prioritized_replay\": True,\n    # Alpha parameter for prioritized replay buffer.\n    \"prioritized_replay_alpha\": 0.6,\n    # Beta parameter for sampling from prioritized replay buffer.\n    \"prioritized_replay_beta\": 0.4,\n    # Epsilon to add to the TD errors when updating priorities.\n    \"prioritized_replay_eps\": 1e-6,\n </s> add  </s> remove     # If True, the execution plan API will not be used. Instead,\n    # a Trainer's `training_iteration` method will be called as-is each\n    # training iteration.\n    \"_disable_execution_plan_api\": False,\n </s> add     \"_disable_execution_plan_api\": True, </s> add         # Experimental flag.\n        # If True, the execution plan API will not be used. Instead,\n        # a Trainer's `training_iteration` method will be called as-is each\n        # training iteration.\n        \"_disable_execution_plan_api\": False, </s> add         # Size of the replay buffer (in time steps).\n        \"buffer_size\": DEPRECATED_VALUE,\n        \"replay_buffer_config\": {\n            \"_enable_replay_buffer_api\": True,\n            \"type\": \"MultiAgentReplayBuffer\",\n            \"capacity\": 1000000,\n        },\n        \"_disable_execution_plan_api\": True, </s> remove         config[\"prioritized_replay\"] = False\n </s> add         config[\"replay_buffer_config\"] = {\n            \"_enable_replay_buffer_api\": True,\n            \"type\": \"MultiAgentReplayBuffer\",\n            \"capacity\": 50000,\n        }", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/ddpg/ddpg.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     #   data.\n <mask>     # - This is False AND restoring from a checkpoint that does contain\n <mask>     #   buffer data.\n <mask>     \"store_buffer_in_checkpoints\": False,\n <mask>     # If True prioritized replay buffer will be used.\n <mask>     \"prioritized_replay\": True,\n <mask>     # Alpha parameter for prioritized replay buffer.\n <mask>     \"prioritized_replay_alpha\": 0.6,\n <mask>     # Beta parameter for sampling from prioritized replay buffer.\n <mask>     \"prioritized_replay_beta\": 0.4,\n <mask>     # Epsilon to add to the TD errors when updating priorities.\n <mask>     \"prioritized_replay_eps\": 1e-6,\n <mask>     # Whether to LZ4 compress observations\n <mask>     \"compress_observations\": False,\n <mask> \n <mask>     # The intensity with which to update the model (vs collecting samples from\n <mask>     # the env). If None, uses the \"natural\" value of:\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> add         # Alpha parameter for prioritized replay buffer.\n        \"prioritized_replay_alpha\": 0.6,\n        # Beta parameter for sampling from prioritized replay buffer.\n        \"prioritized_replay_beta\": 0.4,\n        # Epsilon to add to the TD errors when updating priorities.\n        \"prioritized_replay_eps\": 1e-6, </s> remove     # If True, the execution plan API will not be used. Instead,\n    # a Trainer's `training_iteration` method will be called as-is each\n    # training iteration.\n    \"_disable_execution_plan_api\": False,\n </s> add     \"_disable_execution_plan_api\": True, </s> remove             self.local_replay_buffer.add(batch)\n </s> add             # Use deprecated add_batch() to support old replay buffers for now\n            self.local_replay_buffer.add_batch(batch) </s> remove         \"type\": \"MultiAgentReplayBuffer\",\n </s> add         \"_enable_replay_buffer_api\": True,\n        \"type\": \"MultiAgentPrioritizedReplayBuffer\", </s> remove         # Sample one training MultiAgentBatch from replay buffer.\n        train_batch = self.local_replay_buffer.sample(batch_size)\n </s> add         # Use deprecated replay() to support old replay buffers for now\n        train_batch = self.local_replay_buffer.replay(batch_size) </s> add         # Experimental flag.\n        # If True, the execution plan API will not be used. Instead,\n        # a Trainer's `training_iteration` method will be called as-is each\n        # training iteration.\n        \"_disable_execution_plan_api\": False,", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/ddpg/ddpg.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     # until the minimum timesteps have been executed. Set to 0 for no minimum timesteps.\n <mask>     \"min_sample_timesteps_per_reporting\": 1000,\n <mask> \n <mask>     # Experimental flag.\n <mask>     # If True, the execution plan API will not be used. Instead,\n <mask>     # a Trainer's `training_iteration` method will be called as-is each\n <mask>     # training iteration.\n <mask>     \"_disable_execution_plan_api\": False,\n <mask> })\n <mask> # __sphinx_doc_end__\n <mask> # fmt: on\n <mask> \n <mask> \n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> add         # Experimental flag.\n        # If True, the execution plan API will not be used. Instead,\n        # a Trainer's `training_iteration` method will be called as-is each\n        # training iteration.\n        \"_disable_execution_plan_api\": False, </s> remove         \"type\": \"MultiAgentReplayBuffer\",\n </s> add         \"_enable_replay_buffer_api\": True,\n        \"type\": \"MultiAgentPrioritizedReplayBuffer\", </s> remove     # If True prioritized replay buffer will be used.\n    \"prioritized_replay\": True,\n    # Alpha parameter for prioritized replay buffer.\n    \"prioritized_replay_alpha\": 0.6,\n    # Beta parameter for sampling from prioritized replay buffer.\n    \"prioritized_replay_beta\": 0.4,\n    # Epsilon to add to the TD errors when updating priorities.\n    \"prioritized_replay_eps\": 1e-6,\n </s> add  </s> add         # Alpha parameter for prioritized replay buffer.\n        \"prioritized_replay_alpha\": 0.6,\n        # Beta parameter for sampling from prioritized replay buffer.\n        \"prioritized_replay_beta\": 0.4,\n        # Epsilon to add to the TD errors when updating priorities.\n        \"prioritized_replay_eps\": 1e-6, </s> remove             self.local_replay_buffer.add(batch)\n </s> add             # Use deprecated add_batch() to support old replay buffers for now\n            self.local_replay_buffer.add_batch(batch) </s> remove         # Sample one training MultiAgentBatch from replay buffer.\n        train_batch = self.local_replay_buffer.sample(batch_size)\n </s> add         # Use deprecated replay() to support old replay buffers for now\n        train_batch = self.local_replay_buffer.replay(batch_size)", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/ddpg/ddpg.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace", "code_tokens": " <mask>                     \"'complete_episodes'. Setting \"\n <mask>                     \"batch_mode=complete_episodes.\"\n <mask>                 )\n <mask>                 config[\"batch_mode\"] = \"complete_episodes\"\n <mask> \n <mask>         if config.get(\"prioritized_replay\"):\n <mask>             if config[\"multiagent\"][\"replay_mode\"] == \"lockstep\":\n <mask>                 raise ValueError(\n <mask>                     \"Prioritized replay is not supported when replay_mode=lockstep.\"\n <mask>                 )\n <mask>         else:\n <mask>             if config.get(\"worker_side_prioritization\"):\n <mask>                 raise ValueError(\n <mask>                     \"Worker side prioritization is not supported when \"\n <mask>                     \"prioritized_replay=False.\"\n <mask>                 )\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> add             global_vars: An optional global vars dict to set this\n                worker to. If None, do not update the global_vars. </s> add         # Alpha parameter for prioritized replay buffer.\n        \"prioritized_replay_alpha\": 0.6,\n        # Beta parameter for sampling from prioritized replay buffer.\n        \"prioritized_replay_beta\": 0.4,\n        # Epsilon to add to the TD errors when updating priorities.\n        \"prioritized_replay_eps\": 1e-6, </s> remove         # Sample one training MultiAgentBatch from replay buffer.\n        train_batch = self.local_replay_buffer.sample(batch_size)\n </s> add         # Use deprecated replay() to support old replay buffers for now\n        train_batch = self.local_replay_buffer.replay(batch_size) </s> remove         \"type\": \"MultiAgentReplayBuffer\",\n </s> add         \"_enable_replay_buffer_api\": True,\n        \"type\": \"MultiAgentPrioritizedReplayBuffer\", </s> remove     # If True prioritized replay buffer will be used.\n    \"prioritized_replay\": True,\n    # Alpha parameter for prioritized replay buffer.\n    \"prioritized_replay_alpha\": 0.6,\n    # Beta parameter for sampling from prioritized replay buffer.\n    \"prioritized_replay_beta\": 0.4,\n    # Epsilon to add to the TD errors when updating priorities.\n    \"prioritized_replay_eps\": 1e-6,\n </s> add  </s> add         # Experimental flag.\n        # If True, the execution plan API will not be used. Instead,\n        # a Trainer's `training_iteration` method will be called as-is each\n        # training iteration.\n        \"_disable_execution_plan_api\": False,", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/ddpg/ddpg.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask> from ray.rllib.utils.annotations import override\n <mask> from ray.rllib.utils.typing import TrainerConfigDict\n <mask> \n <mask> TD3_DEFAULT_CONFIG = DDPGTrainer.merge_trainer_configs(\n <mask>     DDPG_CONFIG,\n <mask>     {\n <mask>         # largest changes: twin Q functions, delayed policy updates, and target\n <mask>         # smoothing\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> add from ray.rllib.policy.sample_batch import MultiAgentBatch </s> add     MultiAgentReplayBuffer, </s> remove                     buf = MultiAgentReplayBuffer.get_instance_for_testing()\n                    buf._fake_batch = in_\n </s> add                     buf = trainer.local_replay_buffer\n                    patch_buffer_with_fake_sampling_method(buf, in_) </s> remove from ray.rllib.execution.buffers.multi_agent_replay_buffer import MultiAgentReplayBuffer\n </s> add  </s> remove from typing import Optional\n </s> add from typing import Optional, Any </s> add from ray.rllib.utils.replay_buffers.utils import patch_buffer_with_fake_sampling_method", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/ddpg/td3.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         \"target_network_update_freq\": 0,\n <mask>         \"num_workers\": 0,\n <mask>         \"num_gpus_per_worker\": 0,\n <mask>         \"worker_side_prioritization\": False,\n <mask>         \"buffer_size\": 1000000,\n <mask>         \"prioritized_replay\": False,\n <mask>         \"clip_rewards\": False,\n <mask>         \"use_state_preprocessor\": False,\n <mask>     },\n <mask> )\n <mask> \n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> add         # Size of the replay buffer (in time steps).\n        \"buffer_size\": DEPRECATED_VALUE,\n        \"replay_buffer_config\": {\n            \"_enable_replay_buffer_api\": True,\n            \"type\": \"MultiAgentReplayBuffer\",\n            \"capacity\": 1000000,\n        },\n        \"_disable_execution_plan_api\": True, </s> add         # Experimental flag.\n        # If True, the execution plan API will not be used. Instead,\n        # a Trainer's `training_iteration` method will be called as-is each\n        # training iteration.\n        \"_disable_execution_plan_api\": False, </s> remove     # If True prioritized replay buffer will be used.\n    \"prioritized_replay\": True,\n    # Alpha parameter for prioritized replay buffer.\n    \"prioritized_replay_alpha\": 0.6,\n    # Beta parameter for sampling from prioritized replay buffer.\n    \"prioritized_replay_beta\": 0.4,\n    # Epsilon to add to the TD errors when updating priorities.\n    \"prioritized_replay_eps\": 1e-6,\n </s> add  </s> remove     # If True, the execution plan API will not be used. Instead,\n    # a Trainer's `training_iteration` method will be called as-is each\n    # training iteration.\n    \"_disable_execution_plan_api\": False,\n </s> add     \"_disable_execution_plan_api\": True, </s> remove         \"type\": \"MultiAgentReplayBuffer\",\n </s> add         \"_enable_replay_buffer_api\": True,\n        \"type\": \"MultiAgentPrioritizedReplayBuffer\", </s> add     NUM_ENV_STEPS_TRAINED,", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/ddpg/td3.py"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask>         \"clip_rewards\": False,\n <mask>         \"use_state_preprocessor\": False,\n <mask>     },\n <mask> )\n <mask> \n <mask> \n <mask> class TD3Trainer(DDPGTrainer):\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove         \"buffer_size\": 1000000,\n        \"prioritized_replay\": False,\n </s> add  </s> add         # Experimental flag.\n        # If True, the execution plan API will not be used. Instead,\n        # a Trainer's `training_iteration` method will be called as-is each\n        # training iteration.\n        \"_disable_execution_plan_api\": False, </s> remove     # If True prioritized replay buffer will be used.\n    \"prioritized_replay\": True,\n    # Alpha parameter for prioritized replay buffer.\n    \"prioritized_replay_alpha\": 0.6,\n    # Beta parameter for sampling from prioritized replay buffer.\n    \"prioritized_replay_beta\": 0.4,\n    # Epsilon to add to the TD errors when updating priorities.\n    \"prioritized_replay_eps\": 1e-6,\n </s> add  </s> remove     # If True, the execution plan API will not be used. Instead,\n    # a Trainer's `training_iteration` method will be called as-is each\n    # training iteration.\n    \"_disable_execution_plan_api\": False,\n </s> add     \"_disable_execution_plan_api\": True, </s> add     NUM_ENV_STEPS_TRAINED, </s> remove         \"type\": \"MultiAgentReplayBuffer\",\n </s> add         \"_enable_replay_buffer_api\": True,\n        \"type\": \"MultiAgentPrioritizedReplayBuffer\",", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/ddpg/td3.py"}
{"docstring_tokens": "keep keep keep replace keep replace keep keep", "code_tokens": " <mask>         \"\"\"Test whether an APEX-DDPGTrainer can be built on all frameworks.\"\"\"\n <mask>         config = apex_ddpg.APEX_DDPG_DEFAULT_CONFIG.copy()\n <mask>         config[\"num_workers\"] = 2\n <mask>         config[\"prioritized_replay\"] = True\n <mask>         config[\"min_sample_timesteps_per_reporting\"] = 100\n <mask>         config[\"min_time_s_per_reporting\"] = 1\n <mask>         config[\"learning_starts\"] = 0\n <mask>         config[\"optimizer\"][\"num_replay_buffer_shards\"] = 1\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove         config[\"prioritized_replay\"] = False\n </s> add         config[\"replay_buffer_config\"] = {\n            \"_enable_replay_buffer_api\": True,\n            \"type\": \"MultiAgentReplayBuffer\",\n            \"capacity\": 50000,\n        } </s> remove         # Sample one training MultiAgentBatch from replay buffer.\n        train_batch = self.local_replay_buffer.sample(batch_size)\n </s> add         # Use deprecated replay() to support old replay buffers for now\n        train_batch = self.local_replay_buffer.replay(batch_size) </s> remove         evaluation_interval: 1\n </s> add         evaluation_interval: 10 </s> remove # py_test(\n#    name = \"learning_tests_pendulum_ddpg_fake_gpus\",\n#    main = \"tests/run_regression_tests.py\",\n#    tags = [\"team:ml\", \"learning_tests\", \"learning_tests_pendulum\", \"learning_tests_continuous\", \"fake_gpus\"],\n#    size = \"large\",\n#    srcs = [\"tests/run_regression_tests.py\"],\n#    data = [\"tuned_examples/ddpg/pendulum-ddpg-fake-gpus.yaml\"],\n#    args = [\"--yaml-dir=tuned_examples/ddpg\"]\n# )\n </s> add py_test(\n   name = \"learning_tests_pendulum_ddpg_fake_gpus\",\n   main = \"tests/run_regression_tests.py\",\n   tags = [\"team:ml\", \"learning_tests\", \"learning_tests_pendulum\", \"learning_tests_continuous\", \"fake_gpus\"],\n   size = \"large\",\n   srcs = [\"tests/run_regression_tests.py\"],\n   data = [\"tuned_examples/ddpg/pendulum-ddpg-fake-gpus.yaml\"],\n   args = [\"--yaml-dir=tuned_examples/ddpg\"]\n) </s> remove # py_test(\n#    name = \"learning_tests_pendulum_ddpg\",\n#    main = \"tests/run_regression_tests.py\",\n#    tags = [\"team:ml\", \"learning_tests\", \"learning_tests_pendulum\", \"learning_tests_continuous\"],\n#    size = \"large\",\n#    srcs = [\"tests/run_regression_tests.py\"],\n#    data = glob([\"tuned_examples/ddpg/pendulum-ddpg.yaml\"]),\n#    args = [\"--yaml-dir=tuned_examples/ddpg\"]\n# )\n </s> add py_test(\n   name = \"learning_tests_pendulum_ddpg\",\n   main = \"tests/run_regression_tests.py\",\n   tags = [\"team:ml\", \"learning_tests\", \"learning_tests_pendulum\", \"learning_tests_continuous\"],\n   size = \"large\",\n   srcs = [\"tests/run_regression_tests.py\"],\n   data = glob([\"tuned_examples/ddpg/pendulum-ddpg.yaml\"]),\n   args = [\"--yaml-dir=tuned_examples/ddpg\"]\n)", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/ddpg/tests/test_apex_ddpg.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> import ray\n <mask> import ray.rllib.agents.ddpg as ddpg\n <mask> from ray.rllib.agents.ddpg.ddpg_torch_policy import ddpg_actor_critic_loss as loss_torch\n <mask> from ray.rllib.agents.sac.tests.test_sac import SimpleEnv\n <mask> from ray.rllib.execution.buffers.multi_agent_replay_buffer import MultiAgentReplayBuffer\n <mask> from ray.rllib.policy.sample_batch import SampleBatch\n <mask> from ray.rllib.utils.framework import try_import_tf, try_import_torch\n <mask> from ray.rllib.utils.numpy import fc, huber_loss, l2_loss, relu, sigmoid\n <mask> from ray.rllib.utils.test_utils import (\n <mask>     check,\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove from typing import Optional\n </s> add from typing import Optional, Any </s> add     MultiAgentReplayBuffer, </s> add from ray.rllib.policy.sample_batch import MultiAgentBatch </s> add     NUM_ENV_STEPS_TRAINED, </s> add from ray.rllib.utils.deprecation import DEPRECATED_VALUE </s> add from ray.rllib.utils.replay_buffers.utils import patch_buffer_with_fake_sampling_method", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/ddpg/tests/test_ddpg.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> )\n <mask> from ray.rllib.utils.torch_utils import convert_to_torch_tensor\n <mask> \n <mask> tf1, tf, tfv = try_import_tf()\n <mask> torch, _ = try_import_torch()\n <mask> \n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> add from ray.rllib.policy.sample_batch import MultiAgentBatch </s> remove         config[\"min_time_s_per_reporting\"] = 1\n </s> add  </s> add     NUM_ENV_STEPS_TRAINED, </s> remove from typing import Optional\n </s> add from typing import Optional, Any </s> add     MultiAgentReplayBuffer, </s> add from ray.rllib.utils.deprecation import DEPRECATED_VALUE", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/ddpg/tests/test_ddpg.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         config[\"huber_threshold\"] = 1.0\n <mask>         config[\"gamma\"] = 0.99\n <mask>         # Make this small (seems to introduce errors).\n <mask>         config[\"l2_reg\"] = 1e-10\n <mask>         config[\"prioritized_replay\"] = False\n <mask>         # Use very simple nets.\n <mask>         config[\"actor_hiddens\"] = [10]\n <mask>         config[\"critic_hiddens\"] = [10]\n <mask>         # Make sure, timing differences do not affect trainer.train().\n <mask>         config[\"min_time_s_per_reporting\"] = 0\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove         config[\"min_time_s_per_reporting\"] = 1\n </s> add  </s> remove         # Sample one training MultiAgentBatch from replay buffer.\n        train_batch = self.local_replay_buffer.sample(batch_size)\n </s> add         # Use deprecated replay() to support old replay buffers for now\n        train_batch = self.local_replay_buffer.replay(batch_size) </s> remove         config[\"prioritized_replay\"] = True\n </s> add  </s> remove # py_test(\n#    name = \"learning_tests_pendulum_ddpg\",\n#    main = \"tests/run_regression_tests.py\",\n#    tags = [\"team:ml\", \"learning_tests\", \"learning_tests_pendulum\", \"learning_tests_continuous\"],\n#    size = \"large\",\n#    srcs = [\"tests/run_regression_tests.py\"],\n#    data = glob([\"tuned_examples/ddpg/pendulum-ddpg.yaml\"]),\n#    args = [\"--yaml-dir=tuned_examples/ddpg\"]\n# )\n </s> add py_test(\n   name = \"learning_tests_pendulum_ddpg\",\n   main = \"tests/run_regression_tests.py\",\n   tags = [\"team:ml\", \"learning_tests\", \"learning_tests_pendulum\", \"learning_tests_continuous\"],\n   size = \"large\",\n   srcs = [\"tests/run_regression_tests.py\"],\n   data = glob([\"tuned_examples/ddpg/pendulum-ddpg.yaml\"]),\n   args = [\"--yaml-dir=tuned_examples/ddpg\"]\n) </s> remove # py_test(\n#    name = \"learning_tests_pendulum_ddpg_fake_gpus\",\n#    main = \"tests/run_regression_tests.py\",\n#    tags = [\"team:ml\", \"learning_tests\", \"learning_tests_pendulum\", \"learning_tests_continuous\", \"fake_gpus\"],\n#    size = \"large\",\n#    srcs = [\"tests/run_regression_tests.py\"],\n#    data = [\"tuned_examples/ddpg/pendulum-ddpg-fake-gpus.yaml\"],\n#    args = [\"--yaml-dir=tuned_examples/ddpg\"]\n# )\n </s> add py_test(\n   name = \"learning_tests_pendulum_ddpg_fake_gpus\",\n   main = \"tests/run_regression_tests.py\",\n   tags = [\"team:ml\", \"learning_tests\", \"learning_tests_pendulum\", \"learning_tests_continuous\", \"fake_gpus\"],\n   size = \"large\",\n   srcs = [\"tests/run_regression_tests.py\"],\n   data = [\"tuned_examples/ddpg/pendulum-ddpg-fake-gpus.yaml\"],\n   args = [\"--yaml-dir=tuned_examples/ddpg\"]\n) </s> remove                     buf = MultiAgentReplayBuffer.get_instance_for_testing()\n                    buf._fake_batch = in_\n </s> add                     buf = trainer.local_replay_buffer\n                    patch_buffer_with_fake_sampling_method(buf, in_)", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/ddpg/tests/test_ddpg.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>                     in_ = self._get_batch_helper(obs_size, actions, batch_size)\n <mask>                     tf_inputs.append(in_)\n <mask>                     # Set a fake-batch to use\n <mask>                     # (instead of sampling from replay buffer).\n <mask>                     buf = MultiAgentReplayBuffer.get_instance_for_testing()\n <mask>                     buf._fake_batch = in_\n <mask>                     trainer.train()\n <mask>                     updated_weights = policy.get_weights()\n <mask>                     # Net must have changed.\n <mask>                     if tf_updated_weights:\n <mask>                         check(\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove                     buf = MultiAgentReplayBuffer.get_instance_for_testing()\n                    buf._fake_batch = in_\n </s> add                     buf = trainer.local_replay_buffer\n                    patch_buffer_with_fake_sampling_method(buf, in_) </s> remove         \"type\": \"MultiAgentReplayBuffer\",\n </s> add         \"_enable_replay_buffer_api\": True,\n        \"type\": \"MultiAgentPrioritizedReplayBuffer\", </s> add         # Alpha parameter for prioritized replay buffer.\n        \"prioritized_replay_alpha\": 0.6,\n        # Beta parameter for sampling from prioritized replay buffer.\n        \"prioritized_replay_beta\": 0.4,\n        # Epsilon to add to the TD errors when updating priorities.\n        \"prioritized_replay_eps\": 1e-6, </s> remove         # Sample one training MultiAgentBatch from replay buffer.\n        train_batch = self.local_replay_buffer.sample(batch_size)\n </s> add         # Use deprecated replay() to support old replay buffers for now\n        train_batch = self.local_replay_buffer.replay(batch_size) </s> remove             self.local_replay_buffer.add(batch)\n </s> add             # Use deprecated add_batch() to support old replay buffers for now\n            self.local_replay_buffer.add_batch(batch) </s> remove     # If True, the execution plan API will not be used. Instead,\n    # a Trainer's `training_iteration` method will be called as-is each\n    # training iteration.\n    \"_disable_execution_plan_api\": False,\n </s> add     \"_disable_execution_plan_api\": True,", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/ddpg/tests/test_ddpg.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>                     tf_weights = tf_updated_weights[update_iteration]\n <mask>                     in_ = tf_inputs[update_iteration]\n <mask>                     # Set a fake-batch to use\n <mask>                     # (instead of sampling from replay buffer).\n <mask>                     buf = MultiAgentReplayBuffer.get_instance_for_testing()\n <mask>                     buf._fake_batch = in_\n <mask>                     trainer.train()\n <mask>                     # Compare updated model and target weights.\n <mask>                     for tf_key in tf_weights.keys():\n <mask>                         tf_var = tf_weights[tf_key]\n <mask>                         # Model.\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove                     buf = MultiAgentReplayBuffer.get_instance_for_testing()\n                    buf._fake_batch = in_\n </s> add                     buf = trainer.local_replay_buffer\n                    patch_buffer_with_fake_sampling_method(buf, in_) </s> add         # Alpha parameter for prioritized replay buffer.\n        \"prioritized_replay_alpha\": 0.6,\n        # Beta parameter for sampling from prioritized replay buffer.\n        \"prioritized_replay_beta\": 0.4,\n        # Epsilon to add to the TD errors when updating priorities.\n        \"prioritized_replay_eps\": 1e-6, </s> remove         # Sample one training MultiAgentBatch from replay buffer.\n        train_batch = self.local_replay_buffer.sample(batch_size)\n </s> add         # Use deprecated replay() to support old replay buffers for now\n        train_batch = self.local_replay_buffer.replay(batch_size) </s> remove             self.local_replay_buffer.add(batch)\n </s> add             # Use deprecated add_batch() to support old replay buffers for now\n            self.local_replay_buffer.add_batch(batch) </s> remove     # If True prioritized replay buffer will be used.\n    \"prioritized_replay\": True,\n    # Alpha parameter for prioritized replay buffer.\n    \"prioritized_replay_alpha\": 0.6,\n    # Beta parameter for sampling from prioritized replay buffer.\n    \"prioritized_replay_beta\": 0.4,\n    # Epsilon to add to the TD errors when updating priorities.\n    \"prioritized_replay_eps\": 1e-6,\n </s> add  </s> remove         \"type\": \"MultiAgentReplayBuffer\",\n </s> add         \"_enable_replay_buffer_api\": True,\n        \"type\": \"MultiAgentPrioritizedReplayBuffer\",", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/ddpg/tests/test_ddpg.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask> )\n <mask> from ray.rllib.utils.metrics import (\n <mask>     LAST_TARGET_UPDATE_TS,\n <mask>     NUM_TARGET_UPDATES,\n <mask> )\n <mask> from ray.rllib.utils.deprecation import (\n <mask>     DEPRECATED_VALUE,\n <mask> )\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> add     MultiAgentReplayBuffer, </s> remove from typing import Optional\n </s> add from typing import Optional, Any </s> remove from ray.rllib.execution.buffers.multi_agent_replay_buffer import MultiAgentReplayBuffer\n </s> add  </s> add from ray.rllib.policy.sample_batch import MultiAgentBatch </s> add from ray.rllib.utils.replay_buffers.utils import patch_buffer_with_fake_sampling_method </s> add from ray.rllib.utils.deprecation import DEPRECATED_VALUE", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/dqn/simple_q.py"}
{"docstring_tokens": "keep keep keep keep replace keep replace replace keep keep", "code_tokens": " <mask>             # Update sampling step counters.\n <mask>             self._counters[NUM_ENV_STEPS_SAMPLED] += batch.env_steps()\n <mask>             self._counters[NUM_AGENT_STEPS_SAMPLED] += batch.agent_steps()\n <mask>             # Store new samples in the replay buffer\n <mask>             self.local_replay_buffer.add(batch)\n <mask> \n <mask>         # Sample one training MultiAgentBatch from replay buffer.\n <mask>         train_batch = self.local_replay_buffer.sample(batch_size)\n <mask> \n <mask>         # Learn on the training batch.\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove         cur_ts = self._counters[NUM_ENV_STEPS_SAMPLED]\n </s> add         cur_ts = self._counters[NUM_ENV_STEPS_TRAINED] </s> remove     # If True prioritized replay buffer will be used.\n    \"prioritized_replay\": True,\n    # Alpha parameter for prioritized replay buffer.\n    \"prioritized_replay_alpha\": 0.6,\n    # Beta parameter for sampling from prioritized replay buffer.\n    \"prioritized_replay_beta\": 0.4,\n    # Epsilon to add to the TD errors when updating priorities.\n    \"prioritized_replay_eps\": 1e-6,\n </s> add  </s> add         # Alpha parameter for prioritized replay buffer.\n        \"prioritized_replay_alpha\": 0.6,\n        # Beta parameter for sampling from prioritized replay buffer.\n        \"prioritized_replay_beta\": 0.4,\n        # Epsilon to add to the TD errors when updating priorities.\n        \"prioritized_replay_eps\": 1e-6, </s> remove     # If True, the execution plan API will not be used. Instead,\n    # a Trainer's `training_iteration` method will be called as-is each\n    # training iteration.\n    \"_disable_execution_plan_api\": False,\n </s> add     \"_disable_execution_plan_api\": True, </s> remove         \"type\": \"MultiAgentReplayBuffer\",\n </s> add         \"_enable_replay_buffer_api\": True,\n        \"type\": \"MultiAgentPrioritizedReplayBuffer\",", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/dqn/simple_q.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         # self._counters[NUM_ENV_STEPS_TRAINED] += train_batch.env_steps()\n <mask>         # self._counters[NUM_AGENT_STEPS_TRAINED] += train_batch.agent_steps()\n <mask> \n <mask>         # Update target network every `target_network_update_freq` steps.\n <mask>         cur_ts = self._counters[NUM_ENV_STEPS_SAMPLED]\n <mask>         last_update = self._counters[LAST_TARGET_UPDATE_TS]\n <mask>         if cur_ts - last_update >= self.config[\"target_network_update_freq\"]:\n <mask>             with self._timers[TARGET_NET_UPDATE_TIMER]:\n <mask>                 to_update = local_worker.get_policies_to_train()\n <mask>                 local_worker.foreach_policy_to_train(\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove             self.local_replay_buffer.add(batch)\n </s> add             # Use deprecated add_batch() to support old replay buffers for now\n            self.local_replay_buffer.add_batch(batch) </s> remove         # Sample one training MultiAgentBatch from replay buffer.\n        train_batch = self.local_replay_buffer.sample(batch_size)\n </s> add         # Use deprecated replay() to support old replay buffers for now\n        train_batch = self.local_replay_buffer.replay(batch_size) </s> remove                     buf = MultiAgentReplayBuffer.get_instance_for_testing()\n                    buf._fake_batch = in_\n </s> add                     buf = trainer.local_replay_buffer\n                    patch_buffer_with_fake_sampling_method(buf, in_) </s> remove                     buf = MultiAgentReplayBuffer.get_instance_for_testing()\n                    buf._fake_batch = in_\n </s> add                     buf = trainer.local_replay_buffer\n                    patch_buffer_with_fake_sampling_method(buf, in_) </s> add from ray.rllib.utils.deprecation import DEPRECATED_VALUE </s> remove # py_test(\n#    name = \"learning_tests_pendulum_ddpg\",\n#    main = \"tests/run_regression_tests.py\",\n#    tags = [\"team:ml\", \"learning_tests\", \"learning_tests_pendulum\", \"learning_tests_continuous\"],\n#    size = \"large\",\n#    srcs = [\"tests/run_regression_tests.py\"],\n#    data = glob([\"tuned_examples/ddpg/pendulum-ddpg.yaml\"]),\n#    args = [\"--yaml-dir=tuned_examples/ddpg\"]\n# )\n </s> add py_test(\n   name = \"learning_tests_pendulum_ddpg\",\n   main = \"tests/run_regression_tests.py\",\n   tags = [\"team:ml\", \"learning_tests\", \"learning_tests_pendulum\", \"learning_tests_continuous\"],\n   size = \"large\",\n   srcs = [\"tests/run_regression_tests.py\"],\n   data = glob([\"tuned_examples/ddpg/pendulum-ddpg.yaml\"]),\n   args = [\"--yaml-dir=tuned_examples/ddpg\"]\n)", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/agents/dqn/simple_q.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>                 If None (default), sync weights to/from all policies.\n <mask>             from_worker: Optional RolloutWorker instance to sync from.\n <mask>                 If None (default), sync from this WorkerSet's local worker.\n <mask>         \"\"\"\n <mask>         if self.local_worker() is None and from_worker is None:\n <mask>             raise TypeError(\n <mask>                 \"No `local_worker` in WorkerSet, must provide `from_worker` \"\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove \n        if config.get(\"prioritized_replay\"):\n            if config[\"multiagent\"][\"replay_mode\"] == \"lockstep\":\n                raise ValueError(\n                    \"Prioritized replay is not supported when replay_mode=lockstep.\"\n                )\n        else:\n            if config.get(\"worker_side_prioritization\"):\n                raise ValueError(\n                    \"Worker side prioritization is not supported when \"\n                    \"prioritized_replay=False.\"\n                )\n </s> add  </s> remove         \"type\": \"MultiAgentReplayBuffer\",\n </s> add         \"_enable_replay_buffer_api\": True,\n        \"type\": \"MultiAgentPrioritizedReplayBuffer\", </s> remove     # If True prioritized replay buffer will be used.\n    \"prioritized_replay\": True,\n    # Alpha parameter for prioritized replay buffer.\n    \"prioritized_replay_alpha\": 0.6,\n    # Beta parameter for sampling from prioritized replay buffer.\n    \"prioritized_replay_beta\": 0.4,\n    # Epsilon to add to the TD errors when updating priorities.\n    \"prioritized_replay_eps\": 1e-6,\n </s> add  </s> remove         # Sample one training MultiAgentBatch from replay buffer.\n        train_batch = self.local_replay_buffer.sample(batch_size)\n </s> add         # Use deprecated replay() to support old replay buffers for now\n        train_batch = self.local_replay_buffer.replay(batch_size) </s> add         # Alpha parameter for prioritized replay buffer.\n        \"prioritized_replay_alpha\": 0.6,\n        # Beta parameter for sampling from prioritized replay buffer.\n        \"prioritized_replay_beta\": 0.4,\n        # Epsilon to add to the TD errors when updating priorities.\n        \"prioritized_replay_eps\": 1e-6, </s> remove                     buf = MultiAgentReplayBuffer.get_instance_for_testing()\n                    buf._fake_batch = in_\n </s> add                     buf = trainer.local_replay_buffer\n                    patch_buffer_with_fake_sampling_method(buf, in_)", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/evaluation/worker_set.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         target_network_update_freq: 0\n <mask>         tau: 0.001\n <mask> \n <mask>         # === Replay buffer ===\n <mask>         buffer_size: 10000\n <mask>         prioritized_replay: True\n <mask>         prioritized_replay_alpha: 0.6\n <mask>         prioritized_replay_beta: 0.4\n <mask>         prioritized_replay_eps: 0.000001\n <mask>         clip_rewards: False\n <mask> \n <mask>         # === Optimization ===\n <mask>         actor_lr: 0.001\n <mask>         critic_lr: 0.001\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000 </s> remove         buffer_size: 50000\n        prioritized_replay: False\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 50000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 15000\n        prioritized_replay: true\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 15000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         evaluation_interval: 5\n </s> add         evaluation_interval: 10 </s> remove         # === Evaluation ===\n        evaluation_interval: 1\n        evaluation_num_episodes: 5\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/tuned_examples/ddpg/halfcheetah-ddpg.yaml"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>             ou_sigma: 0.2\n <mask>         min_sample_timesteps_per_reporting: 1000\n <mask>         target_network_update_freq: 0\n <mask>         tau: 0.001\n <mask>         buffer_size: 15000\n <mask>         prioritized_replay: true\n <mask>         prioritized_replay_alpha: 0.6\n <mask>         prioritized_replay_beta: 0.4\n <mask>         prioritized_replay_eps: 0.000001\n <mask>         clip_rewards: false\n <mask>         actor_lr: 0.001\n <mask>         critic_lr: 0.001\n <mask>         use_huber: true\n <mask>         huber_threshold: 1.0\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 50000\n        prioritized_replay: False\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 50000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000 </s> remove         buffer_size: 10000\n </s> add         replay_buffer_config:\n          capacity: 10000 </s> remove         evaluation_interval: 1\n </s> add         evaluation_interval: 10", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/tuned_examples/ddpg/halfcheetah-pybullet-ddpg.yaml"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>             ou_sigma: 0.2\n <mask>         min_sample_timesteps_per_reporting: 1000\n <mask>         target_network_update_freq: 0\n <mask>         tau: 0.001\n <mask>         buffer_size: 10000\n <mask>         prioritized_replay: True\n <mask>         prioritized_replay_alpha: 0.6\n <mask>         prioritized_replay_beta: 0.4\n <mask>         prioritized_replay_eps: 0.000001\n <mask>         clip_rewards: False\n <mask>         actor_lr: 0.001\n <mask>         critic_lr: 0.001\n <mask>         use_huber: False\n <mask>         huber_threshold: 1.0\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove         buffer_size: 15000\n        prioritized_replay: true\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 15000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000 </s> remove         buffer_size: 50000\n        prioritized_replay: False\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 50000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 10000\n </s> add         replay_buffer_config:\n          capacity: 10000 </s> remove         config[\"prioritized_replay\"] = False\n </s> add         config[\"replay_buffer_config\"] = {\n            \"_enable_replay_buffer_api\": True,\n            \"type\": \"MultiAgentReplayBuffer\",\n            \"capacity\": 50000,\n        }", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/tuned_examples/ddpg/hopper-pybullet-ddpg.yaml"}
{"docstring_tokens": "keep keep keep keep replace replace", "code_tokens": " <mask>         exploration_config:\n <mask>             random_timesteps: 1000\n <mask> \n <mask>         # === Evaluation ===\n <mask>         evaluation_interval: 1\n <mask>         evaluation_num_episodes: 5\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove         # === Evaluation ===\n        evaluation_interval: 1\n        evaluation_num_episodes: 5\n </s> add  </s> remove         evaluation_interval: 5\n </s> add         evaluation_interval: 10 </s> remove         evaluation_interval: 1\n </s> add         evaluation_interval: 10 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 50000\n        prioritized_replay: False\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 50000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/tuned_examples/ddpg/invertedpendulum-td3.yaml"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         target_network_update_freq: 0\n <mask>         tau: 0.01\n <mask> \n <mask>         # === Replay buffer ===\n <mask>         buffer_size: 50000\n <mask>         prioritized_replay: False\n <mask>         prioritized_replay_alpha: 0.6\n <mask>         prioritized_replay_beta: 0.4\n <mask>         prioritized_replay_eps: 0.000001\n <mask>         clip_rewards: False\n <mask> \n <mask>         # === Optimization ===\n <mask>         actor_lr: 0.001\n <mask>         critic_lr: 0.001\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 15000\n        prioritized_replay: true\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 15000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         # === Evaluation ===\n        evaluation_interval: 1\n        evaluation_num_episodes: 5\n </s> add  </s> remove         evaluation_interval: 5\n </s> add         evaluation_interval: 10", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/tuned_examples/ddpg/mountaincarcontinuous-ddpg.yaml"}
{"docstring_tokens": "keep keep keep keep replace keep", "code_tokens": " <mask>         exploration_config:\n <mask>             random_timesteps: 10000\n <mask> \n <mask>         # === Evaluation ===\n <mask>         evaluation_interval: 5\n <mask>         evaluation_num_episodes: 10\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove         evaluation_interval: 1\n        evaluation_num_episodes: 5\n </s> add         evaluation_interval: 10\n        evaluation_duration: 5 </s> remove         # === Evaluation ===\n        evaluation_interval: 1\n        evaluation_num_episodes: 5\n </s> add  </s> remove         evaluation_interval: 1\n </s> add         evaluation_interval: 10 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 50000\n        prioritized_replay: False\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 50000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/tuned_examples/ddpg/mujoco-td3.yaml"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         gamma: 0.99\n <mask>         exploration_config:\n <mask>             final_scale: 0.02\n <mask>         min_sample_timesteps_per_reporting: 600\n <mask>         buffer_size: 10000\n <mask>         clip_rewards: false\n <mask>         use_huber: true\n <mask>         learning_starts: 500\n <mask>         train_batch_size: 64\n <mask>         num_workers: 0\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove         buffer_size: 15000\n        prioritized_replay: true\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 15000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         evaluation_interval: 1\n </s> add         evaluation_interval: 10 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         # === Evaluation ===\n        evaluation_interval: 1\n        evaluation_num_episodes: 5\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/tuned_examples/ddpg/pendulum-ddpg-fake-gpus.yaml"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         target_network_update_freq: 0\n <mask>         tau: 0.001\n <mask> \n <mask>         # === Replay buffer ===\n <mask>         buffer_size: 10000\n <mask>         prioritized_replay: True\n <mask>         prioritized_replay_alpha: 0.6\n <mask>         prioritized_replay_beta: 0.4\n <mask>         prioritized_replay_eps: 0.000001\n <mask>         clip_rewards: False\n <mask> \n <mask>         # === Optimization ===\n <mask>         actor_lr: 0.001\n <mask>         critic_lr: 0.001\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 50000\n        prioritized_replay: False\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 50000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 15000\n        prioritized_replay: true\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 15000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         evaluation_interval: 5\n </s> add         evaluation_interval: 10 </s> remove         # === Evaluation ===\n        evaluation_interval: 1\n        evaluation_num_episodes: 5\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/tuned_examples/ddpg/pendulum-ddpg.yaml"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         critic_hiddens: [64, 64]\n <mask>         learning_starts: 5000\n <mask>         exploration_config:\n <mask>             random_timesteps: 5000\n <mask>         evaluation_interval: 1\n <mask>         evaluation_num_episodes: 5\n <mask> \n <mask>         # Fake 2 GPUs.\n <mask>         num_gpus: 2\n <mask>         _fake_gpus: true\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove         # === Evaluation ===\n        evaluation_interval: 1\n        evaluation_num_episodes: 5\n </s> add  </s> remove         evaluation_interval: 1\n        evaluation_num_episodes: 5\n </s> add         evaluation_interval: 10\n        evaluation_duration: 5 </s> remove         evaluation_interval: 5\n </s> add         evaluation_interval: 10 </s> remove         buffer_size: 10000\n </s> add         replay_buffer_config:\n          capacity: 10000 </s> remove         config[\"min_time_s_per_reporting\"] = 1\n </s> add  </s> remove         config[\"prioritized_replay\"] = True\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/tuned_examples/ddpg/pendulum-td3-fake-gpus.yaml"}
{"docstring_tokens": "keep keep keep keep replace replace replace", "code_tokens": " <mask>         # === Exploration ===\n <mask>         learning_starts: 5000\n <mask>         exploration_config:\n <mask>             random_timesteps: 5000\n <mask>         # === Evaluation ===\n <mask>         evaluation_interval: 1\n <mask>         evaluation_num_episodes: 5\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove         evaluation_interval: 1\n        evaluation_num_episodes: 5\n </s> add         evaluation_interval: 10\n        evaluation_duration: 5 </s> remove         evaluation_interval: 5\n </s> add         evaluation_interval: 10 </s> remove         evaluation_interval: 1\n </s> add         evaluation_interval: 10 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000 </s> remove         buffer_size: 10000\n        prioritized_replay: True\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 10000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001 </s> remove         buffer_size: 50000\n        prioritized_replay: False\n        prioritized_replay_alpha: 0.6\n        prioritized_replay_beta: 0.4\n        prioritized_replay_eps: 0.000001\n </s> add         replay_buffer_config:\n          capacity: 50000\n          prioritized_replay_alpha: 0.6\n          prioritized_replay_beta: 0.4\n          prioritized_replay_eps: 0.000001", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/tuned_examples/ddpg/pendulum-td3.yaml"}
{"docstring_tokens": "keep keep replace keep keep keep keep keep", "code_tokens": " <mask> import logging\n <mask> import psutil\n <mask> from typing import Optional\n <mask> \n <mask> from ray.rllib.execution import MultiAgentReplayBuffer as Legacy_MultiAgentReplayBuffer\n <mask> from ray.rllib.execution.buffers.multi_agent_replay_buffer import (\n <mask>     MultiAgentReplayBuffer as LegacyMultiAgentReplayBuffer,\n <mask> )\n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> remove from ray.rllib.execution.buffers.multi_agent_replay_buffer import MultiAgentReplayBuffer\n </s> add  </s> add     MultiAgentReplayBuffer, </s> add     NUM_ENV_STEPS_TRAINED, </s> add from ray.rllib.policy.sample_batch import MultiAgentBatch </s> add from ray.rllib.utils.replay_buffers.utils import patch_buffer_with_fake_sampling_method </s> add from ray.rllib.utils.deprecation import DEPRECATED_VALUE", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/utils/replay_buffers/utils.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask> from ray.rllib.utils.metrics.learner_info import LEARNER_STATS_KEY\n <mask> from ray.rllib.utils.replay_buffers import (\n <mask>     MultiAgentPrioritizedReplayBuffer,\n <mask>     ReplayBuffer,\n <mask> )\n <mask> from ray.rllib.policy.sample_batch import MultiAgentBatch\n <mask> from ray.rllib.utils.typing import ResultDict, SampleBatchType, TrainerConfigDict\n <mask> from ray.util import log_once\n <mask> \n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> add from ray.rllib.policy.sample_batch import MultiAgentBatch </s> remove from ray.rllib.execution.buffers.multi_agent_replay_buffer import MultiAgentReplayBuffer\n </s> add  </s> remove from typing import Optional\n </s> add from typing import Optional, Any </s> add     NUM_ENV_STEPS_TRAINED, </s> add from ray.rllib.utils.deprecation import DEPRECATED_VALUE </s> add from ray.rllib.utils.replay_buffers.utils import patch_buffer_with_fake_sampling_method", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/utils/replay_buffers/utils.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>     MultiAgentReplayBuffer,\n <mask> )\n <mask> from ray.rllib.utils.typing import ResultDict, SampleBatchType, TrainerConfigDict\n <mask> from ray.util import log_once\n <mask> \n <mask> logger = logging.getLogger(__name__)\n <mask> \n <mask> \n </s> [RLlib] DDPG Training iteration fn & Replay Buffer API (#24212) </s> add     MultiAgentReplayBuffer, </s> add from ray.rllib.utils.deprecation import DEPRECATED_VALUE </s> remove from typing import Optional\n </s> add from typing import Optional, Any </s> add from ray.rllib.utils.replay_buffers.utils import patch_buffer_with_fake_sampling_method </s> add     NUM_ENV_STEPS_TRAINED, </s> remove from ray.rllib.execution.buffers.multi_agent_replay_buffer import MultiAgentReplayBuffer\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/86bc9ecce222613a3fac5a26ae8e7637099d55f5", "file_name": "rllib/utils/replay_buffers/utils.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>   bool SubscribeTask(const TaskID &task_id);\n <mask>   /// Unsubscribe from notifications for a task. Returns whether the operation\n <mask>   /// was successful (whether we were subscribed).\n <mask>   bool UnsubscribeTask(const TaskID &task_id);\n <mask>   /// Add a task and its uncommitted lineage to the local stash.\n <mask>   void AddUncommittedLineage(const TaskID &task_id, const Lineage &uncommitted_lineage,\n <mask>                              std::unordered_set<TaskID> &subscribe_tasks);\n <mask> \n <mask>   /// The client ID, used to request notifications for specific tasks.\n <mask>   /// TODO(swang): Move the ClientID into the generic Table implementation.\n <mask>   ClientID client_id_;\n <mask>   /// The durable storage system for task information.\n </s> Flush lineage cache on task submission instead of execution (#4942) </s> add /// Helper method to create a Lineage object with a single task.\nLineage CreateSingletonLineage(const Task &task) {\n  Lineage singleton_lineage;\n  singleton_lineage.SetEntry(task, GcsStatus::UNCOMMITTED);\n  return singleton_lineage;\n}\n </s> remove     RAY_CHECK(lineage_cache.AddWaitingTask(task, empty_lineage));\n </s> add     Lineage lineage = CreateSingletonLineage(task);\n    lineage_cache.AddUncommittedLineage(task.GetTaskSpecification().TaskId(), lineage); </s> remove }\n\nTEST_F(LineageCacheTest, TestWritebackReady) {\n  // Insert a chain of dependent tasks.\n  size_t num_tasks_flushed = 0;\n  std::vector<Task> tasks;\n  InsertTaskChain(lineage_cache_, tasks, 3, std::vector<ObjectID>(), 1);\n </s> add  </s> remove   ASSERT_TRUE(lineage_cache_.AddReadyTask(tasks.front()));\n </s> add   ASSERT_TRUE(lineage_cache_.CommitTask(tasks.front())); </s> remove   Lineage empty_lineage;\n </s> add  </s> remove TEST_F(LineageCacheTest, TestWritebackNoneReady) {\n </s> add TEST_F(LineageCacheTest, TestWritebackReady) {", "html_url": "https://github.com/ray-project/ray/commit/873d45b46750f01e9ded84f9ee8d05328535c75d", "file_name": "src/ray/raylet/lineage_cache.h"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask> }\n <mask> \n <mask> std::vector<ObjectID> InsertTaskChain(LineageCache &lineage_cache,\n <mask>                                       std::vector<Task> &inserted_tasks, int chain_size,\n <mask>                                       const std::vector<ObjectID> &initial_arguments,\n <mask>                                       int64_t num_returns) {\n <mask>   std::vector<ObjectID> arguments = initial_arguments;\n <mask>   for (int i = 0; i < chain_size; i++) {\n </s> Flush lineage cache on task submission instead of execution (#4942) </s> remove   Lineage empty_lineage;\n </s> add  </s> remove     RAY_CHECK(lineage_cache.AddWaitingTask(task, empty_lineage));\n </s> add     Lineage lineage = CreateSingletonLineage(task);\n    lineage_cache.AddUncommittedLineage(task.GetTaskSpecification().TaskId(), lineage); </s> remove TEST_F(LineageCacheTest, TestWritebackNoneReady) {\n </s> add TEST_F(LineageCacheTest, TestWritebackReady) { </s> remove }\n\nTEST_F(LineageCacheTest, TestWritebackReady) {\n  // Insert a chain of dependent tasks.\n  size_t num_tasks_flushed = 0;\n  std::vector<Task> tasks;\n  InsertTaskChain(lineage_cache_, tasks, 3, std::vector<ObjectID>(), 1);\n </s> add  </s> remove   ASSERT_TRUE(lineage_cache_.AddReadyTask(tasks.front()));\n </s> add   ASSERT_TRUE(lineage_cache_.CommitTask(tasks.front())); </s> remove   /// Add a task and its uncommitted lineage to the local stash.\n  void AddUncommittedLineage(const TaskID &task_id, const Lineage &uncommitted_lineage,\n                             std::unordered_set<TaskID> &subscribe_tasks);\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/873d45b46750f01e9ded84f9ee8d05328535c75d", "file_name": "src/ray/raylet/lineage_cache_test.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep replace keep keep keep", "code_tokens": " <mask> std::vector<ObjectID> InsertTaskChain(LineageCache &lineage_cache,\n <mask>                                       std::vector<Task> &inserted_tasks, int chain_size,\n <mask>                                       const std::vector<ObjectID> &initial_arguments,\n <mask>                                       int64_t num_returns) {\n <mask>   Lineage empty_lineage;\n <mask>   std::vector<ObjectID> arguments = initial_arguments;\n <mask>   for (int i = 0; i < chain_size; i++) {\n <mask>     auto task = ExampleTask(arguments, num_returns);\n <mask>     RAY_CHECK(lineage_cache.AddWaitingTask(task, empty_lineage));\n <mask>     inserted_tasks.push_back(task);\n <mask>     arguments.clear();\n <mask>     for (int j = 0; j < task.GetTaskSpecification().NumReturns(); j++) {\n </s> Flush lineage cache on task submission instead of execution (#4942) </s> add /// Helper method to create a Lineage object with a single task.\nLineage CreateSingletonLineage(const Task &task) {\n  Lineage singleton_lineage;\n  singleton_lineage.SetEntry(task, GcsStatus::UNCOMMITTED);\n  return singleton_lineage;\n}\n </s> remove TEST_F(LineageCacheTest, TestWritebackNoneReady) {\n </s> add TEST_F(LineageCacheTest, TestWritebackReady) { </s> remove }\n\nTEST_F(LineageCacheTest, TestWritebackReady) {\n  // Insert a chain of dependent tasks.\n  size_t num_tasks_flushed = 0;\n  std::vector<Task> tasks;\n  InsertTaskChain(lineage_cache_, tasks, 3, std::vector<ObjectID>(), 1);\n </s> add  </s> remove   ASSERT_TRUE(lineage_cache_.AddReadyTask(tasks.front()));\n </s> add   ASSERT_TRUE(lineage_cache_.CommitTask(tasks.front())); </s> remove   /// Add a task and its uncommitted lineage to the local stash.\n  void AddUncommittedLineage(const TaskID &task_id, const Lineage &uncommitted_lineage,\n                             std::unordered_set<TaskID> &subscribe_tasks);\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/873d45b46750f01e9ded84f9ee8d05328535c75d", "file_name": "src/ray/raylet/lineage_cache_test.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>       lineage_cache_.GetUncommittedLineageOrDie(forwarded_task_id, node_id);\n <mask>   ASSERT_EQ(1, uncommitted_lineage_forwarded.GetEntries().size());\n <mask> }\n <mask> \n <mask> TEST_F(LineageCacheTest, TestWritebackNoneReady) {\n <mask>   // Insert a chain of dependent tasks.\n <mask>   size_t num_tasks_flushed = 0;\n <mask>   std::vector<Task> tasks;\n <mask>   InsertTaskChain(lineage_cache_, tasks, 3, std::vector<ObjectID>(), 1);\n <mask> \n </s> Flush lineage cache on task submission instead of execution (#4942) </s> remove }\n\nTEST_F(LineageCacheTest, TestWritebackReady) {\n  // Insert a chain of dependent tasks.\n  size_t num_tasks_flushed = 0;\n  std::vector<Task> tasks;\n  InsertTaskChain(lineage_cache_, tasks, 3, std::vector<ObjectID>(), 1);\n </s> add  </s> remove   ASSERT_TRUE(lineage_cache_.AddReadyTask(tasks.front()));\n </s> add   ASSERT_TRUE(lineage_cache_.CommitTask(tasks.front())); </s> add /// Helper method to create a Lineage object with a single task.\nLineage CreateSingletonLineage(const Task &task) {\n  Lineage singleton_lineage;\n  singleton_lineage.SetEntry(task, GcsStatus::UNCOMMITTED);\n  return singleton_lineage;\n}\n </s> remove     RAY_CHECK(lineage_cache.AddWaitingTask(task, empty_lineage));\n </s> add     Lineage lineage = CreateSingletonLineage(task);\n    lineage_cache.AddUncommittedLineage(task.GetTaskSpecification().TaskId(), lineage); </s> remove   Lineage empty_lineage;\n </s> add  </s> remove   /// Add a task and its uncommitted lineage to the local stash.\n  void AddUncommittedLineage(const TaskID &task_id, const Lineage &uncommitted_lineage,\n                             std::unordered_set<TaskID> &subscribe_tasks);\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/873d45b46750f01e9ded84f9ee8d05328535c75d", "file_name": "src/ray/raylet/lineage_cache_test.cc"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace keep keep replace", "code_tokens": " <mask> \n <mask>   // Check that when no tasks have been marked as ready, we do not flush any\n <mask>   // entries.\n <mask>   ASSERT_EQ(mock_gcs_.TaskTable().size(), num_tasks_flushed);\n <mask> }\n <mask> \n <mask> TEST_F(LineageCacheTest, TestWritebackReady) {\n <mask>   // Insert a chain of dependent tasks.\n <mask>   size_t num_tasks_flushed = 0;\n <mask>   std::vector<Task> tasks;\n <mask>   InsertTaskChain(lineage_cache_, tasks, 3, std::vector<ObjectID>(), 1);\n <mask> \n <mask>   // Check that after marking the first task as ready, we flush only that task.\n <mask>   ASSERT_TRUE(lineage_cache_.AddReadyTask(tasks.front()));\n </s> Flush lineage cache on task submission instead of execution (#4942) </s> remove TEST_F(LineageCacheTest, TestWritebackNoneReady) {\n </s> add TEST_F(LineageCacheTest, TestWritebackReady) { </s> add /// Helper method to create a Lineage object with a single task.\nLineage CreateSingletonLineage(const Task &task) {\n  Lineage singleton_lineage;\n  singleton_lineage.SetEntry(task, GcsStatus::UNCOMMITTED);\n  return singleton_lineage;\n}\n </s> remove   /// Add a task and its uncommitted lineage to the local stash.\n  void AddUncommittedLineage(const TaskID &task_id, const Lineage &uncommitted_lineage,\n                             std::unordered_set<TaskID> &subscribe_tasks);\n </s> add  </s> remove     RAY_CHECK(lineage_cache.AddWaitingTask(task, empty_lineage));\n </s> add     Lineage lineage = CreateSingletonLineage(task);\n    lineage_cache.AddUncommittedLineage(task.GetTaskSpecification().TaskId(), lineage); </s> remove   Lineage empty_lineage;\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/873d45b46750f01e9ded84f9ee8d05328535c75d", "file_name": "src/ray/raylet/lineage_cache_test.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep", "code_tokens": " <mask>     delegate_->HandleRegisterAgent(request, reply, send_reply_callback);\n <mask>   }\n <mask> \n <mask>  private:\n <mask>   std::unique_ptr<AgentManager> &delegate_;\n <mask> };\n <mask> \n <mask> }  // namespace raylet\n <mask> }  // namespace ray\n </s> [runtime env] support create or delete runtime envs in agent (#15904) </s> add void WorkerPool::SetAgentManager(std::shared_ptr<AgentManager> agent_manager) {\n  agent_manager_ = agent_manager;\n}\n </s> remove   agent_manager_.reset(\n      new AgentManager(std::move(options),\n                       /*delay_executor=*/\n                       [this](std::function<void()> task, uint32_t delay_ms) {\n                         return execute_after(io_service_, task, delay_ms);\n                       }));\n </s> add   agent_manager_ = std::make_shared<AgentManager>(\n      std::move(options),\n      /*delay_executor=*/\n      [this](std::function<void()> task, uint32_t delay_ms) {\n        return execute_after(io_service_, task, delay_ms);\n      },\n      /*runtime_env_agent_factory=*/\n      [this](const std::string &ip_address, int port) {\n        RAY_CHECK(!ip_address.empty() && port != 0);\n        return std::shared_ptr<rpc::RuntimeEnvAgentClientInterface>(\n            new rpc::RuntimeEnvAgentClient(ip_address, port, client_call_manager_));\n      });\n  worker_pool_.SetAgentManager(agent_manager_); </s> remove   std::unique_ptr<AgentManager> agent_manager_;\n </s> add   std::shared_ptr<AgentManager> agent_manager_;", "html_url": "https://github.com/ray-project/ray/commit/874e947d6f13963710d249c6d09c35062bc91e14", "file_name": "src/ray/raylet/agent_manager.h"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     }\n <mask>   }\n <mask> \n <mask>   auto options = AgentManager::Options({self_node_id, agent_command_line});\n <mask>   agent_manager_.reset(\n <mask>       new AgentManager(std::move(options),\n <mask>                        /*delay_executor=*/\n <mask>                        [this](std::function<void()> task, uint32_t delay_ms) {\n <mask>                          return execute_after(io_service_, task, delay_ms);\n <mask>                        }));\n <mask> }\n <mask> \n <mask> ray::Status NodeManager::RegisterGcs() {\n <mask>   // Start sending heartbeat here to ensure it happening after raylet being registered.\n <mask>   heartbeat_sender_.reset(new HeartbeatSender(self_node_id_, gcs_client_));\n </s> [runtime env] support create or delete runtime envs in agent (#15904) </s> remove   std::unique_ptr<AgentManager> &delegate_;\n </s> add   std::shared_ptr<AgentManager> &delegate_; </s> add void WorkerPool::SetAgentManager(std::shared_ptr<AgentManager> agent_manager) {\n  agent_manager_ = agent_manager;\n}\n </s> remove   std::unique_ptr<AgentManager> agent_manager_;\n </s> add   std::shared_ptr<AgentManager> agent_manager_;", "html_url": "https://github.com/ray-project/ray/commit/874e947d6f13963710d249c6d09c35062bc91e14", "file_name": "src/ray/raylet/node_manager.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   /// A manager to resolve objects needed by queued tasks and workers that\n <mask>   /// called `ray.get` or `ray.wait`.\n <mask>   DependencyManager dependency_manager_;\n <mask> \n <mask>   std::unique_ptr<AgentManager> agent_manager_;\n <mask> \n <mask>   /// The RPC server.\n <mask>   rpc::GrpcServer node_manager_server_;\n <mask> \n <mask>   /// The node manager RPC service.\n </s> [runtime env] support create or delete runtime envs in agent (#15904) </s> add void WorkerPool::SetAgentManager(std::shared_ptr<AgentManager> agent_manager) {\n  agent_manager_ = agent_manager;\n}\n </s> remove   agent_manager_.reset(\n      new AgentManager(std::move(options),\n                       /*delay_executor=*/\n                       [this](std::function<void()> task, uint32_t delay_ms) {\n                         return execute_after(io_service_, task, delay_ms);\n                       }));\n </s> add   agent_manager_ = std::make_shared<AgentManager>(\n      std::move(options),\n      /*delay_executor=*/\n      [this](std::function<void()> task, uint32_t delay_ms) {\n        return execute_after(io_service_, task, delay_ms);\n      },\n      /*runtime_env_agent_factory=*/\n      [this](const std::string &ip_address, int port) {\n        RAY_CHECK(!ip_address.empty() && port != 0);\n        return std::shared_ptr<rpc::RuntimeEnvAgentClientInterface>(\n            new rpc::RuntimeEnvAgentClient(ip_address, port, client_call_manager_));\n      });\n  worker_pool_.SetAgentManager(agent_manager_); </s> remove   std::unique_ptr<AgentManager> &delegate_;\n </s> add   std::shared_ptr<AgentManager> &delegate_;", "html_url": "https://github.com/ray-project/ray/commit/874e947d6f13963710d249c6d09c35062bc91e14", "file_name": "src/ray/raylet/node_manager.h"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask> }\n <mask> \n <mask> Process WorkerPool::StartWorkerProcess(\n <mask>     const Language &language, const rpc::WorkerType worker_type, const JobID &job_id,\n <mask>     const std::vector<std::string> &dynamic_options,\n <mask>     const std::string &serialized_runtime_env,\n <mask>     std::unordered_map<std::string, std::string> override_environment_variables) {\n <mask>   rpc::JobConfig *job_config = nullptr;\n </s> [runtime env] support create or delete runtime envs in agent (#15904) </s> remove   std::unique_ptr<AgentManager> &delegate_;\n </s> add   std::shared_ptr<AgentManager> &delegate_; </s> remove   agent_manager_.reset(\n      new AgentManager(std::move(options),\n                       /*delay_executor=*/\n                       [this](std::function<void()> task, uint32_t delay_ms) {\n                         return execute_after(io_service_, task, delay_ms);\n                       }));\n </s> add   agent_manager_ = std::make_shared<AgentManager>(\n      std::move(options),\n      /*delay_executor=*/\n      [this](std::function<void()> task, uint32_t delay_ms) {\n        return execute_after(io_service_, task, delay_ms);\n      },\n      /*runtime_env_agent_factory=*/\n      [this](const std::string &ip_address, int port) {\n        RAY_CHECK(!ip_address.empty() && port != 0);\n        return std::shared_ptr<rpc::RuntimeEnvAgentClientInterface>(\n            new rpc::RuntimeEnvAgentClient(ip_address, port, client_call_manager_));\n      });\n  worker_pool_.SetAgentManager(agent_manager_); </s> remove   std::unique_ptr<AgentManager> agent_manager_;\n </s> add   std::shared_ptr<AgentManager> agent_manager_;", "html_url": "https://github.com/ray-project/ray/commit/874e947d6f13963710d249c6d09c35062bc91e14", "file_name": "src/ray/raylet/worker_pool.cc"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         \"cpp/src/arrow/buffer.h\",\n <mask>         \"cpp/src/arrow/io/interfaces.h\",\n <mask>         \"cpp/src/arrow/memory_pool.h\",\n <mask>         \"cpp/src/arrow/status.h\",\n <mask>         \"cpp/src/arrow/util/bit-util.h\",\n <mask>         \"cpp/src/arrow/util/io-util.h\",\n <mask>         \"cpp/src/arrow/util/logging.h\",\n <mask>         \"cpp/src/arrow/util/macros.h\",\n <mask>         \"cpp/src/arrow/util/memory.h\",\n <mask>         \"cpp/src/arrow/util/stl.h\",\n <mask>         \"cpp/src/arrow/util/string.h\",\n </s> Use plasma with batched CreateAndSeal implemented (#5864) </s> remove         \"cpp/src/arrow/util/thread-pool.h\",\n </s> add         \"cpp/src/arrow/util/thread_pool.h\", </s> remove         commit = \"141a213a54f4979ab0b94b94928739359a2ee9ad\",\n </s> add         commit = \"0aad5a08e539a7b7f4abd4ee57e08fe78957d412\", </s> remove exports_files([\"cpp/src/plasma/format/common.fbs\"])\n </s> add exports_files([\"cpp/src/plasma/common.fbs\"]) </s> remove     includes = [\"cpp/src/plasma/format/common.fbs\"],\n </s> add     includes = [\"cpp/src/plasma/common.fbs\"], </s> remove     srcs = [\"cpp/src/plasma/format/plasma.fbs\"],\n </s> add     srcs = [\"cpp/src/plasma/plasma.fbs\"], </s> remove     srcs = [\"cpp/src/plasma/format/common.fbs\"],\n </s> add     srcs = [\"cpp/src/plasma/common.fbs\"],", "html_url": "https://github.com/ray-project/ray/commit/875c84ed634c1beb45d39a8c0c80cb7d968f577d", "file_name": "bazel/BUILD.plasma"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         \"cpp/src/arrow/util/stl.h\",\n <mask>         \"cpp/src/arrow/util/string.h\",\n <mask>         \"cpp/src/arrow/util/string_builder.h\",\n <mask>         \"cpp/src/arrow/util/string_view.h\",\n <mask>         \"cpp/src/arrow/util/thread-pool.h\",\n <mask>         \"cpp/src/arrow/util/type_traits.h\",\n <mask>         \"cpp/src/arrow/util/ubsan.h\",\n <mask>         \"cpp/src/arrow/util/visibility.h\",\n <mask>         \"cpp/src/arrow/util/windows_compatibility.h\",\n <mask>         \"cpp/src/arrow/vendored/string_view.hpp\",\n </s> Use plasma with batched CreateAndSeal implemented (#5864) </s> add         \"cpp/src/arrow/vendored/xxhash.h\",\n        \"cpp/src/arrow/vendored/xxhash/xxh3.h\", </s> remove         \"cpp/src/arrow/util/bit-util.h\",\n        \"cpp/src/arrow/util/io-util.h\",\n </s> add         \"cpp/src/arrow/util/bit_util.h\",\n        \"cpp/src/arrow/util/checked_cast.h\",\n        \"cpp/src/arrow/util/io_util.h\", </s> remove         commit = \"141a213a54f4979ab0b94b94928739359a2ee9ad\",\n </s> add         commit = \"0aad5a08e539a7b7f4abd4ee57e08fe78957d412\", </s> remove exports_files([\"cpp/src/plasma/format/common.fbs\"])\n </s> add exports_files([\"cpp/src/plasma/common.fbs\"]) </s> remove     includes = [\"cpp/src/plasma/format/common.fbs\"],\n </s> add     includes = [\"cpp/src/plasma/common.fbs\"], </s> remove     srcs = [\"cpp/src/plasma/format/plasma.fbs\"],\n </s> add     srcs = [\"cpp/src/plasma/plasma.fbs\"],", "html_url": "https://github.com/ray-project/ray/commit/875c84ed634c1beb45d39a8c0c80cb7d968f577d", "file_name": "bazel/BUILD.plasma"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>         \"cpp/src/arrow/util/ubsan.h\",\n <mask>         \"cpp/src/arrow/util/visibility.h\",\n <mask>         \"cpp/src/arrow/util/windows_compatibility.h\",\n <mask>         \"cpp/src/arrow/vendored/string_view.hpp\",\n <mask>         \"cpp/src/arrow/vendored/xxhash/xxhash.c\",\n <mask>         \"cpp/src/arrow/vendored/xxhash/xxhash.h\",\n <mask>     ],\n <mask>     copts = COPTS,\n <mask>     strip_include_prefix = \"cpp/src\",\n </s> Use plasma with batched CreateAndSeal implemented (#5864) </s> remove         \"cpp/src/arrow/util/thread-pool.h\",\n </s> add         \"cpp/src/arrow/util/thread_pool.h\", </s> remove         commit = \"141a213a54f4979ab0b94b94928739359a2ee9ad\",\n </s> add         commit = \"0aad5a08e539a7b7f4abd4ee57e08fe78957d412\", </s> remove     includes = [\"cpp/src/plasma/format/common.fbs\"],\n </s> add     includes = [\"cpp/src/plasma/common.fbs\"], </s> remove     srcs = [\"cpp/src/plasma/format/plasma.fbs\"],\n </s> add     srcs = [\"cpp/src/plasma/plasma.fbs\"], </s> remove     srcs = [\"cpp/src/plasma/format/common.fbs\"],\n </s> add     srcs = [\"cpp/src/plasma/common.fbs\"], </s> remove exports_files([\"cpp/src/plasma/format/common.fbs\"])\n </s> add exports_files([\"cpp/src/plasma/common.fbs\"])", "html_url": "https://github.com/ray-project/ray/commit/875c84ed634c1beb45d39a8c0c80cb7d968f577d", "file_name": "bazel/BUILD.plasma"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> ]\n <mask> \n <mask> flatbuffer_cc_library(\n <mask>     name = \"common_fbs\",\n <mask>     srcs = [\"cpp/src/plasma/format/common.fbs\"],\n <mask>     flatc_args = FLATC_ARGS,\n <mask>     out_prefix = \"cpp/src/plasma/\",\n <mask> )\n <mask> \n <mask> flatbuffer_cc_library(\n </s> Use plasma with batched CreateAndSeal implemented (#5864) </s> remove     srcs = [\"cpp/src/plasma/format/plasma.fbs\"],\n </s> add     srcs = [\"cpp/src/plasma/plasma.fbs\"], </s> remove     includes = [\"cpp/src/plasma/format/common.fbs\"],\n </s> add     includes = [\"cpp/src/plasma/common.fbs\"], </s> remove         commit = \"141a213a54f4979ab0b94b94928739359a2ee9ad\",\n </s> add         commit = \"0aad5a08e539a7b7f4abd4ee57e08fe78957d412\", </s> remove exports_files([\"cpp/src/plasma/format/common.fbs\"])\n </s> add exports_files([\"cpp/src/plasma/common.fbs\"]) </s> add         \"cpp/src/arrow/vendored/xxhash.h\",\n        \"cpp/src/arrow/vendored/xxhash/xxh3.h\", </s> remove         \"cpp/src/arrow/util/thread-pool.h\",\n </s> add         \"cpp/src/arrow/util/thread_pool.h\",", "html_url": "https://github.com/ray-project/ray/commit/875c84ed634c1beb45d39a8c0c80cb7d968f577d", "file_name": "bazel/BUILD.plasma"}
{"docstring_tokens": "keep keep keep replace keep replace keep", "code_tokens": " <mask> \n <mask> flatbuffer_cc_library(\n <mask>     name = \"plasma_fbs\",\n <mask>     srcs = [\"cpp/src/plasma/format/plasma.fbs\"],\n <mask>     flatc_args = FLATC_ARGS,\n <mask>     includes = [\"cpp/src/plasma/format/common.fbs\"],\n <mask>     out_prefix = \"cpp/src/plasma/\",\n </s> Use plasma with batched CreateAndSeal implemented (#5864) </s> remove     srcs = [\"cpp/src/plasma/format/common.fbs\"],\n </s> add     srcs = [\"cpp/src/plasma/common.fbs\"], </s> remove exports_files([\"cpp/src/plasma/format/common.fbs\"])\n </s> add exports_files([\"cpp/src/plasma/common.fbs\"]) </s> remove         commit = \"141a213a54f4979ab0b94b94928739359a2ee9ad\",\n </s> add         commit = \"0aad5a08e539a7b7f4abd4ee57e08fe78957d412\", </s> add         \"cpp/src/arrow/vendored/xxhash.h\",\n        \"cpp/src/arrow/vendored/xxhash/xxh3.h\", </s> remove         \"cpp/src/arrow/util/thread-pool.h\",\n </s> add         \"cpp/src/arrow/util/thread_pool.h\",", "html_url": "https://github.com/ray-project/ray/commit/875c84ed634c1beb45d39a8c0c80cb7d968f577d", "file_name": "bazel/BUILD.plasma"}
{"docstring_tokens": "keep keep keep keep replace", "code_tokens": " <mask>     includes = [\"cpp/src/plasma/format/common.fbs\"],\n <mask>     out_prefix = \"cpp/src/plasma/\",\n <mask> )\n <mask> \n <mask> exports_files([\"cpp/src/plasma/format/common.fbs\"])\n </s> Use plasma with batched CreateAndSeal implemented (#5864) </s> remove     includes = [\"cpp/src/plasma/format/common.fbs\"],\n </s> add     includes = [\"cpp/src/plasma/common.fbs\"], </s> remove     srcs = [\"cpp/src/plasma/format/plasma.fbs\"],\n </s> add     srcs = [\"cpp/src/plasma/plasma.fbs\"], </s> remove         commit = \"141a213a54f4979ab0b94b94928739359a2ee9ad\",\n </s> add         commit = \"0aad5a08e539a7b7f4abd4ee57e08fe78957d412\", </s> remove     srcs = [\"cpp/src/plasma/format/common.fbs\"],\n </s> add     srcs = [\"cpp/src/plasma/common.fbs\"], </s> add         \"cpp/src/arrow/vendored/xxhash.h\",\n        \"cpp/src/arrow/vendored/xxhash/xxh3.h\", </s> remove         \"cpp/src/arrow/util/thread-pool.h\",\n </s> add         \"cpp/src/arrow/util/thread_pool.h\",", "html_url": "https://github.com/ray-project/ray/commit/875c84ed634c1beb45d39a8c0c80cb7d968f577d", "file_name": "bazel/BUILD.plasma"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     new_git_repository(\n <mask>         name = \"plasma\",\n <mask>         build_file = \"@//bazel:BUILD.plasma\",\n <mask>         commit = \"141a213a54f4979ab0b94b94928739359a2ee9ad\",\n <mask>         remote = \"https://github.com/apache/arrow\",\n <mask>     )\n <mask> \n <mask>     new_git_repository(\n <mask>         name = \"cython\",\n </s> Use plasma with batched CreateAndSeal implemented (#5864) </s> remove     srcs = [\"cpp/src/plasma/format/plasma.fbs\"],\n </s> add     srcs = [\"cpp/src/plasma/plasma.fbs\"], </s> remove     includes = [\"cpp/src/plasma/format/common.fbs\"],\n </s> add     includes = [\"cpp/src/plasma/common.fbs\"], </s> remove     srcs = [\"cpp/src/plasma/format/common.fbs\"],\n </s> add     srcs = [\"cpp/src/plasma/common.fbs\"], </s> remove exports_files([\"cpp/src/plasma/format/common.fbs\"])\n </s> add exports_files([\"cpp/src/plasma/common.fbs\"]) </s> add         \"cpp/src/arrow/vendored/xxhash.h\",\n        \"cpp/src/arrow/vendored/xxhash/xxh3.h\", </s> remove         \"cpp/src/arrow/util/thread-pool.h\",\n </s> add         \"cpp/src/arrow/util/thread_pool.h\",", "html_url": "https://github.com/ray-project/ray/commit/875c84ed634c1beb45d39a8c0c80cb7d968f577d", "file_name": "bazel/ray_deps_setup.bzl"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>         # Makes sure the Node object has valid addresses after setup.\n <mask>         self.validate_ip_port(self.address)\n <mask>         self.validate_ip_port(self.gcs_address)\n <mask> \n <mask>     @staticmethod\n <mask>     def validate_ip_port(ip_port):\n <mask>         \"\"\"Validates the address is in the ip:port format\"\"\"\n <mask>         _, _, port = ip_port.rpartition(\":\")\n </s> [core] Add stats for the gcs backend for telemetry. (#27876)\n\n## Why are these changes needed?\r\n\r\nTo get better understanding of how GCS FT is used, adding this metrics.\r\n\r\nTest:\r\n```\r\ncat /tmp/ray/session_latest/usage_stats.json\r\n{\"usage_stats\": {\"ray_version\": \"3.0.0.dev0\", \"python_version\": \"3.9.12\", \"schema_version\": \"0.1\", \"source\": \"OSS\", \"session_id\": \"70d3ecd3-5b16-40c3-9301-fd05404ea92a\", \"git_commit\": \"{{RAY_COMMIT_SHA}}\", \"os\": \"linux\", \"collect_timestamp_ms\": 1660587366806, \"session_start_timestamp_ms\": 1660587351586, \"cloud_provider\": null, \"min_workers\": null, \"max_workers\": null, \"head_node_instance_type\": null, \"worker_node_instance_types\": null, \"total_num_cpus\": 16, \"total_num_gpus\": null, \"total_memory_gb\": 16.10752945020795, \"total_object_store_memory_gb\": 8.053764724172652, \"library_usages\": [\"serve\"], \"total_success\": 0, \"total_failed\": 13, \"seq_number\": 13, \"extra_usage_tags\": {\"serve_api_version\": \"v1\", \"gcs_storage\": \"redis\", \"serve_num_deployments\": \"1\"}, \"total_num_nodes\": 2, \"total_num_running_jobs\": 2}}\r\n``` </s> remove         assert result == {\"key\": \"val\", \"_test1\": \"val1\", \"_test2\": \"val2\"}\n </s> add         assert result == {\n            \"key\": \"val\",\n            \"_test1\": \"val1\",\n            \"_test2\": \"val2\",\n            \"gcs_storage\": gcs_storage_type,\n        } </s> add @pytest.fixture\ndef gcs_storage_type():\n    storage = \"redis\" if os.environ.get(\"RAY_REDIS_ADDRESS\") else \"memory\"\n    yield storage\n\n </s> add     # The GCS storage type, which could be memory or redis\n    GCS_STORAGE = auto()\n </s> remove     monkeypatch, call_ray_start, reset_usage_stats, ray_client\n </s> add     monkeypatch, call_ray_start, reset_usage_stats, ray_client, gcs_storage_type </s> remove             assert tags == {\"key\": \"val\", \"key2\": \"val2\"}\n </s> add             assert tags == {\n                \"key\": \"val\",\n                \"key2\": \"val2\",\n                \"gcs_storage\": gcs_storage_type,\n            } </s> remove def test_usage_report_e2e(monkeypatch, ray_start_cluster, tmp_path, reset_usage_stats):\n </s> add def test_usage_report_e2e(\n    monkeypatch, ray_start_cluster, tmp_path, reset_usage_stats, gcs_storage_type\n):", "html_url": "https://github.com/ray-project/ray/commit/87ce8480ffa9d4cd3f1331cd1a10574f806d77c7", "file_name": "python/ray/_private/node.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask>     SERVE_NUM_DEPLOYMENTS = auto()\n <mask> \n <mask> \n <mask> def record_extra_usage_tag(key: TagKey, value: str):\n <mask>     \"\"\"Record extra kv usage tag.\n <mask> \n </s> [core] Add stats for the gcs backend for telemetry. (#27876)\n\n## Why are these changes needed?\r\n\r\nTo get better understanding of how GCS FT is used, adding this metrics.\r\n\r\nTest:\r\n```\r\ncat /tmp/ray/session_latest/usage_stats.json\r\n{\"usage_stats\": {\"ray_version\": \"3.0.0.dev0\", \"python_version\": \"3.9.12\", \"schema_version\": \"0.1\", \"source\": \"OSS\", \"session_id\": \"70d3ecd3-5b16-40c3-9301-fd05404ea92a\", \"git_commit\": \"{{RAY_COMMIT_SHA}}\", \"os\": \"linux\", \"collect_timestamp_ms\": 1660587366806, \"session_start_timestamp_ms\": 1660587351586, \"cloud_provider\": null, \"min_workers\": null, \"max_workers\": null, \"head_node_instance_type\": null, \"worker_node_instance_types\": null, \"total_num_cpus\": 16, \"total_num_gpus\": null, \"total_memory_gb\": 16.10752945020795, \"total_object_store_memory_gb\": 8.053764724172652, \"library_usages\": [\"serve\"], \"total_success\": 0, \"total_failed\": 13, \"seq_number\": 13, \"extra_usage_tags\": {\"serve_api_version\": \"v1\", \"gcs_storage\": \"redis\", \"serve_num_deployments\": \"1\"}, \"total_num_nodes\": 2, \"total_num_running_jobs\": 2}}\r\n``` </s> add @pytest.fixture\ndef gcs_storage_type():\n    storage = \"redis\" if os.environ.get(\"RAY_REDIS_ADDRESS\") else \"memory\"\n    yield storage\n\n </s> remove def test_usage_report_e2e(monkeypatch, ray_start_cluster, tmp_path, reset_usage_stats):\n </s> add def test_usage_report_e2e(\n    monkeypatch, ray_start_cluster, tmp_path, reset_usage_stats, gcs_storage_type\n): </s> remove def test_usage_stats_tags(monkeypatch, ray_start_cluster, reset_usage_stats):\n </s> add def test_usage_stats_tags(\n    monkeypatch, ray_start_cluster, reset_usage_stats, gcs_storage_type\n): </s> remove             assert tags == {\"key\": \"val\", \"key2\": \"val2\"}\n </s> add             assert tags == {\n                \"key\": \"val\",\n                \"key2\": \"val2\",\n                \"gcs_storage\": gcs_storage_type,\n            } </s> remove     monkeypatch, call_ray_start, reset_usage_stats, ray_client\n </s> add     monkeypatch, call_ray_start, reset_usage_stats, ray_client, gcs_storage_type </s> add         self._record_stats()", "html_url": "https://github.com/ray-project/ray/commit/87ce8480ffa9d4cd3f1331cd1a10574f806d77c7", "file_name": "python/ray/_private/usage/usage_lib.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask> \n <mask> \n <mask> @pytest.fixture\n <mask> def reset_usage_stats():\n <mask>     yield\n <mask>     # Remove the lib usage so that it will be reset for each test.\n <mask>     ray_usage_lib.LibUsageRecorder(\n <mask>         ray._private.utils.get_ray_temp_dir()\n </s> [core] Add stats for the gcs backend for telemetry. (#27876)\n\n## Why are these changes needed?\r\n\r\nTo get better understanding of how GCS FT is used, adding this metrics.\r\n\r\nTest:\r\n```\r\ncat /tmp/ray/session_latest/usage_stats.json\r\n{\"usage_stats\": {\"ray_version\": \"3.0.0.dev0\", \"python_version\": \"3.9.12\", \"schema_version\": \"0.1\", \"source\": \"OSS\", \"session_id\": \"70d3ecd3-5b16-40c3-9301-fd05404ea92a\", \"git_commit\": \"{{RAY_COMMIT_SHA}}\", \"os\": \"linux\", \"collect_timestamp_ms\": 1660587366806, \"session_start_timestamp_ms\": 1660587351586, \"cloud_provider\": null, \"min_workers\": null, \"max_workers\": null, \"head_node_instance_type\": null, \"worker_node_instance_types\": null, \"total_num_cpus\": 16, \"total_num_gpus\": null, \"total_memory_gb\": 16.10752945020795, \"total_object_store_memory_gb\": 8.053764724172652, \"library_usages\": [\"serve\"], \"total_success\": 0, \"total_failed\": 13, \"seq_number\": 13, \"extra_usage_tags\": {\"serve_api_version\": \"v1\", \"gcs_storage\": \"redis\", \"serve_num_deployments\": \"1\"}, \"total_num_nodes\": 2, \"total_num_running_jobs\": 2}}\r\n``` </s> add     # The GCS storage type, which could be memory or redis\n    GCS_STORAGE = auto()\n </s> add         self._record_stats() </s> add             \"gcs_storage\": gcs_storage_type, </s> remove         assert result == {\"key\": \"val\", \"_test1\": \"val1\", \"_test2\": \"val2\"}\n </s> add         assert result == {\n            \"key\": \"val\",\n            \"_test1\": \"val1\",\n            \"_test2\": \"val2\",\n            \"gcs_storage\": gcs_storage_type,\n        } </s> remove def test_usage_stats_tags(monkeypatch, ray_start_cluster, reset_usage_stats):\n </s> add def test_usage_stats_tags(\n    monkeypatch, ray_start_cluster, reset_usage_stats, gcs_storage_type\n): </s> remove     monkeypatch, call_ray_start, reset_usage_stats, ray_client\n </s> add     monkeypatch, call_ray_start, reset_usage_stats, ray_client, gcs_storage_type", "html_url": "https://github.com/ray-project/ray/commit/87ce8480ffa9d4cd3f1331cd1a10574f806d77c7", "file_name": "python/ray/tests/test_usage_stats.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> \n <mask> @pytest.mark.parametrize(\"ray_client\", [True, False])\n <mask> def test_get_extra_usage_tags_to_report(\n <mask>     monkeypatch, call_ray_start, reset_usage_stats, ray_client\n <mask> ):\n <mask>     with monkeypatch.context() as m:\n <mask>         # Test a normal case.\n <mask>         m.setenv(\"RAY_USAGE_STATS_EXTRA_TAGS\", \"key=val;key2=val2\")\n <mask>         result = ray_usage_lib.get_extra_usage_tags_to_report(\n </s> [core] Add stats for the gcs backend for telemetry. (#27876)\n\n## Why are these changes needed?\r\n\r\nTo get better understanding of how GCS FT is used, adding this metrics.\r\n\r\nTest:\r\n```\r\ncat /tmp/ray/session_latest/usage_stats.json\r\n{\"usage_stats\": {\"ray_version\": \"3.0.0.dev0\", \"python_version\": \"3.9.12\", \"schema_version\": \"0.1\", \"source\": \"OSS\", \"session_id\": \"70d3ecd3-5b16-40c3-9301-fd05404ea92a\", \"git_commit\": \"{{RAY_COMMIT_SHA}}\", \"os\": \"linux\", \"collect_timestamp_ms\": 1660587366806, \"session_start_timestamp_ms\": 1660587351586, \"cloud_provider\": null, \"min_workers\": null, \"max_workers\": null, \"head_node_instance_type\": null, \"worker_node_instance_types\": null, \"total_num_cpus\": 16, \"total_num_gpus\": null, \"total_memory_gb\": 16.10752945020795, \"total_object_store_memory_gb\": 8.053764724172652, \"library_usages\": [\"serve\"], \"total_success\": 0, \"total_failed\": 13, \"seq_number\": 13, \"extra_usage_tags\": {\"serve_api_version\": \"v1\", \"gcs_storage\": \"redis\", \"serve_num_deployments\": \"1\"}, \"total_num_nodes\": 2, \"total_num_running_jobs\": 2}}\r\n``` </s> remove def test_usage_stats_tags(monkeypatch, ray_start_cluster, reset_usage_stats):\n </s> add def test_usage_stats_tags(\n    monkeypatch, ray_start_cluster, reset_usage_stats, gcs_storage_type\n): </s> remove         assert result == {\"key\": \"val\", \"_test1\": \"val1\", \"_test2\": \"val3\"}\n </s> add         assert result == {\n            \"key\": \"val\",\n            \"_test1\": \"val1\",\n            \"_test2\": \"val3\",\n            \"gcs_storage\": gcs_storage_type,\n        } </s> remove def test_usage_report_e2e(monkeypatch, ray_start_cluster, tmp_path, reset_usage_stats):\n </s> add def test_usage_report_e2e(\n    monkeypatch, ray_start_cluster, tmp_path, reset_usage_stats, gcs_storage_type\n): </s> remove         assert result == {\"key\": \"val\", \"_test1\": \"val1\", \"_test2\": \"val2\"}\n </s> add         assert result == {\n            \"key\": \"val\",\n            \"_test1\": \"val1\",\n            \"_test2\": \"val2\",\n            \"gcs_storage\": gcs_storage_type,\n        } </s> add             \"gcs_storage\": gcs_storage_type, </s> add     # The GCS storage type, which could be memory or redis\n    GCS_STORAGE = auto()\n", "html_url": "https://github.com/ray-project/ray/commit/87ce8480ffa9d4cd3f1331cd1a10574f806d77c7", "file_name": "python/ray/tests/test_usage_stats.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         run_string_as_driver(driver)\n <mask>         result = ray_usage_lib.get_extra_usage_tags_to_report(\n <mask>             ray.experimental.internal_kv.internal_kv_get_gcs_client()\n <mask>         )\n <mask>         assert result == {\"key\": \"val\", \"_test1\": \"val1\", \"_test2\": \"val2\"}\n <mask>         # Make sure the value is overwritten.\n <mask>         ray_usage_lib.record_extra_usage_tag(ray_usage_lib.TagKey._TEST2, \"val3\")\n <mask>         result = ray_usage_lib.get_extra_usage_tags_to_report(\n <mask>             ray.experimental.internal_kv.internal_kv_get_gcs_client()\n <mask>         )\n </s> [core] Add stats for the gcs backend for telemetry. (#27876)\n\n## Why are these changes needed?\r\n\r\nTo get better understanding of how GCS FT is used, adding this metrics.\r\n\r\nTest:\r\n```\r\ncat /tmp/ray/session_latest/usage_stats.json\r\n{\"usage_stats\": {\"ray_version\": \"3.0.0.dev0\", \"python_version\": \"3.9.12\", \"schema_version\": \"0.1\", \"source\": \"OSS\", \"session_id\": \"70d3ecd3-5b16-40c3-9301-fd05404ea92a\", \"git_commit\": \"{{RAY_COMMIT_SHA}}\", \"os\": \"linux\", \"collect_timestamp_ms\": 1660587366806, \"session_start_timestamp_ms\": 1660587351586, \"cloud_provider\": null, \"min_workers\": null, \"max_workers\": null, \"head_node_instance_type\": null, \"worker_node_instance_types\": null, \"total_num_cpus\": 16, \"total_num_gpus\": null, \"total_memory_gb\": 16.10752945020795, \"total_object_store_memory_gb\": 8.053764724172652, \"library_usages\": [\"serve\"], \"total_success\": 0, \"total_failed\": 13, \"seq_number\": 13, \"extra_usage_tags\": {\"serve_api_version\": \"v1\", \"gcs_storage\": \"redis\", \"serve_num_deployments\": \"1\"}, \"total_num_nodes\": 2, \"total_num_running_jobs\": 2}}\r\n``` </s> remove         assert result == {\"key\": \"val\", \"_test1\": \"val1\", \"_test2\": \"val3\"}\n </s> add         assert result == {\n            \"key\": \"val\",\n            \"_test1\": \"val1\",\n            \"_test2\": \"val3\",\n            \"gcs_storage\": gcs_storage_type,\n        } </s> remove     monkeypatch, call_ray_start, reset_usage_stats, ray_client\n </s> add     monkeypatch, call_ray_start, reset_usage_stats, ray_client, gcs_storage_type </s> add         self._record_stats() </s> remove             assert tags == {\"key\": \"val\", \"key2\": \"val2\"}\n </s> add             assert tags == {\n                \"key\": \"val\",\n                \"key2\": \"val2\",\n                \"gcs_storage\": gcs_storage_type,\n            } </s> remove def test_usage_report_e2e(monkeypatch, ray_start_cluster, tmp_path, reset_usage_stats):\n </s> add def test_usage_report_e2e(\n    monkeypatch, ray_start_cluster, tmp_path, reset_usage_stats, gcs_storage_type\n): </s> add @pytest.fixture\ndef gcs_storage_type():\n    storage = \"redis\" if os.environ.get(\"RAY_REDIS_ADDRESS\") else \"memory\"\n    yield storage\n\n", "html_url": "https://github.com/ray-project/ray/commit/87ce8480ffa9d4cd3f1331cd1a10574f806d77c7", "file_name": "python/ray/tests/test_usage_stats.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         ray_usage_lib.record_extra_usage_tag(ray_usage_lib.TagKey._TEST2, \"val3\")\n <mask>         result = ray_usage_lib.get_extra_usage_tags_to_report(\n <mask>             ray.experimental.internal_kv.internal_kv_get_gcs_client()\n <mask>         )\n <mask>         assert result == {\"key\": \"val\", \"_test1\": \"val1\", \"_test2\": \"val3\"}\n <mask> \n <mask> \n <mask> def test_usage_stats_enabledness(monkeypatch, tmp_path, reset_usage_stats):\n <mask>     with monkeypatch.context() as m:\n <mask>         m.setenv(\"RAY_USAGE_STATS_ENABLED\", \"1\")\n </s> [core] Add stats for the gcs backend for telemetry. (#27876)\n\n## Why are these changes needed?\r\n\r\nTo get better understanding of how GCS FT is used, adding this metrics.\r\n\r\nTest:\r\n```\r\ncat /tmp/ray/session_latest/usage_stats.json\r\n{\"usage_stats\": {\"ray_version\": \"3.0.0.dev0\", \"python_version\": \"3.9.12\", \"schema_version\": \"0.1\", \"source\": \"OSS\", \"session_id\": \"70d3ecd3-5b16-40c3-9301-fd05404ea92a\", \"git_commit\": \"{{RAY_COMMIT_SHA}}\", \"os\": \"linux\", \"collect_timestamp_ms\": 1660587366806, \"session_start_timestamp_ms\": 1660587351586, \"cloud_provider\": null, \"min_workers\": null, \"max_workers\": null, \"head_node_instance_type\": null, \"worker_node_instance_types\": null, \"total_num_cpus\": 16, \"total_num_gpus\": null, \"total_memory_gb\": 16.10752945020795, \"total_object_store_memory_gb\": 8.053764724172652, \"library_usages\": [\"serve\"], \"total_success\": 0, \"total_failed\": 13, \"seq_number\": 13, \"extra_usage_tags\": {\"serve_api_version\": \"v1\", \"gcs_storage\": \"redis\", \"serve_num_deployments\": \"1\"}, \"total_num_nodes\": 2, \"total_num_running_jobs\": 2}}\r\n``` </s> remove         assert result == {\"key\": \"val\", \"_test1\": \"val1\", \"_test2\": \"val2\"}\n </s> add         assert result == {\n            \"key\": \"val\",\n            \"_test1\": \"val1\",\n            \"_test2\": \"val2\",\n            \"gcs_storage\": gcs_storage_type,\n        } </s> remove def test_usage_stats_tags(monkeypatch, ray_start_cluster, reset_usage_stats):\n </s> add def test_usage_stats_tags(\n    monkeypatch, ray_start_cluster, reset_usage_stats, gcs_storage_type\n): </s> remove     monkeypatch, call_ray_start, reset_usage_stats, ray_client\n </s> add     monkeypatch, call_ray_start, reset_usage_stats, ray_client, gcs_storage_type </s> remove def test_usage_report_e2e(monkeypatch, ray_start_cluster, tmp_path, reset_usage_stats):\n </s> add def test_usage_report_e2e(\n    monkeypatch, ray_start_cluster, tmp_path, reset_usage_stats, gcs_storage_type\n): </s> remove             assert tags == {\"key\": \"val\", \"key2\": \"val2\"}\n </s> add             assert tags == {\n                \"key\": \"val\",\n                \"key2\": \"val2\",\n                \"gcs_storage\": gcs_storage_type,\n            } </s> add     # The GCS storage type, which could be memory or redis\n    GCS_STORAGE = auto()\n", "html_url": "https://github.com/ray-project/ray/commit/87ce8480ffa9d4cd3f1331cd1a10574f806d77c7", "file_name": "python/ray/tests/test_usage_stats.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> @pytest.mark.skipif(\n <mask>     sys.platform == \"win32\",\n <mask>     reason=\"Test depends on runtime env feature not supported on Windows.\",\n <mask> )\n <mask> def test_usage_report_e2e(monkeypatch, ray_start_cluster, tmp_path, reset_usage_stats):\n <mask>     \"\"\"\n <mask>     Test usage report works e2e with env vars.\n <mask>     \"\"\"\n <mask>     cluster_config_file_path = tmp_path / \"ray_bootstrap_config.yaml\"\n <mask>     cluster_config_file_path.write_text(\n </s> [core] Add stats for the gcs backend for telemetry. (#27876)\n\n## Why are these changes needed?\r\n\r\nTo get better understanding of how GCS FT is used, adding this metrics.\r\n\r\nTest:\r\n```\r\ncat /tmp/ray/session_latest/usage_stats.json\r\n{\"usage_stats\": {\"ray_version\": \"3.0.0.dev0\", \"python_version\": \"3.9.12\", \"schema_version\": \"0.1\", \"source\": \"OSS\", \"session_id\": \"70d3ecd3-5b16-40c3-9301-fd05404ea92a\", \"git_commit\": \"{{RAY_COMMIT_SHA}}\", \"os\": \"linux\", \"collect_timestamp_ms\": 1660587366806, \"session_start_timestamp_ms\": 1660587351586, \"cloud_provider\": null, \"min_workers\": null, \"max_workers\": null, \"head_node_instance_type\": null, \"worker_node_instance_types\": null, \"total_num_cpus\": 16, \"total_num_gpus\": null, \"total_memory_gb\": 16.10752945020795, \"total_object_store_memory_gb\": 8.053764724172652, \"library_usages\": [\"serve\"], \"total_success\": 0, \"total_failed\": 13, \"seq_number\": 13, \"extra_usage_tags\": {\"serve_api_version\": \"v1\", \"gcs_storage\": \"redis\", \"serve_num_deployments\": \"1\"}, \"total_num_nodes\": 2, \"total_num_running_jobs\": 2}}\r\n``` </s> remove def test_usage_stats_tags(monkeypatch, ray_start_cluster, reset_usage_stats):\n </s> add def test_usage_stats_tags(\n    monkeypatch, ray_start_cluster, reset_usage_stats, gcs_storage_type\n): </s> remove         assert result == {\"key\": \"val\", \"_test1\": \"val1\", \"_test2\": \"val3\"}\n </s> add         assert result == {\n            \"key\": \"val\",\n            \"_test1\": \"val1\",\n            \"_test2\": \"val3\",\n            \"gcs_storage\": gcs_storage_type,\n        } </s> remove     monkeypatch, call_ray_start, reset_usage_stats, ray_client\n </s> add     monkeypatch, call_ray_start, reset_usage_stats, ray_client, gcs_storage_type </s> remove         assert result == {\"key\": \"val\", \"_test1\": \"val1\", \"_test2\": \"val2\"}\n </s> add         assert result == {\n            \"key\": \"val\",\n            \"_test1\": \"val1\",\n            \"_test2\": \"val2\",\n            \"gcs_storage\": gcs_storage_type,\n        } </s> add     # The GCS storage type, which could be memory or redis\n    GCS_STORAGE = auto()\n </s> add @pytest.fixture\ndef gcs_storage_type():\n    storage = \"redis\" if os.environ.get(\"RAY_REDIS_ADDRESS\") else \"memory\"\n    yield storage\n\n", "html_url": "https://github.com/ray-project/ray/commit/87ce8480ffa9d4cd3f1331cd1a10574f806d77c7", "file_name": "python/ray/tests/test_usage_stats.py"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask>             \"serve_num_deployments\": \"1\",\n <mask>             \"serve_api_version\": \"v1\",\n <mask>         }\n <mask>         assert payload[\"total_num_nodes\"] == 1\n <mask>         assert payload[\"total_num_running_jobs\"] == 1\n <mask>         if os.environ.get(\"RAY_MINIMAL\") == \"1\":\n <mask>             # Since we start a serve actor for mocking a server using runtime env.\n </s> [core] Add stats for the gcs backend for telemetry. (#27876)\n\n## Why are these changes needed?\r\n\r\nTo get better understanding of how GCS FT is used, adding this metrics.\r\n\r\nTest:\r\n```\r\ncat /tmp/ray/session_latest/usage_stats.json\r\n{\"usage_stats\": {\"ray_version\": \"3.0.0.dev0\", \"python_version\": \"3.9.12\", \"schema_version\": \"0.1\", \"source\": \"OSS\", \"session_id\": \"70d3ecd3-5b16-40c3-9301-fd05404ea92a\", \"git_commit\": \"{{RAY_COMMIT_SHA}}\", \"os\": \"linux\", \"collect_timestamp_ms\": 1660587366806, \"session_start_timestamp_ms\": 1660587351586, \"cloud_provider\": null, \"min_workers\": null, \"max_workers\": null, \"head_node_instance_type\": null, \"worker_node_instance_types\": null, \"total_num_cpus\": 16, \"total_num_gpus\": null, \"total_memory_gb\": 16.10752945020795, \"total_object_store_memory_gb\": 8.053764724172652, \"library_usages\": [\"serve\"], \"total_success\": 0, \"total_failed\": 13, \"seq_number\": 13, \"extra_usage_tags\": {\"serve_api_version\": \"v1\", \"gcs_storage\": \"redis\", \"serve_num_deployments\": \"1\"}, \"total_num_nodes\": 2, \"total_num_running_jobs\": 2}}\r\n``` </s> remove     monkeypatch, call_ray_start, reset_usage_stats, ray_client\n </s> add     monkeypatch, call_ray_start, reset_usage_stats, ray_client, gcs_storage_type </s> add @pytest.fixture\ndef gcs_storage_type():\n    storage = \"redis\" if os.environ.get(\"RAY_REDIS_ADDRESS\") else \"memory\"\n    yield storage\n\n </s> remove         assert result == {\"key\": \"val\", \"_test1\": \"val1\", \"_test2\": \"val2\"}\n </s> add         assert result == {\n            \"key\": \"val\",\n            \"_test1\": \"val1\",\n            \"_test2\": \"val2\",\n            \"gcs_storage\": gcs_storage_type,\n        } </s> remove             assert tags == {\"key\": \"val\", \"key2\": \"val2\"}\n </s> add             assert tags == {\n                \"key\": \"val\",\n                \"key2\": \"val2\",\n                \"gcs_storage\": gcs_storage_type,\n            } </s> remove         assert result == {\"key\": \"val\", \"_test1\": \"val1\", \"_test2\": \"val3\"}\n </s> add         assert result == {\n            \"key\": \"val\",\n            \"_test1\": \"val1\",\n            \"_test2\": \"val3\",\n            \"gcs_storage\": gcs_storage_type,\n        } </s> remove def test_usage_report_e2e(monkeypatch, ray_start_cluster, tmp_path, reset_usage_stats):\n </s> add def test_usage_report_e2e(\n    monkeypatch, ray_start_cluster, tmp_path, reset_usage_stats, gcs_storage_type\n):", "html_url": "https://github.com/ray-project/ray/commit/87ce8480ffa9d4cd3f1331cd1a10574f806d77c7", "file_name": "python/ray/tests/test_usage_stats.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         wait_for_condition(verify)\n <mask> \n <mask> \n <mask> def test_usage_stats_tags(monkeypatch, ray_start_cluster, reset_usage_stats):\n <mask>     \"\"\"\n <mask>     Test usage tags are correctly reported.\n <mask>     \"\"\"\n <mask>     with monkeypatch.context() as m:\n <mask>         m.setenv(\"RAY_USAGE_STATS_ENABLED\", \"1\")\n </s> [core] Add stats for the gcs backend for telemetry. (#27876)\n\n## Why are these changes needed?\r\n\r\nTo get better understanding of how GCS FT is used, adding this metrics.\r\n\r\nTest:\r\n```\r\ncat /tmp/ray/session_latest/usage_stats.json\r\n{\"usage_stats\": {\"ray_version\": \"3.0.0.dev0\", \"python_version\": \"3.9.12\", \"schema_version\": \"0.1\", \"source\": \"OSS\", \"session_id\": \"70d3ecd3-5b16-40c3-9301-fd05404ea92a\", \"git_commit\": \"{{RAY_COMMIT_SHA}}\", \"os\": \"linux\", \"collect_timestamp_ms\": 1660587366806, \"session_start_timestamp_ms\": 1660587351586, \"cloud_provider\": null, \"min_workers\": null, \"max_workers\": null, \"head_node_instance_type\": null, \"worker_node_instance_types\": null, \"total_num_cpus\": 16, \"total_num_gpus\": null, \"total_memory_gb\": 16.10752945020795, \"total_object_store_memory_gb\": 8.053764724172652, \"library_usages\": [\"serve\"], \"total_success\": 0, \"total_failed\": 13, \"seq_number\": 13, \"extra_usage_tags\": {\"serve_api_version\": \"v1\", \"gcs_storage\": \"redis\", \"serve_num_deployments\": \"1\"}, \"total_num_nodes\": 2, \"total_num_running_jobs\": 2}}\r\n``` </s> remove def test_usage_report_e2e(monkeypatch, ray_start_cluster, tmp_path, reset_usage_stats):\n </s> add def test_usage_report_e2e(\n    monkeypatch, ray_start_cluster, tmp_path, reset_usage_stats, gcs_storage_type\n): </s> remove         assert result == {\"key\": \"val\", \"_test1\": \"val1\", \"_test2\": \"val3\"}\n </s> add         assert result == {\n            \"key\": \"val\",\n            \"_test1\": \"val1\",\n            \"_test2\": \"val3\",\n            \"gcs_storage\": gcs_storage_type,\n        } </s> remove     monkeypatch, call_ray_start, reset_usage_stats, ray_client\n </s> add     monkeypatch, call_ray_start, reset_usage_stats, ray_client, gcs_storage_type </s> remove             assert tags == {\"key\": \"val\", \"key2\": \"val2\"}\n </s> add             assert tags == {\n                \"key\": \"val\",\n                \"key2\": \"val2\",\n                \"gcs_storage\": gcs_storage_type,\n            } </s> add @pytest.fixture\ndef gcs_storage_type():\n    storage = \"redis\" if os.environ.get(\"RAY_REDIS_ADDRESS\") else \"memory\"\n    yield storage\n\n </s> add     # The GCS storage type, which could be memory or redis\n    GCS_STORAGE = auto()\n", "html_url": "https://github.com/ray-project/ray/commit/87ce8480ffa9d4cd3f1331cd1a10574f806d77c7", "file_name": "python/ray/tests/test_usage_stats.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         def verify():\n <mask>             tags = read_file(temp_dir, \"usage_stats\")[\"extra_usage_tags\"]\n <mask>             num_nodes = read_file(temp_dir, \"usage_stats\")[\"total_num_nodes\"]\n <mask>             assert tags == {\"key\": \"val\", \"key2\": \"val2\"}\n <mask>             assert num_nodes == 2\n <mask>             return True\n <mask> \n <mask>         wait_for_condition(verify)\n <mask> \n </s> [core] Add stats for the gcs backend for telemetry. (#27876)\n\n## Why are these changes needed?\r\n\r\nTo get better understanding of how GCS FT is used, adding this metrics.\r\n\r\nTest:\r\n```\r\ncat /tmp/ray/session_latest/usage_stats.json\r\n{\"usage_stats\": {\"ray_version\": \"3.0.0.dev0\", \"python_version\": \"3.9.12\", \"schema_version\": \"0.1\", \"source\": \"OSS\", \"session_id\": \"70d3ecd3-5b16-40c3-9301-fd05404ea92a\", \"git_commit\": \"{{RAY_COMMIT_SHA}}\", \"os\": \"linux\", \"collect_timestamp_ms\": 1660587366806, \"session_start_timestamp_ms\": 1660587351586, \"cloud_provider\": null, \"min_workers\": null, \"max_workers\": null, \"head_node_instance_type\": null, \"worker_node_instance_types\": null, \"total_num_cpus\": 16, \"total_num_gpus\": null, \"total_memory_gb\": 16.10752945020795, \"total_object_store_memory_gb\": 8.053764724172652, \"library_usages\": [\"serve\"], \"total_success\": 0, \"total_failed\": 13, \"seq_number\": 13, \"extra_usage_tags\": {\"serve_api_version\": \"v1\", \"gcs_storage\": \"redis\", \"serve_num_deployments\": \"1\"}, \"total_num_nodes\": 2, \"total_num_running_jobs\": 2}}\r\n``` </s> remove         assert result == {\"key\": \"val\", \"_test1\": \"val1\", \"_test2\": \"val2\"}\n </s> add         assert result == {\n            \"key\": \"val\",\n            \"_test1\": \"val1\",\n            \"_test2\": \"val2\",\n            \"gcs_storage\": gcs_storage_type,\n        } </s> remove def test_usage_stats_tags(monkeypatch, ray_start_cluster, reset_usage_stats):\n </s> add def test_usage_stats_tags(\n    monkeypatch, ray_start_cluster, reset_usage_stats, gcs_storage_type\n): </s> remove         assert result == {\"key\": \"val\", \"_test1\": \"val1\", \"_test2\": \"val3\"}\n </s> add         assert result == {\n            \"key\": \"val\",\n            \"_test1\": \"val1\",\n            \"_test2\": \"val3\",\n            \"gcs_storage\": gcs_storage_type,\n        } </s> add     # The GCS storage type, which could be memory or redis\n    GCS_STORAGE = auto()\n </s> add @pytest.fixture\ndef gcs_storage_type():\n    storage = \"redis\" if os.environ.get(\"RAY_REDIS_ADDRESS\") else \"memory\"\n    yield storage\n\n </s> remove     monkeypatch, call_ray_start, reset_usage_stats, ray_client\n </s> add     monkeypatch, call_ray_start, reset_usage_stats, ray_client, gcs_storage_type", "html_url": "https://github.com/ray-project/ray/commit/87ce8480ffa9d4cd3f1331cd1a10574f806d77c7", "file_name": "python/ray/tests/test_usage_stats.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> \n <mask>         try:\n <mask>             self.process_runner.check_call([\n <mask>                 KUBECTL_RSYNC,\n <mask>                 flags,\n <mask>                 source,\n </s> Kubernetes rsync verbosity fixed (#11716) </s> add             flags = \"-aqz\" if is_rsync_silent() else \"-avz\" </s> remove                 \"-avz\",\n </s> add                 flags, </s> remove                 \"-avz\",\n </s> add                 flags,", "html_url": "https://github.com/ray-project/ray/commit/8816d34541b9569ad32f655a1208845f0fd6e46a", "file_name": "python/ray/autoscaler/_private/command_runner.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         try:\n <mask>             self.process_runner.check_call([\n <mask>                 KUBECTL_RSYNC,\n <mask>                 \"-avz\",\n <mask>                 source,\n <mask>                 \"{}@{}:{}\".format(self.node_id, self.namespace, target),\n <mask>             ])\n <mask>         except Exception as e:\n <mask>             warnings.warn(\n </s> Kubernetes rsync verbosity fixed (#11716) </s> add             flags = \"-aqz\" if is_rsync_silent() else \"-avz\" </s> add             flags = \"-aqz\" if is_rsync_silent() else \"-avz\" </s> remove                 \"-avz\",\n </s> add                 flags,", "html_url": "https://github.com/ray-project/ray/commit/8816d34541b9569ad32f655a1208845f0fd6e46a", "file_name": "python/ray/autoscaler/_private/command_runner.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>             target = \"/root\" + target[1:]\n <mask> \n <mask>         try:\n <mask>             self.process_runner.check_call([\n <mask>                 KUBECTL_RSYNC,\n <mask>                 flags,\n <mask>                 \"{}@{}:{}\".format(self.node_id, self.namespace, source),\n <mask>                 target,\n <mask>             ])\n </s> Kubernetes rsync verbosity fixed (#11716) </s> add             flags = \"-aqz\" if is_rsync_silent() else \"-avz\" </s> remove                 \"-avz\",\n </s> add                 flags, </s> remove                 \"-avz\",\n </s> add                 flags,", "html_url": "https://github.com/ray-project/ray/commit/8816d34541b9569ad32f655a1208845f0fd6e46a", "file_name": "python/ray/autoscaler/_private/command_runner.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         try:\n <mask>             self.process_runner.check_call([\n <mask>                 KUBECTL_RSYNC,\n <mask>                 \"-avz\",\n <mask>                 \"{}@{}:{}\".format(self.node_id, self.namespace, source),\n <mask>                 target,\n <mask>             ])\n <mask>         except Exception as e:\n <mask>             warnings.warn(\n </s> Kubernetes rsync verbosity fixed (#11716) </s> add             flags = \"-aqz\" if is_rsync_silent() else \"-avz\" </s> add             flags = \"-aqz\" if is_rsync_silent() else \"-avz\" </s> remove                 \"-avz\",\n </s> add                 flags,", "html_url": "https://github.com/ray-project/ray/commit/8816d34541b9569ad32f655a1208845f0fd6e46a", "file_name": "python/ray/autoscaler/_private/command_runner.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask> import os\n <mask> import logging\n <mask> from os.path import dirname\n <mask> import sys\n <mask> \n <mask> logger = logging.getLogger(__name__)\n <mask> \n <mask> # MUST add pickle5 to the import path because it will be imported by some\n <mask> # raylet modules.\n </s> Fix WSL patch (it doesn't fall under \"win32\") (#9195) </s> add if (platform.system() == \"Linux\"\n        and \"Microsoft\".lower() in platform.release().lower()):\n    import ray.compat  # noqa: E402\n    ray.compat.patch_psutil()\n </s> remove     ray.compat.patch_psutil()\n </s> add  </s> remove             import psutil._pslinux\n        except ImportError:\n            psutil = None\n        psutil_open_binary = None\n        if psutil:\n            try:\n                psutil_open_binary = psutil._pslinux.open_binary\n            except AttributeError:\n                pass\n        # Only patch it if it doesn't seem to have been patched already\n        if psutil_open_binary and psutil_open_binary.__name__ == \"open_binary\":\n\n            def psutil_open_binary_patched(fname, *args, **kwargs):\n                f = psutil_open_binary(fname, *args, **kwargs)\n                if fname == \"/proc/meminfo\":\n                    with f:\n                        # Make sure there's a space after colons\n                        return io.BytesIO(f.read().replace(b\":\", b\": \"))\n                return f\n\n            psutil._pslinux.open_binary = psutil_open_binary_patched\n </s> add             psutil_open_binary = psutil._pslinux.open_binary\n        except AttributeError:\n            pass\n    # Only patch it if it doesn't seem to have been patched already\n    if psutil_open_binary and psutil_open_binary.__name__ == \"open_binary\":\n\n        def psutil_open_binary_patched(fname, *args, **kwargs):\n            f = psutil_open_binary(fname, *args, **kwargs)\n            if fname == \"/proc/meminfo\":\n                with f:\n                    # Make sure there's a space after colons\n                    return io.BytesIO(f.read().replace(b\":\", b\": \"))\n            return f\n\n        psutil._pslinux.open_binary = psutil_open_binary_patched </s> remove     if (platform.system() == \"Linux\"\n            and \"Microsoft\".lower() in platform.release().lower()):\n        # WSL's /proc/meminfo has an inconsistency where it\n        # nondeterministically omits a space after colons (after \"SwapFree:\"\n        # in my case).\n        # psutil then splits on spaces and then parses the wrong field,\n        # crashing on the 'int(fields[1])' expression in\n        # psutil._pslinux.virtual_memory().\n        # Workaround: We ensure there is a space following each colon.\n </s> add     \"\"\"WSL's /proc/meminfo has an inconsistency where it\n    nondeterministically omits a space after colons (after \"SwapFree:\"\n    in my case).\n    psutil then splits on spaces and then parses the wrong field,\n    crashing on the 'int(fields[1])' expression in\n    psutil._pslinux.virtual_memory().\n    Workaround: We ensure there is a space following each colon.\n    \"\"\"\n    assert (platform.system() == \"Linux\"\n            and \"Microsoft\".lower() in platform.release().lower())\n\n    try:\n        import psutil._pslinux\n    except ImportError:\n        psutil = None\n    psutil_open_binary = None\n    if psutil:", "html_url": "https://github.com/ray-project/ray/commit/882f60012fca805cad30f0e9e5d281117e10eae4", "file_name": "python/ray/__init__.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> sys.path.insert(0, thirdparty_files)\n <mask> \n <mask> if sys.platform == \"win32\":\n <mask>     import ray.compat  # noqa: E402\n <mask>     ray.compat.patch_psutil()\n <mask>     ray.compat.patch_redis_empty_recv()\n <mask> \n <mask> # Expose ray ABI symbols which may be dependent by other shared\n <mask> # libraries such as _streaming.so. See BUILD.bazel:_raylet\n <mask> python_shared_lib_suffix = \".so\" if sys.platform != \"win32\" else \".pyd\"\n </s> Fix WSL patch (it doesn't fall under \"win32\") (#9195) </s> add if (platform.system() == \"Linux\"\n        and \"Microsoft\".lower() in platform.release().lower()):\n    import ray.compat  # noqa: E402\n    ray.compat.patch_psutil()\n </s> remove             import psutil._pslinux\n        except ImportError:\n            psutil = None\n        psutil_open_binary = None\n        if psutil:\n            try:\n                psutil_open_binary = psutil._pslinux.open_binary\n            except AttributeError:\n                pass\n        # Only patch it if it doesn't seem to have been patched already\n        if psutil_open_binary and psutil_open_binary.__name__ == \"open_binary\":\n\n            def psutil_open_binary_patched(fname, *args, **kwargs):\n                f = psutil_open_binary(fname, *args, **kwargs)\n                if fname == \"/proc/meminfo\":\n                    with f:\n                        # Make sure there's a space after colons\n                        return io.BytesIO(f.read().replace(b\":\", b\": \"))\n                return f\n\n            psutil._pslinux.open_binary = psutil_open_binary_patched\n </s> add             psutil_open_binary = psutil._pslinux.open_binary\n        except AttributeError:\n            pass\n    # Only patch it if it doesn't seem to have been patched already\n    if psutil_open_binary and psutil_open_binary.__name__ == \"open_binary\":\n\n        def psutil_open_binary_patched(fname, *args, **kwargs):\n            f = psutil_open_binary(fname, *args, **kwargs)\n            if fname == \"/proc/meminfo\":\n                with f:\n                    # Make sure there's a space after colons\n                    return io.BytesIO(f.read().replace(b\":\", b\": \"))\n            return f\n\n        psutil._pslinux.open_binary = psutil_open_binary_patched </s> remove     if (platform.system() == \"Linux\"\n            and \"Microsoft\".lower() in platform.release().lower()):\n        # WSL's /proc/meminfo has an inconsistency where it\n        # nondeterministically omits a space after colons (after \"SwapFree:\"\n        # in my case).\n        # psutil then splits on spaces and then parses the wrong field,\n        # crashing on the 'int(fields[1])' expression in\n        # psutil._pslinux.virtual_memory().\n        # Workaround: We ensure there is a space following each colon.\n </s> add     \"\"\"WSL's /proc/meminfo has an inconsistency where it\n    nondeterministically omits a space after colons (after \"SwapFree:\"\n    in my case).\n    psutil then splits on spaces and then parses the wrong field,\n    crashing on the 'int(fields[1])' expression in\n    psutil._pslinux.virtual_memory().\n    Workaround: We ensure there is a space following each colon.\n    \"\"\"\n    assert (platform.system() == \"Linux\"\n            and \"Microsoft\".lower() in platform.release().lower())\n\n    try:\n        import psutil._pslinux\n    except ImportError:\n        psutil = None\n    psutil_open_binary = None\n    if psutil: </s> add import platform", "html_url": "https://github.com/ray-project/ray/commit/882f60012fca805cad30f0e9e5d281117e10eae4", "file_name": "python/ray/__init__.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>     import ray.compat  # noqa: E402\n <mask>     ray.compat.patch_redis_empty_recv()\n <mask> \n <mask> # Expose ray ABI symbols which may be dependent by other shared\n <mask> # libraries such as _streaming.so. See BUILD.bazel:_raylet\n <mask> python_shared_lib_suffix = \".so\" if sys.platform != \"win32\" else \".pyd\"\n <mask> so_path = os.path.join(dirname(__file__), \"_raylet\" + python_shared_lib_suffix)\n <mask> if os.path.exists(so_path):\n <mask>     import ctypes\n </s> Fix WSL patch (it doesn't fall under \"win32\") (#9195) </s> remove     ray.compat.patch_psutil()\n </s> add  </s> add import platform </s> remove             import psutil._pslinux\n        except ImportError:\n            psutil = None\n        psutil_open_binary = None\n        if psutil:\n            try:\n                psutil_open_binary = psutil._pslinux.open_binary\n            except AttributeError:\n                pass\n        # Only patch it if it doesn't seem to have been patched already\n        if psutil_open_binary and psutil_open_binary.__name__ == \"open_binary\":\n\n            def psutil_open_binary_patched(fname, *args, **kwargs):\n                f = psutil_open_binary(fname, *args, **kwargs)\n                if fname == \"/proc/meminfo\":\n                    with f:\n                        # Make sure there's a space after colons\n                        return io.BytesIO(f.read().replace(b\":\", b\": \"))\n                return f\n\n            psutil._pslinux.open_binary = psutil_open_binary_patched\n </s> add             psutil_open_binary = psutil._pslinux.open_binary\n        except AttributeError:\n            pass\n    # Only patch it if it doesn't seem to have been patched already\n    if psutil_open_binary and psutil_open_binary.__name__ == \"open_binary\":\n\n        def psutil_open_binary_patched(fname, *args, **kwargs):\n            f = psutil_open_binary(fname, *args, **kwargs)\n            if fname == \"/proc/meminfo\":\n                with f:\n                    # Make sure there's a space after colons\n                    return io.BytesIO(f.read().replace(b\":\", b\": \"))\n            return f\n\n        psutil._pslinux.open_binary = psutil_open_binary_patched </s> remove     if (platform.system() == \"Linux\"\n            and \"Microsoft\".lower() in platform.release().lower()):\n        # WSL's /proc/meminfo has an inconsistency where it\n        # nondeterministically omits a space after colons (after \"SwapFree:\"\n        # in my case).\n        # psutil then splits on spaces and then parses the wrong field,\n        # crashing on the 'int(fields[1])' expression in\n        # psutil._pslinux.virtual_memory().\n        # Workaround: We ensure there is a space following each colon.\n </s> add     \"\"\"WSL's /proc/meminfo has an inconsistency where it\n    nondeterministically omits a space after colons (after \"SwapFree:\"\n    in my case).\n    psutil then splits on spaces and then parses the wrong field,\n    crashing on the 'int(fields[1])' expression in\n    psutil._pslinux.virtual_memory().\n    Workaround: We ensure there is a space following each colon.\n    \"\"\"\n    assert (platform.system() == \"Linux\"\n            and \"Microsoft\".lower() in platform.release().lower())\n\n    try:\n        import psutil._pslinux\n    except ImportError:\n        psutil = None\n    psutil_open_binary = None\n    if psutil:", "html_url": "https://github.com/ray-project/ray/commit/882f60012fca805cad30f0e9e5d281117e10eae4", "file_name": "python/ray/__init__.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace", "code_tokens": " <mask>     redis.connection.recv = redis_recv\n <mask> \n <mask> \n <mask> def patch_psutil():\n <mask>     if (platform.system() == \"Linux\"\n <mask>             and \"Microsoft\".lower() in platform.release().lower()):\n <mask>         # WSL's /proc/meminfo has an inconsistency where it\n <mask>         # nondeterministically omits a space after colons (after \"SwapFree:\"\n <mask>         # in my case).\n <mask>         # psutil then splits on spaces and then parses the wrong field,\n <mask>         # crashing on the 'int(fields[1])' expression in\n <mask>         # psutil._pslinux.virtual_memory().\n <mask>         # Workaround: We ensure there is a space following each colon.\n <mask>         try:\n <mask>             import psutil._pslinux\n <mask>         except ImportError:\n <mask>             psutil = None\n <mask>         psutil_open_binary = None\n <mask>         if psutil:\n <mask>             try:\n <mask>                 psutil_open_binary = psutil._pslinux.open_binary\n <mask>             except AttributeError:\n <mask>                 pass\n <mask>         # Only patch it if it doesn't seem to have been patched already\n <mask>         if psutil_open_binary and psutil_open_binary.__name__ == \"open_binary\":\n <mask> \n <mask>             def psutil_open_binary_patched(fname, *args, **kwargs):\n <mask>                 f = psutil_open_binary(fname, *args, **kwargs)\n <mask>                 if fname == \"/proc/meminfo\":\n <mask>                     with f:\n <mask>                         # Make sure there's a space after colons\n <mask>                         return io.BytesIO(f.read().replace(b\":\", b\": \"))\n <mask>                 return f\n <mask> \n <mask>             psutil._pslinux.open_binary = psutil_open_binary_patched\n </s> Fix WSL patch (it doesn't fall under \"win32\") (#9195) </s> add if (platform.system() == \"Linux\"\n        and \"Microsoft\".lower() in platform.release().lower()):\n    import ray.compat  # noqa: E402\n    ray.compat.patch_psutil()\n </s> remove     ray.compat.patch_psutil()\n </s> add  </s> add import platform", "html_url": "https://github.com/ray-project/ray/commit/882f60012fca805cad30f0e9e5d281117e10eae4", "file_name": "python/ray/compat.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask> import ray.dashboard.utils as dashboard_utils\n <mask> from ray.dashboard.modules.job.common import (\n <mask>     JobSubmitRequest,\n <mask>     JobSubmitResponse,\n <mask>     JobLogsResponse,\n <mask> )\n <mask> from ray.dashboard.modules.job.job_manager import JobManager\n <mask> from ray.dashboard.modules.job.pydantic_models import JobType\n <mask> from ray.dashboard.modules.job.utils import parse_and_validate_request, find_job_by_ids\n </s> [Job Submission][refactor 4/N] Complete the remaining interfaces on JobAgent (#28533)\n\nSigned-off-by: Catch-Bull <burglarralgrub@gmail.com>\r\njust need to implement stop_job, and I remove get_job_info because we can access JobInfoStorage without call `ray.init`. </s> add         if job.type is not JobType.SUBMISSION:\n            return Response(\n                text=\"Can only stop submission type jobs\",\n                status=aiohttp.web.HTTPBadRequest.status_code,\n            )\n\n        try:\n            stopped = self.get_job_manager().stop_job(job.submission_id)\n            resp = JobStopResponse(stopped=stopped)\n        except Exception:\n            return Response(\n                text=traceback.format_exc(),\n                status=aiohttp.web.HTTPInternalServerError.status_code,\n            ) </s> remove             text=json.dumps(job.dict()),\n            content_type=\"application/json\",\n </s> add             text=json.dumps(dataclasses.asdict(resp)), content_type=\"application/json\" </s> remove     @routes.get(\"/api/job_agent/jobs/{job_or_submission_id}\")\n </s> add     @routes.post(\"/api/job_agent/jobs/{job_or_submission_id}/stop\") </s> remove     async def get_job_info(self, req: Request) -> Response:\n </s> add     async def stop_job(self, req: Request) -> Response: </s> remove                 await self._raise_error(resp)\n </s> add                 self._raise_error(resp) </s> remove                 return JobDetails(**result_json)\n </s> add                 return JobStopResponse(**result_json)", "html_url": "https://github.com/ray-project/ray/commit/8840be1942a69b2595a05c5c5556b0daec7abbcd", "file_name": "dashboard/modules/job/job_agent.py"}
{"docstring_tokens": "keep keep keep keep replace keep replace", "code_tokens": " <mask>             content_type=\"application/json\",\n <mask>             status=aiohttp.web.HTTPOk.status_code,\n <mask>         )\n <mask> \n <mask>     @routes.get(\"/api/job_agent/jobs/{job_or_submission_id}\")\n <mask>     @optional_utils.init_ray_and_catch_exceptions()\n <mask>     async def get_job_info(self, req: Request) -> Response:\n </s> [Job Submission][refactor 4/N] Complete the remaining interfaces on JobAgent (#28533)\n\nSigned-off-by: Catch-Bull <burglarralgrub@gmail.com>\r\njust need to implement stop_job, and I remove get_job_info because we can access JobInfoStorage without call `ray.init`. </s> remove             text=json.dumps(job.dict()),\n            content_type=\"application/json\",\n </s> add             text=json.dumps(dataclasses.asdict(resp)), content_type=\"application/json\" </s> remove     async def get_job_info(self, job_id: str) -> JobDetails:\n        async with self._session.get(\n            f\"{self._agent_address}/api/job_agent/jobs/{job_id}\"\n </s> add     async def stop_job_internal(self, job_id: str) -> JobStopResponse:\n\n        logger.debug(f\"Stopping job with job_id={job_id}.\")\n\n        async with self._session.post(\n            f\"{self._agent_address}/api/job_agent/jobs/{job_id}/stop\" </s> remove                 await self._raise_error(resp)\n </s> add                 self._raise_error(resp) </s> remove                 return JobDetails(**result_json)\n </s> add                 return JobStopResponse(**result_json) </s> add         if job.type is not JobType.SUBMISSION:\n            return Response(\n                text=\"Can only stop submission type jobs\",\n                status=aiohttp.web.HTTPBadRequest.status_code,\n            )\n\n        try:\n            stopped = self.get_job_manager().stop_job(job.submission_id)\n            resp = JobStopResponse(stopped=stopped)\n        except Exception:\n            return Response(\n                text=traceback.format_exc(),\n                status=aiohttp.web.HTTPInternalServerError.status_code,\n            )", "html_url": "https://github.com/ray-project/ray/commit/8840be1942a69b2595a05c5c5556b0daec7abbcd", "file_name": "dashboard/modules/job/job_agent.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>                 text=f\"Job {job_or_submission_id} does not exist\",\n <mask>                 status=aiohttp.web.HTTPNotFound.status_code,\n <mask>             )\n <mask> \n <mask>         return Response(\n <mask>             text=json.dumps(dataclasses.asdict(resp)), content_type=\"application/json\"\n <mask>         )\n </s> [Job Submission][refactor 4/N] Complete the remaining interfaces on JobAgent (#28533)\n\nSigned-off-by: Catch-Bull <burglarralgrub@gmail.com>\r\njust need to implement stop_job, and I remove get_job_info because we can access JobInfoStorage without call `ray.init`. </s> remove             text=json.dumps(job.dict()),\n            content_type=\"application/json\",\n </s> add             text=json.dumps(dataclasses.asdict(resp)), content_type=\"application/json\" </s> remove                 return JobDetails(**result_json)\n </s> add                 return JobStopResponse(**result_json) </s> remove                 await self._raise_error(resp)\n </s> add                 self._raise_error(resp) </s> remove     async def get_job_info(self, job_id: str) -> JobDetails:\n        async with self._session.get(\n            f\"{self._agent_address}/api/job_agent/jobs/{job_id}\"\n </s> add     async def stop_job_internal(self, job_id: str) -> JobStopResponse:\n\n        logger.debug(f\"Stopping job with job_id={job_id}.\")\n\n        async with self._session.post(\n            f\"{self._agent_address}/api/job_agent/jobs/{job_id}/stop\" </s> remove     @routes.get(\"/api/job_agent/jobs/{job_or_submission_id}\")\n </s> add     @routes.post(\"/api/job_agent/jobs/{job_or_submission_id}/stop\") </s> remove     async def get_job_info(self, req: Request) -> Response:\n </s> add     async def stop_job(self, req: Request) -> Response:", "html_url": "https://github.com/ray-project/ray/commit/8840be1942a69b2595a05c5c5556b0daec7abbcd", "file_name": "dashboard/modules/job/job_agent.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>                 status=aiohttp.web.HTTPNotFound.status_code,\n <mask>             )\n <mask> \n <mask>         return Response(\n <mask>             text=json.dumps(job.dict()),\n <mask>             content_type=\"application/json\",\n <mask>         )\n <mask> \n <mask>     @routes.get(\"/api/job_agent/jobs/{job_or_submission_id}/logs\")\n <mask>     @optional_utils.init_ray_and_catch_exceptions()\n <mask>     async def get_job_logs(self, req: Request) -> Response:\n </s> [Job Submission][refactor 4/N] Complete the remaining interfaces on JobAgent (#28533)\n\nSigned-off-by: Catch-Bull <burglarralgrub@gmail.com>\r\njust need to implement stop_job, and I remove get_job_info because we can access JobInfoStorage without call `ray.init`. </s> remove     @routes.get(\"/api/job_agent/jobs/{job_or_submission_id}\")\n </s> add     @routes.post(\"/api/job_agent/jobs/{job_or_submission_id}/stop\") </s> remove     async def get_job_info(self, req: Request) -> Response:\n </s> add     async def stop_job(self, req: Request) -> Response: </s> add         if job.type is not JobType.SUBMISSION:\n            return Response(\n                text=\"Can only stop submission type jobs\",\n                status=aiohttp.web.HTTPBadRequest.status_code,\n            )\n\n        try:\n            stopped = self.get_job_manager().stop_job(job.submission_id)\n            resp = JobStopResponse(stopped=stopped)\n        except Exception:\n            return Response(\n                text=traceback.format_exc(),\n                status=aiohttp.web.HTTPInternalServerError.status_code,\n            ) </s> remove                 return JobDetails(**result_json)\n </s> add                 return JobStopResponse(**result_json) </s> remove                 await self._raise_error(resp)\n </s> add                 self._raise_error(resp) </s> remove     async def get_job_info(self, job_id: str) -> JobDetails:\n        async with self._session.get(\n            f\"{self._agent_address}/api/job_agent/jobs/{job_id}\"\n </s> add     async def stop_job_internal(self, job_id: str) -> JobStopResponse:\n\n        logger.debug(f\"Stopping job with job_id={job_id}.\")\n\n        async with self._session.post(\n            f\"{self._agent_address}/api/job_agent/jobs/{job_id}/stop\"", "html_url": "https://github.com/ray-project/ray/commit/8840be1942a69b2595a05c5c5556b0daec7abbcd", "file_name": "dashboard/modules/job/job_agent.py"}
{"docstring_tokens": "keep keep replace replace replace keep keep keep replace keep keep keep keep", "code_tokens": " <mask>                 await self._raise_error(resp)\n <mask> \n <mask>     async def get_job_info(self, job_id: str) -> JobDetails:\n <mask>         async with self._session.get(\n <mask>             f\"{self._agent_address}/api/job_agent/jobs/{job_id}\"\n <mask>         ) as resp:\n <mask>             if resp.status == 200:\n <mask>                 result_json = await resp.json()\n <mask>                 return JobDetails(**result_json)\n <mask>             else:\n <mask>                 await self._raise_error(resp)\n <mask> \n <mask>     async def get_job_logs(self, job_id: str) -> str:\n </s> [Job Submission][refactor 4/N] Complete the remaining interfaces on JobAgent (#28533)\n\nSigned-off-by: Catch-Bull <burglarralgrub@gmail.com>\r\njust need to implement stop_job, and I remove get_job_info because we can access JobInfoStorage without call `ray.init`. </s> remove                 await self._raise_error(resp)\n </s> add                 self._raise_error(resp) </s> remove     async def get_job_info(self, req: Request) -> Response:\n </s> add     async def stop_job(self, req: Request) -> Response: </s> remove     @routes.get(\"/api/job_agent/jobs/{job_or_submission_id}\")\n </s> add     @routes.post(\"/api/job_agent/jobs/{job_or_submission_id}/stop\") </s> remove             text=json.dumps(job.dict()),\n            content_type=\"application/json\",\n </s> add             text=json.dumps(dataclasses.asdict(resp)), content_type=\"application/json\" </s> add         if job.type is not JobType.SUBMISSION:\n            return Response(\n                text=\"Can only stop submission type jobs\",\n                status=aiohttp.web.HTTPBadRequest.status_code,\n            )\n\n        try:\n            stopped = self.get_job_manager().stop_job(job.submission_id)\n            resp = JobStopResponse(stopped=stopped)\n        except Exception:\n            return Response(\n                text=traceback.format_exc(),\n                status=aiohttp.web.HTTPInternalServerError.status_code,\n            )", "html_url": "https://github.com/ray-project/ray/commit/8840be1942a69b2595a05c5c5556b0daec7abbcd", "file_name": "dashboard/modules/job/job_head.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             if resp.status == 200:\n <mask>                 result_json = await resp.json()\n <mask>                 return JobDetails(**result_json)\n <mask>             else:\n <mask>                 await self._raise_error(resp)\n <mask> \n <mask>     async def get_job_logs(self, job_id: str) -> str:\n <mask>         async with self._session.get(\n <mask>             f\"{self._agent_address}/api/job_agent/jobs/{job_id}/logs\"\n <mask>         ) as resp:\n </s> [Job Submission][refactor 4/N] Complete the remaining interfaces on JobAgent (#28533)\n\nSigned-off-by: Catch-Bull <burglarralgrub@gmail.com>\r\njust need to implement stop_job, and I remove get_job_info because we can access JobInfoStorage without call `ray.init`. </s> remove                 return JobDetails(**result_json)\n </s> add                 return JobStopResponse(**result_json) </s> remove     async def get_job_info(self, job_id: str) -> JobDetails:\n        async with self._session.get(\n            f\"{self._agent_address}/api/job_agent/jobs/{job_id}\"\n </s> add     async def stop_job_internal(self, job_id: str) -> JobStopResponse:\n\n        logger.debug(f\"Stopping job with job_id={job_id}.\")\n\n        async with self._session.post(\n            f\"{self._agent_address}/api/job_agent/jobs/{job_id}/stop\" </s> remove     async def get_job_info(self, req: Request) -> Response:\n </s> add     async def stop_job(self, req: Request) -> Response: </s> remove     @routes.get(\"/api/job_agent/jobs/{job_or_submission_id}\")\n </s> add     @routes.post(\"/api/job_agent/jobs/{job_or_submission_id}/stop\") </s> remove             text=json.dumps(job.dict()),\n            content_type=\"application/json\",\n </s> add             text=json.dumps(dataclasses.asdict(resp)), content_type=\"application/json\" </s> add         if job.type is not JobType.SUBMISSION:\n            return Response(\n                text=\"Can only stop submission type jobs\",\n                status=aiohttp.web.HTTPBadRequest.status_code,\n            )\n\n        try:\n            stopped = self.get_job_manager().stop_job(job.submission_id)\n            resp = JobStopResponse(stopped=stopped)\n        except Exception:\n            return Response(\n                text=traceback.format_exc(),\n                status=aiohttp.web.HTTPInternalServerError.status_code,\n            )", "html_url": "https://github.com/ray-project/ray/commit/8840be1942a69b2595a05c5c5556b0daec7abbcd", "file_name": "dashboard/modules/job/job_head.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>             error_msg = traceback.format_exc()\n <mask>             print(\"Error recovering trial from checkpoint, abort:\", error_msg)\n <mask>             self._stop_trial(trial, error=True, error_msg=error_msg)\n <mask> \n <mask>     def _get_runnable(self):\n <mask>         return self._scheduler_alg.choose_trial_to_run(self)\n <mask> \n <mask>     def _commit_resources(self, resources):\n <mask>         self._committed_resources = Resources(\n <mask>             self._committed_resources.cpu + resources.cpu_total(),\n <mask>             self._committed_resources.gpu + resources.gpu_total())\n <mask> \n </s> [tune] HyperOpt Support (v2) (#1763) </s> remove         the PENDING or PAUSED state.\"\"\"\n </s> add         the PENDING or PAUSED state. This function must be idempotent.\n\n        If no trial is ready, return None.\"\"\" </s> remove     def choose_trial_to_run(self, trial_runner, trials):\n </s> add     def add_experiment(self, experiment, trial_runner):\n        \"\"\"Adds an experiment to the scheduler.\n\n        The scheduler is responsible for adding the trials of the experiment\n        to the runner, which can be done immediately (if there are a finite\n        set of trials), or over time (if there is an infinite stream of trials\n        or if the scheduler is iterative in nature).\n        \"\"\"\n        generator = generate_trials(experiment.spec, experiment.name)\n        while True:\n            try:\n                trial_runner.add_trial(next(generator))\n            except StopIteration:\n                break\n\n    def choose_trial_to_run(self, trial_runner): </s> add     \"HyperOpt\": HyperOptScheduler, </s> remove         scheduler, launch_web_server=with_server, server_port=server_port)\n\n    if type(experiments) is dict:\n        for name, spec in experiments.items():\n            for trial in generate_trials(spec, name):\n                trial.set_verbose(verbose)\n                runner.add_trial(trial)\n    elif (type(experiments) is list and\n          all(isinstance(exp, Experiment) for exp in experiments)):\n        for experiment in experiments:\n            for trial in experiment.trials():\n                trial.set_verbose(verbose)\n                runner.add_trial(trial)\n    elif isinstance(experiments, Experiment):\n        for trial in experiments.trials():\n            trial.set_verbose(verbose)\n            runner.add_trial(trial)\n </s> add         scheduler, launch_web_server=with_server, server_port=server_port,\n        verbose=verbose)\n    exp_list = experiments\n    if isinstance(experiments, Experiment):\n        exp_list = [experiments]\n    elif type(experiments) is dict:\n        exp_list = [Experiment.from_json(name, spec)\n                    for name, spec in experiments.items()]\n\n    if (type(exp_list) is list and\n            all(isinstance(exp, Experiment) for exp in exp_list)):\n        for experiment in exp_list:\n            scheduler.add_experiment(experiment, runner)\n    else:\n        raise TuneError(\"Invalid argument: {}\".format(experiments)) </s> remove             AsyncHyperBand, or HyperBand.\n </s> add             AsyncHyperBand, HyperBand, or HyperOpt. </s> remove from ray.tune.variant_generator import generate_trials\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/888e70f1bee44e6988054631382143cb5939b377", "file_name": "python/ray/tune/trial_runner.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask> from __future__ import print_function\n <mask> \n <mask> from ray.tune.trial import Trial\n <mask> \n <mask> \n <mask> class TrialScheduler(object):\n <mask>     CONTINUE = \"CONTINUE\"\n <mask>     PAUSE = \"PAUSE\"\n </s> [tune] HyperOpt Support (v2) (#1763) </s> remove from ray.tune import TuneError\n </s> add from ray.tune.error import TuneError </s> add from ray.tune.hpo_scheduler import HyperOptScheduler </s> remove from ray.tune.variant_generator import generate_trials\n </s> add  </s> remove         scheduler, launch_web_server=with_server, server_port=server_port)\n\n    if type(experiments) is dict:\n        for name, spec in experiments.items():\n            for trial in generate_trials(spec, name):\n                trial.set_verbose(verbose)\n                runner.add_trial(trial)\n    elif (type(experiments) is list and\n          all(isinstance(exp, Experiment) for exp in experiments)):\n        for experiment in experiments:\n            for trial in experiment.trials():\n                trial.set_verbose(verbose)\n                runner.add_trial(trial)\n    elif isinstance(experiments, Experiment):\n        for trial in experiments.trials():\n            trial.set_verbose(verbose)\n            runner.add_trial(trial)\n </s> add         scheduler, launch_web_server=with_server, server_port=server_port,\n        verbose=verbose)\n    exp_list = experiments\n    if isinstance(experiments, Experiment):\n        exp_list = [experiments]\n    elif type(experiments) is dict:\n        exp_list = [Experiment.from_json(name, spec)\n                    for name, spec in experiments.items()]\n\n    if (type(exp_list) is list and\n            all(isinstance(exp, Experiment) for exp in exp_list)):\n        for experiment in exp_list:\n            scheduler.add_experiment(experiment, runner)\n    else:\n        raise TuneError(\"Invalid argument: {}\".format(experiments)) </s> remove             AsyncHyperBand, or HyperBand.\n </s> add             AsyncHyperBand, HyperBand, or HyperOpt. </s> add     \"HyperOpt\": HyperOptScheduler,", "html_url": "https://github.com/ray-project/ray/commit/888e70f1bee44e6988054631382143cb5939b377", "file_name": "python/ray/tune/trial_scheduler.py"}
{"docstring_tokens": "keep keep replace keep keep keep replace keep keep keep", "code_tokens": " <mask>         raise NotImplementedError\n <mask> \n <mask>     def choose_trial_to_run(self, trial_runner, trials):\n <mask>         \"\"\"Called to choose a new trial to run.\n <mask> \n <mask>         This should return one of the trials in trial_runner that is in\n <mask>         the PENDING or PAUSED state.\"\"\"\n <mask> \n <mask>         raise NotImplementedError\n <mask> \n </s> [tune] HyperOpt Support (v2) (#1763) </s> remove             AsyncHyperBand, or HyperBand.\n </s> add             AsyncHyperBand, HyperBand, or HyperOpt. </s> remove         scheduler, launch_web_server=with_server, server_port=server_port)\n\n    if type(experiments) is dict:\n        for name, spec in experiments.items():\n            for trial in generate_trials(spec, name):\n                trial.set_verbose(verbose)\n                runner.add_trial(trial)\n    elif (type(experiments) is list and\n          all(isinstance(exp, Experiment) for exp in experiments)):\n        for experiment in experiments:\n            for trial in experiment.trials():\n                trial.set_verbose(verbose)\n                runner.add_trial(trial)\n    elif isinstance(experiments, Experiment):\n        for trial in experiments.trials():\n            trial.set_verbose(verbose)\n            runner.add_trial(trial)\n </s> add         scheduler, launch_web_server=with_server, server_port=server_port,\n        verbose=verbose)\n    exp_list = experiments\n    if isinstance(experiments, Experiment):\n        exp_list = [experiments]\n    elif type(experiments) is dict:\n        exp_list = [Experiment.from_json(name, spec)\n                    for name, spec in experiments.items()]\n\n    if (type(exp_list) is list and\n            all(isinstance(exp, Experiment) for exp in exp_list)):\n        for experiment in exp_list:\n            scheduler.add_experiment(experiment, runner)\n    else:\n        raise TuneError(\"Invalid argument: {}\".format(experiments)) </s> remove     def _get_runnable(self):\n        return self._scheduler_alg.choose_trial_to_run(self)\n\n </s> add  </s> add     \"HyperOpt\": HyperOptScheduler, </s> remove from ray.tune.variant_generator import generate_trials\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/888e70f1bee44e6988054631382143cb5939b377", "file_name": "python/ray/tune/trial_scheduler.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> from __future__ import print_function\n <mask> \n <mask> import time\n <mask> \n <mask> from ray.tune import TuneError\n <mask> from ray.tune.hyperband import HyperBandScheduler\n <mask> from ray.tune.async_hyperband import AsyncHyperBandScheduler\n <mask> from ray.tune.median_stopping_rule import MedianStoppingRule\n <mask> from ray.tune.trial import Trial, DEBUG_PRINT_INTERVAL\n <mask> from ray.tune.log_sync import wait_for_log_sync\n </s> [tune] HyperOpt Support (v2) (#1763) </s> add from ray.tune.hpo_scheduler import HyperOptScheduler </s> add from ray.tune.variant_generator import generate_trials </s> remove from ray.tune.variant_generator import generate_trials\n </s> add  </s> remove     def choose_trial_to_run(self, trial_runner, trials):\n </s> add     def add_experiment(self, experiment, trial_runner):\n        \"\"\"Adds an experiment to the scheduler.\n\n        The scheduler is responsible for adding the trials of the experiment\n        to the runner, which can be done immediately (if there are a finite\n        set of trials), or over time (if there is an infinite stream of trials\n        or if the scheduler is iterative in nature).\n        \"\"\"\n        generator = generate_trials(experiment.spec, experiment.name)\n        while True:\n            try:\n                trial_runner.add_trial(next(generator))\n            except StopIteration:\n                break\n\n    def choose_trial_to_run(self, trial_runner): </s> remove         scheduler, launch_web_server=with_server, server_port=server_port)\n\n    if type(experiments) is dict:\n        for name, spec in experiments.items():\n            for trial in generate_trials(spec, name):\n                trial.set_verbose(verbose)\n                runner.add_trial(trial)\n    elif (type(experiments) is list and\n          all(isinstance(exp, Experiment) for exp in experiments)):\n        for experiment in experiments:\n            for trial in experiment.trials():\n                trial.set_verbose(verbose)\n                runner.add_trial(trial)\n    elif isinstance(experiments, Experiment):\n        for trial in experiments.trials():\n            trial.set_verbose(verbose)\n            runner.add_trial(trial)\n </s> add         scheduler, launch_web_server=with_server, server_port=server_port,\n        verbose=verbose)\n    exp_list = experiments\n    if isinstance(experiments, Experiment):\n        exp_list = [experiments]\n    elif type(experiments) is dict:\n        exp_list = [Experiment.from_json(name, spec)\n                    for name, spec in experiments.items()]\n\n    if (type(exp_list) is list and\n            all(isinstance(exp, Experiment) for exp in exp_list)):\n        for experiment in exp_list:\n            scheduler.add_experiment(experiment, runner)\n    else:\n        raise TuneError(\"Invalid argument: {}\".format(experiments)) </s> remove             AsyncHyperBand, or HyperBand.\n </s> add             AsyncHyperBand, HyperBand, or HyperOpt.", "html_url": "https://github.com/ray-project/ray/commit/888e70f1bee44e6988054631382143cb5939b377", "file_name": "python/ray/tune/tune.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask> from ray.tune.hyperband import HyperBandScheduler\n <mask> from ray.tune.async_hyperband import AsyncHyperBandScheduler\n <mask> from ray.tune.median_stopping_rule import MedianStoppingRule\n <mask> from ray.tune.trial import Trial, DEBUG_PRINT_INTERVAL\n <mask> from ray.tune.log_sync import wait_for_log_sync\n <mask> from ray.tune.trial_runner import TrialRunner\n <mask> from ray.tune.trial_scheduler import FIFOScheduler\n <mask> from ray.tune.web_server import TuneServer\n </s> [tune] HyperOpt Support (v2) (#1763) </s> remove from ray.tune import TuneError\n </s> add from ray.tune.error import TuneError </s> remove from ray.tune.variant_generator import generate_trials\n </s> add  </s> add from ray.tune.variant_generator import generate_trials </s> remove         scheduler, launch_web_server=with_server, server_port=server_port)\n\n    if type(experiments) is dict:\n        for name, spec in experiments.items():\n            for trial in generate_trials(spec, name):\n                trial.set_verbose(verbose)\n                runner.add_trial(trial)\n    elif (type(experiments) is list and\n          all(isinstance(exp, Experiment) for exp in experiments)):\n        for experiment in experiments:\n            for trial in experiment.trials():\n                trial.set_verbose(verbose)\n                runner.add_trial(trial)\n    elif isinstance(experiments, Experiment):\n        for trial in experiments.trials():\n            trial.set_verbose(verbose)\n            runner.add_trial(trial)\n </s> add         scheduler, launch_web_server=with_server, server_port=server_port,\n        verbose=verbose)\n    exp_list = experiments\n    if isinstance(experiments, Experiment):\n        exp_list = [experiments]\n    elif type(experiments) is dict:\n        exp_list = [Experiment.from_json(name, spec)\n                    for name, spec in experiments.items()]\n\n    if (type(exp_list) is list and\n            all(isinstance(exp, Experiment) for exp in exp_list)):\n        for experiment in exp_list:\n            scheduler.add_experiment(experiment, runner)\n    else:\n        raise TuneError(\"Invalid argument: {}\".format(experiments)) </s> remove             AsyncHyperBand, or HyperBand.\n </s> add             AsyncHyperBand, HyperBand, or HyperOpt. </s> add     \"HyperOpt\": HyperOptScheduler,", "html_url": "https://github.com/ray-project/ray/commit/888e70f1bee44e6988054631382143cb5939b377", "file_name": "python/ray/tune/tune.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> from ray.tune.log_sync import wait_for_log_sync\n <mask> from ray.tune.trial_runner import TrialRunner\n <mask> from ray.tune.trial_scheduler import FIFOScheduler\n <mask> from ray.tune.web_server import TuneServer\n <mask> from ray.tune.variant_generator import generate_trials\n <mask> from ray.tune.experiment import Experiment\n <mask> \n <mask> \n <mask> _SCHEDULERS = {\n <mask>     \"FIFO\": FIFOScheduler,\n </s> [tune] HyperOpt Support (v2) (#1763) </s> add from ray.tune.hpo_scheduler import HyperOptScheduler </s> add from ray.tune.variant_generator import generate_trials </s> remove from ray.tune import TuneError\n </s> add from ray.tune.error import TuneError </s> remove         scheduler, launch_web_server=with_server, server_port=server_port)\n\n    if type(experiments) is dict:\n        for name, spec in experiments.items():\n            for trial in generate_trials(spec, name):\n                trial.set_verbose(verbose)\n                runner.add_trial(trial)\n    elif (type(experiments) is list and\n          all(isinstance(exp, Experiment) for exp in experiments)):\n        for experiment in experiments:\n            for trial in experiment.trials():\n                trial.set_verbose(verbose)\n                runner.add_trial(trial)\n    elif isinstance(experiments, Experiment):\n        for trial in experiments.trials():\n            trial.set_verbose(verbose)\n            runner.add_trial(trial)\n </s> add         scheduler, launch_web_server=with_server, server_port=server_port,\n        verbose=verbose)\n    exp_list = experiments\n    if isinstance(experiments, Experiment):\n        exp_list = [experiments]\n    elif type(experiments) is dict:\n        exp_list = [Experiment.from_json(name, spec)\n                    for name, spec in experiments.items()]\n\n    if (type(exp_list) is list and\n            all(isinstance(exp, Experiment) for exp in exp_list)):\n        for experiment in exp_list:\n            scheduler.add_experiment(experiment, runner)\n    else:\n        raise TuneError(\"Invalid argument: {}\".format(experiments)) </s> remove             AsyncHyperBand, or HyperBand.\n </s> add             AsyncHyperBand, HyperBand, or HyperOpt. </s> add     \"HyperOpt\": HyperOptScheduler,", "html_url": "https://github.com/ray-project/ray/commit/888e70f1bee44e6988054631382143cb5939b377", "file_name": "python/ray/tune/tune.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask>     \"HyperBand\": HyperBandScheduler,\n <mask>     \"AsyncHyperBand\": AsyncHyperBandScheduler,\n <mask> }\n <mask> \n <mask> \n <mask> def _make_scheduler(args):\n </s> [tune] HyperOpt Support (v2) (#1763) </s> remove     def _get_runnable(self):\n        return self._scheduler_alg.choose_trial_to_run(self)\n\n </s> add  </s> remove         the PENDING or PAUSED state.\"\"\"\n </s> add         the PENDING or PAUSED state. This function must be idempotent.\n\n        If no trial is ready, return None.\"\"\" </s> remove     def choose_trial_to_run(self, trial_runner, trials):\n </s> add     def add_experiment(self, experiment, trial_runner):\n        \"\"\"Adds an experiment to the scheduler.\n\n        The scheduler is responsible for adding the trials of the experiment\n        to the runner, which can be done immediately (if there are a finite\n        set of trials), or over time (if there is an infinite stream of trials\n        or if the scheduler is iterative in nature).\n        \"\"\"\n        generator = generate_trials(experiment.spec, experiment.name)\n        while True:\n            try:\n                trial_runner.add_trial(next(generator))\n            except StopIteration:\n                break\n\n    def choose_trial_to_run(self, trial_runner): </s> remove         scheduler, launch_web_server=with_server, server_port=server_port)\n\n    if type(experiments) is dict:\n        for name, spec in experiments.items():\n            for trial in generate_trials(spec, name):\n                trial.set_verbose(verbose)\n                runner.add_trial(trial)\n    elif (type(experiments) is list and\n          all(isinstance(exp, Experiment) for exp in experiments)):\n        for experiment in experiments:\n            for trial in experiment.trials():\n                trial.set_verbose(verbose)\n                runner.add_trial(trial)\n    elif isinstance(experiments, Experiment):\n        for trial in experiments.trials():\n            trial.set_verbose(verbose)\n            runner.add_trial(trial)\n </s> add         scheduler, launch_web_server=with_server, server_port=server_port,\n        verbose=verbose)\n    exp_list = experiments\n    if isinstance(experiments, Experiment):\n        exp_list = [experiments]\n    elif type(experiments) is dict:\n        exp_list = [Experiment.from_json(name, spec)\n                    for name, spec in experiments.items()]\n\n    if (type(exp_list) is list and\n            all(isinstance(exp, Experiment) for exp in exp_list)):\n        for experiment in exp_list:\n            scheduler.add_experiment(experiment, runner)\n    else:\n        raise TuneError(\"Invalid argument: {}\".format(experiments)) </s> remove             AsyncHyperBand, or HyperBand.\n </s> add             AsyncHyperBand, HyperBand, or HyperOpt. </s> remove from ray.tune.variant_generator import generate_trials\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/888e70f1bee44e6988054631382143cb5939b377", "file_name": "python/ray/tune/tune.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     Args:\n <mask>         experiments (Experiment | list | dict): Experiments to run.\n <mask>         scheduler (TrialScheduler): Scheduler for executing\n <mask>             the experiment. Choose among FIFO (default), MedianStopping,\n <mask>             AsyncHyperBand, or HyperBand.\n <mask>         with_server (bool): Starts a background Tune server. Needed for\n <mask>             using the Client API.\n <mask>         server_port (int): Port number for launching TuneServer.\n <mask>         verbose (bool): How much output should be printed for each trial.\n <mask>     \"\"\"\n </s> [tune] HyperOpt Support (v2) (#1763) </s> remove     def choose_trial_to_run(self, trial_runner, trials):\n </s> add     def add_experiment(self, experiment, trial_runner):\n        \"\"\"Adds an experiment to the scheduler.\n\n        The scheduler is responsible for adding the trials of the experiment\n        to the runner, which can be done immediately (if there are a finite\n        set of trials), or over time (if there is an infinite stream of trials\n        or if the scheduler is iterative in nature).\n        \"\"\"\n        generator = generate_trials(experiment.spec, experiment.name)\n        while True:\n            try:\n                trial_runner.add_trial(next(generator))\n            except StopIteration:\n                break\n\n    def choose_trial_to_run(self, trial_runner): </s> remove         the PENDING or PAUSED state.\"\"\"\n </s> add         the PENDING or PAUSED state. This function must be idempotent.\n\n        If no trial is ready, return None.\"\"\" </s> remove         scheduler, launch_web_server=with_server, server_port=server_port)\n\n    if type(experiments) is dict:\n        for name, spec in experiments.items():\n            for trial in generate_trials(spec, name):\n                trial.set_verbose(verbose)\n                runner.add_trial(trial)\n    elif (type(experiments) is list and\n          all(isinstance(exp, Experiment) for exp in experiments)):\n        for experiment in experiments:\n            for trial in experiment.trials():\n                trial.set_verbose(verbose)\n                runner.add_trial(trial)\n    elif isinstance(experiments, Experiment):\n        for trial in experiments.trials():\n            trial.set_verbose(verbose)\n            runner.add_trial(trial)\n </s> add         scheduler, launch_web_server=with_server, server_port=server_port,\n        verbose=verbose)\n    exp_list = experiments\n    if isinstance(experiments, Experiment):\n        exp_list = [experiments]\n    elif type(experiments) is dict:\n        exp_list = [Experiment.from_json(name, spec)\n                    for name, spec in experiments.items()]\n\n    if (type(exp_list) is list and\n            all(isinstance(exp, Experiment) for exp in exp_list)):\n        for experiment in exp_list:\n            scheduler.add_experiment(experiment, runner)\n    else:\n        raise TuneError(\"Invalid argument: {}\".format(experiments)) </s> add     \"HyperOpt\": HyperOptScheduler, </s> remove from ray.tune.variant_generator import generate_trials\n </s> add  </s> add from ray.tune.hpo_scheduler import HyperOptScheduler", "html_url": "https://github.com/ray-project/ray/commit/888e70f1bee44e6988054631382143cb5939b377", "file_name": "python/ray/tune/tune.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     if scheduler is None:\n <mask>         scheduler = FIFOScheduler()\n <mask> \n <mask>     runner = TrialRunner(\n <mask>         scheduler, launch_web_server=with_server, server_port=server_port)\n <mask> \n <mask>     if type(experiments) is dict:\n <mask>         for name, spec in experiments.items():\n <mask>             for trial in generate_trials(spec, name):\n <mask>                 trial.set_verbose(verbose)\n <mask>                 runner.add_trial(trial)\n <mask>     elif (type(experiments) is list and\n <mask>           all(isinstance(exp, Experiment) for exp in experiments)):\n <mask>         for experiment in experiments:\n <mask>             for trial in experiment.trials():\n <mask>                 trial.set_verbose(verbose)\n <mask>                 runner.add_trial(trial)\n <mask>     elif isinstance(experiments, Experiment):\n <mask>         for trial in experiments.trials():\n <mask>             trial.set_verbose(verbose)\n <mask>             runner.add_trial(trial)\n <mask> \n <mask>     print(runner.debug_string(max_debug=99999))\n <mask> \n <mask>     last_debug = 0\n <mask>     while not runner.is_finished():\n </s> [tune] HyperOpt Support (v2) (#1763) </s> remove     def choose_trial_to_run(self, trial_runner, trials):\n </s> add     def add_experiment(self, experiment, trial_runner):\n        \"\"\"Adds an experiment to the scheduler.\n\n        The scheduler is responsible for adding the trials of the experiment\n        to the runner, which can be done immediately (if there are a finite\n        set of trials), or over time (if there is an infinite stream of trials\n        or if the scheduler is iterative in nature).\n        \"\"\"\n        generator = generate_trials(experiment.spec, experiment.name)\n        while True:\n            try:\n                trial_runner.add_trial(next(generator))\n            except StopIteration:\n                break\n\n    def choose_trial_to_run(self, trial_runner): </s> remove         the PENDING or PAUSED state.\"\"\"\n </s> add         the PENDING or PAUSED state. This function must be idempotent.\n\n        If no trial is ready, return None.\"\"\" </s> remove             AsyncHyperBand, or HyperBand.\n </s> add             AsyncHyperBand, HyperBand, or HyperOpt. </s> remove     def _get_runnable(self):\n        return self._scheduler_alg.choose_trial_to_run(self)\n\n </s> add  </s> add     \"HyperOpt\": HyperOptScheduler, </s> remove from ray.tune.variant_generator import generate_trials\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/888e70f1bee44e6988054631382143cb5939b377", "file_name": "python/ray/tune/tune.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                                     \"name\",\n <mask>                                     \"function\",\n <mask>                                     \"num_return_vals\",\n <mask>                                     \"module\",\n <mask>                                     \"function_export_counter\",\n <mask>                                     \"num_cpus\",\n <mask>                                     \"num_gpus\"])\n <mask>   function_id = photon.ObjectID(function_id_str)\n <mask>   function_name = function_name.decode(\"ascii\")\n <mask>   num_return_vals = int(num_return_vals)\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove   function_export_counter = int(function_export_counter)\n </s> add  </s> remove       function_name = worker.function_names[function_id.id()]\n </s> add       function_name, function_executor = worker.functions[worker.task_driver_id.id()][function_id.id()] </s> remove   worker.function_names[function_id.id()] = function_name\n  worker.num_return_vals[function_id.id()] = num_return_vals\n  worker.function_export_counters[function_id.id()] = function_export_counter\n </s> add  </s> remove   # overwritten if the function is unpickled successfully.\n </s> add   # overwritten if the function is successfully registered. </s> remove         function_id = FunctionID((hashlib.sha256(func_name.encode(\"ascii\")).digest())[:20])\n </s> add         # Compute the function ID as a hash of the function name as well as the\n        # source code. We could in principle hash in the values in the closure\n        # of the function, but that is likely to introduce non-determinism in\n        # the computation of the function ID.\n        function_id_hash = hashlib.sha1()\n        function_id_hash.update(func_name.encode(\"ascii\"))\n        function_id_hash.update(inspect.getsource(func).encode(\"ascii\"))\n        function_id = function_id_hash.digest()\n        assert len(function_id) == 20\n        function_id = FunctionID(function_id) </s> remove   worker.num_return_vals[function_id.id()] = num_return_vals\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep replace keep replace replace replace keep", "code_tokens": " <mask>   num_cpus = int(num_cpus)\n <mask>   num_gpus = int(num_gpus)\n <mask>   module = module.decode(\"ascii\")\n <mask>   function_export_counter = int(function_export_counter)\n <mask> \n <mask>   worker.function_names[function_id.id()] = function_name\n <mask>   worker.num_return_vals[function_id.id()] = num_return_vals\n <mask>   worker.function_export_counters[function_id.id()] = function_export_counter\n <mask>   # This is a placeholder in case the function can't be unpickled. This will be\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove   # overwritten if the function is unpickled successfully.\n </s> add   # overwritten if the function is successfully registered. </s> remove   worker.functions[function_id.id()] = remote(num_return_vals=num_return_vals,\n                                              function_id=function_id,\n                                              num_cpus=num_cpus,\n                                              num_gpus=num_gpus)(lambda *xs: f())\n </s> add   remote_f_placeholder = remote(function_id=function_id)(lambda *xs: f())\n  worker.functions[driver_id][function_id.id()] = (function_name, remote_f_placeholder)\n  worker.function_properties[driver_id][function_id.id()] = (num_return_vals, num_cpus, num_gpus) </s> add     num_cpus (int): The number of CPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver.\n    num_gpus (int): The number of GPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver. </s> remove     # This example is somewhat contrived. It should be successfully pickled, and\n    # then it should throw an exception when it is unpickled. This may depend a\n    # bit on the specifics of our pickler.\n    def reducer(*args):\n      raise Exception(\"There is a problem here.\")\n    class Foo(object):\n      def __init__(self):\n        self.__name__ = \"Foo_object\"\n        self.func_doc = \"\"\n        self.__globals__ = {}\n      def __reduce__(self):\n        return reducer, ()\n      def __call__(self):\n        return\n    f = ray.remote(Foo())\n </s> add     # Create the contents of a temporary Python file.\n    temporary_python_file = \"\"\"\ndef temporary_helper_function():\n  return 1\n\"\"\"\n\n    f = tempfile.NamedTemporaryFile(suffix=\".py\")\n    f.write(temporary_python_file.encode(\"ascii\"))\n    f.flush()\n    directory = os.path.dirname(f.name)\n    # Get the module name and strip \".py\" from the end.\n    module_name = os.path.basename(f.name)[:-3]\n    sys.path.append(directory)\n    module = __import__(module_name)\n\n    # Define a function that closes over this temporary module. This should fail\n    # when it is unpickled.\n    @ray.remote\n    def g():\n      return module.temporary_python_file()\n </s> remove       function_name = worker.function_names[function_id.id()]\n </s> add       function_name, function_executor = worker.functions[worker.task_driver_id.id()][function_id.id()]", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep replace keep keep replace replace replace replace keep", "code_tokens": " <mask>   worker.num_return_vals[function_id.id()] = num_return_vals\n <mask>   worker.function_export_counters[function_id.id()] = function_export_counter\n <mask>   # This is a placeholder in case the function can't be unpickled. This will be\n <mask>   # overwritten if the function is unpickled successfully.\n <mask>   def f():\n <mask>     raise Exception(\"This function was not imported properly.\")\n <mask>   worker.functions[function_id.id()] = remote(num_return_vals=num_return_vals,\n <mask>                                               function_id=function_id,\n <mask>                                               num_cpus=num_cpus,\n <mask>                                               num_gpus=num_gpus)(lambda *xs: f())\n <mask> \n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove   worker.function_names[function_id.id()] = function_name\n  worker.num_return_vals[function_id.id()] = num_return_vals\n  worker.function_export_counters[function_id.id()] = function_export_counter\n </s> add  </s> remove   function_export_counter = int(function_export_counter)\n </s> add  </s> remove     worker.functions[function_id.id()] = remote(num_return_vals=num_return_vals,\n                                                function_id=function_id,\n                                                num_cpus=num_cpus,\n                                                num_gpus=num_gpus)(function)\n </s> add     worker.functions[driver_id][function_id.id()] = (function_name, remote(function_id=function_id)(function)) </s> remove     # This example is somewhat contrived. It should be successfully pickled, and\n    # then it should throw an exception when it is unpickled. This may depend a\n    # bit on the specifics of our pickler.\n    def reducer(*args):\n      raise Exception(\"There is a problem here.\")\n    class Foo(object):\n      def __init__(self):\n        self.__name__ = \"Foo_object\"\n        self.func_doc = \"\"\n        self.__globals__ = {}\n      def __reduce__(self):\n        return reducer, ()\n      def __call__(self):\n        return\n    f = ray.remote(Foo())\n </s> add     # Create the contents of a temporary Python file.\n    temporary_python_file = \"\"\"\ndef temporary_helper_function():\n  return 1\n\"\"\"\n\n    f = tempfile.NamedTemporaryFile(suffix=\".py\")\n    f.write(temporary_python_file.encode(\"ascii\"))\n    f.flush()\n    directory = os.path.dirname(f.name)\n    # Get the module name and strip \".py\" from the end.\n    module_name = os.path.basename(f.name)[:-3]\n    sys.path.append(directory)\n    module = __import__(module_name)\n\n    # Define a function that closes over this temporary module. This should fail\n    # when it is unpickled.\n    @ray.remote\n    def g():\n      return module.temporary_python_file()\n </s> add     num_cpus (int): The number of CPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver.\n    num_gpus (int): The number of GPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver.", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>                                       \"function_name\": function_name})\n <mask>   else:\n <mask>     # TODO(rkn): Why is the below line necessary?\n <mask>     function.__module__ = module\n <mask>     worker.functions[function_id.id()] = remote(num_return_vals=num_return_vals,\n <mask>                                                 function_id=function_id,\n <mask>                                                 num_cpus=num_cpus,\n <mask>                                                 num_gpus=num_gpus)(function)\n <mask>     # Add the function to the function table.\n <mask>     worker.redis_client.rpush(\"FunctionTable:{}\".format(function_id.id()), worker.worker_id)\n <mask> \n <mask> def fetch_and_register_environment_variable(key, worker=global_worker):\n <mask>   \"\"\"Import an environment variable.\"\"\"\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove   # overwritten if the function is unpickled successfully.\n </s> add   # overwritten if the function is successfully registered. </s> remove   worker.functions[function_id.id()] = remote(num_return_vals=num_return_vals,\n                                              function_id=function_id,\n                                              num_cpus=num_cpus,\n                                              num_gpus=num_gpus)(lambda *xs: f())\n </s> add   remote_f_placeholder = remote(function_id=function_id)(lambda *xs: f())\n  worker.functions[driver_id][function_id.id()] = (function_name, remote_f_placeholder)\n  worker.function_properties[driver_id][function_id.id()] = (num_return_vals, num_cpus, num_gpus) </s> remove   worker.function_names[function_id.id()] = function_name\n  worker.num_return_vals[function_id.id()] = num_return_vals\n  worker.function_export_counters[function_id.id()] = function_export_counter\n </s> add  </s> remove     # This example is somewhat contrived. It should be successfully pickled, and\n    # then it should throw an exception when it is unpickled. This may depend a\n    # bit on the specifics of our pickler.\n    def reducer(*args):\n      raise Exception(\"There is a problem here.\")\n    class Foo(object):\n      def __init__(self):\n        self.__name__ = \"Foo_object\"\n        self.func_doc = \"\"\n        self.__globals__ = {}\n      def __reduce__(self):\n        return reducer, ()\n      def __call__(self):\n        return\n    f = ray.remote(Foo())\n </s> add     # Create the contents of a temporary Python file.\n    temporary_python_file = \"\"\"\ndef temporary_helper_function():\n  return 1\n\"\"\"\n\n    f = tempfile.NamedTemporaryFile(suffix=\".py\")\n    f.write(temporary_python_file.encode(\"ascii\"))\n    f.flush()\n    directory = os.path.dirname(f.name)\n    # Get the module name and strip \".py\" from the end.\n    module_name = os.path.basename(f.name)[:-3]\n    sys.path.append(directory)\n    module = __import__(module_name)\n\n    # Define a function that closes over this temporary module. This should fail\n    # when it is unpickled.\n    @ray.remote\n    def g():\n      return module.temporary_python_file()\n </s> remove     self.assertTrue(b\"There is a problem here.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"No module named\", ray.error_info()[0][b\"message\"])\n    self.assertIn(b\"No module named\", ray.error_info()[1][b\"message\"]) </s> remove         function_id = FunctionID((hashlib.sha256(func_name.encode(\"ascii\")).digest())[:20])\n </s> add         # Compute the function ID as a hash of the function name as well as the\n        # source code. We could in principle hash in the values in the closure\n        # of the function, but that is likely to introduce non-determinism in\n        # the computation of the function ID.\n        function_id_hash = hashlib.sha1()\n        function_id_hash.update(func_name.encode(\"ascii\"))\n        function_id_hash.update(inspect.getsource(func).encode(\"ascii\"))\n        function_id = function_id_hash.digest()\n        assert len(function_id) == 20\n        function_id = FunctionID(function_id)", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>   # and before the call to import_pubsub_client.listen will still be processed\n <mask>   # in the loop.\n <mask>   worker.import_pubsub_client.psubscribe(\"__keyspace@0__:Exports\")\n <mask>   worker_info_key = \"WorkerInfo:{}\".format(worker.worker_id)\n <mask>   worker.redis_client.hset(worker_info_key, \"export_counter\", 0)\n <mask>   worker.worker_import_counter = 0\n <mask>   # The number of imports is similar to the worker_import_counter except that it\n <mask>   # also counts actors.\n <mask>   num_imported = 0\n <mask> \n <mask>   # Get the exports that occurred before the call to psubscribe.\n <mask>   with worker.lock:\n <mask>     export_keys = worker.redis_client.lrange(\"Exports\", 0, -1)\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove     # Check that the number of imports we have is at least as great as the\n    # export counter for the task. If not, wait until we have imported enough.\n    # We will push warnings to the user if we spend too long in this loop.\n    with log_span(\"ray:wait_for_import_counter\", worker=worker):\n      wait_for_valid_import_counter(function_id, task.driver_id().id(), worker=worker)\n </s> add     # Wait until the function to be executed has actually been registered on\n    # this worker. We will push warnings to the user if we spend too long in\n    # this loop.\n    with log_span(\"ray:wait_for_function\", worker=worker):\n      wait_for_function(function_id, task.driver_id().id(), worker=worker) </s> remove     # This example is somewhat contrived. It should be successfully pickled, and\n    # then it should throw an exception when it is unpickled. This may depend a\n    # bit on the specifics of our pickler.\n    def reducer(*args):\n      raise Exception(\"There is a problem here.\")\n    class Foo(object):\n      def __init__(self):\n        self.__name__ = \"Foo_object\"\n        self.func_doc = \"\"\n        self.__globals__ = {}\n      def __reduce__(self):\n        return reducer, ()\n      def __call__(self):\n        return\n    f = ray.remote(Foo())\n </s> add     # Create the contents of a temporary Python file.\n    temporary_python_file = \"\"\"\ndef temporary_helper_function():\n  return 1\n\"\"\"\n\n    f = tempfile.NamedTemporaryFile(suffix=\".py\")\n    f.write(temporary_python_file.encode(\"ascii\"))\n    f.flush()\n    directory = os.path.dirname(f.name)\n    # Get the module name and strip \".py\" from the end.\n    module_name = os.path.basename(f.name)[:-3]\n    sys.path.append(directory)\n    module = __import__(module_name)\n\n    # Define a function that closes over this temporary module. This should fail\n    # when it is unpickled.\n    @ray.remote\n    def g():\n      return module.temporary_python_file()\n </s> remove     subprocess.Popen([stop_ray_script]).wait()\n </s> add   def testRemoteFunctionIsolation(self):\n    # This test will run multiple remote functions with the same names in two\n    # different drivers.\n    # Connect a driver to the Ray cluster.\n    ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE)\n\n    # Start another driver and make sure that it can define and call its own\n    # commands with the same names.\n    driver_script = \"\"\"\nimport ray\nimport time\nray.init(redis_address=\"{}\")\n@ray.remote\ndef f():\n  return 3\n@ray.remote\ndef g(x, y):\n  return 4\nfor _ in range(10000):\n  result = ray.get([f.remote(), g.remote(0, 0)])\n  assert result == [3, 4]\nprint(\"success\")\n\"\"\".format(self.redis_address)\n\n    # Save the driver script as a file so we can call it using subprocess.\n    with tempfile.NamedTemporaryFile() as f:\n      f.write(driver_script.encode(\"ascii\"))\n      f.flush()\n      out = subprocess.check_output([\"python\", f.name]).decode(\"ascii\")\n\n    @ray.remote\n    def f():\n      return 1\n\n    @ray.remote\n    def g(x):\n      return 2\n\n    for _ in range(10000):\n      result = ray.get([f.remote(), g.remote(0)])\n      self.assertEqual(result, [1, 2])\n\n    # Make sure the other driver succeeded.\n    self.assertIn(\"success\", out)\n\n    ray.worker.cleanup() </s> remove       function_name = worker.function_names[function_id.id()]\n </s> add       function_name, function_executor = worker.functions[worker.task_driver_id.id()][function_id.id()] </s> remove     redis_address = redis_address.split(\"\\\"\")[0]\n </s> add     self.redis_address = redis_address.split(\"\\\"\")[0]\n\n  def tearDown(self):\n    # Kill the Ray cluster.\n    subprocess.Popen([stop_ray_script]).wait()\n\n  def testErrorIsolation(self): </s> remove     self.assertTrue(b\"The initializer failed.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"The initializer failed.\", ray.error_info()[0][b\"message\"])", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         if worker.actor_id == actor_id:\n <mask>           worker.fetch_and_register[\"Actor\"](key, worker)\n <mask>       else:\n <mask>         raise Exception(\"This code should be unreachable.\")\n <mask>       # Actors do not contribute to the import counter.\n <mask>       if not key.startswith(b\"Actor\"):\n <mask>         worker.redis_client.hincrby(worker_info_key, \"export_counter\", 1)\n <mask>         worker.worker_import_counter += 1\n <mask>       num_imported += 1\n <mask> \n <mask>   for msg in worker.import_pubsub_client.listen():\n <mask>     with worker.lock:\n <mask>       if msg[\"type\"] == \"psubscribe\":\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove         # Actors do not contribute to the import counter.\n        if not key.startswith(b\"Actor\"):\n          worker.redis_client.hincrby(worker_info_key, \"export_counter\", 1)\n          worker.worker_import_counter += 1\n </s> add  </s> remove       if worker.actor_id == NIL_ACTOR_ID and function_id.id() in worker.functions and (worker.function_export_counters[function_id.id()] <= worker.worker_import_counter):\n </s> add       if worker.actor_id == NIL_ACTOR_ID and function_id.id() in worker.functions[driver_id]: </s> remove   worker.driver_export_counter += 1\n </s> add  </s> remove   worker.redis_client.hset(worker_info_key, \"export_counter\", 0)\n  worker.worker_import_counter = 0\n  # The number of imports is similar to the worker_import_counter except that it\n  # also counts actors.\n </s> add   # Keep track of the number of imports that we've imported. </s> remove     subprocess.Popen([stop_ray_script]).wait()\n </s> add   def testRemoteFunctionIsolation(self):\n    # This test will run multiple remote functions with the same names in two\n    # different drivers.\n    # Connect a driver to the Ray cluster.\n    ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE)\n\n    # Start another driver and make sure that it can define and call its own\n    # commands with the same names.\n    driver_script = \"\"\"\nimport ray\nimport time\nray.init(redis_address=\"{}\")\n@ray.remote\ndef f():\n  return 3\n@ray.remote\ndef g(x, y):\n  return 4\nfor _ in range(10000):\n  result = ray.get([f.remote(), g.remote(0, 0)])\n  assert result == [3, 4]\nprint(\"success\")\n\"\"\".format(self.redis_address)\n\n    # Save the driver script as a file so we can call it using subprocess.\n    with tempfile.NamedTemporaryFile() as f:\n      f.write(driver_script.encode(\"ascii\"))\n      f.flush()\n      out = subprocess.check_output([\"python\", f.name]).decode(\"ascii\")\n\n    @ray.remote\n    def f():\n      return 1\n\n    @ray.remote\n    def g(x):\n      return 2\n\n    for _ in range(10000):\n      result = ray.get([f.remote(), g.remote(0)])\n      self.assertEqual(result, [1, 2])\n\n    # Make sure the other driver succeeded.\n    self.assertIn(\"success\", out)\n\n    ray.worker.cleanup() </s> remove         arguments = get_arguments_for_execution(worker.functions[function_id.id()], args, worker)\n </s> add         arguments = get_arguments_for_execution(function_name, args, worker)", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>           if worker.actor_id == actor_id:\n <mask>             worker.fetch_and_register[\"Actor\"](key, worker)\n <mask>         else:\n <mask>           raise Exception(\"This code should be unreachable.\")\n <mask>         # Actors do not contribute to the import counter.\n <mask>         if not key.startswith(b\"Actor\"):\n <mask>           worker.redis_client.hincrby(worker_info_key, \"export_counter\", 1)\n <mask>           worker.worker_import_counter += 1\n <mask>         num_imported += 1\n <mask> \n <mask> def connect(info, object_id_seed=None, mode=WORKER_MODE, worker=global_worker, actor_id=NIL_ACTOR_ID):\n <mask>   \"\"\"Connect this worker to the local scheduler, to Plasma, and to Redis.\n <mask> \n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove       # Actors do not contribute to the import counter.\n      if not key.startswith(b\"Actor\"):\n        worker.redis_client.hincrby(worker_info_key, \"export_counter\", 1)\n        worker.worker_import_counter += 1\n </s> add  </s> remove   worker.driver_export_counter += 1\n </s> add  </s> remove   worker.redis_client.hset(worker_info_key, \"export_counter\", 0)\n  worker.worker_import_counter = 0\n  # The number of imports is similar to the worker_import_counter except that it\n  # also counts actors.\n </s> add   # Keep track of the number of imports that we've imported. </s> remove         raise RayGetArgumentError(function.__name__, i, arg, argument)\n </s> add         raise RayGetArgumentError(function_name, i, arg, argument) </s> remove     # This example is somewhat contrived. It should be successfully pickled, and\n    # then it should throw an exception when it is unpickled. This may depend a\n    # bit on the specifics of our pickler.\n    def reducer(*args):\n      raise Exception(\"There is a problem here.\")\n    class Foo(object):\n      def __init__(self):\n        self.__name__ = \"Foo_object\"\n        self.func_doc = \"\"\n        self.__globals__ = {}\n      def __reduce__(self):\n        return reducer, ()\n      def __call__(self):\n        return\n    f = ray.remote(Foo())\n </s> add     # Create the contents of a temporary Python file.\n    temporary_python_file = \"\"\"\ndef temporary_helper_function():\n  return 1\n\"\"\"\n\n    f = tempfile.NamedTemporaryFile(suffix=\".py\")\n    f.write(temporary_python_file.encode(\"ascii\"))\n    f.flush()\n    directory = os.path.dirname(f.name)\n    # Get the module name and strip \".py\" from the end.\n    module_name = os.path.basename(f.name)[:-3]\n    sys.path.append(directory)\n    module = __import__(module_name)\n\n    # Define a function that closes over this temporary module. This should fail\n    # when it is unpickled.\n    @ray.remote\n    def g():\n      return module.temporary_python_file()\n </s> remove                                   \"function_export_counter\": worker.driver_export_counter,\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>     ready_ids = [photon.ObjectID(object_id) for object_id in ready_ids]\n <mask>     remaining_ids = [photon.ObjectID(object_id) for object_id in remaining_ids]\n <mask>     return ready_ids, remaining_ids\n <mask> \n <mask> def wait_for_valid_import_counter(function_id, driver_id, timeout=5, worker=global_worker):\n <mask>   \"\"\"Wait until this worker has imported enough to execute the function.\n <mask> \n <mask>   This method will simply loop until the import thread has imported enough of\n <mask>   the exports to execute the function. If we spend too long in this loop, that\n <mask>   may indicate a problem somewhere and we will push an error message to the\n <mask>   user.\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove   This method will simply loop until the import thread has imported enough of\n  the exports to execute the function. If we spend too long in this loop, that\n  may indicate a problem somewhere and we will push an error message to the\n  user.\n </s> add   This method will simply loop until the import thread has imported the relevant\n  function. If we spend too long in this loop, that may indicate a problem\n  somewhere and we will push an error message to the user. </s> remove     # Check that the number of imports we have is at least as great as the\n    # export counter for the task. If not, wait until we have imported enough.\n    # We will push warnings to the user if we spend too long in this loop.\n    with log_span(\"ray:wait_for_import_counter\", worker=worker):\n      wait_for_valid_import_counter(function_id, task.driver_id().id(), worker=worker)\n </s> add     # Wait until the function to be executed has actually been registered on\n    # this worker. We will push warnings to the user if we spend too long in\n    # this loop.\n    with log_span(\"ray:wait_for_function\", worker=worker):\n      wait_for_function(function_id, task.driver_id().id(), worker=worker) </s> add     num_cpus (int): The number of CPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver.\n    num_gpus (int): The number of GPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver. </s> remove def get_arguments_for_execution(function, serialized_args, worker=global_worker):\n </s> add def get_arguments_for_execution(function_name, serialized_args, worker=global_worker): </s> remove   worker.functions[function_id.id()] = remote(num_return_vals=num_return_vals,\n                                              function_id=function_id,\n                                              num_cpus=num_cpus,\n                                              num_gpus=num_gpus)(lambda *xs: f())\n </s> add   remote_f_placeholder = remote(function_id=function_id)(lambda *xs: f())\n  worker.functions[driver_id][function_id.id()] = (function_name, remote_f_placeholder)\n  worker.function_properties[driver_id][function_id.id()] = (num_return_vals, num_cpus, num_gpus) </s> remove     # This example is somewhat contrived. It should be successfully pickled, and\n    # then it should throw an exception when it is unpickled. This may depend a\n    # bit on the specifics of our pickler.\n    def reducer(*args):\n      raise Exception(\"There is a problem here.\")\n    class Foo(object):\n      def __init__(self):\n        self.__name__ = \"Foo_object\"\n        self.func_doc = \"\"\n        self.__globals__ = {}\n      def __reduce__(self):\n        return reducer, ()\n      def __call__(self):\n        return\n    f = ray.remote(Foo())\n </s> add     # Create the contents of a temporary Python file.\n    temporary_python_file = \"\"\"\ndef temporary_helper_function():\n  return 1\n\"\"\"\n\n    f = tempfile.NamedTemporaryFile(suffix=\".py\")\n    f.write(temporary_python_file.encode(\"ascii\"))\n    f.flush()\n    directory = os.path.dirname(f.name)\n    # Get the module name and strip \".py\" from the end.\n    module_name = os.path.basename(f.name)[:-3]\n    sys.path.append(directory)\n    module = __import__(module_name)\n\n    # Define a function that closes over this temporary module. This should fail\n    # when it is unpickled.\n    @ray.remote\n    def g():\n      return module.temporary_python_file()\n", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> def wait_for_valid_import_counter(function_id, driver_id, timeout=5, worker=global_worker):\n <mask>   \"\"\"Wait until this worker has imported enough to execute the function.\n <mask> \n <mask>   This method will simply loop until the import thread has imported enough of\n <mask>   the exports to execute the function. If we spend too long in this loop, that\n <mask>   may indicate a problem somewhere and we will push an error message to the\n <mask>   user.\n <mask> \n <mask>   If this worker is an actor, then this will wait until the actor has been\n <mask>   defined.\n <mask> \n <mask>   Args:\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove def wait_for_valid_import_counter(function_id, driver_id, timeout=5, worker=global_worker):\n  \"\"\"Wait until this worker has imported enough to execute the function.\n </s> add def wait_for_function(function_id, driver_id, timeout=5, worker=global_worker):\n  \"\"\"Wait until the function to be executed is present on this worker. </s> remove     # Check that the number of imports we have is at least as great as the\n    # export counter for the task. If not, wait until we have imported enough.\n    # We will push warnings to the user if we spend too long in this loop.\n    with log_span(\"ray:wait_for_import_counter\", worker=worker):\n      wait_for_valid_import_counter(function_id, task.driver_id().id(), worker=worker)\n </s> add     # Wait until the function to be executed has actually been registered on\n    # this worker. We will push warnings to the user if we spend too long in\n    # this loop.\n    with log_span(\"ray:wait_for_function\", worker=worker):\n      wait_for_function(function_id, task.driver_id().id(), worker=worker) </s> add     num_cpus (int): The number of CPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver.\n    num_gpus (int): The number of GPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver. </s> remove         raise RayGetArgumentError(function.__name__, i, arg, argument)\n </s> add         raise RayGetArgumentError(function_name, i, arg, argument) </s> remove def get_arguments_for_execution(function, serialized_args, worker=global_worker):\n </s> add def get_arguments_for_execution(function_name, serialized_args, worker=global_worker): </s> remove   worker.functions[function_id.id()] = remote(num_return_vals=num_return_vals,\n                                              function_id=function_id,\n                                              num_cpus=num_cpus,\n                                              num_gpus=num_gpus)(lambda *xs: f())\n </s> add   remote_f_placeholder = remote(function_id=function_id)(lambda *xs: f())\n  worker.functions[driver_id][function_id.id()] = (function_name, remote_f_placeholder)\n  worker.function_properties[driver_id][function_id.id()] = (num_return_vals, num_cpus, num_gpus)", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   warning_sent = False\n <mask>   num_warnings_sent = 0\n <mask>   while True:\n <mask>     with worker.lock:\n <mask>       if worker.actor_id == NIL_ACTOR_ID and function_id.id() in worker.functions and (worker.function_export_counters[function_id.id()] <= worker.worker_import_counter):\n <mask>         break\n <mask>       elif worker.actor_id != NIL_ACTOR_ID and worker.actor_id in worker.actors:\n <mask>         break\n <mask>       if time.time() - start_time > timeout * (num_warnings_sent + 1):\n <mask>         if function_id.id() not in worker.functions:\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove         if function_id.id() not in worker.functions:\n          warning_message = \"This worker was asked to execute a function that it does not have registered. You may have to restart Ray.\"\n        else:\n          warning_message = \"This worker's import counter is too small.\"\n </s> add         warning_message = \"This worker was asked to execute a function that it does not have registered. You may have to restart Ray.\" </s> remove       # Actors do not contribute to the import counter.\n      if not key.startswith(b\"Actor\"):\n        worker.redis_client.hincrby(worker_info_key, \"export_counter\", 1)\n        worker.worker_import_counter += 1\n </s> add  </s> remove         objectids = _submit_task(function_id, func_name, args, num_cpus, num_gpus)\n </s> add         objectids = _submit_task(function_id, func_name, args) </s> remove   worker.redis_client.hset(worker_info_key, \"export_counter\", 0)\n  worker.worker_import_counter = 0\n  # The number of imports is similar to the worker_import_counter except that it\n  # also counts actors.\n </s> add   # Keep track of the number of imports that we've imported. </s> remove         # Actors do not contribute to the import counter.\n        if not key.startswith(b\"Actor\"):\n          worker.redis_client.hincrby(worker_info_key, \"export_counter\", 1)\n          worker.worker_import_counter += 1\n </s> add  </s> remove           outputs = worker.functions[task.function_id().id()](worker.actors[task.actor_id().id()], *arguments)\n </s> add           outputs = function_executor(worker.actors[task.actor_id().id()], *arguments)", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep replace replace replace replace keep replace keep keep keep", "code_tokens": " <mask>       elif worker.actor_id != NIL_ACTOR_ID and worker.actor_id in worker.actors:\n <mask>         break\n <mask>       if time.time() - start_time > timeout * (num_warnings_sent + 1):\n <mask>         if function_id.id() not in worker.functions:\n <mask>           warning_message = \"This worker was asked to execute a function that it does not have registered. You may have to restart Ray.\"\n <mask>         else:\n <mask>           warning_message = \"This worker's import counter is too small.\"\n <mask>         if not warning_sent:\n <mask>           worker.push_error_to_driver(driver_id, \"import_counter\",\n <mask>                                       warning_message)\n <mask>         warning_sent = True\n <mask>     time.sleep(0.001)\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove       if worker.actor_id == NIL_ACTOR_ID and function_id.id() in worker.functions and (worker.function_export_counters[function_id.id()] <= worker.worker_import_counter):\n </s> add       if worker.actor_id == NIL_ACTOR_ID and function_id.id() in worker.functions[driver_id]: </s> remove     # Check that the number of imports we have is at least as great as the\n    # export counter for the task. If not, wait until we have imported enough.\n    # We will push warnings to the user if we spend too long in this loop.\n    with log_span(\"ray:wait_for_import_counter\", worker=worker):\n      wait_for_valid_import_counter(function_id, task.driver_id().id(), worker=worker)\n </s> add     # Wait until the function to be executed has actually been registered on\n    # this worker. We will push warnings to the user if we spend too long in\n    # this loop.\n    with log_span(\"ray:wait_for_function\", worker=worker):\n      wait_for_function(function_id, task.driver_id().id(), worker=worker) </s> remove def wait_for_valid_import_counter(function_id, driver_id, timeout=5, worker=global_worker):\n  \"\"\"Wait until this worker has imported enough to execute the function.\n </s> add def wait_for_function(function_id, driver_id, timeout=5, worker=global_worker):\n  \"\"\"Wait until the function to be executed is present on this worker. </s> remove   # overwritten if the function is unpickled successfully.\n </s> add   # overwritten if the function is successfully registered. </s> remove       # Actors do not contribute to the import counter.\n      if not key.startswith(b\"Actor\"):\n        worker.redis_client.hincrby(worker_info_key, \"export_counter\", 1)\n        worker.worker_import_counter += 1\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep replace keep keep keep replace keep keep keep", "code_tokens": " <mask>       return_object_ids = task.returns()\n <mask>       function_name = worker.function_names[function_id.id()]\n <mask> \n <mask>       # Get task arguments from the object store.\n <mask>       with log_span(\"ray:task:get_arguments\", worker=worker):\n <mask>         arguments = get_arguments_for_execution(worker.functions[function_id.id()], args, worker)\n <mask> \n <mask>       # Execute the task.\n <mask>       with log_span(\"ray:task:execute\", worker=worker):\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove           outputs = worker.functions[task.function_id().id()].executor(arguments)\n </s> add           outputs = function_executor.executor(arguments) </s> remove           outputs = worker.functions[task.function_id().id()](worker.actors[task.actor_id().id()], *arguments)\n </s> add           outputs = function_executor(worker.actors[task.actor_id().id()], *arguments) </s> remove     # Check that the number of imports we have is at least as great as the\n    # export counter for the task. If not, wait until we have imported enough.\n    # We will push warnings to the user if we spend too long in this loop.\n    with log_span(\"ray:wait_for_import_counter\", worker=worker):\n      wait_for_valid_import_counter(function_id, task.driver_id().id(), worker=worker)\n </s> add     # Wait until the function to be executed has actually been registered on\n    # this worker. We will push warnings to the user if we spend too long in\n    # this loop.\n    with log_span(\"ray:wait_for_function\", worker=worker):\n      wait_for_function(function_id, task.driver_id().id(), worker=worker) </s> remove       contents = {\"function_name\": worker.function_names[function_id.id()],\n </s> add       function_name, _ = worker.functions[task.driver_id().id()][function_id.id()]\n      contents = {\"function_name\": function_name, </s> remove   def testErrorIsolation(self):\n </s> add   def setUp(self):", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep replace keep replace keep keep keep keep", "code_tokens": " <mask>       with log_span(\"ray:task:execute\", worker=worker):\n <mask>         if task.actor_id().id() == NIL_ACTOR_ID:\n <mask>           outputs = worker.functions[task.function_id().id()].executor(arguments)\n <mask>         else:\n <mask>           outputs = worker.functions[task.function_id().id()](worker.actors[task.actor_id().id()], *arguments)\n <mask> \n <mask>       # Store the outputs in the local object store.\n <mask>       with log_span(\"ray:task:store_outputs\", worker=worker):\n <mask>         if len(return_object_ids) == 1:\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove         arguments = get_arguments_for_execution(worker.functions[function_id.id()], args, worker)\n </s> add         arguments = get_arguments_for_execution(function_name, args, worker) </s> remove       function_name = worker.function_names[function_id.id()]\n </s> add       function_name, function_executor = worker.functions[worker.task_driver_id.id()][function_id.id()] </s> remove       contents = {\"function_name\": worker.function_names[function_id.id()],\n </s> add       function_name, _ = worker.functions[task.driver_id().id()][function_id.id()]\n      contents = {\"function_name\": function_name, </s> remove     # Check that the number of imports we have is at least as great as the\n    # export counter for the task. If not, wait until we have imported enough.\n    # We will push warnings to the user if we spend too long in this loop.\n    with log_span(\"ray:wait_for_import_counter\", worker=worker):\n      wait_for_valid_import_counter(function_id, task.driver_id().id(), worker=worker)\n </s> add     # Wait until the function to be executed has actually been registered on\n    # this worker. We will push warnings to the user if we spend too long in\n    # this loop.\n    with log_span(\"ray:wait_for_function\", worker=worker):\n      wait_for_function(function_id, task.driver_id().id(), worker=worker) </s> remove       # Actors do not contribute to the import counter.\n      if not key.startswith(b\"Actor\"):\n        worker.redis_client.hincrby(worker_info_key, \"export_counter\", 1)\n        worker.worker_import_counter += 1\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     with log_span(\"ray:get_task\", worker=worker):\n <mask>       task = worker.photon_client.get_task()\n <mask> \n <mask>     function_id = task.function_id()\n <mask>     # Check that the number of imports we have is at least as great as the\n <mask>     # export counter for the task. If not, wait until we have imported enough.\n <mask>     # We will push warnings to the user if we spend too long in this loop.\n <mask>     with log_span(\"ray:wait_for_import_counter\", worker=worker):\n <mask>       wait_for_valid_import_counter(function_id, task.driver_id().id(), worker=worker)\n <mask> \n <mask>     # Execute the task.\n <mask>     # TODO(rkn): Consider acquiring this lock with a timeout and pushing a\n <mask>     # warning to the user if we are waiting too long to acquire the lock because\n <mask>     # that may indicate that the system is hanging, and it'd be good to know\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove   This method will simply loop until the import thread has imported enough of\n  the exports to execute the function. If we spend too long in this loop, that\n  may indicate a problem somewhere and we will push an error message to the\n  user.\n </s> add   This method will simply loop until the import thread has imported the relevant\n  function. If we spend too long in this loop, that may indicate a problem\n  somewhere and we will push an error message to the user. </s> remove def wait_for_valid_import_counter(function_id, driver_id, timeout=5, worker=global_worker):\n  \"\"\"Wait until this worker has imported enough to execute the function.\n </s> add def wait_for_function(function_id, driver_id, timeout=5, worker=global_worker):\n  \"\"\"Wait until the function to be executed is present on this worker. </s> remove           outputs = worker.functions[task.function_id().id()].executor(arguments)\n </s> add           outputs = function_executor.executor(arguments) </s> remove   worker.redis_client.hset(worker_info_key, \"export_counter\", 0)\n  worker.worker_import_counter = 0\n  # The number of imports is similar to the worker_import_counter except that it\n  # also counts actors.\n </s> add   # Keep track of the number of imports that we've imported. </s> remove         arguments = get_arguments_for_execution(worker.functions[function_id.id()], args, worker)\n </s> add         arguments = get_arguments_for_execution(function_name, args, worker) </s> remove         raise RayGetArgumentError(function.__name__, i, arg, argument)\n </s> add         raise RayGetArgumentError(function_name, i, arg, argument)", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     log(event_type=\"ray:acquire_lock\", kind=LOG_SPAN_START, worker=worker)\n <mask>     with worker.lock:\n <mask>       log(event_type=\"ray:acquire_lock\", kind=LOG_SPAN_END, worker=worker)\n <mask> \n <mask>       contents = {\"function_name\": worker.function_names[function_id.id()],\n <mask>                   \"task_id\": task.task_id().hex()}\n <mask>       with log_span(\"ray:task\", contents=contents, worker=worker):\n <mask>         process_task(task)\n <mask> \n <mask>     # Push all of the log events to the global state store.\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove def _submit_task(function_id, func_name, args, num_cpus, num_gpus, worker=global_worker):\n </s> add def _submit_task(function_id, func_name, args, worker=global_worker): </s> remove     # Check that the number of imports we have is at least as great as the\n    # export counter for the task. If not, wait until we have imported enough.\n    # We will push warnings to the user if we spend too long in this loop.\n    with log_span(\"ray:wait_for_import_counter\", worker=worker):\n      wait_for_valid_import_counter(function_id, task.driver_id().id(), worker=worker)\n </s> add     # Wait until the function to be executed has actually been registered on\n    # this worker. We will push warnings to the user if we spend too long in\n    # this loop.\n    with log_span(\"ray:wait_for_function\", worker=worker):\n      wait_for_function(function_id, task.driver_id().id(), worker=worker) </s> remove           outputs = worker.functions[task.function_id().id()].executor(arguments)\n </s> add           outputs = function_executor.executor(arguments) </s> remove         arguments = get_arguments_for_execution(worker.functions[function_id.id()], args, worker)\n </s> add         arguments = get_arguments_for_execution(function_name, args, worker) </s> remove           outputs = worker.functions[task.function_id().id()](worker.actors[task.actor_id().id()], *arguments)\n </s> add           outputs = function_executor(worker.actors[task.actor_id().id()], *arguments) </s> remove       function_name = worker.function_names[function_id.id()]\n </s> add       function_name, function_executor = worker.functions[worker.task_driver_id.id()][function_id.id()]", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     # Push all of the log events to the global state store.\n <mask>     flush_log()\n <mask> \n <mask> def _submit_task(function_id, func_name, args, num_cpus, num_gpus, worker=global_worker):\n <mask>   \"\"\"This is a wrapper around worker.submit_task.\n <mask> \n <mask>   We use this wrapper so that in the remote decorator, we can call _submit_task\n <mask>   instead of worker.submit_task. The difference is that when we attempt to\n <mask>   serialize remote functions, we don't attempt to serialize the worker object,\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove   return worker.submit_task(function_id, func_name, args, num_cpus, num_gpus)\n </s> add   return worker.submit_task(function_id, func_name, args) </s> remove       contents = {\"function_name\": worker.function_names[function_id.id()],\n </s> add       function_name, _ = worker.functions[task.driver_id().id()][function_id.id()]\n      contents = {\"function_name\": function_name, </s> add     num_cpus (int): The number of CPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver.\n    num_gpus (int): The number of GPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver. </s> remove         function_id = FunctionID((hashlib.sha256(func_name.encode(\"ascii\")).digest())[:20])\n </s> add         # Compute the function ID as a hash of the function name as well as the\n        # source code. We could in principle hash in the values in the closure\n        # of the function, but that is likely to introduce non-determinism in\n        # the computation of the function ID.\n        function_id_hash = hashlib.sha1()\n        function_id_hash.update(func_name.encode(\"ascii\"))\n        function_id_hash.update(inspect.getsource(func).encode(\"ascii\"))\n        function_id = function_id_hash.digest()\n        assert len(function_id) == 20\n        function_id = FunctionID(function_id) </s> remove     # Check that the number of imports we have is at least as great as the\n    # export counter for the task. If not, wait until we have imported enough.\n    # We will push warnings to the user if we spend too long in this loop.\n    with log_span(\"ray:wait_for_import_counter\", worker=worker):\n      wait_for_valid_import_counter(function_id, task.driver_id().id(), worker=worker)\n </s> add     # Wait until the function to be executed has actually been registered on\n    # this worker. We will push warnings to the user if we spend too long in\n    # this loop.\n    with log_span(\"ray:wait_for_function\", worker=worker):\n      wait_for_function(function_id, task.driver_id().id(), worker=worker) </s> remove   This method will simply loop until the import thread has imported enough of\n  the exports to execute the function. If we spend too long in this loop, that\n  may indicate a problem somewhere and we will push an error message to the\n  user.\n </s> add   This method will simply loop until the import thread has imported the relevant\n  function. If we spend too long in this loop, that may indicate a problem\n  somewhere and we will push an error message to the user.", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   instead of worker.submit_task. The difference is that when we attempt to\n <mask>   serialize remote functions, we don't attempt to serialize the worker object,\n <mask>   which cannot be serialized.\n <mask>   \"\"\"\n <mask>   return worker.submit_task(function_id, func_name, args, num_cpus, num_gpus)\n <mask> \n <mask> def _mode(worker=global_worker):\n <mask>   \"\"\"This is a wrapper around worker.mode.\n <mask> \n <mask>   We use this wrapper so that in the remote decorator, we can call _mode()\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove def _submit_task(function_id, func_name, args, num_cpus, num_gpus, worker=global_worker):\n </s> add def _submit_task(function_id, func_name, args, worker=global_worker): </s> add     num_cpus (int): The number of CPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver.\n    num_gpus (int): The number of GPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver. </s> remove     subprocess.Popen([stop_ray_script]).wait()\n </s> add   def testRemoteFunctionIsolation(self):\n    # This test will run multiple remote functions with the same names in two\n    # different drivers.\n    # Connect a driver to the Ray cluster.\n    ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE)\n\n    # Start another driver and make sure that it can define and call its own\n    # commands with the same names.\n    driver_script = \"\"\"\nimport ray\nimport time\nray.init(redis_address=\"{}\")\n@ray.remote\ndef f():\n  return 3\n@ray.remote\ndef g(x, y):\n  return 4\nfor _ in range(10000):\n  result = ray.get([f.remote(), g.remote(0, 0)])\n  assert result == [3, 4]\nprint(\"success\")\n\"\"\".format(self.redis_address)\n\n    # Save the driver script as a file so we can call it using subprocess.\n    with tempfile.NamedTemporaryFile() as f:\n      f.write(driver_script.encode(\"ascii\"))\n      f.flush()\n      out = subprocess.check_output([\"python\", f.name]).decode(\"ascii\")\n\n    @ray.remote\n    def f():\n      return 1\n\n    @ray.remote\n    def g(x):\n      return 2\n\n    for _ in range(10000):\n      result = ray.get([f.remote(), g.remote(0)])\n      self.assertEqual(result, [1, 2])\n\n    # Make sure the other driver succeeded.\n    self.assertIn(\"success\", out)\n\n    ray.worker.cleanup() </s> remove         function_id = FunctionID((hashlib.sha256(func_name.encode(\"ascii\")).digest())[:20])\n </s> add         # Compute the function ID as a hash of the function name as well as the\n        # source code. We could in principle hash in the values in the closure\n        # of the function, but that is likely to introduce non-determinism in\n        # the computation of the function ID.\n        function_id_hash = hashlib.sha1()\n        function_id_hash.update(func_name.encode(\"ascii\"))\n        function_id_hash.update(inspect.getsource(func).encode(\"ascii\"))\n        function_id = function_id_hash.digest()\n        assert len(function_id) == 20\n        function_id = FunctionID(function_id) </s> remove     # Check that the number of imports we have is at least as great as the\n    # export counter for the task. If not, wait until we have imported enough.\n    # We will push warnings to the user if we spend too long in this loop.\n    with log_span(\"ray:wait_for_import_counter\", worker=worker):\n      wait_for_valid_import_counter(function_id, task.driver_id().id(), worker=worker)\n </s> add     # Wait until the function to be executed has actually been registered on\n    # this worker. We will push warnings to the user if we spend too long in\n    # this loop.\n    with log_span(\"ray:wait_for_function\", worker=worker):\n      wait_for_function(function_id, task.driver_id().id(), worker=worker) </s> remove   worker.functions[function_id.id()] = remote(num_return_vals=num_return_vals,\n                                              function_id=function_id,\n                                              num_cpus=num_cpus,\n                                              num_gpus=num_gpus)(lambda *xs: f())\n </s> add   remote_f_placeholder = remote(function_id=function_id)(lambda *xs: f())\n  worker.functions[driver_id][function_id.id()] = (function_name, remote_f_placeholder)\n  worker.function_properties[driver_id][function_id.id()] = (num_return_vals, num_cpus, num_gpus)", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                                   \"name\": name,\n <mask>                                   \"initializer\": pickling.dumps(environment_variable.initializer),\n <mask>                                   \"reinitializer\": pickling.dumps(environment_variable.reinitializer)})\n <mask>   worker.redis_client.rpush(\"Exports\", key)\n <mask>   worker.driver_export_counter += 1\n <mask> \n <mask> def export_remote_function(function_id, func_name, func, num_return_vals, num_cpus, num_gpus, worker=global_worker):\n <mask>   check_main_thread()\n <mask>   if _mode(worker) not in [SCRIPT_MODE, SILENT_MODE]:\n <mask>     raise Exception(\"export_remote_function can only be called on a driver.\")\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> add   worker.function_properties[worker.task_driver_id.id()][function_id.id()] = (num_return_vals, num_cpus, num_gpus) </s> remove   worker.num_return_vals[function_id.id()] = num_return_vals\n </s> add  </s> remove                                   \"function_export_counter\": worker.driver_export_counter,\n </s> add  </s> remove   worker.driver_export_counter += 1\n </s> add  </s> remove def _submit_task(function_id, func_name, args, num_cpus, num_gpus, worker=global_worker):\n </s> add def _submit_task(function_id, func_name, args, worker=global_worker): </s> remove       # Actors do not contribute to the import counter.\n      if not key.startswith(b\"Actor\"):\n        worker.redis_client.hincrby(worker_info_key, \"export_counter\", 1)\n        worker.worker_import_counter += 1\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>   check_main_thread()\n <mask>   if _mode(worker) not in [SCRIPT_MODE, SILENT_MODE]:\n <mask>     raise Exception(\"export_remote_function can only be called on a driver.\")\n <mask>   key = \"RemoteFunction:{}\".format(function_id.id())\n <mask>   pickled_func = pickling.dumps(func)\n <mask>   worker.redis_client.hmset(key, {\"driver_id\": worker.task_driver_id.id(),\n <mask>                                   \"function_id\": function_id.id(),\n <mask>                                   \"name\": func_name,\n <mask>                                   \"module\": func.__module__,\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove   worker.num_return_vals[function_id.id()] = num_return_vals\n </s> add  </s> remove   worker.driver_export_counter += 1\n </s> add  </s> remove                                   \"function_export_counter\": worker.driver_export_counter,\n </s> add  </s> add     num_cpus (int): The number of CPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver.\n    num_gpus (int): The number of GPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver. </s> remove   worker.function_names[function_id.id()] = function_name\n  worker.num_return_vals[function_id.id()] = num_return_vals\n  worker.function_export_counters[function_id.id()] = function_export_counter\n </s> add  </s> remove   # overwritten if the function is unpickled successfully.\n </s> add   # overwritten if the function is successfully registered.", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   check_main_thread()\n <mask>   if _mode(worker) not in [SCRIPT_MODE, SILENT_MODE]:\n <mask>     raise Exception(\"export_remote_function can only be called on a driver.\")\n <mask>   key = \"RemoteFunction:{}\".format(function_id.id())\n <mask>   worker.num_return_vals[function_id.id()] = num_return_vals\n <mask>   pickled_func = pickling.dumps(func)\n <mask>   worker.redis_client.hmset(key, {\"driver_id\": worker.task_driver_id.id(),\n <mask>                                   \"function_id\": function_id.id(),\n <mask>                                   \"name\": func_name,\n <mask>                                   \"module\": func.__module__,\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> add   worker.function_properties[worker.task_driver_id.id()][function_id.id()] = (num_return_vals, num_cpus, num_gpus) </s> remove   worker.driver_export_counter += 1\n </s> add  </s> remove                                   \"function_export_counter\": worker.driver_export_counter,\n </s> add  </s> remove   worker.function_names[function_id.id()] = function_name\n  worker.num_return_vals[function_id.id()] = num_return_vals\n  worker.function_export_counters[function_id.id()] = function_export_counter\n </s> add  </s> remove   function_export_counter = int(function_export_counter)\n </s> add  </s> remove   # overwritten if the function is unpickled successfully.\n </s> add   # overwritten if the function is successfully registered.", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep replace keep keep", "code_tokens": " <mask>                                   \"name\": func_name,\n <mask>                                   \"module\": func.__module__,\n <mask>                                   \"function\": pickled_func,\n <mask>                                   \"num_return_vals\": num_return_vals,\n <mask>                                   \"function_export_counter\": worker.driver_export_counter,\n <mask>                                   \"num_cpus\": num_cpus,\n <mask>                                   \"num_gpus\": num_gpus})\n <mask>   worker.redis_client.rpush(\"Exports\", key)\n <mask>   worker.driver_export_counter += 1\n <mask> \n <mask> def remote(*args, **kwargs):\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove   worker.driver_export_counter += 1\n </s> add  </s> add   worker.function_properties[worker.task_driver_id.id()][function_id.id()] = (num_return_vals, num_cpus, num_gpus) </s> remove   worker.num_return_vals[function_id.id()] = num_return_vals\n </s> add  </s> remove       # Actors do not contribute to the import counter.\n      if not key.startswith(b\"Actor\"):\n        worker.redis_client.hincrby(worker_info_key, \"export_counter\", 1)\n        worker.worker_import_counter += 1\n </s> add  </s> remove         # Actors do not contribute to the import counter.\n        if not key.startswith(b\"Actor\"):\n          worker.redis_client.hincrby(worker_info_key, \"export_counter\", 1)\n          worker.worker_import_counter += 1\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask> \n <mask>   Args:\n <mask>     num_return_vals (int): The number of object IDs that a call to this function\n <mask>       should return.\n <mask>   \"\"\"\n <mask>   worker = global_worker\n <mask>   def make_remote_decorator(num_return_vals, num_cpus, num_gpus, func_id=None):\n <mask>     def remote_decorator(func):\n <mask>       func_name = \"{}.{}\".format(func.__module__, func.__name__)\n <mask>       if func_id is None:\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove         function_id = FunctionID((hashlib.sha256(func_name.encode(\"ascii\")).digest())[:20])\n </s> add         # Compute the function ID as a hash of the function name as well as the\n        # source code. We could in principle hash in the values in the closure\n        # of the function, but that is likely to introduce non-determinism in\n        # the computation of the function ID.\n        function_id_hash = hashlib.sha1()\n        function_id_hash.update(func_name.encode(\"ascii\"))\n        function_id_hash.update(inspect.getsource(func).encode(\"ascii\"))\n        function_id = function_id_hash.digest()\n        assert len(function_id) == 20\n        function_id = FunctionID(function_id) </s> remove   worker.redis_client.hset(worker_info_key, \"export_counter\", 0)\n  worker.worker_import_counter = 0\n  # The number of imports is similar to the worker_import_counter except that it\n  # also counts actors.\n </s> add   # Keep track of the number of imports that we've imported. </s> remove   return worker.submit_task(function_id, func_name, args, num_cpus, num_gpus)\n </s> add   return worker.submit_task(function_id, func_name, args) </s> remove def _submit_task(function_id, func_name, args, num_cpus, num_gpus, worker=global_worker):\n </s> add def _submit_task(function_id, func_name, args, worker=global_worker): </s> remove     # This example is somewhat contrived. It should be successfully pickled, and\n    # then it should throw an exception when it is unpickled. This may depend a\n    # bit on the specifics of our pickler.\n    def reducer(*args):\n      raise Exception(\"There is a problem here.\")\n    class Foo(object):\n      def __init__(self):\n        self.__name__ = \"Foo_object\"\n        self.func_doc = \"\"\n        self.__globals__ = {}\n      def __reduce__(self):\n        return reducer, ()\n      def __call__(self):\n        return\n    f = ray.remote(Foo())\n </s> add     # Create the contents of a temporary Python file.\n    temporary_python_file = \"\"\"\ndef temporary_helper_function():\n  return 1\n\"\"\"\n\n    f = tempfile.NamedTemporaryFile(suffix=\".py\")\n    f.write(temporary_python_file.encode(\"ascii\"))\n    f.flush()\n    directory = os.path.dirname(f.name)\n    # Get the module name and strip \".py\" from the end.\n    module_name = os.path.basename(f.name)[:-3]\n    sys.path.append(directory)\n    module = __import__(module_name)\n\n    # Define a function that closes over this temporary module. This should fail\n    # when it is unpickled.\n    @ray.remote\n    def g():\n      return module.temporary_python_file()\n </s> remove     function (Callable): The remote function whose arguments are being\n      retrieved.\n </s> add     function_name (str): The name of the remote function whose arguments are\n      being retrieved.", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   def make_remote_decorator(num_return_vals, num_cpus, num_gpus, func_id=None):\n <mask>     def remote_decorator(func):\n <mask>       func_name = \"{}.{}\".format(func.__module__, func.__name__)\n <mask>       if func_id is None:\n <mask>         function_id = FunctionID((hashlib.sha256(func_name.encode(\"ascii\")).digest())[:20])\n <mask>       else:\n <mask>         function_id = func_id\n <mask> \n <mask>       def func_call(*args, **kwargs):\n <mask>         \"\"\"This gets run immediately when a worker calls a remote function.\"\"\"\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> add     num_cpus (int): The number of CPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver.\n    num_gpus (int): The number of GPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver. </s> remove                                     \"function_export_counter\",\n </s> add  </s> remove   worker.driver_export_counter += 1\n </s> add  </s> remove def _submit_task(function_id, func_name, args, num_cpus, num_gpus, worker=global_worker):\n </s> add def _submit_task(function_id, func_name, args, worker=global_worker): </s> remove       function_name = worker.function_names[function_id.id()]\n </s> add       function_name, function_executor = worker.functions[worker.task_driver_id.id()][function_id.id()] </s> remove   return worker.submit_task(function_id, func_name, args, num_cpus, num_gpus)\n </s> add   return worker.submit_task(function_id, func_name, args)", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>           finally:\n <mask>             _env()._reinitialize()\n <mask>             _env()._running_remote_function_locally = False\n <mask>           return result\n <mask>         objectids = _submit_task(function_id, func_name, args, num_cpus, num_gpus)\n <mask>         if len(objectids) == 1:\n <mask>           return objectids[0]\n <mask>         elif len(objectids) > 1:\n <mask>           return objectids\n <mask>       def func_executor(arguments):\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove   return worker.submit_task(function_id, func_name, args, num_cpus, num_gpus)\n </s> add   return worker.submit_task(function_id, func_name, args) </s> remove       if worker.actor_id == NIL_ACTOR_ID and function_id.id() in worker.functions and (worker.function_export_counters[function_id.id()] <= worker.worker_import_counter):\n </s> add       if worker.actor_id == NIL_ACTOR_ID and function_id.id() in worker.functions[driver_id]: </s> remove     subprocess.Popen([stop_ray_script]).wait()\n </s> add   def testRemoteFunctionIsolation(self):\n    # This test will run multiple remote functions with the same names in two\n    # different drivers.\n    # Connect a driver to the Ray cluster.\n    ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE)\n\n    # Start another driver and make sure that it can define and call its own\n    # commands with the same names.\n    driver_script = \"\"\"\nimport ray\nimport time\nray.init(redis_address=\"{}\")\n@ray.remote\ndef f():\n  return 3\n@ray.remote\ndef g(x, y):\n  return 4\nfor _ in range(10000):\n  result = ray.get([f.remote(), g.remote(0, 0)])\n  assert result == [3, 4]\nprint(\"success\")\n\"\"\".format(self.redis_address)\n\n    # Save the driver script as a file so we can call it using subprocess.\n    with tempfile.NamedTemporaryFile() as f:\n      f.write(driver_script.encode(\"ascii\"))\n      f.flush()\n      out = subprocess.check_output([\"python\", f.name]).decode(\"ascii\")\n\n    @ray.remote\n    def f():\n      return 1\n\n    @ray.remote\n    def g(x):\n      return 2\n\n    for _ in range(10000):\n      result = ray.get([f.remote(), g.remote(0)])\n      self.assertEqual(result, [1, 2])\n\n    # Make sure the other driver succeeded.\n    self.assertIn(\"success\", out)\n\n    ray.worker.cleanup() </s> remove           outputs = worker.functions[task.function_id().id()](worker.actors[task.actor_id().id()], *arguments)\n </s> add           outputs = function_executor(worker.actors[task.actor_id().id()], *arguments) </s> remove def _submit_task(function_id, func_name, args, num_cpus, num_gpus, worker=global_worker):\n </s> add def _submit_task(function_id, func_name, args, worker=global_worker): </s> remove     # This example is somewhat contrived. It should be successfully pickled, and\n    # then it should throw an exception when it is unpickled. This may depend a\n    # bit on the specifics of our pickler.\n    def reducer(*args):\n      raise Exception(\"There is a problem here.\")\n    class Foo(object):\n      def __init__(self):\n        self.__name__ = \"Foo_object\"\n        self.func_doc = \"\"\n        self.__globals__ = {}\n      def __reduce__(self):\n        return reducer, ()\n      def __call__(self):\n        return\n    f = ray.remote(Foo())\n </s> add     # Create the contents of a temporary Python file.\n    temporary_python_file = \"\"\"\ndef temporary_helper_function():\n  return 1\n\"\"\"\n\n    f = tempfile.NamedTemporaryFile(suffix=\".py\")\n    f.write(temporary_python_file.encode(\"ascii\"))\n    f.flush()\n    directory = os.path.dirname(f.name)\n    # Get the module name and strip \".py\" from the end.\n    module_name = os.path.basename(f.name)[:-3]\n    sys.path.append(directory)\n    module = __import__(module_name)\n\n    # Define a function that closes over this temporary module. This should fail\n    # when it is unpickled.\n    @ray.remote\n    def g():\n      return module.temporary_python_file()\n", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   # check if the user specified a variable number of arguments and any keyword arguments\n <mask>   if has_vararg_param and any([d != funcsigs._empty for _, d in keyword_defaults]):\n <mask>     raise \"Function {} has a *args argument as well as a keyword argument, which is currently not supported.\".format(name)\n <mask> \n <mask> def get_arguments_for_execution(function, serialized_args, worker=global_worker):\n <mask>   \"\"\"Retrieve the arguments for the remote function.\n <mask> \n <mask>   This retrieves the values for the arguments to the remote function that were\n <mask>   passed in as object IDs. Argumens that were passed by value are not changed.\n <mask>   This is called by the worker that is executing the remote function.\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove     function (Callable): The remote function whose arguments are being\n      retrieved.\n </s> add     function_name (str): The name of the remote function whose arguments are\n      being retrieved. </s> add     num_cpus (int): The number of CPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver.\n    num_gpus (int): The number of GPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver. </s> remove def wait_for_valid_import_counter(function_id, driver_id, timeout=5, worker=global_worker):\n  \"\"\"Wait until this worker has imported enough to execute the function.\n </s> add def wait_for_function(function_id, driver_id, timeout=5, worker=global_worker):\n  \"\"\"Wait until the function to be executed is present on this worker. </s> remove         function_id = FunctionID((hashlib.sha256(func_name.encode(\"ascii\")).digest())[:20])\n </s> add         # Compute the function ID as a hash of the function name as well as the\n        # source code. We could in principle hash in the values in the closure\n        # of the function, but that is likely to introduce non-determinism in\n        # the computation of the function ID.\n        function_id_hash = hashlib.sha1()\n        function_id_hash.update(func_name.encode(\"ascii\"))\n        function_id_hash.update(inspect.getsource(func).encode(\"ascii\"))\n        function_id = function_id_hash.digest()\n        assert len(function_id) == 20\n        function_id = FunctionID(function_id) </s> remove         raise RayGetArgumentError(function.__name__, i, arg, argument)\n </s> add         raise RayGetArgumentError(function_name, i, arg, argument) </s> remove     # Check that the number of imports we have is at least as great as the\n    # export counter for the task. If not, wait until we have imported enough.\n    # We will push warnings to the user if we spend too long in this loop.\n    with log_span(\"ray:wait_for_import_counter\", worker=worker):\n      wait_for_valid_import_counter(function_id, task.driver_id().id(), worker=worker)\n </s> add     # Wait until the function to be executed has actually been registered on\n    # this worker. We will push warnings to the user if we spend too long in\n    # this loop.\n    with log_span(\"ray:wait_for_function\", worker=worker):\n      wait_for_function(function_id, task.driver_id().id(), worker=worker)", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>   passed in as object IDs. Argumens that were passed by value are not changed.\n <mask>   This is called by the worker that is executing the remote function.\n <mask> \n <mask>   Args:\n <mask>     function (Callable): The remote function whose arguments are being\n <mask>       retrieved.\n <mask>     serialized_args (List): The arguments to the function. These are either\n <mask>       strings representing serialized objects passed by value or they are\n <mask>       ObjectIDs.\n <mask> \n <mask>   Returns:\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove def get_arguments_for_execution(function, serialized_args, worker=global_worker):\n </s> add def get_arguments_for_execution(function_name, serialized_args, worker=global_worker): </s> add     num_cpus (int): The number of CPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver.\n    num_gpus (int): The number of GPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver. </s> remove         raise RayGetArgumentError(function.__name__, i, arg, argument)\n </s> add         raise RayGetArgumentError(function_name, i, arg, argument) </s> remove   This method will simply loop until the import thread has imported enough of\n  the exports to execute the function. If we spend too long in this loop, that\n  may indicate a problem somewhere and we will push an error message to the\n  user.\n </s> add   This method will simply loop until the import thread has imported the relevant\n  function. If we spend too long in this loop, that may indicate a problem\n  somewhere and we will push an error message to the user. </s> remove def _submit_task(function_id, func_name, args, num_cpus, num_gpus, worker=global_worker):\n </s> add def _submit_task(function_id, func_name, args, worker=global_worker): </s> remove   return worker.submit_task(function_id, func_name, args, num_cpus, num_gpus)\n </s> add   return worker.submit_task(function_id, func_name, args)", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>       argument = worker.get_object([arg])[0]\n <mask>       if isinstance(argument, RayTaskError):\n <mask>         # If the result is a RayTaskError, then the task that created this\n <mask>         # object failed, and we should propagate the error message here.\n <mask>         raise RayGetArgumentError(function.__name__, i, arg, argument)\n <mask>     else:\n <mask>       # pass the argument by value\n <mask>       argument = arg\n <mask> \n <mask>     arguments.append(argument)\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove def get_arguments_for_execution(function, serialized_args, worker=global_worker):\n </s> add def get_arguments_for_execution(function_name, serialized_args, worker=global_worker): </s> remove   This method will simply loop until the import thread has imported enough of\n  the exports to execute the function. If we spend too long in this loop, that\n  may indicate a problem somewhere and we will push an error message to the\n  user.\n </s> add   This method will simply loop until the import thread has imported the relevant\n  function. If we spend too long in this loop, that may indicate a problem\n  somewhere and we will push an error message to the user. </s> remove     # Check that the number of imports we have is at least as great as the\n    # export counter for the task. If not, wait until we have imported enough.\n    # We will push warnings to the user if we spend too long in this loop.\n    with log_span(\"ray:wait_for_import_counter\", worker=worker):\n      wait_for_valid_import_counter(function_id, task.driver_id().id(), worker=worker)\n </s> add     # Wait until the function to be executed has actually been registered on\n    # this worker. We will push warnings to the user if we spend too long in\n    # this loop.\n    with log_span(\"ray:wait_for_function\", worker=worker):\n      wait_for_function(function_id, task.driver_id().id(), worker=worker) </s> remove     # This example is somewhat contrived. It should be successfully pickled, and\n    # then it should throw an exception when it is unpickled. This may depend a\n    # bit on the specifics of our pickler.\n    def reducer(*args):\n      raise Exception(\"There is a problem here.\")\n    class Foo(object):\n      def __init__(self):\n        self.__name__ = \"Foo_object\"\n        self.func_doc = \"\"\n        self.__globals__ = {}\n      def __reduce__(self):\n        return reducer, ()\n      def __call__(self):\n        return\n    f = ray.remote(Foo())\n </s> add     # Create the contents of a temporary Python file.\n    temporary_python_file = \"\"\"\ndef temporary_helper_function():\n  return 1\n\"\"\"\n\n    f = tempfile.NamedTemporaryFile(suffix=\".py\")\n    f.write(temporary_python_file.encode(\"ascii\"))\n    f.flush()\n    directory = os.path.dirname(f.name)\n    # Get the module name and strip \".py\" from the end.\n    module_name = os.path.basename(f.name)[:-3]\n    sys.path.append(directory)\n    module = __import__(module_name)\n\n    # Define a function that closes over this temporary module. This should fail\n    # when it is unpickled.\n    @ray.remote\n    def g():\n      return module.temporary_python_file()\n </s> remove     self.assertTrue(b\"The initializer failed.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"The initializer failed.\", ray.error_info()[0][b\"message\"]) </s> remove def wait_for_valid_import_counter(function_id, driver_id, timeout=5, worker=global_worker):\n  \"\"\"Wait until this worker has imported enough to execute the function.\n </s> add def wait_for_function(function_id, driver_id, timeout=5, worker=global_worker):\n  \"\"\"Wait until the function to be executed is present on this worker.", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> }\n <mask> \n <mask> /**\n <mask>  * Give a task directly to another local scheduler. This is currently only used\n <mask>  * for assigning actor tasks to the local scheduer responsible for that actor.\n <mask>  *\n <mask>  * @param state The scheduler state.\n <mask>  * @param algorithm_state The scheduling algorithm state.\n <mask>  * @param spec The task specification to schedule.\n <mask>  * @param local_scheduler_id The ID of the local scheduler to give the task to.\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> add     num_cpus (int): The number of CPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver.\n    num_gpus (int): The number of GPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver. </s> remove     function (Callable): The remote function whose arguments are being\n      retrieved.\n </s> add     function_name (str): The name of the remote function whose arguments are\n      being retrieved. </s> remove       if worker.actor_id == NIL_ACTOR_ID and function_id.id() in worker.functions and (worker.function_export_counters[function_id.id()] <= worker.worker_import_counter):\n </s> add       if worker.actor_id == NIL_ACTOR_ID and function_id.id() in worker.functions[driver_id]: </s> remove def _submit_task(function_id, func_name, args, num_cpus, num_gpus, worker=global_worker):\n </s> add def _submit_task(function_id, func_name, args, worker=global_worker): </s> remove         if function_id.id() not in worker.functions:\n          warning_message = \"This worker was asked to execute a function that it does not have registered. You may have to restart Ray.\"\n        else:\n          warning_message = \"This worker's import counter is too small.\"\n </s> add         warning_message = \"This worker was asked to execute a function that it does not have registered. You may have to restart Ray.\" </s> remove   worker.redis_client.hset(worker_info_key, \"export_counter\", 0)\n  worker.worker_import_counter = 0\n  # The number of imports is similar to the worker_import_counter except that it\n  # also counts actors.\n </s> add   # Keep track of the number of imports that we've imported.", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "src/photon/photon_algorithm.c"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> from __future__ import absolute_import\n <mask> from __future__ import division\n <mask> from __future__ import print_function\n <mask> \n <mask> import unittest\n <mask> import ray\n <mask> import sys\n <mask> import time\n <mask> \n <mask> if sys.version_info >= (3, 0):\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> add import unittest </s> add import tempfile </s> remove     subprocess.Popen([stop_ray_script]).wait()\n </s> add   def testRemoteFunctionIsolation(self):\n    # This test will run multiple remote functions with the same names in two\n    # different drivers.\n    # Connect a driver to the Ray cluster.\n    ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE)\n\n    # Start another driver and make sure that it can define and call its own\n    # commands with the same names.\n    driver_script = \"\"\"\nimport ray\nimport time\nray.init(redis_address=\"{}\")\n@ray.remote\ndef f():\n  return 3\n@ray.remote\ndef g(x, y):\n  return 4\nfor _ in range(10000):\n  result = ray.get([f.remote(), g.remote(0, 0)])\n  assert result == [3, 4]\nprint(\"success\")\n\"\"\".format(self.redis_address)\n\n    # Save the driver script as a file so we can call it using subprocess.\n    with tempfile.NamedTemporaryFile() as f:\n      f.write(driver_script.encode(\"ascii\"))\n      f.flush()\n      out = subprocess.check_output([\"python\", f.name]).decode(\"ascii\")\n\n    @ray.remote\n    def f():\n      return 1\n\n    @ray.remote\n    def g(x):\n      return 2\n\n    for _ in range(10000):\n      result = ray.get([f.remote(), g.remote(0)])\n      self.assertEqual(result, [1, 2])\n\n    # Make sure the other driver succeeded.\n    self.assertIn(\"success\", out)\n\n    ray.worker.cleanup() </s> remove       # Actors do not contribute to the import counter.\n      if not key.startswith(b\"Actor\"):\n        worker.redis_client.hincrby(worker_info_key, \"export_counter\", 1)\n        worker.worker_import_counter += 1\n </s> add  </s> remove           worker.push_error_to_driver(driver_id, \"import_counter\",\n </s> add           worker.push_error_to_driver(driver_id, \"wait_for_function\", </s> remove   This method will simply loop until the import thread has imported enough of\n  the exports to execute the function. If we spend too long in this loop, that\n  may indicate a problem somewhere and we will push an error message to the\n  user.\n </s> add   This method will simply loop until the import thread has imported the relevant\n  function. If we spend too long in this loop, that may indicate a problem\n  somewhere and we will push an error message to the user.", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "test/failure_test.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> import ray\n <mask> import sys\n <mask> import time\n <mask> import unittest\n <mask> \n <mask> if sys.version_info >= (3, 0):\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> add import unittest </s> remove import unittest\n </s> add import os </s> remove     subprocess.Popen([stop_ray_script]).wait()\n </s> add   def testRemoteFunctionIsolation(self):\n    # This test will run multiple remote functions with the same names in two\n    # different drivers.\n    # Connect a driver to the Ray cluster.\n    ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE)\n\n    # Start another driver and make sure that it can define and call its own\n    # commands with the same names.\n    driver_script = \"\"\"\nimport ray\nimport time\nray.init(redis_address=\"{}\")\n@ray.remote\ndef f():\n  return 3\n@ray.remote\ndef g(x, y):\n  return 4\nfor _ in range(10000):\n  result = ray.get([f.remote(), g.remote(0, 0)])\n  assert result == [3, 4]\nprint(\"success\")\n\"\"\".format(self.redis_address)\n\n    # Save the driver script as a file so we can call it using subprocess.\n    with tempfile.NamedTemporaryFile() as f:\n      f.write(driver_script.encode(\"ascii\"))\n      f.flush()\n      out = subprocess.check_output([\"python\", f.name]).decode(\"ascii\")\n\n    @ray.remote\n    def f():\n      return 1\n\n    @ray.remote\n    def g(x):\n      return 2\n\n    for _ in range(10000):\n      result = ray.get([f.remote(), g.remote(0)])\n      self.assertEqual(result, [1, 2])\n\n    # Make sure the other driver succeeded.\n    self.assertIn(\"success\", out)\n\n    ray.worker.cleanup() </s> remove       # Actors do not contribute to the import counter.\n      if not key.startswith(b\"Actor\"):\n        worker.redis_client.hincrby(worker_info_key, \"export_counter\", 1)\n        worker.worker_import_counter += 1\n </s> add  </s> remove           worker.push_error_to_driver(driver_id, \"import_counter\",\n </s> add           worker.push_error_to_driver(driver_id, \"wait_for_function\", </s> remove   This method will simply loop until the import thread has imported enough of\n  the exports to execute the function. If we spend too long in this loop, that\n  may indicate a problem somewhere and we will push an error message to the\n  user.\n </s> add   This method will simply loop until the import thread has imported the relevant\n  function. If we spend too long in this loop, that may indicate a problem\n  somewhere and we will push an error message to the user.", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "test/failure_test.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask> import ray\n <mask> import sys\n <mask> import tempfile\n <mask> import time\n <mask> \n <mask> if sys.version_info >= (3, 0):\n <mask>   from importlib import reload\n <mask> \n <mask> import ray.test.test_functions as test_functions\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> add import tempfile </s> remove import unittest\n </s> add import os </s> remove     subprocess.Popen([stop_ray_script]).wait()\n </s> add   def testRemoteFunctionIsolation(self):\n    # This test will run multiple remote functions with the same names in two\n    # different drivers.\n    # Connect a driver to the Ray cluster.\n    ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE)\n\n    # Start another driver and make sure that it can define and call its own\n    # commands with the same names.\n    driver_script = \"\"\"\nimport ray\nimport time\nray.init(redis_address=\"{}\")\n@ray.remote\ndef f():\n  return 3\n@ray.remote\ndef g(x, y):\n  return 4\nfor _ in range(10000):\n  result = ray.get([f.remote(), g.remote(0, 0)])\n  assert result == [3, 4]\nprint(\"success\")\n\"\"\".format(self.redis_address)\n\n    # Save the driver script as a file so we can call it using subprocess.\n    with tempfile.NamedTemporaryFile() as f:\n      f.write(driver_script.encode(\"ascii\"))\n      f.flush()\n      out = subprocess.check_output([\"python\", f.name]).decode(\"ascii\")\n\n    @ray.remote\n    def f():\n      return 1\n\n    @ray.remote\n    def g(x):\n      return 2\n\n    for _ in range(10000):\n      result = ray.get([f.remote(), g.remote(0)])\n      self.assertEqual(result, [1, 2])\n\n    # Make sure the other driver succeeded.\n    self.assertIn(\"success\", out)\n\n    ray.worker.cleanup() </s> remove       # Actors do not contribute to the import counter.\n      if not key.startswith(b\"Actor\"):\n        worker.redis_client.hincrby(worker_info_key, \"export_counter\", 1)\n        worker.worker_import_counter += 1\n </s> add  </s> remove           worker.push_error_to_driver(driver_id, \"import_counter\",\n </s> add           worker.push_error_to_driver(driver_id, \"wait_for_function\", </s> remove   This method will simply loop until the import thread has imported enough of\n  the exports to execute the function. If we spend too long in this loop, that\n  may indicate a problem somewhere and we will push an error message to the\n  user.\n </s> add   This method will simply loop until the import thread has imported the relevant\n  function. If we spend too long in this loop, that may indicate a problem\n  somewhere and we will push an error message to the user.", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "test/failure_test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     wait_for_errors(b\"task\", 2)\n <mask>     result = ray.error_info()\n <mask>     self.assertEqual(len(relevant_errors(b\"task\")), 2)\n <mask>     for task in relevant_errors(b\"task\"):\n <mask>       self.assertTrue(b\"Test function 1 intentionally failed.\" in task.get(b\"message\"))\n <mask> \n <mask>     x = test_functions.throw_exception_fct2.remote()\n <mask>     try:\n <mask>       ray.get(x)\n <mask>     except Exception as e:\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove       self.assertTrue(\"Test function 2 intentionally failed.\" in str(e))\n </s> add       self.assertIn(\"Test function 2 intentionally failed.\", str(e)) </s> remove         self.assertTrue(\"Test function 3 intentionally failed.\" in str(e))\n </s> add         self.assertIn(\"Test function 3 intentionally failed.\", str(e)) </s> remove     self.assertTrue(b\"Function to run failed.\" in ray.error_info()[0][b\"message\"])\n    self.assertTrue(b\"Function to run failed.\" in ray.error_info()[1][b\"message\"])\n </s> add     self.assertIn(b\"Function to run failed.\", ray.error_info()[0][b\"message\"])\n    self.assertIn(b\"Function to run failed.\", ray.error_info()[1][b\"message\"]) </s> remove     self.assertTrue(b\"The initializer failed.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"The initializer failed.\", ray.error_info()[0][b\"message\"]) </s> remove     self.assertTrue(b\"There is a problem here.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"No module named\", ray.error_info()[0][b\"message\"])\n    self.assertIn(b\"No module named\", ray.error_info()[1][b\"message\"]) </s> remove     subprocess.Popen([stop_ray_script]).wait()\n </s> add   def testRemoteFunctionIsolation(self):\n    # This test will run multiple remote functions with the same names in two\n    # different drivers.\n    # Connect a driver to the Ray cluster.\n    ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE)\n\n    # Start another driver and make sure that it can define and call its own\n    # commands with the same names.\n    driver_script = \"\"\"\nimport ray\nimport time\nray.init(redis_address=\"{}\")\n@ray.remote\ndef f():\n  return 3\n@ray.remote\ndef g(x, y):\n  return 4\nfor _ in range(10000):\n  result = ray.get([f.remote(), g.remote(0, 0)])\n  assert result == [3, 4]\nprint(\"success\")\n\"\"\".format(self.redis_address)\n\n    # Save the driver script as a file so we can call it using subprocess.\n    with tempfile.NamedTemporaryFile() as f:\n      f.write(driver_script.encode(\"ascii\"))\n      f.flush()\n      out = subprocess.check_output([\"python\", f.name]).decode(\"ascii\")\n\n    @ray.remote\n    def f():\n      return 1\n\n    @ray.remote\n    def g(x):\n      return 2\n\n    for _ in range(10000):\n      result = ray.get([f.remote(), g.remote(0)])\n      self.assertEqual(result, [1, 2])\n\n    # Make sure the other driver succeeded.\n    self.assertIn(\"success\", out)\n\n    ray.worker.cleanup()", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "test/failure_test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     x = test_functions.throw_exception_fct2.remote()\n <mask>     try:\n <mask>       ray.get(x)\n <mask>     except Exception as e:\n <mask>       self.assertTrue(\"Test function 2 intentionally failed.\" in str(e))\n <mask>     else:\n <mask>       self.assertTrue(False) # ray.get should throw an exception\n <mask> \n <mask>     x, y, z = test_functions.throw_exception_fct3.remote(1.0)\n <mask>     for ref in [x, y, z]:\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove         self.assertTrue(\"Test function 3 intentionally failed.\" in str(e))\n </s> add         self.assertIn(\"Test function 3 intentionally failed.\", str(e)) </s> remove       self.assertTrue(b\"Test function 1 intentionally failed.\" in task.get(b\"message\"))\n </s> add       self.assertIn(b\"Test function 1 intentionally failed.\", task.get(b\"message\")) </s> remove     # This example is somewhat contrived. It should be successfully pickled, and\n    # then it should throw an exception when it is unpickled. This may depend a\n    # bit on the specifics of our pickler.\n    def reducer(*args):\n      raise Exception(\"There is a problem here.\")\n    class Foo(object):\n      def __init__(self):\n        self.__name__ = \"Foo_object\"\n        self.func_doc = \"\"\n        self.__globals__ = {}\n      def __reduce__(self):\n        return reducer, ()\n      def __call__(self):\n        return\n    f = ray.remote(Foo())\n </s> add     # Create the contents of a temporary Python file.\n    temporary_python_file = \"\"\"\ndef temporary_helper_function():\n  return 1\n\"\"\"\n\n    f = tempfile.NamedTemporaryFile(suffix=\".py\")\n    f.write(temporary_python_file.encode(\"ascii\"))\n    f.flush()\n    directory = os.path.dirname(f.name)\n    # Get the module name and strip \".py\" from the end.\n    module_name = os.path.basename(f.name)[:-3]\n    sys.path.append(directory)\n    module = __import__(module_name)\n\n    # Define a function that closes over this temporary module. This should fail\n    # when it is unpickled.\n    @ray.remote\n    def g():\n      return module.temporary_python_file()\n </s> remove   worker.functions[function_id.id()] = remote(num_return_vals=num_return_vals,\n                                              function_id=function_id,\n                                              num_cpus=num_cpus,\n                                              num_gpus=num_gpus)(lambda *xs: f())\n </s> add   remote_f_placeholder = remote(function_id=function_id)(lambda *xs: f())\n  worker.functions[driver_id][function_id.id()] = (function_name, remote_f_placeholder)\n  worker.function_properties[driver_id][function_id.id()] = (num_return_vals, num_cpus, num_gpus) </s> remove     self.assertTrue(b\"There is a problem here.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"No module named\", ray.error_info()[0][b\"message\"])\n    self.assertIn(b\"No module named\", ray.error_info()[1][b\"message\"]) </s> remove         function_id = FunctionID((hashlib.sha256(func_name.encode(\"ascii\")).digest())[:20])\n </s> add         # Compute the function ID as a hash of the function name as well as the\n        # source code. We could in principle hash in the values in the closure\n        # of the function, but that is likely to introduce non-determinism in\n        # the computation of the function ID.\n        function_id_hash = hashlib.sha1()\n        function_id_hash.update(func_name.encode(\"ascii\"))\n        function_id_hash.update(inspect.getsource(func).encode(\"ascii\"))\n        function_id = function_id_hash.digest()\n        assert len(function_id) == 20\n        function_id = FunctionID(function_id)", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "test/failure_test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     for ref in [x, y, z]:\n <mask>       try:\n <mask>         ray.get(ref)\n <mask>       except Exception as e:\n <mask>         self.assertTrue(\"Test function 3 intentionally failed.\" in str(e))\n <mask>       else:\n <mask>         self.assertTrue(False) # ray.get should throw an exception\n <mask> \n <mask>     ray.worker.cleanup()\n <mask> \n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove       self.assertTrue(\"Test function 2 intentionally failed.\" in str(e))\n </s> add       self.assertIn(\"Test function 2 intentionally failed.\", str(e)) </s> remove       self.assertTrue(b\"Test function 1 intentionally failed.\" in task.get(b\"message\"))\n </s> add       self.assertIn(b\"Test function 1 intentionally failed.\", task.get(b\"message\")) </s> remove       self.assertRaises(Exception, lambda : ray.get(f.remote()))\n </s> add       self.assertRaises(Exception, lambda : ray.get(g.remote()))\n\n    f.close() </s> remove     # This example is somewhat contrived. It should be successfully pickled, and\n    # then it should throw an exception when it is unpickled. This may depend a\n    # bit on the specifics of our pickler.\n    def reducer(*args):\n      raise Exception(\"There is a problem here.\")\n    class Foo(object):\n      def __init__(self):\n        self.__name__ = \"Foo_object\"\n        self.func_doc = \"\"\n        self.__globals__ = {}\n      def __reduce__(self):\n        return reducer, ()\n      def __call__(self):\n        return\n    f = ray.remote(Foo())\n </s> add     # Create the contents of a temporary Python file.\n    temporary_python_file = \"\"\"\ndef temporary_helper_function():\n  return 1\n\"\"\"\n\n    f = tempfile.NamedTemporaryFile(suffix=\".py\")\n    f.write(temporary_python_file.encode(\"ascii\"))\n    f.flush()\n    directory = os.path.dirname(f.name)\n    # Get the module name and strip \".py\" from the end.\n    module_name = os.path.basename(f.name)[:-3]\n    sys.path.append(directory)\n    module = __import__(module_name)\n\n    # Define a function that closes over this temporary module. This should fail\n    # when it is unpickled.\n    @ray.remote\n    def g():\n      return module.temporary_python_file()\n </s> remove     self.assertTrue(b\"There is a problem here.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"No module named\", ray.error_info()[0][b\"message\"])\n    self.assertIn(b\"No module named\", ray.error_info()[1][b\"message\"]) </s> remove     self.assertTrue(b\"Function to run failed.\" in ray.error_info()[0][b\"message\"])\n    self.assertTrue(b\"Function to run failed.\" in ray.error_info()[1][b\"message\"])\n </s> add     self.assertIn(b\"Function to run failed.\", ray.error_info()[0][b\"message\"])\n    self.assertIn(b\"Function to run failed.\", ray.error_info()[1][b\"message\"])", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "test/failure_test.py"}
{"docstring_tokens": "keep keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep replace keep", "code_tokens": " <mask>     ray.init(num_workers=2, driver_mode=ray.SILENT_MODE)\n <mask> \n <mask>     # This example is somewhat contrived. It should be successfully pickled, and\n <mask>     # then it should throw an exception when it is unpickled. This may depend a\n <mask>     # bit on the specifics of our pickler.\n <mask>     def reducer(*args):\n <mask>       raise Exception(\"There is a problem here.\")\n <mask>     class Foo(object):\n <mask>       def __init__(self):\n <mask>         self.__name__ = \"Foo_object\"\n <mask>         self.func_doc = \"\"\n <mask>         self.__globals__ = {}\n <mask>       def __reduce__(self):\n <mask>         return reducer, ()\n <mask>       def __call__(self):\n <mask>         return\n <mask>     f = ray.remote(Foo())\n <mask>     wait_for_errors(b\"register_remote_function\", 2)\n <mask>     self.assertTrue(b\"There is a problem here.\" in ray.error_info()[0][b\"message\"])\n <mask> \n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove     self.assertTrue(b\"The initializer failed.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"The initializer failed.\", ray.error_info()[0][b\"message\"]) </s> remove   worker.functions[function_id.id()] = remote(num_return_vals=num_return_vals,\n                                              function_id=function_id,\n                                              num_cpus=num_cpus,\n                                              num_gpus=num_gpus)(lambda *xs: f())\n </s> add   remote_f_placeholder = remote(function_id=function_id)(lambda *xs: f())\n  worker.functions[driver_id][function_id.id()] = (function_name, remote_f_placeholder)\n  worker.function_properties[driver_id][function_id.id()] = (num_return_vals, num_cpus, num_gpus) </s> remove   # overwritten if the function is unpickled successfully.\n </s> add   # overwritten if the function is successfully registered. </s> remove     subprocess.Popen([stop_ray_script]).wait()\n </s> add   def testRemoteFunctionIsolation(self):\n    # This test will run multiple remote functions with the same names in two\n    # different drivers.\n    # Connect a driver to the Ray cluster.\n    ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE)\n\n    # Start another driver and make sure that it can define and call its own\n    # commands with the same names.\n    driver_script = \"\"\"\nimport ray\nimport time\nray.init(redis_address=\"{}\")\n@ray.remote\ndef f():\n  return 3\n@ray.remote\ndef g(x, y):\n  return 4\nfor _ in range(10000):\n  result = ray.get([f.remote(), g.remote(0, 0)])\n  assert result == [3, 4]\nprint(\"success\")\n\"\"\".format(self.redis_address)\n\n    # Save the driver script as a file so we can call it using subprocess.\n    with tempfile.NamedTemporaryFile() as f:\n      f.write(driver_script.encode(\"ascii\"))\n      f.flush()\n      out = subprocess.check_output([\"python\", f.name]).decode(\"ascii\")\n\n    @ray.remote\n    def f():\n      return 1\n\n    @ray.remote\n    def g(x):\n      return 2\n\n    for _ in range(10000):\n      result = ray.get([f.remote(), g.remote(0)])\n      self.assertEqual(result, [1, 2])\n\n    # Make sure the other driver succeeded.\n    self.assertIn(\"success\", out)\n\n    ray.worker.cleanup() </s> add     num_cpus (int): The number of CPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver.\n    num_gpus (int): The number of GPUs needed to execute this function. This\n      should only be passed in when defining the remote function on the driver.", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "test/failure_test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     # Check that if we try to call the function it throws an exception and does\n <mask>     # not hang.\n <mask>     for _ in range(10):\n <mask>       self.assertRaises(Exception, lambda : ray.get(f.remote()))\n <mask> \n <mask>     ray.worker.cleanup()\n <mask> \n <mask>   def testFailImportingEnvironmentVariable(self):\n <mask>     ray.init(num_workers=2, driver_mode=ray.SILENT_MODE)\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove     self.assertTrue(b\"There is a problem here.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"No module named\", ray.error_info()[0][b\"message\"])\n    self.assertIn(b\"No module named\", ray.error_info()[1][b\"message\"]) </s> remove     # This example is somewhat contrived. It should be successfully pickled, and\n    # then it should throw an exception when it is unpickled. This may depend a\n    # bit on the specifics of our pickler.\n    def reducer(*args):\n      raise Exception(\"There is a problem here.\")\n    class Foo(object):\n      def __init__(self):\n        self.__name__ = \"Foo_object\"\n        self.func_doc = \"\"\n        self.__globals__ = {}\n      def __reduce__(self):\n        return reducer, ()\n      def __call__(self):\n        return\n    f = ray.remote(Foo())\n </s> add     # Create the contents of a temporary Python file.\n    temporary_python_file = \"\"\"\ndef temporary_helper_function():\n  return 1\n\"\"\"\n\n    f = tempfile.NamedTemporaryFile(suffix=\".py\")\n    f.write(temporary_python_file.encode(\"ascii\"))\n    f.flush()\n    directory = os.path.dirname(f.name)\n    # Get the module name and strip \".py\" from the end.\n    module_name = os.path.basename(f.name)[:-3]\n    sys.path.append(directory)\n    module = __import__(module_name)\n\n    # Define a function that closes over this temporary module. This should fail\n    # when it is unpickled.\n    @ray.remote\n    def g():\n      return module.temporary_python_file()\n </s> add     # Clean up the junk we added to sys.path.\n    sys.path.pop(-1) </s> remove     subprocess.Popen([stop_ray_script]).wait()\n </s> add   def testRemoteFunctionIsolation(self):\n    # This test will run multiple remote functions with the same names in two\n    # different drivers.\n    # Connect a driver to the Ray cluster.\n    ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE)\n\n    # Start another driver and make sure that it can define and call its own\n    # commands with the same names.\n    driver_script = \"\"\"\nimport ray\nimport time\nray.init(redis_address=\"{}\")\n@ray.remote\ndef f():\n  return 3\n@ray.remote\ndef g(x, y):\n  return 4\nfor _ in range(10000):\n  result = ray.get([f.remote(), g.remote(0, 0)])\n  assert result == [3, 4]\nprint(\"success\")\n\"\"\".format(self.redis_address)\n\n    # Save the driver script as a file so we can call it using subprocess.\n    with tempfile.NamedTemporaryFile() as f:\n      f.write(driver_script.encode(\"ascii\"))\n      f.flush()\n      out = subprocess.check_output([\"python\", f.name]).decode(\"ascii\")\n\n    @ray.remote\n    def f():\n      return 1\n\n    @ray.remote\n    def g(x):\n      return 2\n\n    for _ in range(10000):\n      result = ray.get([f.remote(), g.remote(0)])\n      self.assertEqual(result, [1, 2])\n\n    # Make sure the other driver succeeded.\n    self.assertIn(\"success\", out)\n\n    ray.worker.cleanup() </s> remove     self.assertTrue(b\"The reinitializer failed.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"The reinitializer failed.\", ray.error_info()[0][b\"message\"]) </s> remove     self.assertTrue(b\"The initializer failed.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"The initializer failed.\", ray.error_info()[0][b\"message\"])", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "test/failure_test.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask> \n <mask>     f.close()\n <mask> \n <mask>     ray.worker.cleanup()\n <mask> \n <mask>   def testFailImportingEnvironmentVariable(self):\n <mask>     ray.init(num_workers=2, driver_mode=ray.SILENT_MODE)\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove       self.assertRaises(Exception, lambda : ray.get(f.remote()))\n </s> add       self.assertRaises(Exception, lambda : ray.get(g.remote()))\n\n    f.close() </s> remove     self.assertTrue(b\"The reinitializer failed.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"The reinitializer failed.\", ray.error_info()[0][b\"message\"]) </s> remove     self.assertTrue(b\"The initializer failed.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"The initializer failed.\", ray.error_info()[0][b\"message\"]) </s> remove     ray.init(redis_address=redis_address, driver_mode=ray.SILENT_MODE)\n </s> add     ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE) </s> remove     subprocess.Popen([stop_ray_script]).wait()\n </s> add   def testRemoteFunctionIsolation(self):\n    # This test will run multiple remote functions with the same names in two\n    # different drivers.\n    # Connect a driver to the Ray cluster.\n    ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE)\n\n    # Start another driver and make sure that it can define and call its own\n    # commands with the same names.\n    driver_script = \"\"\"\nimport ray\nimport time\nray.init(redis_address=\"{}\")\n@ray.remote\ndef f():\n  return 3\n@ray.remote\ndef g(x, y):\n  return 4\nfor _ in range(10000):\n  result = ray.get([f.remote(), g.remote(0, 0)])\n  assert result == [3, 4]\nprint(\"success\")\n\"\"\".format(self.redis_address)\n\n    # Save the driver script as a file so we can call it using subprocess.\n    with tempfile.NamedTemporaryFile() as f:\n      f.write(driver_script.encode(\"ascii\"))\n      f.flush()\n      out = subprocess.check_output([\"python\", f.name]).decode(\"ascii\")\n\n    @ray.remote\n    def f():\n      return 1\n\n    @ray.remote\n    def g(x):\n      return 2\n\n    for _ in range(10000):\n      result = ray.get([f.remote(), g.remote(0)])\n      self.assertEqual(result, [1, 2])\n\n    # Make sure the other driver succeeded.\n    self.assertIn(\"success\", out)\n\n    ray.worker.cleanup() </s> remove         self.assertTrue(\"Test function 3 intentionally failed.\" in str(e))\n </s> add         self.assertIn(\"Test function 3 intentionally failed.\", str(e))", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "test/failure_test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>       return 0\n <mask>     ray.env.foo = ray.EnvironmentVariable(initializer)\n <mask>     wait_for_errors(b\"register_environment_variable\", 2)\n <mask>     # Check that the error message is in the task info.\n <mask>     self.assertTrue(b\"The initializer failed.\" in ray.error_info()[0][b\"message\"])\n <mask> \n <mask>     ray.worker.cleanup()\n <mask> \n <mask>   def testFailReinitializingVariable(self):\n <mask>     ray.init(num_workers=2, driver_mode=ray.SILENT_MODE)\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove     self.assertTrue(b\"The reinitializer failed.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"The reinitializer failed.\", ray.error_info()[0][b\"message\"]) </s> remove     self.assertTrue(b\"Function to run failed.\" in ray.error_info()[0][b\"message\"])\n    self.assertTrue(b\"Function to run failed.\" in ray.error_info()[1][b\"message\"])\n </s> add     self.assertIn(b\"Function to run failed.\", ray.error_info()[0][b\"message\"])\n    self.assertIn(b\"Function to run failed.\", ray.error_info()[1][b\"message\"]) </s> remove     self.assertTrue(b\"There is a problem here.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"No module named\", ray.error_info()[0][b\"message\"])\n    self.assertIn(b\"No module named\", ray.error_info()[1][b\"message\"]) </s> remove       self.assertRaises(Exception, lambda : ray.get(f.remote()))\n </s> add       self.assertRaises(Exception, lambda : ray.get(g.remote()))\n\n    f.close() </s> remove     # This example is somewhat contrived. It should be successfully pickled, and\n    # then it should throw an exception when it is unpickled. This may depend a\n    # bit on the specifics of our pickler.\n    def reducer(*args):\n      raise Exception(\"There is a problem here.\")\n    class Foo(object):\n      def __init__(self):\n        self.__name__ = \"Foo_object\"\n        self.func_doc = \"\"\n        self.__globals__ = {}\n      def __reduce__(self):\n        return reducer, ()\n      def __call__(self):\n        return\n    f = ray.remote(Foo())\n </s> add     # Create the contents of a temporary Python file.\n    temporary_python_file = \"\"\"\ndef temporary_helper_function():\n  return 1\n\"\"\"\n\n    f = tempfile.NamedTemporaryFile(suffix=\".py\")\n    f.write(temporary_python_file.encode(\"ascii\"))\n    f.flush()\n    directory = os.path.dirname(f.name)\n    # Get the module name and strip \".py\" from the end.\n    module_name = os.path.basename(f.name)[:-3]\n    sys.path.append(directory)\n    module = __import__(module_name)\n\n    # Define a function that closes over this temporary module. This should fail\n    # when it is unpickled.\n    @ray.remote\n    def g():\n      return module.temporary_python_file()\n </s> add     # Clean up the junk we added to sys.path.\n    sys.path.pop(-1)", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "test/failure_test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>       ray.env.foo\n <mask>     use_foo.remote()\n <mask>     wait_for_errors(b\"reinitialize_environment_variable\", 1)\n <mask>     # Check that the error message is in the task info.\n <mask>     self.assertTrue(b\"The reinitializer failed.\" in ray.error_info()[0][b\"message\"])\n <mask> \n <mask>     ray.worker.cleanup()\n <mask> \n <mask>   def testFailedFunctionToRun(self):\n <mask>     ray.init(num_workers=2, driver_mode=ray.SILENT_MODE)\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove     self.assertTrue(b\"The initializer failed.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"The initializer failed.\", ray.error_info()[0][b\"message\"]) </s> remove     self.assertTrue(b\"Function to run failed.\" in ray.error_info()[0][b\"message\"])\n    self.assertTrue(b\"Function to run failed.\" in ray.error_info()[1][b\"message\"])\n </s> add     self.assertIn(b\"Function to run failed.\", ray.error_info()[0][b\"message\"])\n    self.assertIn(b\"Function to run failed.\", ray.error_info()[1][b\"message\"]) </s> remove       self.assertRaises(Exception, lambda : ray.get(f.remote()))\n </s> add       self.assertRaises(Exception, lambda : ray.get(g.remote()))\n\n    f.close() </s> add     # Clean up the junk we added to sys.path.\n    sys.path.pop(-1) </s> remove     self.assertTrue(b\"There is a problem here.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"No module named\", ray.error_info()[0][b\"message\"])\n    self.assertIn(b\"No module named\", ray.error_info()[1][b\"message\"]) </s> remove         raise RayGetArgumentError(function.__name__, i, arg, argument)\n </s> add         raise RayGetArgumentError(function_name, i, arg, argument)", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "test/failure_test.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>     ray.worker.global_worker.run_function_on_all_workers(f)\n <mask>     wait_for_errors(b\"function_to_run\", 2)\n <mask>     # Check that the error message is in the task info.\n <mask>     self.assertEqual(len(ray.error_info()), 2)\n <mask>     self.assertTrue(b\"Function to run failed.\" in ray.error_info()[0][b\"message\"])\n <mask>     self.assertTrue(b\"Function to run failed.\" in ray.error_info()[1][b\"message\"])\n <mask> \n <mask>     ray.worker.cleanup()\n <mask> \n <mask> class ActorTest(unittest.TestCase):\n <mask> \n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove     self.assertTrue(b\"The initializer failed.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"The initializer failed.\", ray.error_info()[0][b\"message\"]) </s> remove     self.assertTrue(b\"The reinitializer failed.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"The reinitializer failed.\", ray.error_info()[0][b\"message\"]) </s> remove     self.assertTrue(b\"There is a problem here.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"No module named\", ray.error_info()[0][b\"message\"])\n    self.assertIn(b\"No module named\", ray.error_info()[1][b\"message\"]) </s> remove       self.assertTrue(b\"Test function 1 intentionally failed.\" in task.get(b\"message\"))\n </s> add       self.assertIn(b\"Test function 1 intentionally failed.\", task.get(b\"message\")) </s> remove     subprocess.Popen([stop_ray_script]).wait()\n </s> add   def testRemoteFunctionIsolation(self):\n    # This test will run multiple remote functions with the same names in two\n    # different drivers.\n    # Connect a driver to the Ray cluster.\n    ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE)\n\n    # Start another driver and make sure that it can define and call its own\n    # commands with the same names.\n    driver_script = \"\"\"\nimport ray\nimport time\nray.init(redis_address=\"{}\")\n@ray.remote\ndef f():\n  return 3\n@ray.remote\ndef g(x, y):\n  return 4\nfor _ in range(10000):\n  result = ray.get([f.remote(), g.remote(0, 0)])\n  assert result == [3, 4]\nprint(\"success\")\n\"\"\".format(self.redis_address)\n\n    # Save the driver script as a file so we can call it using subprocess.\n    with tempfile.NamedTemporaryFile() as f:\n      f.write(driver_script.encode(\"ascii\"))\n      f.flush()\n      out = subprocess.check_output([\"python\", f.name]).decode(\"ascii\")\n\n    @ray.remote\n    def f():\n      return 1\n\n    @ray.remote\n    def g(x):\n      return 2\n\n    for _ in range(10000):\n      result = ray.get([f.remote(), g.remote(0)])\n      self.assertEqual(result, [1, 2])\n\n    # Make sure the other driver succeeded.\n    self.assertIn(\"success\", out)\n\n    ray.worker.cleanup() </s> remove         function_id = FunctionID((hashlib.sha256(func_name.encode(\"ascii\")).digest())[:20])\n </s> add         # Compute the function ID as a hash of the function name as well as the\n        # source code. We could in principle hash in the values in the closure\n        # of the function, but that is likely to introduce non-determinism in\n        # the computation of the function ID.\n        function_id_hash = hashlib.sha1()\n        function_id_hash.update(func_name.encode(\"ascii\"))\n        function_id_hash.update(inspect.getsource(func).encode(\"ascii\"))\n        function_id = function_id_hash.digest()\n        assert len(function_id) == 20\n        function_id = FunctionID(function_id)", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "test/failure_test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> stop_ray_script = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"../scripts/stop_ray.sh\")\n <mask> \n <mask> class MultiNodeTest(unittest.TestCase):\n <mask> \n <mask>   def testErrorIsolation(self):\n <mask>     # Start the Ray processes on this machine.\n <mask>     out = subprocess.check_output([start_ray_script, \"--head\"]).decode(\"ascii\")\n <mask>     # Get the redis address from the output.\n <mask>     redis_substring_prefix = \"redis_address=\\\"\"\n <mask>     redis_address_location = out.find(redis_substring_prefix) + len(redis_substring_prefix)\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove     redis_address = redis_address.split(\"\\\"\")[0]\n </s> add     self.redis_address = redis_address.split(\"\\\"\")[0]\n\n  def tearDown(self):\n    # Kill the Ray cluster.\n    subprocess.Popen([stop_ray_script]).wait()\n\n  def testErrorIsolation(self): </s> remove     ray.init(redis_address=redis_address, driver_mode=ray.SILENT_MODE)\n </s> add     ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE) </s> remove         arguments = get_arguments_for_execution(worker.functions[function_id.id()], args, worker)\n </s> add         arguments = get_arguments_for_execution(function_name, args, worker) </s> remove     # This example is somewhat contrived. It should be successfully pickled, and\n    # then it should throw an exception when it is unpickled. This may depend a\n    # bit on the specifics of our pickler.\n    def reducer(*args):\n      raise Exception(\"There is a problem here.\")\n    class Foo(object):\n      def __init__(self):\n        self.__name__ = \"Foo_object\"\n        self.func_doc = \"\"\n        self.__globals__ = {}\n      def __reduce__(self):\n        return reducer, ()\n      def __call__(self):\n        return\n    f = ray.remote(Foo())\n </s> add     # Create the contents of a temporary Python file.\n    temporary_python_file = \"\"\"\ndef temporary_helper_function():\n  return 1\n\"\"\"\n\n    f = tempfile.NamedTemporaryFile(suffix=\".py\")\n    f.write(temporary_python_file.encode(\"ascii\"))\n    f.flush()\n    directory = os.path.dirname(f.name)\n    # Get the module name and strip \".py\" from the end.\n    module_name = os.path.basename(f.name)[:-3]\n    sys.path.append(directory)\n    module = __import__(module_name)\n\n    # Define a function that closes over this temporary module. This should fail\n    # when it is unpickled.\n    @ray.remote\n    def g():\n      return module.temporary_python_file()\n </s> remove       function_name = worker.function_names[function_id.id()]\n </s> add       function_name, function_executor = worker.functions[worker.task_driver_id.id()][function_id.id()] </s> remove     subprocess.Popen([stop_ray_script]).wait()\n </s> add   def testRemoteFunctionIsolation(self):\n    # This test will run multiple remote functions with the same names in two\n    # different drivers.\n    # Connect a driver to the Ray cluster.\n    ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE)\n\n    # Start another driver and make sure that it can define and call its own\n    # commands with the same names.\n    driver_script = \"\"\"\nimport ray\nimport time\nray.init(redis_address=\"{}\")\n@ray.remote\ndef f():\n  return 3\n@ray.remote\ndef g(x, y):\n  return 4\nfor _ in range(10000):\n  result = ray.get([f.remote(), g.remote(0, 0)])\n  assert result == [3, 4]\nprint(\"success\")\n\"\"\".format(self.redis_address)\n\n    # Save the driver script as a file so we can call it using subprocess.\n    with tempfile.NamedTemporaryFile() as f:\n      f.write(driver_script.encode(\"ascii\"))\n      f.flush()\n      out = subprocess.check_output([\"python\", f.name]).decode(\"ascii\")\n\n    @ray.remote\n    def f():\n      return 1\n\n    @ray.remote\n    def g(x):\n      return 2\n\n    for _ in range(10000):\n      result = ray.get([f.remote(), g.remote(0)])\n      self.assertEqual(result, [1, 2])\n\n    # Make sure the other driver succeeded.\n    self.assertIn(\"success\", out)\n\n    ray.worker.cleanup()", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "test/multi_node_test.py"}
{"docstring_tokens": "keep keep keep keep replace keep replace", "code_tokens": " <mask>     # Get the redis address from the output.\n <mask>     redis_substring_prefix = \"redis_address=\\\"\"\n <mask>     redis_address_location = out.find(redis_substring_prefix) + len(redis_substring_prefix)\n <mask>     redis_address = out[redis_address_location:]\n <mask>     redis_address = redis_address.split(\"\\\"\")[0]\n <mask>     # Connect a driver to the Ray cluster.\n <mask>     ray.init(redis_address=redis_address, driver_mode=ray.SILENT_MODE)\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove   def testErrorIsolation(self):\n </s> add   def setUp(self): </s> remove     subprocess.Popen([stop_ray_script]).wait()\n </s> add   def testRemoteFunctionIsolation(self):\n    # This test will run multiple remote functions with the same names in two\n    # different drivers.\n    # Connect a driver to the Ray cluster.\n    ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE)\n\n    # Start another driver and make sure that it can define and call its own\n    # commands with the same names.\n    driver_script = \"\"\"\nimport ray\nimport time\nray.init(redis_address=\"{}\")\n@ray.remote\ndef f():\n  return 3\n@ray.remote\ndef g(x, y):\n  return 4\nfor _ in range(10000):\n  result = ray.get([f.remote(), g.remote(0, 0)])\n  assert result == [3, 4]\nprint(\"success\")\n\"\"\".format(self.redis_address)\n\n    # Save the driver script as a file so we can call it using subprocess.\n    with tempfile.NamedTemporaryFile() as f:\n      f.write(driver_script.encode(\"ascii\"))\n      f.flush()\n      out = subprocess.check_output([\"python\", f.name]).decode(\"ascii\")\n\n    @ray.remote\n    def f():\n      return 1\n\n    @ray.remote\n    def g(x):\n      return 2\n\n    for _ in range(10000):\n      result = ray.get([f.remote(), g.remote(0)])\n      self.assertEqual(result, [1, 2])\n\n    # Make sure the other driver succeeded.\n    self.assertIn(\"success\", out)\n\n    ray.worker.cleanup() </s> remove         arguments = get_arguments_for_execution(worker.functions[function_id.id()], args, worker)\n </s> add         arguments = get_arguments_for_execution(function_name, args, worker) </s> remove       function_name = worker.function_names[function_id.id()]\n </s> add       function_name, function_executor = worker.functions[worker.task_driver_id.id()][function_id.id()] </s> remove     # This example is somewhat contrived. It should be successfully pickled, and\n    # then it should throw an exception when it is unpickled. This may depend a\n    # bit on the specifics of our pickler.\n    def reducer(*args):\n      raise Exception(\"There is a problem here.\")\n    class Foo(object):\n      def __init__(self):\n        self.__name__ = \"Foo_object\"\n        self.func_doc = \"\"\n        self.__globals__ = {}\n      def __reduce__(self):\n        return reducer, ()\n      def __call__(self):\n        return\n    f = ray.remote(Foo())\n </s> add     # Create the contents of a temporary Python file.\n    temporary_python_file = \"\"\"\ndef temporary_helper_function():\n  return 1\n\"\"\"\n\n    f = tempfile.NamedTemporaryFile(suffix=\".py\")\n    f.write(temporary_python_file.encode(\"ascii\"))\n    f.flush()\n    directory = os.path.dirname(f.name)\n    # Get the module name and strip \".py\" from the end.\n    module_name = os.path.basename(f.name)[:-3]\n    sys.path.append(directory)\n    module = __import__(module_name)\n\n    # Define a function that closes over this temporary module. This should fail\n    # when it is unpickled.\n    @ray.remote\n    def g():\n      return module.temporary_python_file()\n", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "test/multi_node_test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> assert \"{}\" in ray.error_info()[0][b\"message\"].decode(\"ascii\")\n <mask> \n <mask> print(\"success\")\n <mask> \"\"\".format(redis_address, error_string2, error_string2)\n <mask> \n <mask>     # Save the driver script as a file so we can call it using subprocess.\n <mask>     with tempfile.NamedTemporaryFile() as f:\n <mask>       f.write(driver_script.encode(\"ascii\"))\n <mask>       f.flush()\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove     subprocess.Popen([stop_ray_script]).wait()\n </s> add   def testRemoteFunctionIsolation(self):\n    # This test will run multiple remote functions with the same names in two\n    # different drivers.\n    # Connect a driver to the Ray cluster.\n    ray.init(redis_address=self.redis_address, driver_mode=ray.SILENT_MODE)\n\n    # Start another driver and make sure that it can define and call its own\n    # commands with the same names.\n    driver_script = \"\"\"\nimport ray\nimport time\nray.init(redis_address=\"{}\")\n@ray.remote\ndef f():\n  return 3\n@ray.remote\ndef g(x, y):\n  return 4\nfor _ in range(10000):\n  result = ray.get([f.remote(), g.remote(0, 0)])\n  assert result == [3, 4]\nprint(\"success\")\n\"\"\".format(self.redis_address)\n\n    # Save the driver script as a file so we can call it using subprocess.\n    with tempfile.NamedTemporaryFile() as f:\n      f.write(driver_script.encode(\"ascii\"))\n      f.flush()\n      out = subprocess.check_output([\"python\", f.name]).decode(\"ascii\")\n\n    @ray.remote\n    def f():\n      return 1\n\n    @ray.remote\n    def g(x):\n      return 2\n\n    for _ in range(10000):\n      result = ray.get([f.remote(), g.remote(0)])\n      self.assertEqual(result, [1, 2])\n\n    # Make sure the other driver succeeded.\n    self.assertIn(\"success\", out)\n\n    ray.worker.cleanup() </s> remove         function_id = FunctionID((hashlib.sha256(func_name.encode(\"ascii\")).digest())[:20])\n </s> add         # Compute the function ID as a hash of the function name as well as the\n        # source code. We could in principle hash in the values in the closure\n        # of the function, but that is likely to introduce non-determinism in\n        # the computation of the function ID.\n        function_id_hash = hashlib.sha1()\n        function_id_hash.update(func_name.encode(\"ascii\"))\n        function_id_hash.update(inspect.getsource(func).encode(\"ascii\"))\n        function_id = function_id_hash.digest()\n        assert len(function_id) == 20\n        function_id = FunctionID(function_id) </s> remove def _submit_task(function_id, func_name, args, num_cpus, num_gpus, worker=global_worker):\n </s> add def _submit_task(function_id, func_name, args, worker=global_worker): </s> remove     # Check that the number of imports we have is at least as great as the\n    # export counter for the task. If not, wait until we have imported enough.\n    # We will push warnings to the user if we spend too long in this loop.\n    with log_span(\"ray:wait_for_import_counter\", worker=worker):\n      wait_for_valid_import_counter(function_id, task.driver_id().id(), worker=worker)\n </s> add     # Wait until the function to be executed has actually been registered on\n    # this worker. We will push warnings to the user if we spend too long in\n    # this loop.\n    with log_span(\"ray:wait_for_function\", worker=worker):\n      wait_for_function(function_id, task.driver_id().id(), worker=worker) </s> remove   return worker.submit_task(function_id, func_name, args, num_cpus, num_gpus)\n </s> add   return worker.submit_task(function_id, func_name, args) </s> remove   worker.redis_client.hset(worker_info_key, \"export_counter\", 0)\n  worker.worker_import_counter = 0\n  # The number of imports is similar to the worker_import_counter except that it\n  # also counts actors.\n </s> add   # Keep track of the number of imports that we've imported.", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "test/multi_node_test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     self.assertEqual(len(ray.error_info()), 1)\n <mask>     self.assertIn(error_string1, ray.error_info()[0][b\"message\"].decode(\"ascii\"))\n <mask> \n <mask>     ray.worker.cleanup()\n <mask>     subprocess.Popen([stop_ray_script]).wait()\n <mask> \n <mask> class StartRayScriptTest(unittest.TestCase):\n <mask> \n <mask>   def testCallingStartRayHead(self):\n <mask>     # Test that we can call start-ray.sh with various command line parameters.\n </s> Simplify imports and exports and provide driver isolation for remote functions. (#288)\n\n* Remove import counter and export counter.\r\n\r\n* Provide isolation between drivers for remote functions.\r\n\r\n* Add test for driver function isolation.\r\n\r\n* Hash source code into function ID to reduce likelihood of collisions.\r\n\r\n* Fix failure test example.\r\n\r\n* Replace assertTrue with assertIn to improve failure messages in tests.\r\n\r\n* Fix failure test. </s> remove     self.assertTrue(b\"Function to run failed.\" in ray.error_info()[0][b\"message\"])\n    self.assertTrue(b\"Function to run failed.\" in ray.error_info()[1][b\"message\"])\n </s> add     self.assertIn(b\"Function to run failed.\", ray.error_info()[0][b\"message\"])\n    self.assertIn(b\"Function to run failed.\", ray.error_info()[1][b\"message\"]) </s> remove \"\"\".format(redis_address, error_string2, error_string2)\n </s> add \"\"\".format(self.redis_address, error_string2, error_string2) </s> remove     redis_address = redis_address.split(\"\\\"\")[0]\n </s> add     self.redis_address = redis_address.split(\"\\\"\")[0]\n\n  def tearDown(self):\n    # Kill the Ray cluster.\n    subprocess.Popen([stop_ray_script]).wait()\n\n  def testErrorIsolation(self): </s> remove       self.assertRaises(Exception, lambda : ray.get(f.remote()))\n </s> add       self.assertRaises(Exception, lambda : ray.get(g.remote()))\n\n    f.close() </s> remove     self.assertTrue(b\"The reinitializer failed.\" in ray.error_info()[0][b\"message\"])\n </s> add     self.assertIn(b\"The reinitializer failed.\", ray.error_info()[0][b\"message\"]) </s> remove def _submit_task(function_id, func_name, args, num_cpus, num_gpus, worker=global_worker):\n </s> add def _submit_task(function_id, func_name, args, worker=global_worker):", "html_url": "https://github.com/ray-project/ray/commit/88a5b4e77bb628b56514ec948044f7ee8ef61bdb", "file_name": "test/multi_node_test.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>         nodes = self.workers()\n <mask>         print(self.debug_string(nodes))\n <mask>         self.load_metrics.prune_active_ips(\n <mask>             [self.provider.internal_ip(node_id) for node_id in nodes])\n <mask> \n <mask>         # Terminate any idle or out of date nodes\n <mask>         last_used = self.load_metrics.last_used_time_by_ip\n <mask>         horizon = time.time() - (60 * self.config[\"idle_timeout_minutes\"])\n <mask>         num_terminated = 0\n <mask>         for node_id in nodes:\n </s> autoscaler: count head node, don't kill below target (fixes #2317) (#2320)\n\nSpecifically, subtracts 1 from the target number of workers, taking into\r\naccount that the head node has some computational resources.\r\n\r\nDo not kill an idle node if it would drop us below the target number of\r\nnodes (in which case we just immediately relaunch). </s> remove                     len(nodes) - num_terminated > self.config[\"min_workers\"]:\n </s> add                     len(nodes) - num_terminated > target_workers: </s> remove             self.launch_new_node(min(max_allowed, target_num - num_nodes))\n </s> add             num_launches = min(max_allowed, target_workers - num_workers)\n            self.launch_new_node(num_launches) </s> remove         target_num = self.target_num_workers()\n        num_nodes = len(nodes) + num_pending\n        if num_nodes < target_num:\n </s> add         num_workers = len(nodes) + num_pending\n        if num_workers < target_workers: </s> remove         ideal_num_workers = int(np.ceil(cur_used / float(target_frac)))\n </s> add         ideal_num_nodes = int(np.ceil(cur_used / float(target_frac)))\n        ideal_num_workers = ideal_num_nodes - 1  # subtract 1 for head node </s> remove         self.assertEqual(len(self.provider.nodes({})), 2)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 1)\n\n    def testDontScaleBelowTarget(self):\n        config = SMALL_CLUSTER.copy()\n        config[\"min_workers\"] = 0\n        config[\"max_workers\"] = 2\n        config[\"target_utilization_fraction\"] = 0.5\n        config_path = self.write_config(config)\n        self.provider = MockProvider()\n        lm = LoadMetrics()\n        autoscaler = StandardAutoscaler(\n            config_path, lm, max_failures=0, update_interval_s=0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n        autoscaler.update()\n        self.assertEqual(autoscaler.num_launches_pending.value, 0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n\n        # Scales up as nodes are reported as used\n        local_ip = services.get_node_ip_address()\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 0})  # head\n        # 1.0 nodes used => target nodes = 2 => target workers = 1\n        autoscaler.update()\n        self.waitForNodes(1)\n\n        # Make new node idle, and never used.\n        # Should hold steady as target is still 2.\n        lm.update(\"172.0.0.0\", {\"CPU\": 0}, {\"CPU\": 0})\n        lm.last_used_time_by_ip[\"172.0.0.0\"] = 0\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 1)\n\n        # Reduce load on head => target nodes = 1 => target workers = 0\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 1})\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 0) </s> remove         self.assertEqual(len(self.provider.nodes({})), 6)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 5)", "html_url": "https://github.com/ray-project/ray/commit/89460b8d11bdd6194e7752cff55229714a6ccd60", "file_name": "python/ray/autoscaler/autoscaler.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         num_terminated = 0\n <mask>         for node_id in nodes:\n <mask>             node_ip = self.provider.internal_ip(node_id)\n <mask>             if node_ip in last_used and last_used[node_ip] < horizon and \\\n <mask>                     len(nodes) - num_terminated > self.config[\"min_workers\"]:\n <mask>                 num_terminated += 1\n <mask>                 print(\"StandardAutoscaler: Terminating idle node: \"\n <mask>                       \"{}\".format(node_id))\n <mask>                 self.provider.terminate_node(node_id)\n <mask>             elif not self.launch_config_ok(node_id):\n </s> autoscaler: count head node, don't kill below target (fixes #2317) (#2320)\n\nSpecifically, subtracts 1 from the target number of workers, taking into\r\naccount that the head node has some computational resources.\r\n\r\nDo not kill an idle node if it would drop us below the target number of\r\nnodes (in which case we just immediately relaunch). </s> add         target_workers = self.target_num_workers() </s> remove             self.launch_new_node(min(max_allowed, target_num - num_nodes))\n </s> add             num_launches = min(max_allowed, target_workers - num_workers)\n            self.launch_new_node(num_launches) </s> remove         target_num = self.target_num_workers()\n        num_nodes = len(nodes) + num_pending\n        if num_nodes < target_num:\n </s> add         num_workers = len(nodes) + num_pending\n        if num_workers < target_workers: </s> remove         self.assertEqual(len(self.provider.nodes({})), 2)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 1)\n\n    def testDontScaleBelowTarget(self):\n        config = SMALL_CLUSTER.copy()\n        config[\"min_workers\"] = 0\n        config[\"max_workers\"] = 2\n        config[\"target_utilization_fraction\"] = 0.5\n        config_path = self.write_config(config)\n        self.provider = MockProvider()\n        lm = LoadMetrics()\n        autoscaler = StandardAutoscaler(\n            config_path, lm, max_failures=0, update_interval_s=0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n        autoscaler.update()\n        self.assertEqual(autoscaler.num_launches_pending.value, 0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n\n        # Scales up as nodes are reported as used\n        local_ip = services.get_node_ip_address()\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 0})  # head\n        # 1.0 nodes used => target nodes = 2 => target workers = 1\n        autoscaler.update()\n        self.waitForNodes(1)\n\n        # Make new node idle, and never used.\n        # Should hold steady as target is still 2.\n        lm.update(\"172.0.0.0\", {\"CPU\": 0}, {\"CPU\": 0})\n        lm.last_used_time_by_ip[\"172.0.0.0\"] = 0\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 1)\n\n        # Reduce load on head => target nodes = 1 => target workers = 0\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 1})\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 0) </s> remove         config[\"min_workers\"] = 2\n </s> add         config[\"min_workers\"] = 1 </s> remove         ideal_num_workers = int(np.ceil(cur_used / float(target_frac)))\n </s> add         ideal_num_nodes = int(np.ceil(cur_used / float(target_frac)))\n        ideal_num_workers = ideal_num_nodes - 1  # subtract 1 for head node", "html_url": "https://github.com/ray-project/ray/commit/89460b8d11bdd6194e7752cff55229714a6ccd60", "file_name": "python/ray/autoscaler/autoscaler.py"}
{"docstring_tokens": "keep keep keep replace replace replace keep keep replace", "code_tokens": " <mask>             print(self.debug_string(nodes))\n <mask> \n <mask>         # Launch new nodes if needed\n <mask>         target_num = self.target_num_workers()\n <mask>         num_nodes = len(nodes) + num_pending\n <mask>         if num_nodes < target_num:\n <mask>             max_allowed = min(self.max_launch_batch,\n <mask>                               self.max_concurrent_launches - num_pending)\n <mask>             self.launch_new_node(min(max_allowed, target_num - num_nodes))\n </s> autoscaler: count head node, don't kill below target (fixes #2317) (#2320)\n\nSpecifically, subtracts 1 from the target number of workers, taking into\r\naccount that the head node has some computational resources.\r\n\r\nDo not kill an idle node if it would drop us below the target number of\r\nnodes (in which case we just immediately relaunch). </s> remove                     len(nodes) - num_terminated > self.config[\"min_workers\"]:\n </s> add                     len(nodes) - num_terminated > target_workers: </s> add         target_workers = self.target_num_workers() </s> remove         self.assertEqual(len(self.provider.nodes({})), 2)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 1)\n\n    def testDontScaleBelowTarget(self):\n        config = SMALL_CLUSTER.copy()\n        config[\"min_workers\"] = 0\n        config[\"max_workers\"] = 2\n        config[\"target_utilization_fraction\"] = 0.5\n        config_path = self.write_config(config)\n        self.provider = MockProvider()\n        lm = LoadMetrics()\n        autoscaler = StandardAutoscaler(\n            config_path, lm, max_failures=0, update_interval_s=0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n        autoscaler.update()\n        self.assertEqual(autoscaler.num_launches_pending.value, 0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n\n        # Scales up as nodes are reported as used\n        local_ip = services.get_node_ip_address()\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 0})  # head\n        # 1.0 nodes used => target nodes = 2 => target workers = 1\n        autoscaler.update()\n        self.waitForNodes(1)\n\n        # Make new node idle, and never used.\n        # Should hold steady as target is still 2.\n        lm.update(\"172.0.0.0\", {\"CPU\": 0}, {\"CPU\": 0})\n        lm.last_used_time_by_ip[\"172.0.0.0\"] = 0\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 1)\n\n        # Reduce load on head => target nodes = 1 => target workers = 0\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 1})\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 0) </s> remove         ideal_num_workers = int(np.ceil(cur_used / float(target_frac)))\n </s> add         ideal_num_nodes = int(np.ceil(cur_used / float(target_frac)))\n        ideal_num_workers = ideal_num_nodes - 1  # subtract 1 for head node </s> remove         self.assertEqual(len(self.provider.nodes({})), 6)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 5)", "html_url": "https://github.com/ray-project/ray/commit/89460b8d11bdd6194e7752cff55229714a6ccd60", "file_name": "python/ray/autoscaler/autoscaler.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     def target_num_workers(self):\n <mask>         target_frac = self.config[\"target_utilization_fraction\"]\n <mask>         cur_used = self.load_metrics.approx_workers_used()\n <mask>         ideal_num_workers = int(np.ceil(cur_used / float(target_frac)))\n <mask>         return min(self.config[\"max_workers\"],\n <mask>                    max(self.config[\"min_workers\"], ideal_num_workers))\n <mask> \n <mask>     def launch_config_ok(self, node_id):\n <mask>         launch_conf = self.provider.node_tags(node_id).get(\n </s> autoscaler: count head node, don't kill below target (fixes #2317) (#2320)\n\nSpecifically, subtracts 1 from the target number of workers, taking into\r\naccount that the head node has some computational resources.\r\n\r\nDo not kill an idle node if it would drop us below the target number of\r\nnodes (in which case we just immediately relaunch). </s> remove         config[\"min_workers\"] = 2\n </s> add         config[\"min_workers\"] = 1 </s> remove         self.assertEqual(len(self.provider.nodes({})), 2)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 1)\n\n    def testDontScaleBelowTarget(self):\n        config = SMALL_CLUSTER.copy()\n        config[\"min_workers\"] = 0\n        config[\"max_workers\"] = 2\n        config[\"target_utilization_fraction\"] = 0.5\n        config_path = self.write_config(config)\n        self.provider = MockProvider()\n        lm = LoadMetrics()\n        autoscaler = StandardAutoscaler(\n            config_path, lm, max_failures=0, update_interval_s=0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n        autoscaler.update()\n        self.assertEqual(autoscaler.num_launches_pending.value, 0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n\n        # Scales up as nodes are reported as used\n        local_ip = services.get_node_ip_address()\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 0})  # head\n        # 1.0 nodes used => target nodes = 2 => target workers = 1\n        autoscaler.update()\n        self.waitForNodes(1)\n\n        # Make new node idle, and never used.\n        # Should hold steady as target is still 2.\n        lm.update(\"172.0.0.0\", {\"CPU\": 0}, {\"CPU\": 0})\n        lm.last_used_time_by_ip[\"172.0.0.0\"] = 0\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 1)\n\n        # Reduce load on head => target nodes = 1 => target workers = 0\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 1})\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 0) </s> remove         self.assertEqual(len(self.provider.nodes({})), 4)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 3) </s> add         target_workers = self.target_num_workers() </s> remove         target_num = self.target_num_workers()\n        num_nodes = len(nodes) + num_pending\n        if num_nodes < target_num:\n </s> add         num_workers = len(nodes) + num_pending\n        if num_workers < target_workers: </s> remove             self.launch_new_node(min(max_allowed, target_num - num_nodes))\n </s> add             num_launches = min(max_allowed, target_workers - num_workers)\n            self.launch_new_node(num_launches)", "html_url": "https://github.com/ray-project/ray/commit/89460b8d11bdd6194e7752cff55229714a6ccd60", "file_name": "python/ray/autoscaler/autoscaler.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask> \n <mask> import ray\n <mask> from ray.autoscaler.autoscaler import StandardAutoscaler, LoadMetrics, \\\n <mask>     fillout_defaults, validate_config\n <mask> from ray.autoscaler.tags import TAG_RAY_NODE_TYPE, TAG_RAY_NODE_STATUS\n <mask> from ray.autoscaler.node_provider import NODE_PROVIDERS, NodeProvider\n <mask> from ray.autoscaler.updater import NodeUpdaterThread\n <mask> \n </s> autoscaler: count head node, don't kill below target (fixes #2317) (#2320)\n\nSpecifically, subtracts 1 from the target number of workers, taking into\r\naccount that the head node has some computational resources.\r\n\r\nDo not kill an idle node if it would drop us below the target number of\r\nnodes (in which case we just immediately relaunch). </s> remove                     len(nodes) - num_terminated > self.config[\"min_workers\"]:\n </s> add                     len(nodes) - num_terminated > target_workers: </s> remove         self.assertEqual(len(self.provider.nodes({})), 2)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 1)\n\n    def testDontScaleBelowTarget(self):\n        config = SMALL_CLUSTER.copy()\n        config[\"min_workers\"] = 0\n        config[\"max_workers\"] = 2\n        config[\"target_utilization_fraction\"] = 0.5\n        config_path = self.write_config(config)\n        self.provider = MockProvider()\n        lm = LoadMetrics()\n        autoscaler = StandardAutoscaler(\n            config_path, lm, max_failures=0, update_interval_s=0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n        autoscaler.update()\n        self.assertEqual(autoscaler.num_launches_pending.value, 0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n\n        # Scales up as nodes are reported as used\n        local_ip = services.get_node_ip_address()\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 0})  # head\n        # 1.0 nodes used => target nodes = 2 => target workers = 1\n        autoscaler.update()\n        self.waitForNodes(1)\n\n        # Make new node idle, and never used.\n        # Should hold steady as target is still 2.\n        lm.update(\"172.0.0.0\", {\"CPU\": 0}, {\"CPU\": 0})\n        lm.last_used_time_by_ip[\"172.0.0.0\"] = 0\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 1)\n\n        # Reduce load on head => target nodes = 1 => target workers = 0\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 1})\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 0) </s> remove         self.assertEqual(len(self.provider.nodes({})), 4)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 3) </s> remove         self.assertEqual(len(self.provider.nodes({})), 6)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 5) </s> remove         self.waitForNodes(6)\n </s> add         self.waitForNodes(5) </s> remove         self.waitForNodes(4)\n        lm.update(\"172.0.0.2\", {\"CPU\": 2}, {\"CPU\": 0})\n </s> add         self.waitForNodes(3)\n        lm.update(\"172.0.0.1\", {\"CPU\": 2}, {\"CPU\": 0})", "html_url": "https://github.com/ray-project/ray/commit/89460b8d11bdd6194e7752cff55229714a6ccd60", "file_name": "test/autoscaler_test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         self.waitFor(lambda: len(runner.calls) > 0)\n <mask> \n <mask>     def testScaleUpBasedOnLoad(self):\n <mask>         config = SMALL_CLUSTER.copy()\n <mask>         config[\"min_workers\"] = 2\n <mask>         config[\"max_workers\"] = 10\n <mask>         config[\"target_utilization_fraction\"] = 0.5\n <mask>         config_path = self.write_config(config)\n <mask>         self.provider = MockProvider()\n <mask>         lm = LoadMetrics()\n </s> autoscaler: count head node, don't kill below target (fixes #2317) (#2320)\n\nSpecifically, subtracts 1 from the target number of workers, taking into\r\naccount that the head node has some computational resources.\r\n\r\nDo not kill an idle node if it would drop us below the target number of\r\nnodes (in which case we just immediately relaunch). </s> remove         self.assertEqual(len(self.provider.nodes({})), 2)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 1)\n\n    def testDontScaleBelowTarget(self):\n        config = SMALL_CLUSTER.copy()\n        config[\"min_workers\"] = 0\n        config[\"max_workers\"] = 2\n        config[\"target_utilization_fraction\"] = 0.5\n        config_path = self.write_config(config)\n        self.provider = MockProvider()\n        lm = LoadMetrics()\n        autoscaler = StandardAutoscaler(\n            config_path, lm, max_failures=0, update_interval_s=0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n        autoscaler.update()\n        self.assertEqual(autoscaler.num_launches_pending.value, 0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n\n        # Scales up as nodes are reported as used\n        local_ip = services.get_node_ip_address()\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 0})  # head\n        # 1.0 nodes used => target nodes = 2 => target workers = 1\n        autoscaler.update()\n        self.waitForNodes(1)\n\n        # Make new node idle, and never used.\n        # Should hold steady as target is still 2.\n        lm.update(\"172.0.0.0\", {\"CPU\": 0}, {\"CPU\": 0})\n        lm.last_used_time_by_ip[\"172.0.0.0\"] = 0\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 1)\n\n        # Reduce load on head => target nodes = 1 => target workers = 0\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 1})\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 0) </s> remove         ideal_num_workers = int(np.ceil(cur_used / float(target_frac)))\n </s> add         ideal_num_nodes = int(np.ceil(cur_used / float(target_frac)))\n        ideal_num_workers = ideal_num_nodes - 1  # subtract 1 for head node </s> remove                     len(nodes) - num_terminated > self.config[\"min_workers\"]:\n </s> add                     len(nodes) - num_terminated > target_workers: </s> remove         self.assertEqual(len(self.provider.nodes({})), 4)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 3) </s> add         target_workers = self.target_num_workers() </s> remove         target_num = self.target_num_workers()\n        num_nodes = len(nodes) + num_pending\n        if num_nodes < target_num:\n </s> add         num_workers = len(nodes) + num_pending\n        if num_workers < target_workers:", "html_url": "https://github.com/ray-project/ray/commit/89460b8d11bdd6194e7752cff55229714a6ccd60", "file_name": "test/autoscaler_test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         autoscaler = StandardAutoscaler(\n <mask>             config_path, lm, max_failures=0, update_interval_s=0)\n <mask>         self.assertEqual(len(self.provider.nodes({})), 0)\n <mask>         autoscaler.update()\n <mask>         self.waitForNodes(2)\n <mask>         autoscaler.update()\n <mask>         self.assertEqual(autoscaler.num_launches_pending.value, 0)\n <mask>         self.assertEqual(len(self.provider.nodes({})), 2)\n <mask> \n <mask>         # Scales up as nodes are reported as used\n </s> autoscaler: count head node, don't kill below target (fixes #2317) (#2320)\n\nSpecifically, subtracts 1 from the target number of workers, taking into\r\naccount that the head node has some computational resources.\r\n\r\nDo not kill an idle node if it would drop us below the target number of\r\nnodes (in which case we just immediately relaunch). </s> remove         self.assertEqual(len(self.provider.nodes({})), 2)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 1)\n\n    def testDontScaleBelowTarget(self):\n        config = SMALL_CLUSTER.copy()\n        config[\"min_workers\"] = 0\n        config[\"max_workers\"] = 2\n        config[\"target_utilization_fraction\"] = 0.5\n        config_path = self.write_config(config)\n        self.provider = MockProvider()\n        lm = LoadMetrics()\n        autoscaler = StandardAutoscaler(\n            config_path, lm, max_failures=0, update_interval_s=0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n        autoscaler.update()\n        self.assertEqual(autoscaler.num_launches_pending.value, 0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n\n        # Scales up as nodes are reported as used\n        local_ip = services.get_node_ip_address()\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 0})  # head\n        # 1.0 nodes used => target nodes = 2 => target workers = 1\n        autoscaler.update()\n        self.waitForNodes(1)\n\n        # Make new node idle, and never used.\n        # Should hold steady as target is still 2.\n        lm.update(\"172.0.0.0\", {\"CPU\": 0}, {\"CPU\": 0})\n        lm.last_used_time_by_ip[\"172.0.0.0\"] = 0\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 1)\n\n        # Reduce load on head => target nodes = 1 => target workers = 0\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 1})\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 0) </s> remove         self.assertEqual(len(self.provider.nodes({})), 2)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 1) </s> remove         lm.update(\"172.0.0.0\", {\"CPU\": 2}, {\"CPU\": 0})\n        lm.update(\"172.0.0.1\", {\"CPU\": 2}, {\"CPU\": 0})\n </s> add         local_ip = services.get_node_ip_address()\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 0})  # head\n        lm.update(\"172.0.0.0\", {\"CPU\": 2}, {\"CPU\": 0})  # worker 1 </s> remove         self.assertEqual(len(self.provider.nodes({})), 6)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 5) </s> remove         self.assertEqual(len(self.provider.nodes({})), 4)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 3) </s> remove         self.waitForNodes(4)\n        lm.update(\"172.0.0.2\", {\"CPU\": 2}, {\"CPU\": 0})\n </s> add         self.waitForNodes(3)\n        lm.update(\"172.0.0.1\", {\"CPU\": 2}, {\"CPU\": 0})", "html_url": "https://github.com/ray-project/ray/commit/89460b8d11bdd6194e7752cff55229714a6ccd60", "file_name": "test/autoscaler_test.py"}
{"docstring_tokens": "keep keep keep replace keep keep replace replace keep keep", "code_tokens": " <mask>         self.waitForNodes(2)\n <mask>         autoscaler.update()\n <mask>         self.assertEqual(autoscaler.num_launches_pending.value, 0)\n <mask>         self.assertEqual(len(self.provider.nodes({})), 2)\n <mask> \n <mask>         # Scales up as nodes are reported as used\n <mask>         lm.update(\"172.0.0.0\", {\"CPU\": 2}, {\"CPU\": 0})\n <mask>         lm.update(\"172.0.0.1\", {\"CPU\": 2}, {\"CPU\": 0})\n <mask>         autoscaler.update()\n <mask>         self.waitForNodes(4)\n </s> autoscaler: count head node, don't kill below target (fixes #2317) (#2320)\n\nSpecifically, subtracts 1 from the target number of workers, taking into\r\naccount that the head node has some computational resources.\r\n\r\nDo not kill an idle node if it would drop us below the target number of\r\nnodes (in which case we just immediately relaunch). </s> remove         self.waitForNodes(4)\n        lm.update(\"172.0.0.2\", {\"CPU\": 2}, {\"CPU\": 0})\n </s> add         self.waitForNodes(3)\n        lm.update(\"172.0.0.1\", {\"CPU\": 2}, {\"CPU\": 0}) </s> remove         self.waitForNodes(6)\n </s> add         self.waitForNodes(5) </s> remove         self.waitForNodes(2)\n </s> add         self.waitForNodes(1) </s> remove         self.assertEqual(len(self.provider.nodes({})), 2)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 1)\n\n    def testDontScaleBelowTarget(self):\n        config = SMALL_CLUSTER.copy()\n        config[\"min_workers\"] = 0\n        config[\"max_workers\"] = 2\n        config[\"target_utilization_fraction\"] = 0.5\n        config_path = self.write_config(config)\n        self.provider = MockProvider()\n        lm = LoadMetrics()\n        autoscaler = StandardAutoscaler(\n            config_path, lm, max_failures=0, update_interval_s=0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n        autoscaler.update()\n        self.assertEqual(autoscaler.num_launches_pending.value, 0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n\n        # Scales up as nodes are reported as used\n        local_ip = services.get_node_ip_address()\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 0})  # head\n        # 1.0 nodes used => target nodes = 2 => target workers = 1\n        autoscaler.update()\n        self.waitForNodes(1)\n\n        # Make new node idle, and never used.\n        # Should hold steady as target is still 2.\n        lm.update(\"172.0.0.0\", {\"CPU\": 0}, {\"CPU\": 0})\n        lm.last_used_time_by_ip[\"172.0.0.0\"] = 0\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 1)\n\n        # Reduce load on head => target nodes = 1 => target workers = 0\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 1})\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 0) </s> remove         self.assertEqual(len(self.provider.nodes({})), 6)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 5)", "html_url": "https://github.com/ray-project/ray/commit/89460b8d11bdd6194e7752cff55229714a6ccd60", "file_name": "test/autoscaler_test.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         # Scales up as nodes are reported as used\n <mask>         lm.update(\"172.0.0.0\", {\"CPU\": 2}, {\"CPU\": 0})\n <mask>         lm.update(\"172.0.0.1\", {\"CPU\": 2}, {\"CPU\": 0})\n <mask>         autoscaler.update()\n <mask>         self.waitForNodes(4)\n <mask>         lm.update(\"172.0.0.2\", {\"CPU\": 2}, {\"CPU\": 0})\n <mask>         autoscaler.update()\n <mask>         self.waitForNodes(6)\n <mask> \n <mask>         # Holds steady when load is removed\n <mask>         lm.update(\"172.0.0.0\", {\"CPU\": 2}, {\"CPU\": 2})\n </s> autoscaler: count head node, don't kill below target (fixes #2317) (#2320)\n\nSpecifically, subtracts 1 from the target number of workers, taking into\r\naccount that the head node has some computational resources.\r\n\r\nDo not kill an idle node if it would drop us below the target number of\r\nnodes (in which case we just immediately relaunch). </s> remove         self.waitForNodes(6)\n </s> add         self.waitForNodes(5) </s> remove         lm.update(\"172.0.0.0\", {\"CPU\": 2}, {\"CPU\": 0})\n        lm.update(\"172.0.0.1\", {\"CPU\": 2}, {\"CPU\": 0})\n </s> add         local_ip = services.get_node_ip_address()\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 0})  # head\n        lm.update(\"172.0.0.0\", {\"CPU\": 2}, {\"CPU\": 0})  # worker 1 </s> remove         self.assertEqual(len(self.provider.nodes({})), 2)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 1) </s> remove         self.assertEqual(len(self.provider.nodes({})), 6)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 5) </s> remove         self.assertEqual(len(self.provider.nodes({})), 2)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 1)\n\n    def testDontScaleBelowTarget(self):\n        config = SMALL_CLUSTER.copy()\n        config[\"min_workers\"] = 0\n        config[\"max_workers\"] = 2\n        config[\"target_utilization_fraction\"] = 0.5\n        config_path = self.write_config(config)\n        self.provider = MockProvider()\n        lm = LoadMetrics()\n        autoscaler = StandardAutoscaler(\n            config_path, lm, max_failures=0, update_interval_s=0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n        autoscaler.update()\n        self.assertEqual(autoscaler.num_launches_pending.value, 0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n\n        # Scales up as nodes are reported as used\n        local_ip = services.get_node_ip_address()\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 0})  # head\n        # 1.0 nodes used => target nodes = 2 => target workers = 1\n        autoscaler.update()\n        self.waitForNodes(1)\n\n        # Make new node idle, and never used.\n        # Should hold steady as target is still 2.\n        lm.update(\"172.0.0.0\", {\"CPU\": 0}, {\"CPU\": 0})\n        lm.last_used_time_by_ip[\"172.0.0.0\"] = 0\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 1)\n\n        # Reduce load on head => target nodes = 1 => target workers = 0\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 1})\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 0) </s> remove         self.waitForNodes(2)\n </s> add         self.waitForNodes(1)", "html_url": "https://github.com/ray-project/ray/commit/89460b8d11bdd6194e7752cff55229714a6ccd60", "file_name": "test/autoscaler_test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         autoscaler.update()\n <mask>         self.waitForNodes(4)\n <mask>         lm.update(\"172.0.0.2\", {\"CPU\": 2}, {\"CPU\": 0})\n <mask>         autoscaler.update()\n <mask>         self.waitForNodes(6)\n <mask> \n <mask>         # Holds steady when load is removed\n <mask>         lm.update(\"172.0.0.0\", {\"CPU\": 2}, {\"CPU\": 2})\n <mask>         lm.update(\"172.0.0.1\", {\"CPU\": 2}, {\"CPU\": 2})\n <mask>         autoscaler.update()\n </s> autoscaler: count head node, don't kill below target (fixes #2317) (#2320)\n\nSpecifically, subtracts 1 from the target number of workers, taking into\r\naccount that the head node has some computational resources.\r\n\r\nDo not kill an idle node if it would drop us below the target number of\r\nnodes (in which case we just immediately relaunch). </s> remove         self.waitForNodes(4)\n        lm.update(\"172.0.0.2\", {\"CPU\": 2}, {\"CPU\": 0})\n </s> add         self.waitForNodes(3)\n        lm.update(\"172.0.0.1\", {\"CPU\": 2}, {\"CPU\": 0}) </s> remove         lm.update(\"172.0.0.0\", {\"CPU\": 2}, {\"CPU\": 0})\n        lm.update(\"172.0.0.1\", {\"CPU\": 2}, {\"CPU\": 0})\n </s> add         local_ip = services.get_node_ip_address()\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 0})  # head\n        lm.update(\"172.0.0.0\", {\"CPU\": 2}, {\"CPU\": 0})  # worker 1 </s> remove         self.assertEqual(len(self.provider.nodes({})), 6)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 5) </s> remove         self.assertEqual(len(self.provider.nodes({})), 2)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 1) </s> remove         self.assertEqual(len(self.provider.nodes({})), 2)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 1)\n\n    def testDontScaleBelowTarget(self):\n        config = SMALL_CLUSTER.copy()\n        config[\"min_workers\"] = 0\n        config[\"max_workers\"] = 2\n        config[\"target_utilization_fraction\"] = 0.5\n        config_path = self.write_config(config)\n        self.provider = MockProvider()\n        lm = LoadMetrics()\n        autoscaler = StandardAutoscaler(\n            config_path, lm, max_failures=0, update_interval_s=0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n        autoscaler.update()\n        self.assertEqual(autoscaler.num_launches_pending.value, 0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n\n        # Scales up as nodes are reported as used\n        local_ip = services.get_node_ip_address()\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 0})  # head\n        # 1.0 nodes used => target nodes = 2 => target workers = 1\n        autoscaler.update()\n        self.waitForNodes(1)\n\n        # Make new node idle, and never used.\n        # Should hold steady as target is still 2.\n        lm.update(\"172.0.0.0\", {\"CPU\": 0}, {\"CPU\": 0})\n        lm.last_used_time_by_ip[\"172.0.0.0\"] = 0\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 1)\n\n        # Reduce load on head => target nodes = 1 => target workers = 0\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 1})\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 0) </s> remove         self.waitForNodes(2)\n </s> add         self.waitForNodes(1)", "html_url": "https://github.com/ray-project/ray/commit/89460b8d11bdd6194e7752cff55229714a6ccd60", "file_name": "test/autoscaler_test.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         lm.update(\"172.0.0.0\", {\"CPU\": 2}, {\"CPU\": 2})\n <mask>         lm.update(\"172.0.0.1\", {\"CPU\": 2}, {\"CPU\": 2})\n <mask>         autoscaler.update()\n <mask>         self.assertEqual(autoscaler.num_launches_pending.value, 0)\n <mask>         self.assertEqual(len(self.provider.nodes({})), 6)\n <mask> \n <mask>         # Scales down as nodes become unused\n <mask>         lm.last_used_time_by_ip[\"172.0.0.0\"] = 0\n <mask>         lm.last_used_time_by_ip[\"172.0.0.1\"] = 0\n <mask>         autoscaler.update()\n </s> autoscaler: count head node, don't kill below target (fixes #2317) (#2320)\n\nSpecifically, subtracts 1 from the target number of workers, taking into\r\naccount that the head node has some computational resources.\r\n\r\nDo not kill an idle node if it would drop us below the target number of\r\nnodes (in which case we just immediately relaunch). </s> remove         self.waitForNodes(6)\n </s> add         self.waitForNodes(5) </s> remove         self.waitForNodes(4)\n        lm.update(\"172.0.0.2\", {\"CPU\": 2}, {\"CPU\": 0})\n </s> add         self.waitForNodes(3)\n        lm.update(\"172.0.0.1\", {\"CPU\": 2}, {\"CPU\": 0}) </s> remove         self.assertEqual(len(self.provider.nodes({})), 4)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 3) </s> remove         self.assertEqual(len(self.provider.nodes({})), 2)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 1)\n\n    def testDontScaleBelowTarget(self):\n        config = SMALL_CLUSTER.copy()\n        config[\"min_workers\"] = 0\n        config[\"max_workers\"] = 2\n        config[\"target_utilization_fraction\"] = 0.5\n        config_path = self.write_config(config)\n        self.provider = MockProvider()\n        lm = LoadMetrics()\n        autoscaler = StandardAutoscaler(\n            config_path, lm, max_failures=0, update_interval_s=0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n        autoscaler.update()\n        self.assertEqual(autoscaler.num_launches_pending.value, 0)\n        self.assertEqual(len(self.provider.nodes({})), 0)\n\n        # Scales up as nodes are reported as used\n        local_ip = services.get_node_ip_address()\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 0})  # head\n        # 1.0 nodes used => target nodes = 2 => target workers = 1\n        autoscaler.update()\n        self.waitForNodes(1)\n\n        # Make new node idle, and never used.\n        # Should hold steady as target is still 2.\n        lm.update(\"172.0.0.0\", {\"CPU\": 0}, {\"CPU\": 0})\n        lm.last_used_time_by_ip[\"172.0.0.0\"] = 0\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 1)\n\n        # Reduce load on head => target nodes = 1 => target workers = 0\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 1})\n        autoscaler.update()\n        self.assertEqual(len(self.provider.nodes({})), 0) </s> remove         lm.update(\"172.0.0.0\", {\"CPU\": 2}, {\"CPU\": 0})\n        lm.update(\"172.0.0.1\", {\"CPU\": 2}, {\"CPU\": 0})\n </s> add         local_ip = services.get_node_ip_address()\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 0})  # head\n        lm.update(\"172.0.0.0\", {\"CPU\": 2}, {\"CPU\": 0})  # worker 1 </s> remove         self.assertEqual(len(self.provider.nodes({})), 2)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 1)", "html_url": "https://github.com/ray-project/ray/commit/89460b8d11bdd6194e7752cff55229714a6ccd60", "file_name": "test/autoscaler_test.py"}
{"docstring_tokens": "keep keep keep replace keep keep keep keep replace keep keep", "code_tokens": " <mask>         lm.last_used_time_by_ip[\"172.0.0.1\"] = 0\n <mask>         autoscaler.update()\n <mask>         self.assertEqual(autoscaler.num_launches_pending.value, 0)\n <mask>         self.assertEqual(len(self.provider.nodes({})), 4)\n <mask>         lm.last_used_time_by_ip[\"172.0.0.2\"] = 0\n <mask>         lm.last_used_time_by_ip[\"172.0.0.3\"] = 0\n <mask>         autoscaler.update()\n <mask>         self.assertEqual(autoscaler.num_launches_pending.value, 0)\n <mask>         self.assertEqual(len(self.provider.nodes({})), 2)\n <mask> \n <mask>     def testRecoverUnhealthyWorkers(self):\n </s> autoscaler: count head node, don't kill below target (fixes #2317) (#2320)\n\nSpecifically, subtracts 1 from the target number of workers, taking into\r\naccount that the head node has some computational resources.\r\n\r\nDo not kill an idle node if it would drop us below the target number of\r\nnodes (in which case we just immediately relaunch). </s> remove         self.assertEqual(len(self.provider.nodes({})), 6)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 5) </s> remove         self.waitForNodes(2)\n </s> add         self.waitForNodes(1) </s> remove         self.assertEqual(len(self.provider.nodes({})), 2)\n </s> add         self.assertEqual(len(self.provider.nodes({})), 1) </s> remove         lm.update(\"172.0.0.0\", {\"CPU\": 2}, {\"CPU\": 0})\n        lm.update(\"172.0.0.1\", {\"CPU\": 2}, {\"CPU\": 0})\n </s> add         local_ip = services.get_node_ip_address()\n        lm.update(local_ip, {\"CPU\": 2}, {\"CPU\": 0})  # head\n        lm.update(\"172.0.0.0\", {\"CPU\": 2}, {\"CPU\": 0})  # worker 1 </s> remove         config[\"min_workers\"] = 2\n </s> add         config[\"min_workers\"] = 1", "html_url": "https://github.com/ray-project/ray/commit/89460b8d11bdd6194e7752cff55229714a6ccd60", "file_name": "test/autoscaler_test.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask>    projects.rst\n <mask>    signals.rst\n <mask>    async_api.rst\n <mask>    serve.rst\n <mask>    multiprocessing.rst\n <mask> \n <mask> .. toctree::\n <mask>    :maxdepth: -1\n </s> Add experimental parallel iterators API (#6644) </s> add py_test(\n    name = \"test_iter\",\n    size = \"small\",\n    srcs = [\"test_iter.py\"],\n    tags = [\"exclusive\"],\n    deps = [\"//:ray_lib\"],\n)\n </s> remove     \"TensorFlowVariables\", \"get_actor\", \"register_actor\", \"get\", \"wait\",\n    \"set_flushing_policy\", \"GcsFlushPolicy\", \"SimpleGcsFlushPolicy\",\n    \"set_resource\", \"ActorPool\"\n </s> add     \"TensorFlowVariables\",\n    \"get_actor\",\n    \"register_actor\",\n    \"get\",\n    \"wait\",\n    \"set_flushing_policy\",\n    \"GcsFlushPolicy\",\n    \"SimpleGcsFlushPolicy\",\n    \"set_resource\",\n    \"ActorPool\",\n    \"iter\", </s> add from . import iter", "html_url": "https://github.com/ray-project/ray/commit/895f2727fb39eee9d56756451f1405525c7dfa87", "file_name": "doc/source/index.rst"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> from .actor_pool import ActorPool\n <mask> from .dynamic_resources import set_resource\n <mask> \n <mask> \n <mask> def TensorFlowVariables(*args, **kwargs):\n <mask>     raise DeprecationWarning(\n </s> Add experimental parallel iterators API (#6644) </s> add py_test(\n    name = \"test_iter\",\n    size = \"small\",\n    srcs = [\"test_iter.py\"],\n    tags = [\"exclusive\"],\n    deps = [\"//:ray_lib\"],\n)\n </s> remove     \"TensorFlowVariables\", \"get_actor\", \"register_actor\", \"get\", \"wait\",\n    \"set_flushing_policy\", \"GcsFlushPolicy\", \"SimpleGcsFlushPolicy\",\n    \"set_resource\", \"ActorPool\"\n </s> add     \"TensorFlowVariables\",\n    \"get_actor\",\n    \"register_actor\",\n    \"get\",\n    \"wait\",\n    \"set_flushing_policy\",\n    \"GcsFlushPolicy\",\n    \"SimpleGcsFlushPolicy\",\n    \"set_resource\",\n    \"ActorPool\",\n    \"iter\", </s> add    iter.rst", "html_url": "https://github.com/ray-project/ray/commit/895f2727fb39eee9d56756451f1405525c7dfa87", "file_name": "python/ray/experimental/__init__.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep", "code_tokens": " <mask>         \" do 'from ray.experimental.tf_utils import TensorFlowVariables'.\")\n <mask> \n <mask> \n <mask> __all__ = [\n <mask>     \"TensorFlowVariables\", \"get_actor\", \"register_actor\", \"get\", \"wait\",\n <mask>     \"set_flushing_policy\", \"GcsFlushPolicy\", \"SimpleGcsFlushPolicy\",\n <mask>     \"set_resource\", \"ActorPool\"\n <mask> ]\n </s> Add experimental parallel iterators API (#6644) </s> add py_test(\n    name = \"test_iter\",\n    size = \"small\",\n    srcs = [\"test_iter.py\"],\n    tags = [\"exclusive\"],\n    deps = [\"//:ray_lib\"],\n)\n </s> add from . import iter </s> add    iter.rst", "html_url": "https://github.com/ray-project/ray/commit/895f2727fb39eee9d56756451f1405525c7dfa87", "file_name": "python/ray/experimental/__init__.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>     deps = [\"//:ray_lib\"],\n <mask> )\n <mask> \n <mask> py_test(\n <mask>     name = \"test_actor_resources\",\n <mask>     size = \"medium\",\n <mask>     srcs = [\"test_actor_resources.py\"],\n </s> Add experimental parallel iterators API (#6644) </s> remove     \"TensorFlowVariables\", \"get_actor\", \"register_actor\", \"get\", \"wait\",\n    \"set_flushing_policy\", \"GcsFlushPolicy\", \"SimpleGcsFlushPolicy\",\n    \"set_resource\", \"ActorPool\"\n </s> add     \"TensorFlowVariables\",\n    \"get_actor\",\n    \"register_actor\",\n    \"get\",\n    \"wait\",\n    \"set_flushing_policy\",\n    \"GcsFlushPolicy\",\n    \"SimpleGcsFlushPolicy\",\n    \"set_resource\",\n    \"ActorPool\",\n    \"iter\", </s> add from . import iter </s> add    iter.rst", "html_url": "https://github.com/ray-project/ray/commit/895f2727fb39eee9d56756451f1405525c7dfa87", "file_name": "python/ray/tests/BUILD"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> -----------------\n <mask> \n <mask> PyCaret is an open source low-code machine learning library in Python that aims to reduce the hypothesis to insights cycle time in a ML experiment. It enables data scientists to perform end-to-end experiments quickly and efficiently.\n <mask> \n <mask> Github: `https://github.com/pycaret/pycaret/ <https://github.com/pycaret/pycaret/>`_\n <mask> \n <mask> Seldon Alibi |seldon|\n <mask> ---------------------\n <mask> \n <mask> Alibi is an open source Python library aimed at machine learning model inspection and interpretation. The focus of the library is to provide high-quality implementations of black-box, white-box, local and global explanation methods for classification and regression models.\n </s> [docs] Add RayDP link to community integrations (#10851) </s> remove Github: `https://github.com/SeldonIO/alibi <https://github.com/SeldonIO/alibi>`__\n </s> add GitHub: `https://github.com/SeldonIO/alibi <https://github.com/SeldonIO/alibi>`__ </s> add .. |raydp| image:: images/intel.png\n    :class: inline-figure\n    :height: 30\n", "html_url": "https://github.com/ray-project/ray/commit/89da3f9ba7f68c16e7467dc545ed5c9510670400", "file_name": "doc/source/ray-libraries.rst"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> ---------------------\n <mask> \n <mask> Alibi is an open source Python library aimed at machine learning model inspection and interpretation. The focus of the library is to provide high-quality implementations of black-box, white-box, local and global explanation methods for classification and regression models.\n <mask> \n <mask> Github: `https://github.com/SeldonIO/alibi <https://github.com/SeldonIO/alibi>`__\n <mask> \n <mask> Spacy |spacy|\n <mask> -------------\n <mask> spaCy is a library for advanced Natural Language Processing in Python and Cython. It's built on the very latest research, and was designed from day one to be used in real products.\n <mask> \n </s> [docs] Add RayDP link to community integrations (#10851) </s> remove Github: `https://github.com/pycaret/pycaret/ <https://github.com/pycaret/pycaret/>`_\n </s> add GitHub: `https://github.com/pycaret/pycaret <https://github.com/pycaret/pycaret>`_\n\nRayDP |raydp|\n-------------\n\nRayDP (\"Spark on Ray\") enables you to easily use Spark inside a Ray program. You can use Spark to read the input data, process the data using SQL, Spark DataFrame, or Pandas (via Koalas) API, extract and transform features using Spark MLLib, and use RayDP Estimator API for distributed training on the preprocessed dataset.\n\nGitHub: `https://github.com/Intel-bigdata/oap-raydp <https://github.com/Intel-bigdata/oap-raydp>`_ </s> add .. |raydp| image:: images/intel.png\n    :class: inline-figure\n    :height: 30\n", "html_url": "https://github.com/ray-project/ray/commit/89da3f9ba7f68c16e7467dc545ed5c9510670400", "file_name": "doc/source/ray-libraries.rst"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask>     :height: 30\n <mask> \n <mask> .. |seldon| image:: images/seldon.png\n <mask>     :class: inline-figure\n <mask>     :height: 30\n <mask> \n <mask> .. |spacy| image:: images/spacy.png\n </s> [docs] Add RayDP link to community integrations (#10851) </s> remove Github: `https://github.com/SeldonIO/alibi <https://github.com/SeldonIO/alibi>`__\n </s> add GitHub: `https://github.com/SeldonIO/alibi <https://github.com/SeldonIO/alibi>`__ </s> remove Github: `https://github.com/pycaret/pycaret/ <https://github.com/pycaret/pycaret/>`_\n </s> add GitHub: `https://github.com/pycaret/pycaret <https://github.com/pycaret/pycaret>`_\n\nRayDP |raydp|\n-------------\n\nRayDP (\"Spark on Ray\") enables you to easily use Spark inside a Ray program. You can use Spark to read the input data, process the data using SQL, Spark DataFrame, or Pandas (via Koalas) API, extract and transform features using Spark MLLib, and use RayDP Estimator API for distributed training on the preprocessed dataset.\n\nGitHub: `https://github.com/Intel-bigdata/oap-raydp <https://github.com/Intel-bigdata/oap-raydp>`_", "html_url": "https://github.com/ray-project/ray/commit/89da3f9ba7f68c16e7467dc545ed5c9510670400", "file_name": "doc/source/ray-libraries.rst"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask> \n <mask> def convert_ndarray_to_tf_tensor(\n <mask>     ndarray: np.ndarray,\n <mask>     dtype: Optional[tf.dtypes.DType] = None,\n <mask> ) -> tf.Tensor:\n <mask>     \"\"\"Convert a NumPy ndarray to a TensorFlow Tensor.\n <mask> \n <mask>     Args:\n <mask>         ndarray: A NumPy ndarray that we wish to convert to a TensorFlow Tensor.\n </s> [Datasets] Add ragged tensor support for `to_tf` (#29476)\n\nIf your dataset contains heterogeneously shaped tensors (e.g., from differently-sized images), then to_tf doesn't work. </s> add         type_spec: A type spec that specifies the shape and dtype of the returned\n            tensor. If you specify ``dtype``, the dtype stored in the type spec is\n            ignored. </s> remove     return tf.convert_to_tensor(ndarray, dtype=dtype)\n </s> add     if is_ragged:\n        return tf.ragged.constant(ndarray, dtype=dtype)\n    else:\n        return tf.convert_to_tensor(ndarray, dtype=dtype) </s> add     if dtype is None and type_spec is not None:\n        dtype = type_spec.dtype\n\n    is_ragged = isinstance(type_spec, tf.RaggedTensorSpec) </s> add     def get_tensor_spec(\n        dtype: Union[np.dtype, pa.DataType], *, name: str\n    ) -> tf.TypeSpec:\n        shape, dtype = get_shape(dtype), get_dtype(dtype)\n        # Batch dimension is always `None`. So, if there's more than one `None`-valued\n        # dimension, then the tensor is ragged.\n        is_ragged = sum(dim is None for dim in shape) > 1\n        if is_ragged:\n            type_spec = tf.RaggedTensorSpec(shape, dtype=dtype)\n        else:\n            type_spec = tf.TensorSpec(shape, dtype=dtype, name=name)\n        return type_spec\n </s> remove         def get_columns_from_batch(\n            batch: Dict[str, tf.Tensor], *, columns: Union[str, List[str]]\n </s> add         def convert_batch_to_tensors(\n            batch: Dict[str, np.ndarray],\n            *,\n            columns: Union[str, List[str]],\n            type_spec: Union[tf.TypeSpec, Dict[str, tf.TypeSpec]], </s> remove                 return batch[columns]\n            return {column: batch[column] for column in columns}\n </s> add                 return convert_ndarray_to_tf_tensor(batch[columns], type_spec=type_spec)\n            return {\n                convert_ndarray_to_tf_tensor(batch[column], type_spec=type_spec[column])\n                for column in columns\n            }", "html_url": "https://github.com/ray-project/ray/commit/89f17082078c81f403a3f658665b7c16f63cff03", "file_name": "python/ray/air/_internal/tensorflow_utils.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask>     Args:\n <mask>         ndarray: A NumPy ndarray that we wish to convert to a TensorFlow Tensor.\n <mask>         dtype: A TensorFlow dtype for the created tensor; if None, the dtype will be\n <mask>             inferred from the NumPy ndarray data.\n <mask> \n <mask>     Returns: A TensorFlow Tensor.\n <mask>     \"\"\"\n <mask>     if dtype is None and type_spec is not None:\n </s> [Datasets] Add ragged tensor support for `to_tf` (#29476)\n\nIf your dataset contains heterogeneously shaped tensors (e.g., from differently-sized images), then to_tf doesn't work. </s> add     type_spec: Optional[tf.TypeSpec] = None, </s> add     if dtype is None and type_spec is not None:\n        dtype = type_spec.dtype\n\n    is_ragged = isinstance(type_spec, tf.RaggedTensorSpec) </s> remove     return tf.convert_to_tensor(ndarray, dtype=dtype)\n </s> add     if is_ragged:\n        return tf.ragged.constant(ndarray, dtype=dtype)\n    else:\n        return tf.convert_to_tensor(ndarray, dtype=dtype) </s> add     def get_tensor_spec(\n        dtype: Union[np.dtype, pa.DataType], *, name: str\n    ) -> tf.TypeSpec:\n        shape, dtype = get_shape(dtype), get_dtype(dtype)\n        # Batch dimension is always `None`. So, if there's more than one `None`-valued\n        # dimension, then the tensor is ragged.\n        is_ragged = sum(dim is None for dim in shape) > 1\n        if is_ragged:\n            type_spec = tf.RaggedTensorSpec(shape, dtype=dtype)\n        else:\n            type_spec = tf.TensorSpec(shape, dtype=dtype, name=name)\n        return type_spec\n </s> remove         return tf.TensorSpec(get_shape(dtype), dtype=get_dtype(dtype), name=name)\n </s> add         return get_tensor_spec(dtype, name=name) </s> remove         from ray.air._internal.tensorflow_utils import get_type_spec\n </s> add         from ray.air._internal.tensorflow_utils import (\n            get_type_spec,\n            convert_ndarray_to_tf_tensor,\n        )", "html_url": "https://github.com/ray-project/ray/commit/89f17082078c81f403a3f658665b7c16f63cff03", "file_name": "python/ray/air/_internal/tensorflow_utils.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     Returns: A TensorFlow Tensor.\n <mask>     \"\"\"\n <mask>     ndarray = _unwrap_ndarray_object_type_if_needed(ndarray)\n <mask>     if is_ragged:\n <mask>         return tf.ragged.constant(ndarray, dtype=dtype)\n <mask>     else:\n <mask>         return tf.convert_to_tensor(ndarray, dtype=dtype)\n <mask> \n </s> [Datasets] Add ragged tensor support for `to_tf` (#29476)\n\nIf your dataset contains heterogeneously shaped tensors (e.g., from differently-sized images), then to_tf doesn't work. </s> remove     return tf.convert_to_tensor(ndarray, dtype=dtype)\n </s> add     if is_ragged:\n        return tf.ragged.constant(ndarray, dtype=dtype)\n    else:\n        return tf.convert_to_tensor(ndarray, dtype=dtype) </s> add         type_spec: A type spec that specifies the shape and dtype of the returned\n            tensor. If you specify ``dtype``, the dtype stored in the type spec is\n            ignored. </s> add     type_spec: Optional[tf.TypeSpec] = None, </s> add     def get_tensor_spec(\n        dtype: Union[np.dtype, pa.DataType], *, name: str\n    ) -> tf.TypeSpec:\n        shape, dtype = get_shape(dtype), get_dtype(dtype)\n        # Batch dimension is always `None`. So, if there's more than one `None`-valued\n        # dimension, then the tensor is ragged.\n        is_ragged = sum(dim is None for dim in shape) > 1\n        if is_ragged:\n            type_spec = tf.RaggedTensorSpec(shape, dtype=dtype)\n        else:\n            type_spec = tf.TensorSpec(shape, dtype=dtype, name=name)\n        return type_spec\n </s> remove         from ray.air._internal.tensorflow_utils import get_type_spec\n </s> add         from ray.air._internal.tensorflow_utils import (\n            get_type_spec,\n            convert_ndarray_to_tf_tensor,\n        ) </s> remove         return tf.TensorSpec(get_shape(dtype), dtype=get_dtype(dtype), name=name)\n </s> add         return get_tensor_spec(dtype, name=name)", "html_url": "https://github.com/ray-project/ray/commit/89f17082078c81f403a3f658665b7c16f63cff03", "file_name": "python/ray/air/_internal/tensorflow_utils.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     Returns: A TensorFlow Tensor.\n <mask>     \"\"\"\n <mask>     ndarray = _unwrap_ndarray_object_type_if_needed(ndarray)\n <mask>     return tf.convert_to_tensor(ndarray, dtype=dtype)\n <mask> \n <mask> \n <mask> def convert_ndarray_batch_to_tf_tensor_batch(\n <mask>     ndarrays: Union[np.ndarray, Dict[str, np.ndarray]],\n <mask>     dtypes: Optional[Union[tf.dtypes.DType, Dict[str, tf.dtypes.DType]]] = None,\n </s> [Datasets] Add ragged tensor support for `to_tf` (#29476)\n\nIf your dataset contains heterogeneously shaped tensors (e.g., from differently-sized images), then to_tf doesn't work. </s> add     if dtype is None and type_spec is not None:\n        dtype = type_spec.dtype\n\n    is_ragged = isinstance(type_spec, tf.RaggedTensorSpec) </s> add         type_spec: A type spec that specifies the shape and dtype of the returned\n            tensor. If you specify ``dtype``, the dtype stored in the type spec is\n            ignored. </s> add     type_spec: Optional[tf.TypeSpec] = None, </s> remove         def get_columns_from_batch(\n            batch: Dict[str, tf.Tensor], *, columns: Union[str, List[str]]\n </s> add         def convert_batch_to_tensors(\n            batch: Dict[str, np.ndarray],\n            *,\n            columns: Union[str, List[str]],\n            type_spec: Union[tf.TypeSpec, Dict[str, tf.TypeSpec]], </s> remove                 return batch[columns]\n            return {column: batch[column] for column in columns}\n </s> add                 return convert_ndarray_to_tf_tensor(batch[columns], type_spec=type_spec)\n            return {\n                convert_ndarray_to_tf_tensor(batch[column], type_spec=type_spec[column])\n                for column in columns\n            } </s> remove         from ray.air._internal.tensorflow_utils import get_type_spec\n </s> add         from ray.air._internal.tensorflow_utils import (\n            get_type_spec,\n            convert_ndarray_to_tf_tensor,\n        )", "html_url": "https://github.com/ray-project/ray/commit/89f17082078c81f403a3f658665b7c16f63cff03", "file_name": "python/ray/air/_internal/tensorflow_utils.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>         if isinstance(dtype, TensorDtype):\n <mask>             shape += dtype.element_shape\n <mask>         return shape\n <mask> \n <mask>     if isinstance(columns, str):\n <mask>         name, dtype = columns, dtypes[columns]\n <mask>         return get_tensor_spec(dtype, name=name)\n <mask> \n <mask>     return {\n </s> [Datasets] Add ragged tensor support for `to_tf` (#29476)\n\nIf your dataset contains heterogeneously shaped tensors (e.g., from differently-sized images), then to_tf doesn't work. </s> remove         return tf.TensorSpec(get_shape(dtype), dtype=get_dtype(dtype), name=name)\n </s> add         return get_tensor_spec(dtype, name=name) </s> remove         name: tf.TensorSpec(get_shape(dtype), dtype=get_dtype(dtype), name=name)\n </s> add         name: get_tensor_spec(dtype, name=name) </s> add         type_spec: A type spec that specifies the shape and dtype of the returned\n            tensor. If you specify ``dtype``, the dtype stored in the type spec is\n            ignored. </s> remove                 return batch[columns]\n            return {column: batch[column] for column in columns}\n </s> add                 return convert_ndarray_to_tf_tensor(batch[columns], type_spec=type_spec)\n            return {\n                convert_ndarray_to_tf_tensor(batch[column], type_spec=type_spec[column])\n                for column in columns\n            } </s> remove         def get_columns_from_batch(\n            batch: Dict[str, tf.Tensor], *, columns: Union[str, List[str]]\n </s> add         def convert_batch_to_tensors(\n            batch: Dict[str, np.ndarray],\n            *,\n            columns: Union[str, List[str]],\n            type_spec: Union[tf.TypeSpec, Dict[str, tf.TypeSpec]], </s> add     if dtype is None and type_spec is not None:\n        dtype = type_spec.dtype\n\n    is_ragged = isinstance(type_spec, tf.RaggedTensorSpec)", "html_url": "https://github.com/ray-project/ray/commit/89f17082078c81f403a3f658665b7c16f63cff03", "file_name": "python/ray/air/_internal/tensorflow_utils.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep replace keep keep", "code_tokens": " <mask>         return shape\n <mask> \n <mask>     if isinstance(columns, str):\n <mask>         name, dtype = columns, dtypes[columns]\n <mask>         return tf.TensorSpec(get_shape(dtype), dtype=get_dtype(dtype), name=name)\n <mask> \n <mask>     return {\n <mask>         name: tf.TensorSpec(get_shape(dtype), dtype=get_dtype(dtype), name=name)\n <mask>         for name, dtype in dtypes.items()\n <mask>         if name in columns\n </s> [Datasets] Add ragged tensor support for `to_tf` (#29476)\n\nIf your dataset contains heterogeneously shaped tensors (e.g., from differently-sized images), then to_tf doesn't work. </s> add     def get_tensor_spec(\n        dtype: Union[np.dtype, pa.DataType], *, name: str\n    ) -> tf.TypeSpec:\n        shape, dtype = get_shape(dtype), get_dtype(dtype)\n        # Batch dimension is always `None`. So, if there's more than one `None`-valued\n        # dimension, then the tensor is ragged.\n        is_ragged = sum(dim is None for dim in shape) > 1\n        if is_ragged:\n            type_spec = tf.RaggedTensorSpec(shape, dtype=dtype)\n        else:\n            type_spec = tf.TensorSpec(shape, dtype=dtype, name=name)\n        return type_spec\n </s> remove                 return batch[columns]\n            return {column: batch[column] for column in columns}\n </s> add                 return convert_ndarray_to_tf_tensor(batch[columns], type_spec=type_spec)\n            return {\n                convert_ndarray_to_tf_tensor(batch[column], type_spec=type_spec[column])\n                for column in columns\n            } </s> add         type_spec: A type spec that specifies the shape and dtype of the returned\n            tensor. If you specify ``dtype``, the dtype stored in the type spec is\n            ignored. </s> add     if dtype is None and type_spec is not None:\n        dtype = type_spec.dtype\n\n    is_ragged = isinstance(type_spec, tf.RaggedTensorSpec) </s> remove         def get_columns_from_batch(\n            batch: Dict[str, tf.Tensor], *, columns: Union[str, List[str]]\n </s> add         def convert_batch_to_tensors(\n            batch: Dict[str, np.ndarray],\n            *,\n            columns: Union[str, List[str]],\n            type_spec: Union[tf.TypeSpec, Dict[str, tf.TypeSpec]],", "html_url": "https://github.com/ray-project/ray/commit/89f17082078c81f403a3f658665b7c16f63cff03", "file_name": "python/ray/air/_internal/tensorflow_utils.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             :meth:`~ray.data.Dataset.iter_tf_batches`\n <mask>                 Call this method if you need more flexibility.\n <mask> \n <mask>         \"\"\"  # noqa: E501\n <mask>         from ray.air._internal.tensorflow_utils import get_type_spec\n <mask> \n <mask>         try:\n <mask>             import tensorflow as tf\n <mask>         except ImportError:\n <mask>             raise ValueError(\"tensorflow must be installed!\")\n </s> [Datasets] Add ragged tensor support for `to_tf` (#29476)\n\nIf your dataset contains heterogeneously shaped tensors (e.g., from differently-sized images), then to_tf doesn't work. </s> remove from ray.data.preprocessors import Concatenator\n </s> add  </s> remove from ray.train.tensorflow import TensorflowTrainer\n </s> add  </s> add from ray.data.extensions import TensorArray\nfrom ray.data.preprocessors import Concatenator\nfrom ray.train.tensorflow import TensorflowTrainer </s> add         type_spec: A type spec that specifies the shape and dtype of the returned\n            tensor. If you specify ``dtype``, the dtype stored in the type spec is\n            ignored. </s> add     def get_tensor_spec(\n        dtype: Union[np.dtype, pa.DataType], *, name: str\n    ) -> tf.TypeSpec:\n        shape, dtype = get_shape(dtype), get_dtype(dtype)\n        # Batch dimension is always `None`. So, if there's more than one `None`-valued\n        # dimension, then the tensor is ragged.\n        is_ragged = sum(dim is None for dim in shape) > 1\n        if is_ragged:\n            type_spec = tf.RaggedTensorSpec(shape, dtype=dtype)\n        else:\n            type_spec = tf.TensorSpec(shape, dtype=dtype, name=name)\n        return type_spec\n </s> add     if dtype is None and type_spec is not None:\n        dtype = type_spec.dtype\n\n    is_ragged = isinstance(type_spec, tf.RaggedTensorSpec)", "html_url": "https://github.com/ray-project/ray/commit/89f17082078c81f403a3f658665b7c16f63cff03", "file_name": "python/ray/data/dataset.py"}
{"docstring_tokens": "keep keep replace replace keep keep replace replace keep keep keep", "code_tokens": " <mask>         validate_columns(label_columns)\n <mask> \n <mask>         def get_columns_from_batch(\n <mask>             batch: Dict[str, tf.Tensor], *, columns: Union[str, List[str]]\n <mask>         ) -> Union[tf.Tensor, Dict[str, tf.Tensor]]:\n <mask>             if isinstance(columns, str):\n <mask>                 return batch[columns]\n <mask>             return {column: batch[column] for column in columns}\n <mask> \n <mask>         def generator():\n <mask>             for batch in self.iter_tf_batches(\n </s> [Datasets] Add ragged tensor support for `to_tf` (#29476)\n\nIf your dataset contains heterogeneously shaped tensors (e.g., from differently-sized images), then to_tf doesn't work. </s> remove             for batch in self.iter_tf_batches(\n </s> add             for batch in self.iter_batches( </s> remove     return tf.convert_to_tensor(ndarray, dtype=dtype)\n </s> add     if is_ragged:\n        return tf.ragged.constant(ndarray, dtype=dtype)\n    else:\n        return tf.convert_to_tensor(ndarray, dtype=dtype) </s> add     def get_tensor_spec(\n        dtype: Union[np.dtype, pa.DataType], *, name: str\n    ) -> tf.TypeSpec:\n        shape, dtype = get_shape(dtype), get_dtype(dtype)\n        # Batch dimension is always `None`. So, if there's more than one `None`-valued\n        # dimension, then the tensor is ragged.\n        is_ragged = sum(dim is None for dim in shape) > 1\n        if is_ragged:\n            type_spec = tf.RaggedTensorSpec(shape, dtype=dtype)\n        else:\n            type_spec = tf.TensorSpec(shape, dtype=dtype, name=name)\n        return type_spec\n </s> remove         return tf.TensorSpec(get_shape(dtype), dtype=get_dtype(dtype), name=name)\n </s> add         return get_tensor_spec(dtype, name=name) </s> remove         name: tf.TensorSpec(get_shape(dtype), dtype=get_dtype(dtype), name=name)\n </s> add         name: get_tensor_spec(dtype, name=name)", "html_url": "https://github.com/ray-project/ray/commit/89f17082078c81f403a3f658665b7c16f63cff03", "file_name": "python/ray/data/dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                 return batch[columns]\n <mask>             return {column: batch[column] for column in columns}\n <mask> \n <mask>         def generator():\n <mask>             for batch in self.iter_tf_batches(\n <mask>                 prefetch_blocks=prefetch_blocks,\n <mask>                 batch_size=batch_size,\n <mask>                 drop_last=drop_last,\n <mask>                 local_shuffle_buffer_size=local_shuffle_buffer_size,\n <mask>                 local_shuffle_seed=local_shuffle_seed,\n </s> [Datasets] Add ragged tensor support for `to_tf` (#29476)\n\nIf your dataset contains heterogeneously shaped tensors (e.g., from differently-sized images), then to_tf doesn't work. </s> remove                 return batch[columns]\n            return {column: batch[column] for column in columns}\n </s> add                 return convert_ndarray_to_tf_tensor(batch[columns], type_spec=type_spec)\n            return {\n                convert_ndarray_to_tf_tensor(batch[column], type_spec=type_spec[column])\n                for column in columns\n            } </s> remove         def get_columns_from_batch(\n            batch: Dict[str, tf.Tensor], *, columns: Union[str, List[str]]\n </s> add         def convert_batch_to_tensors(\n            batch: Dict[str, np.ndarray],\n            *,\n            columns: Union[str, List[str]],\n            type_spec: Union[tf.TypeSpec, Dict[str, tf.TypeSpec]], </s> add                 batch_format=\"numpy\", </s> remove                 features = get_columns_from_batch(batch, columns=feature_columns)\n                labels = get_columns_from_batch(batch, columns=label_columns)\n </s> add                 features = convert_batch_to_tensors(\n                    batch, columns=feature_columns, type_spec=feature_type_spec\n                )\n                labels = convert_batch_to_tensors(\n                    batch, columns=label_columns, type_spec=label_type_spec\n                ) </s> remove         name: tf.TensorSpec(get_shape(dtype), dtype=get_dtype(dtype), name=name)\n </s> add         name: get_tensor_spec(dtype, name=name) </s> remove         return tf.TensorSpec(get_shape(dtype), dtype=get_dtype(dtype), name=name)\n </s> add         return get_tensor_spec(dtype, name=name)", "html_url": "https://github.com/ray-project/ray/commit/89f17082078c81f403a3f658665b7c16f63cff03", "file_name": "python/ray/data/dataset.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>                 local_shuffle_buffer_size=local_shuffle_buffer_size,\n <mask>                 local_shuffle_seed=local_shuffle_seed,\n <mask>             ):\n <mask>                 assert isinstance(batch, dict)\n <mask>                 features = convert_batch_to_tensors(\n <mask>                     batch, columns=feature_columns, type_spec=feature_type_spec\n <mask>                 )\n <mask>                 labels = convert_batch_to_tensors(\n </s> [Datasets] Add ragged tensor support for `to_tf` (#29476)\n\nIf your dataset contains heterogeneously shaped tensors (e.g., from differently-sized images), then to_tf doesn't work. </s> remove                 features = get_columns_from_batch(batch, columns=feature_columns)\n                labels = get_columns_from_batch(batch, columns=label_columns)\n </s> add                 features = convert_batch_to_tensors(\n                    batch, columns=feature_columns, type_spec=feature_type_spec\n                )\n                labels = convert_batch_to_tensors(\n                    batch, columns=label_columns, type_spec=label_type_spec\n                ) </s> remove             for batch in self.iter_tf_batches(\n </s> add             for batch in self.iter_batches( </s> remove         def get_columns_from_batch(\n            batch: Dict[str, tf.Tensor], *, columns: Union[str, List[str]]\n </s> add         def convert_batch_to_tensors(\n            batch: Dict[str, np.ndarray],\n            *,\n            columns: Union[str, List[str]],\n            type_spec: Union[tf.TypeSpec, Dict[str, tf.TypeSpec]], </s> remove         from ray.air._internal.tensorflow_utils import get_type_spec\n </s> add         from ray.air._internal.tensorflow_utils import (\n            get_type_spec,\n            convert_ndarray_to_tf_tensor,\n        ) </s> add     type_spec: Optional[tf.TypeSpec] = None, </s> remove                 return batch[columns]\n            return {column: batch[column] for column in columns}\n </s> add                 return convert_ndarray_to_tf_tensor(batch[columns], type_spec=type_spec)\n            return {\n                convert_ndarray_to_tf_tensor(batch[column], type_spec=type_spec[column])\n                for column in columns\n            }", "html_url": "https://github.com/ray-project/ray/commit/89f17082078c81f403a3f658665b7c16f63cff03", "file_name": "python/ray/data/dataset.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>                 local_shuffle_buffer_size=local_shuffle_buffer_size,\n <mask>                 local_shuffle_seed=local_shuffle_seed,\n <mask>             ):\n <mask>                 assert isinstance(batch, dict)\n <mask>                 features = get_columns_from_batch(batch, columns=feature_columns)\n <mask>                 labels = get_columns_from_batch(batch, columns=label_columns)\n <mask>                 yield features, labels\n <mask> \n <mask>         feature_type_spec = get_type_spec(schema, columns=feature_columns)\n <mask>         label_type_spec = get_type_spec(schema, columns=label_columns)\n <mask>         output_signature = (feature_type_spec, label_type_spec)\n </s> [Datasets] Add ragged tensor support for `to_tf` (#29476)\n\nIf your dataset contains heterogeneously shaped tensors (e.g., from differently-sized images), then to_tf doesn't work. </s> add                 batch_format=\"numpy\", </s> remove             for batch in self.iter_tf_batches(\n </s> add             for batch in self.iter_batches( </s> add from ray.data.extensions import TensorArray\nfrom ray.data.preprocessors import Concatenator\nfrom ray.train.tensorflow import TensorflowTrainer </s> remove from ray.train.tensorflow import TensorflowTrainer\n </s> add  </s> remove from ray.data.preprocessors import Concatenator\n </s> add  </s> remove                 return batch[columns]\n            return {column: batch[column] for column in columns}\n </s> add                 return convert_ndarray_to_tf_tensor(batch[columns], type_spec=type_spec)\n            return {\n                convert_ndarray_to_tf_tensor(batch[column], type_spec=type_spec[column])\n                for column in columns\n            }", "html_url": "https://github.com/ray-project/ray/commit/89f17082078c81f403a3f658665b7c16f63cff03", "file_name": "python/ray/data/dataset.py"}
{"docstring_tokens": "keep keep replace keep keep keep keep keep keep keep keep replace keep", "code_tokens": " <mask> \n <mask> import ray\n <mask> from ray.data.preprocessors import Concatenator\n <mask> from ray.air import session\n <mask> from ray.train.tensorflow import TensorflowTrainer\n <mask> from ray.air.config import ScalingConfig\n <mask> from ray.air.constants import TENSOR_COLUMN_NAME\n <mask> \n <mask> import ray\n <mask> from ray.data.preprocessors import Concatenator\n <mask> from ray.air import session\n <mask> from ray.train.tensorflow import TensorflowTrainer\n <mask> from ray.air.config import ScalingConfig\n </s> [Datasets] Add ragged tensor support for `to_tf` (#29476)\n\nIf your dataset contains heterogeneously shaped tensors (e.g., from differently-sized images), then to_tf doesn't work. </s> add from ray.data.extensions import TensorArray\nfrom ray.data.preprocessors import Concatenator\nfrom ray.train.tensorflow import TensorflowTrainer </s> remove         from ray.air._internal.tensorflow_utils import get_type_spec\n </s> add         from ray.air._internal.tensorflow_utils import (\n            get_type_spec,\n            convert_ndarray_to_tf_tensor,\n        ) </s> add         type_spec: A type spec that specifies the shape and dtype of the returned\n            tensor. If you specify ``dtype``, the dtype stored in the type spec is\n            ignored. </s> remove                 features = get_columns_from_batch(batch, columns=feature_columns)\n                labels = get_columns_from_batch(batch, columns=label_columns)\n </s> add                 features = convert_batch_to_tensors(\n                    batch, columns=feature_columns, type_spec=feature_type_spec\n                )\n                labels = convert_batch_to_tensors(\n                    batch, columns=label_columns, type_spec=label_type_spec\n                ) </s> add                 batch_format=\"numpy\",", "html_url": "https://github.com/ray-project/ray/commit/89f17082078c81f403a3f658665b7c16f63cff03", "file_name": "python/ray/data/tests/test_dataset_tf.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask> from ray.air.config import ScalingConfig\n <mask> from ray.air.constants import TENSOR_COLUMN_NAME\n <mask> \n <mask> \n <mask> class TestToTF:\n <mask>     def test_autosharding_is_disabled(self):\n </s> [Datasets] Add ragged tensor support for `to_tf` (#29476)\n\nIf your dataset contains heterogeneously shaped tensors (e.g., from differently-sized images), then to_tf doesn't work. </s> remove from ray.train.tensorflow import TensorflowTrainer\n </s> add  </s> remove from ray.data.preprocessors import Concatenator\n </s> add  </s> remove         from ray.air._internal.tensorflow_utils import get_type_spec\n </s> add         from ray.air._internal.tensorflow_utils import (\n            get_type_spec,\n            convert_ndarray_to_tf_tensor,\n        ) </s> add         type_spec: A type spec that specifies the shape and dtype of the returned\n            tensor. If you specify ``dtype``, the dtype stored in the type spec is\n            ignored. </s> remove         def get_columns_from_batch(\n            batch: Dict[str, tf.Tensor], *, columns: Union[str, List[str]]\n </s> add         def convert_batch_to_tensors(\n            batch: Dict[str, np.ndarray],\n            *,\n            columns: Union[str, List[str]],\n            type_spec: Union[tf.TypeSpec, Dict[str, tf.TypeSpec]], </s> remove                 return batch[columns]\n            return {column: batch[column] for column in columns}\n </s> add                 return convert_ndarray_to_tf_tensor(batch[columns], type_spec=type_spec)\n            return {\n                convert_ndarray_to_tf_tensor(batch[column], type_spec=type_spec[column])\n                for column in columns\n            }", "html_url": "https://github.com/ray-project/ray/commit/89f17082078c81f403a3f658665b7c16f63cff03", "file_name": "python/ray/data/tests/test_dataset_tf.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     import pyarrow\n <mask> \n <mask> \n <mask> @PublicAPI\n <mask> class TextDatasource(BinaryDatasource):\n <mask>     \"\"\"Text datasource, for reading and writing text files.\"\"\"\n <mask> \n <mask>     _COLUMN_NAME = \"text\"\n <mask> \n <mask>     def _read_file(\n </s> [Data] Remove dead code from `BinaryDatasource` (#38234)\n\nBinaryDatasource has a separate code path if output_arrow_format is False, but this variable is always True, so the code path is dead. This PR cleans it up.\r\n\r\nSigned-off-by: Balaji Veeramani <balaji@anyscale.com> </s> remove         block = super()._read_file(f, path, **reader_args)\n        assert len(block) == 1\n        data = block[0]\n </s> add         data = f.readall() </s> remove \n    def _rows_per_file(self):\n        return None\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8a2113574e14bd2111985f333fa5e4c12a341272", "file_name": "python/ray/data/datasource/text_datasource.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     def _read_file(\n <mask>         self, f: \"pyarrow.NativeFile\", path: str, **reader_args\n <mask>     ) -> List[str]:\n <mask>         block = super()._read_file(f, path, **reader_args)\n <mask>         assert len(block) == 1\n <mask>         data = block[0]\n <mask> \n <mask>         builder = DelegatingBlockBuilder()\n <mask> \n <mask>         drop_empty_lines = reader_args[\"drop_empty_lines\"]\n <mask>         lines = data.decode(reader_args[\"encoding\"]).split(\"\\n\")\n </s> [Data] Remove dead code from `BinaryDatasource` (#38234)\n\nBinaryDatasource has a separate code path if output_arrow_format is False, but this variable is always True, so the code path is dead. This PR cleans it up.\r\n\r\nSigned-off-by: Balaji Veeramani <balaji@anyscale.com> </s> remove \n    def _rows_per_file(self):\n        return None\n </s> add  </s> remove class TextDatasource(BinaryDatasource):\n </s> add class TextDatasource(FileBasedDatasource):", "html_url": "https://github.com/ray-project/ray/commit/8a2113574e14bd2111985f333fa5e4c12a341272", "file_name": "python/ray/data/datasource/text_datasource.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace", "code_tokens": " <mask>             builder.add(item)\n <mask> \n <mask>         block = builder.build()\n <mask>         return block\n <mask> \n <mask>     def _rows_per_file(self):\n <mask>         return None\n </s> [Data] Remove dead code from `BinaryDatasource` (#38234)\n\nBinaryDatasource has a separate code path if output_arrow_format is False, but this variable is always True, so the code path is dead. This PR cleans it up.\r\n\r\nSigned-off-by: Balaji Veeramani <balaji@anyscale.com> </s> remove         block = super()._read_file(f, path, **reader_args)\n        assert len(block) == 1\n        data = block[0]\n </s> add         data = f.readall() </s> remove class TextDatasource(BinaryDatasource):\n </s> add class TextDatasource(FileBasedDatasource):", "html_url": "https://github.com/ray-project/ray/commit/8a2113574e14bd2111985f333fa5e4c12a341272", "file_name": "python/ray/data/datasource/text_datasource.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                 rewards[i] += gamma**j * rewards[i + j]\n <mask> \n <mask> \n <mask> def postprocess_nstep_and_prio(policy, batch, other_agent=None, episode=None):\n <mask>     # N-step Q adjustments\n <mask>     if policy.config[\"n_step\"] > 1:\n <mask>         _adjust_nstep(policy.config[\"n_step\"], policy.config[\"gamma\"],\n <mask>                       batch[SampleBatch.CUR_OBS], batch[SampleBatch.ACTIONS],\n <mask>                       batch[SampleBatch.REWARDS], batch[SampleBatch.NEXT_OBS],\n <mask>                       batch[SampleBatch.DONES])\n </s> [RLlib] SAC n_step > 1. (#10567) </s> remove     # Prioritize on the worker side\n </s> add     # Prioritize on the worker side. </s> remove     assert policy.config[\"n_step\"] == 1, \"TODO(hartikainen) n_step > 1\"\n\n </s> add  </s> remove     assert policy.config[\"n_step\"] == 1, \"TODO(hartikainen) n_step > 1\"\n\n </s> add  </s> remove         \"\"\"Tests the Categorical ActionDistribution (tf only).\"\"\"\n        num_samples = 100000\n        logits = tf1.placeholder(tf.float32, shape=(None, 10))\n        z = 8 * (np.random.rand(10) - 0.5)\n        data = np.tile(z, (num_samples, 1))\n        c = Categorical(logits, {})  # dummy config dict\n        sample_op = c.sample()\n        sess = tf1.Session()\n        sess.run(tf1.global_variables_initializer())\n        samples = sess.run(sample_op, feed_dict={logits: data})\n        counts = np.zeros(10)\n        for sample in samples:\n            counts[sample] += 1.0\n        probs = np.exp(z) / np.sum(np.exp(z))\n        self.assertTrue(np.sum(np.abs(probs - counts / num_samples)) <= 0.01)\n </s> add         batch_size = 10000\n        num_categories = 4\n        # Create categorical distribution with n categories.\n        inputs_space = Box(-1.0, 2.0, shape=(batch_size, num_categories))\n        values_space = Box(\n            0, num_categories - 1, shape=(batch_size, ), dtype=np.int32)\n\n        inputs = inputs_space.sample()\n\n        for fw, sess in framework_iterator(session=True):\n            # Create the correct distribution object.\n            cls = Categorical if fw != \"torch\" else TorchCategorical\n            categorical = cls(inputs, {})\n\n            # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1))\n\n            # Batch of size=3 and deterministic (True).\n            expected = np.transpose(np.argmax(inputs, axis=-1))\n            # Sample, expect always max value\n            # (max likelihood for deterministic draw).\n            out = categorical.deterministic_sample()\n            check(out, expected)\n\n            # Batch of size=3 and non-deterministic -> expect roughly the mean.\n            out = categorical.sample()\n            check(\n                tf.reduce_mean(out)\n                if fw != \"torch\" else torch.mean(out.float()),\n                1.0,\n                decimals=0)\n\n            # Test log-likelihood outputs.\n            probs = softmax(inputs)\n            values = values_space.sample()\n\n            out = categorical.logp(values\n                                   if fw != \"torch\" else torch.Tensor(values))\n            expected = []\n            for i in range(batch_size):\n                expected.append(np.sum(np.log(np.array(probs[i][values[i]]))))\n            check(out, expected, decimals=4)\n\n            # Test entropy outputs.\n            out = categorical.entropy()\n            expected_entropy = -np.sum(probs * np.log(probs), -1)\n            check(out, expected_entropy) </s> add                 # Make sure bounds make sense and are actually also being\n                # sampled.\n                if isinstance(bounds[0], int):\n                    assert isinstance(bounds[1], int)\n                    assert bounds[0] in sample_check\n                    assert bounds[1] in sample_check </s> remove         for fw in framework_iterator():\n </s> add         for fw, sess in framework_iterator(session=True):", "html_url": "https://github.com/ray-project/ray/commit/8a891b3c304e4043dd27409d993582290da7c439", "file_name": "rllib/agents/dqn/dqn_tf_policy.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     if PRIO_WEIGHTS not in batch:\n <mask>         batch[PRIO_WEIGHTS] = np.ones_like(batch[SampleBatch.REWARDS])\n <mask> \n <mask>     # Prioritize on the worker side\n <mask>     if batch.count > 0 and policy.config[\"worker_side_prioritization\"]:\n <mask>         td_errors = policy.compute_td_error(\n <mask>             batch[SampleBatch.CUR_OBS], batch[SampleBatch.ACTIONS],\n <mask>             batch[SampleBatch.REWARDS], batch[SampleBatch.NEXT_OBS],\n <mask>             batch[SampleBatch.DONES], batch[PRIO_WEIGHTS])\n </s> [RLlib] SAC n_step > 1. (#10567) </s> remove     # N-step Q adjustments\n </s> add     # N-step Q adjustments. </s> remove         \"\"\"Tests the Categorical ActionDistribution (tf only).\"\"\"\n        num_samples = 100000\n        logits = tf1.placeholder(tf.float32, shape=(None, 10))\n        z = 8 * (np.random.rand(10) - 0.5)\n        data = np.tile(z, (num_samples, 1))\n        c = Categorical(logits, {})  # dummy config dict\n        sample_op = c.sample()\n        sess = tf1.Session()\n        sess.run(tf1.global_variables_initializer())\n        samples = sess.run(sample_op, feed_dict={logits: data})\n        counts = np.zeros(10)\n        for sample in samples:\n            counts[sample] += 1.0\n        probs = np.exp(z) / np.sum(np.exp(z))\n        self.assertTrue(np.sum(np.abs(probs - counts / num_samples)) <= 0.01)\n </s> add         batch_size = 10000\n        num_categories = 4\n        # Create categorical distribution with n categories.\n        inputs_space = Box(-1.0, 2.0, shape=(batch_size, num_categories))\n        values_space = Box(\n            0, num_categories - 1, shape=(batch_size, ), dtype=np.int32)\n\n        inputs = inputs_space.sample()\n\n        for fw, sess in framework_iterator(session=True):\n            # Create the correct distribution object.\n            cls = Categorical if fw != \"torch\" else TorchCategorical\n            categorical = cls(inputs, {})\n\n            # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1))\n\n            # Batch of size=3 and deterministic (True).\n            expected = np.transpose(np.argmax(inputs, axis=-1))\n            # Sample, expect always max value\n            # (max likelihood for deterministic draw).\n            out = categorical.deterministic_sample()\n            check(out, expected)\n\n            # Batch of size=3 and non-deterministic -> expect roughly the mean.\n            out = categorical.sample()\n            check(\n                tf.reduce_mean(out)\n                if fw != \"torch\" else torch.mean(out.float()),\n                1.0,\n                decimals=0)\n\n            # Test log-likelihood outputs.\n            probs = softmax(inputs)\n            values = values_space.sample()\n\n            out = categorical.logp(values\n                                   if fw != \"torch\" else torch.Tensor(values))\n            expected = []\n            for i in range(batch_size):\n                expected.append(np.sum(np.log(np.array(probs[i][values[i]]))))\n            check(out, expected, decimals=4)\n\n            # Test entropy outputs.\n            out = categorical.entropy()\n            expected_entropy = -np.sum(probs * np.log(probs), -1)\n            check(out, expected_entropy) </s> remove         for fw in framework_iterator():\n </s> add         for fw, sess in framework_iterator(session=True): </s> remove         dist = distribution_cls(inputs, {})\n </s> add         dist = distribution_cls(inputs, {}, **(extra_kwargs or {})) </s> add                 # Make sure bounds make sense and are actually also being\n                # sampled.\n                if isinstance(bounds[0], int):\n                    assert isinstance(bounds[1], int)\n                    assert bounds[0] in sample_check\n                    assert bounds[1] in sample_check </s> add             # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1),\n                extra_kwargs={\"input_lens\": input_lengths})\n", "html_url": "https://github.com/ray-project/ray/commit/8a891b3c304e4043dd27409d993582290da7c439", "file_name": "rllib/agents/dqn/dqn_tf_policy.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         q_tp1_best = tf.squeeze(input=q_tp1, axis=len(q_tp1.shape) - 1)\n <mask>         q_tp1_best_masked = (1.0 - tf.cast(train_batch[SampleBatch.DONES],\n <mask>                                            tf.float32)) * q_tp1_best\n <mask> \n <mask>     assert policy.config[\"n_step\"] == 1, \"TODO(hartikainen) n_step > 1\"\n <mask> \n <mask>     # compute RHS of bellman equation\n <mask>     q_t_selected_target = tf.stop_gradient(\n <mask>         train_batch[SampleBatch.REWARDS] +\n <mask>         policy.config[\"gamma\"]**policy.config[\"n_step\"] * q_tp1_best_masked)\n <mask> \n </s> [RLlib] SAC n_step > 1. (#10567) </s> remove     assert policy.config[\"n_step\"] == 1, \"TODO(hartikainen) n_step > 1\"\n\n </s> add  </s> remove         \"\"\"Tests the Categorical ActionDistribution (tf only).\"\"\"\n        num_samples = 100000\n        logits = tf1.placeholder(tf.float32, shape=(None, 10))\n        z = 8 * (np.random.rand(10) - 0.5)\n        data = np.tile(z, (num_samples, 1))\n        c = Categorical(logits, {})  # dummy config dict\n        sample_op = c.sample()\n        sess = tf1.Session()\n        sess.run(tf1.global_variables_initializer())\n        samples = sess.run(sample_op, feed_dict={logits: data})\n        counts = np.zeros(10)\n        for sample in samples:\n            counts[sample] += 1.0\n        probs = np.exp(z) / np.sum(np.exp(z))\n        self.assertTrue(np.sum(np.abs(probs - counts / num_samples)) <= 0.01)\n </s> add         batch_size = 10000\n        num_categories = 4\n        # Create categorical distribution with n categories.\n        inputs_space = Box(-1.0, 2.0, shape=(batch_size, num_categories))\n        values_space = Box(\n            0, num_categories - 1, shape=(batch_size, ), dtype=np.int32)\n\n        inputs = inputs_space.sample()\n\n        for fw, sess in framework_iterator(session=True):\n            # Create the correct distribution object.\n            cls = Categorical if fw != \"torch\" else TorchCategorical\n            categorical = cls(inputs, {})\n\n            # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1))\n\n            # Batch of size=3 and deterministic (True).\n            expected = np.transpose(np.argmax(inputs, axis=-1))\n            # Sample, expect always max value\n            # (max likelihood for deterministic draw).\n            out = categorical.deterministic_sample()\n            check(out, expected)\n\n            # Batch of size=3 and non-deterministic -> expect roughly the mean.\n            out = categorical.sample()\n            check(\n                tf.reduce_mean(out)\n                if fw != \"torch\" else torch.mean(out.float()),\n                1.0,\n                decimals=0)\n\n            # Test log-likelihood outputs.\n            probs = softmax(inputs)\n            values = values_space.sample()\n\n            out = categorical.logp(values\n                                   if fw != \"torch\" else torch.Tensor(values))\n            expected = []\n            for i in range(batch_size):\n                expected.append(np.sum(np.log(np.array(probs[i][values[i]]))))\n            check(out, expected, decimals=4)\n\n            # Test entropy outputs.\n            out = categorical.entropy()\n            expected_entropy = -np.sum(probs * np.log(probs), -1)\n            check(out, expected_entropy) </s> add             # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1),\n                extra_kwargs={\"input_lens\": input_lengths})\n </s> remove     # N-step Q adjustments\n </s> add     # N-step Q adjustments. </s> remove         for fw in framework_iterator():\n </s> add         for fw, sess in framework_iterator(session=True): </s> add                 # Make sure bounds make sense and are actually also being\n                # sampled.\n                if isinstance(bounds[0], int):\n                    assert isinstance(bounds[1], int)\n                    assert bounds[0] in sample_check\n                    assert bounds[1] in sample_check", "html_url": "https://github.com/ray-project/ray/commit/8a891b3c304e4043dd27409d993582290da7c439", "file_name": "rllib/agents/sac/sac_tf_policy.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         q_tp1_best = torch.squeeze(input=q_tp1, dim=-1)\n <mask>         q_tp1_best_masked = (1.0 - train_batch[SampleBatch.DONES].float()) * \\\n <mask>             q_tp1_best\n <mask> \n <mask>     assert policy.config[\"n_step\"] == 1, \"TODO(hartikainen) n_step > 1\"\n <mask> \n <mask>     # compute RHS of bellman equation\n <mask>     q_t_selected_target = (\n <mask>         train_batch[SampleBatch.REWARDS] +\n <mask>         (policy.config[\"gamma\"]**policy.config[\"n_step\"]) * q_tp1_best_masked\n <mask>     ).detach()\n </s> [RLlib] SAC n_step > 1. (#10567) </s> remove     assert policy.config[\"n_step\"] == 1, \"TODO(hartikainen) n_step > 1\"\n\n </s> add  </s> remove         \"\"\"Tests the Categorical ActionDistribution (tf only).\"\"\"\n        num_samples = 100000\n        logits = tf1.placeholder(tf.float32, shape=(None, 10))\n        z = 8 * (np.random.rand(10) - 0.5)\n        data = np.tile(z, (num_samples, 1))\n        c = Categorical(logits, {})  # dummy config dict\n        sample_op = c.sample()\n        sess = tf1.Session()\n        sess.run(tf1.global_variables_initializer())\n        samples = sess.run(sample_op, feed_dict={logits: data})\n        counts = np.zeros(10)\n        for sample in samples:\n            counts[sample] += 1.0\n        probs = np.exp(z) / np.sum(np.exp(z))\n        self.assertTrue(np.sum(np.abs(probs - counts / num_samples)) <= 0.01)\n </s> add         batch_size = 10000\n        num_categories = 4\n        # Create categorical distribution with n categories.\n        inputs_space = Box(-1.0, 2.0, shape=(batch_size, num_categories))\n        values_space = Box(\n            0, num_categories - 1, shape=(batch_size, ), dtype=np.int32)\n\n        inputs = inputs_space.sample()\n\n        for fw, sess in framework_iterator(session=True):\n            # Create the correct distribution object.\n            cls = Categorical if fw != \"torch\" else TorchCategorical\n            categorical = cls(inputs, {})\n\n            # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1))\n\n            # Batch of size=3 and deterministic (True).\n            expected = np.transpose(np.argmax(inputs, axis=-1))\n            # Sample, expect always max value\n            # (max likelihood for deterministic draw).\n            out = categorical.deterministic_sample()\n            check(out, expected)\n\n            # Batch of size=3 and non-deterministic -> expect roughly the mean.\n            out = categorical.sample()\n            check(\n                tf.reduce_mean(out)\n                if fw != \"torch\" else torch.mean(out.float()),\n                1.0,\n                decimals=0)\n\n            # Test log-likelihood outputs.\n            probs = softmax(inputs)\n            values = values_space.sample()\n\n            out = categorical.logp(values\n                                   if fw != \"torch\" else torch.Tensor(values))\n            expected = []\n            for i in range(batch_size):\n                expected.append(np.sum(np.log(np.array(probs[i][values[i]]))))\n            check(out, expected, decimals=4)\n\n            # Test entropy outputs.\n            out = categorical.entropy()\n            expected_entropy = -np.sum(probs * np.log(probs), -1)\n            check(out, expected_entropy) </s> remove     # N-step Q adjustments\n </s> add     # N-step Q adjustments. </s> remove         for fw in framework_iterator():\n </s> add         for fw, sess in framework_iterator(session=True): </s> add             # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1),\n                extra_kwargs={\"input_lens\": input_lengths})\n </s> add                 # Make sure bounds make sense and are actually also being\n                # sampled.\n                if isinstance(bounds[0], int):\n                    assert isinstance(bounds[1], int)\n                    assert bounds[0] in sample_check\n                    assert bounds[1] in sample_check", "html_url": "https://github.com/ray-project/ray/commit/8a891b3c304e4043dd27409d993582290da7c439", "file_name": "rllib/agents/sac/sac_torch_policy.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                         distribution_cls,\n <mask>                         network_output_shape,\n <mask>                         fw,\n <mask>                         sess=None,\n <mask>                         bounds=None):\n <mask>         extreme_values = [\n <mask>             0.0,\n <mask>             float(LARGE_INTEGER),\n <mask>             -float(LARGE_INTEGER),\n <mask>             1.1e-34,\n </s> [RLlib] SAC n_step > 1. (#10567) </s> remove         for fw in framework_iterator():\n </s> add         for fw, sess in framework_iterator(session=True): </s> remove         \"\"\"Tests the Categorical ActionDistribution (tf only).\"\"\"\n        num_samples = 100000\n        logits = tf1.placeholder(tf.float32, shape=(None, 10))\n        z = 8 * (np.random.rand(10) - 0.5)\n        data = np.tile(z, (num_samples, 1))\n        c = Categorical(logits, {})  # dummy config dict\n        sample_op = c.sample()\n        sess = tf1.Session()\n        sess.run(tf1.global_variables_initializer())\n        samples = sess.run(sample_op, feed_dict={logits: data})\n        counts = np.zeros(10)\n        for sample in samples:\n            counts[sample] += 1.0\n        probs = np.exp(z) / np.sum(np.exp(z))\n        self.assertTrue(np.sum(np.abs(probs - counts / num_samples)) <= 0.01)\n </s> add         batch_size = 10000\n        num_categories = 4\n        # Create categorical distribution with n categories.\n        inputs_space = Box(-1.0, 2.0, shape=(batch_size, num_categories))\n        values_space = Box(\n            0, num_categories - 1, shape=(batch_size, ), dtype=np.int32)\n\n        inputs = inputs_space.sample()\n\n        for fw, sess in framework_iterator(session=True):\n            # Create the correct distribution object.\n            cls = Categorical if fw != \"torch\" else TorchCategorical\n            categorical = cls(inputs, {})\n\n            # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1))\n\n            # Batch of size=3 and deterministic (True).\n            expected = np.transpose(np.argmax(inputs, axis=-1))\n            # Sample, expect always max value\n            # (max likelihood for deterministic draw).\n            out = categorical.deterministic_sample()\n            check(out, expected)\n\n            # Batch of size=3 and non-deterministic -> expect roughly the mean.\n            out = categorical.sample()\n            check(\n                tf.reduce_mean(out)\n                if fw != \"torch\" else torch.mean(out.float()),\n                1.0,\n                decimals=0)\n\n            # Test log-likelihood outputs.\n            probs = softmax(inputs)\n            values = values_space.sample()\n\n            out = categorical.logp(values\n                                   if fw != \"torch\" else torch.Tensor(values))\n            expected = []\n            for i in range(batch_size):\n                expected.append(np.sum(np.log(np.array(probs[i][values[i]]))))\n            check(out, expected, decimals=4)\n\n            # Test entropy outputs.\n            out = categorical.entropy()\n            expected_entropy = -np.sum(probs * np.log(probs), -1)\n            check(out, expected_entropy) </s> remove         dist = distribution_cls(inputs, {})\n </s> add         dist = distribution_cls(inputs, {}, **(extra_kwargs or {})) </s> remove     assert policy.config[\"n_step\"] == 1, \"TODO(hartikainen) n_step > 1\"\n\n </s> add  </s> remove     assert policy.config[\"n_step\"] == 1, \"TODO(hartikainen) n_step > 1\"\n\n </s> add  </s> add             # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1),\n                extra_kwargs={\"input_lens\": input_lengths})\n", "html_url": "https://github.com/ray-project/ray/commit/8a891b3c304e4043dd27409d993582290da7c439", "file_name": "rllib/models/tests/test_distributions.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                 # the log of a positive number >= 1.\n <mask>                 inputs[batch_item][num] = np.log(\n <mask>                     max(1, np.random.choice((extreme_values))))\n <mask> \n <mask>         dist = distribution_cls(inputs, {})\n <mask>         for _ in range(100):\n <mask>             sample = dist.sample()\n <mask>             if fw != \"tf\":\n <mask>                 sample_check = sample.numpy()\n <mask>             else:\n </s> [RLlib] SAC n_step > 1. (#10567) </s> add                 # Make sure bounds make sense and are actually also being\n                # sampled.\n                if isinstance(bounds[0], int):\n                    assert isinstance(bounds[1], int)\n                    assert bounds[0] in sample_check\n                    assert bounds[1] in sample_check </s> remove         \"\"\"Tests the Categorical ActionDistribution (tf only).\"\"\"\n        num_samples = 100000\n        logits = tf1.placeholder(tf.float32, shape=(None, 10))\n        z = 8 * (np.random.rand(10) - 0.5)\n        data = np.tile(z, (num_samples, 1))\n        c = Categorical(logits, {})  # dummy config dict\n        sample_op = c.sample()\n        sess = tf1.Session()\n        sess.run(tf1.global_variables_initializer())\n        samples = sess.run(sample_op, feed_dict={logits: data})\n        counts = np.zeros(10)\n        for sample in samples:\n            counts[sample] += 1.0\n        probs = np.exp(z) / np.sum(np.exp(z))\n        self.assertTrue(np.sum(np.abs(probs - counts / num_samples)) <= 0.01)\n </s> add         batch_size = 10000\n        num_categories = 4\n        # Create categorical distribution with n categories.\n        inputs_space = Box(-1.0, 2.0, shape=(batch_size, num_categories))\n        values_space = Box(\n            0, num_categories - 1, shape=(batch_size, ), dtype=np.int32)\n\n        inputs = inputs_space.sample()\n\n        for fw, sess in framework_iterator(session=True):\n            # Create the correct distribution object.\n            cls = Categorical if fw != \"torch\" else TorchCategorical\n            categorical = cls(inputs, {})\n\n            # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1))\n\n            # Batch of size=3 and deterministic (True).\n            expected = np.transpose(np.argmax(inputs, axis=-1))\n            # Sample, expect always max value\n            # (max likelihood for deterministic draw).\n            out = categorical.deterministic_sample()\n            check(out, expected)\n\n            # Batch of size=3 and non-deterministic -> expect roughly the mean.\n            out = categorical.sample()\n            check(\n                tf.reduce_mean(out)\n                if fw != \"torch\" else torch.mean(out.float()),\n                1.0,\n                decimals=0)\n\n            # Test log-likelihood outputs.\n            probs = softmax(inputs)\n            values = values_space.sample()\n\n            out = categorical.logp(values\n                                   if fw != \"torch\" else torch.Tensor(values))\n            expected = []\n            for i in range(batch_size):\n                expected.append(np.sum(np.log(np.array(probs[i][values[i]]))))\n            check(out, expected, decimals=4)\n\n            # Test entropy outputs.\n            out = categorical.entropy()\n            expected_entropy = -np.sum(probs * np.log(probs), -1)\n            check(out, expected_entropy) </s> remove         for fw in framework_iterator():\n </s> add         for fw, sess in framework_iterator(session=True): </s> add             # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1),\n                extra_kwargs={\"input_lens\": input_lengths})\n </s> remove     # Prioritize on the worker side\n </s> add     # Prioritize on the worker side. </s> remove     assert policy.config[\"n_step\"] == 1, \"TODO(hartikainen) n_step > 1\"\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8a891b3c304e4043dd27409d993582290da7c439", "file_name": "rllib/models/tests/test_distributions.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask>             assert np.all(np.isfinite(sample_check))\n <mask>             if bounds:\n <mask>                 assert np.min(sample_check) >= bounds[0]\n <mask>                 assert np.max(sample_check) <= bounds[1]\n <mask>             logp = dist.logp(sample)\n <mask>             if fw != \"tf\":\n <mask>                 logp_check = logp.numpy()\n <mask>             else:\n </s> [RLlib] SAC n_step > 1. (#10567) </s> remove         dist = distribution_cls(inputs, {})\n </s> add         dist = distribution_cls(inputs, {}, **(extra_kwargs or {})) </s> remove         \"\"\"Tests the Categorical ActionDistribution (tf only).\"\"\"\n        num_samples = 100000\n        logits = tf1.placeholder(tf.float32, shape=(None, 10))\n        z = 8 * (np.random.rand(10) - 0.5)\n        data = np.tile(z, (num_samples, 1))\n        c = Categorical(logits, {})  # dummy config dict\n        sample_op = c.sample()\n        sess = tf1.Session()\n        sess.run(tf1.global_variables_initializer())\n        samples = sess.run(sample_op, feed_dict={logits: data})\n        counts = np.zeros(10)\n        for sample in samples:\n            counts[sample] += 1.0\n        probs = np.exp(z) / np.sum(np.exp(z))\n        self.assertTrue(np.sum(np.abs(probs - counts / num_samples)) <= 0.01)\n </s> add         batch_size = 10000\n        num_categories = 4\n        # Create categorical distribution with n categories.\n        inputs_space = Box(-1.0, 2.0, shape=(batch_size, num_categories))\n        values_space = Box(\n            0, num_categories - 1, shape=(batch_size, ), dtype=np.int32)\n\n        inputs = inputs_space.sample()\n\n        for fw, sess in framework_iterator(session=True):\n            # Create the correct distribution object.\n            cls = Categorical if fw != \"torch\" else TorchCategorical\n            categorical = cls(inputs, {})\n\n            # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1))\n\n            # Batch of size=3 and deterministic (True).\n            expected = np.transpose(np.argmax(inputs, axis=-1))\n            # Sample, expect always max value\n            # (max likelihood for deterministic draw).\n            out = categorical.deterministic_sample()\n            check(out, expected)\n\n            # Batch of size=3 and non-deterministic -> expect roughly the mean.\n            out = categorical.sample()\n            check(\n                tf.reduce_mean(out)\n                if fw != \"torch\" else torch.mean(out.float()),\n                1.0,\n                decimals=0)\n\n            # Test log-likelihood outputs.\n            probs = softmax(inputs)\n            values = values_space.sample()\n\n            out = categorical.logp(values\n                                   if fw != \"torch\" else torch.Tensor(values))\n            expected = []\n            for i in range(batch_size):\n                expected.append(np.sum(np.log(np.array(probs[i][values[i]]))))\n            check(out, expected, decimals=4)\n\n            # Test entropy outputs.\n            out = categorical.entropy()\n            expected_entropy = -np.sum(probs * np.log(probs), -1)\n            check(out, expected_entropy) </s> remove     assert policy.config[\"n_step\"] == 1, \"TODO(hartikainen) n_step > 1\"\n\n </s> add  </s> remove     assert policy.config[\"n_step\"] == 1, \"TODO(hartikainen) n_step > 1\"\n\n </s> add  </s> remove         for fw in framework_iterator():\n </s> add         for fw, sess in framework_iterator(session=True): </s> remove     # Prioritize on the worker side\n </s> add     # Prioritize on the worker side.", "html_url": "https://github.com/ray-project/ray/commit/8a891b3c304e4043dd27409d993582290da7c439", "file_name": "rllib/models/tests/test_distributions.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>             assert not np.any(np.isnan(logp_check))\n <mask>             assert np.all(np.isfinite(logp_check))\n <mask> \n <mask>     def test_categorical(self):\n <mask>         \"\"\"Tests the Categorical ActionDistribution (tf only).\"\"\"\n <mask>         num_samples = 100000\n <mask>         logits = tf1.placeholder(tf.float32, shape=(None, 10))\n <mask>         z = 8 * (np.random.rand(10) - 0.5)\n <mask>         data = np.tile(z, (num_samples, 1))\n <mask>         c = Categorical(logits, {})  # dummy config dict\n <mask>         sample_op = c.sample()\n <mask>         sess = tf1.Session()\n <mask>         sess.run(tf1.global_variables_initializer())\n <mask>         samples = sess.run(sample_op, feed_dict={logits: data})\n <mask>         counts = np.zeros(10)\n <mask>         for sample in samples:\n <mask>             counts[sample] += 1.0\n <mask>         probs = np.exp(z) / np.sum(np.exp(z))\n <mask>         self.assertTrue(np.sum(np.abs(probs - counts / num_samples)) <= 0.01)\n <mask> \n <mask>     def test_multi_categorical(self):\n <mask>         batch_size = 100\n <mask>         num_categories = 3\n <mask>         num_sub_distributions = 5\n </s> [RLlib] SAC n_step > 1. (#10567) </s> remove         for fw in framework_iterator():\n </s> add         for fw, sess in framework_iterator(session=True): </s> remove         dist = distribution_cls(inputs, {})\n </s> add         dist = distribution_cls(inputs, {}, **(extra_kwargs or {})) </s> remove     assert policy.config[\"n_step\"] == 1, \"TODO(hartikainen) n_step > 1\"\n\n </s> add  </s> remove     assert policy.config[\"n_step\"] == 1, \"TODO(hartikainen) n_step > 1\"\n\n </s> add  </s> add             # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1),\n                extra_kwargs={\"input_lens\": input_lengths})\n </s> add                 # Make sure bounds make sense and are actually also being\n                # sampled.\n                if isinstance(bounds[0], int):\n                    assert isinstance(bounds[1], int)\n                    assert bounds[0] in sample_check\n                    assert bounds[1] in sample_check", "html_url": "https://github.com/ray-project/ray/commit/8a891b3c304e4043dd27409d993582290da7c439", "file_name": "rllib/models/tests/test_distributions.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         inputs = inputs_space.sample()\n <mask>         input_lengths = [num_categories] * num_sub_distributions\n <mask>         inputs_split = np.split(inputs, num_sub_distributions, axis=1)\n <mask> \n <mask>         for fw in framework_iterator():\n <mask>             # Create the correct distribution object.\n <mask>             cls = MultiCategorical if fw != \"torch\" else TorchMultiCategorical\n <mask>             multi_categorical = cls(inputs, None, input_lengths)\n <mask> \n <mask>             # Batch of size=3 and deterministic (True).\n </s> [RLlib] SAC n_step > 1. (#10567) </s> add             # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1),\n                extra_kwargs={\"input_lens\": input_lengths})\n </s> remove         \"\"\"Tests the Categorical ActionDistribution (tf only).\"\"\"\n        num_samples = 100000\n        logits = tf1.placeholder(tf.float32, shape=(None, 10))\n        z = 8 * (np.random.rand(10) - 0.5)\n        data = np.tile(z, (num_samples, 1))\n        c = Categorical(logits, {})  # dummy config dict\n        sample_op = c.sample()\n        sess = tf1.Session()\n        sess.run(tf1.global_variables_initializer())\n        samples = sess.run(sample_op, feed_dict={logits: data})\n        counts = np.zeros(10)\n        for sample in samples:\n            counts[sample] += 1.0\n        probs = np.exp(z) / np.sum(np.exp(z))\n        self.assertTrue(np.sum(np.abs(probs - counts / num_samples)) <= 0.01)\n </s> add         batch_size = 10000\n        num_categories = 4\n        # Create categorical distribution with n categories.\n        inputs_space = Box(-1.0, 2.0, shape=(batch_size, num_categories))\n        values_space = Box(\n            0, num_categories - 1, shape=(batch_size, ), dtype=np.int32)\n\n        inputs = inputs_space.sample()\n\n        for fw, sess in framework_iterator(session=True):\n            # Create the correct distribution object.\n            cls = Categorical if fw != \"torch\" else TorchCategorical\n            categorical = cls(inputs, {})\n\n            # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1))\n\n            # Batch of size=3 and deterministic (True).\n            expected = np.transpose(np.argmax(inputs, axis=-1))\n            # Sample, expect always max value\n            # (max likelihood for deterministic draw).\n            out = categorical.deterministic_sample()\n            check(out, expected)\n\n            # Batch of size=3 and non-deterministic -> expect roughly the mean.\n            out = categorical.sample()\n            check(\n                tf.reduce_mean(out)\n                if fw != \"torch\" else torch.mean(out.float()),\n                1.0,\n                decimals=0)\n\n            # Test log-likelihood outputs.\n            probs = softmax(inputs)\n            values = values_space.sample()\n\n            out = categorical.logp(values\n                                   if fw != \"torch\" else torch.Tensor(values))\n            expected = []\n            for i in range(batch_size):\n                expected.append(np.sum(np.log(np.array(probs[i][values[i]]))))\n            check(out, expected, decimals=4)\n\n            # Test entropy outputs.\n            out = categorical.entropy()\n            expected_entropy = -np.sum(probs * np.log(probs), -1)\n            check(out, expected_entropy) </s> remove         dist = distribution_cls(inputs, {})\n </s> add         dist = distribution_cls(inputs, {}, **(extra_kwargs or {})) </s> remove     # Prioritize on the worker side\n </s> add     # Prioritize on the worker side. </s> add                 # Make sure bounds make sense and are actually also being\n                # sampled.\n                if isinstance(bounds[0], int):\n                    assert isinstance(bounds[1], int)\n                    assert bounds[0] in sample_check\n                    assert bounds[1] in sample_check </s> remove     assert policy.config[\"n_step\"] == 1, \"TODO(hartikainen) n_step > 1\"\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8a891b3c304e4043dd27409d993582290da7c439", "file_name": "rllib/models/tests/test_distributions.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>             cls = MultiCategorical if fw != \"torch\" else TorchMultiCategorical\n <mask>             multi_categorical = cls(inputs, None, input_lengths)\n <mask> \n <mask>             # Batch of size=3 and deterministic (True).\n <mask>             expected = np.transpose(np.argmax(inputs_split, axis=-1))\n <mask>             # Sample, expect always max value\n <mask>             # (max likelihood for deterministic draw).\n <mask>             out = multi_categorical.deterministic_sample()\n <mask>             check(out, expected)\n </s> [RLlib] SAC n_step > 1. (#10567) </s> remove         for fw in framework_iterator():\n </s> add         for fw, sess in framework_iterator(session=True): </s> remove         \"\"\"Tests the Categorical ActionDistribution (tf only).\"\"\"\n        num_samples = 100000\n        logits = tf1.placeholder(tf.float32, shape=(None, 10))\n        z = 8 * (np.random.rand(10) - 0.5)\n        data = np.tile(z, (num_samples, 1))\n        c = Categorical(logits, {})  # dummy config dict\n        sample_op = c.sample()\n        sess = tf1.Session()\n        sess.run(tf1.global_variables_initializer())\n        samples = sess.run(sample_op, feed_dict={logits: data})\n        counts = np.zeros(10)\n        for sample in samples:\n            counts[sample] += 1.0\n        probs = np.exp(z) / np.sum(np.exp(z))\n        self.assertTrue(np.sum(np.abs(probs - counts / num_samples)) <= 0.01)\n </s> add         batch_size = 10000\n        num_categories = 4\n        # Create categorical distribution with n categories.\n        inputs_space = Box(-1.0, 2.0, shape=(batch_size, num_categories))\n        values_space = Box(\n            0, num_categories - 1, shape=(batch_size, ), dtype=np.int32)\n\n        inputs = inputs_space.sample()\n\n        for fw, sess in framework_iterator(session=True):\n            # Create the correct distribution object.\n            cls = Categorical if fw != \"torch\" else TorchCategorical\n            categorical = cls(inputs, {})\n\n            # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1))\n\n            # Batch of size=3 and deterministic (True).\n            expected = np.transpose(np.argmax(inputs, axis=-1))\n            # Sample, expect always max value\n            # (max likelihood for deterministic draw).\n            out = categorical.deterministic_sample()\n            check(out, expected)\n\n            # Batch of size=3 and non-deterministic -> expect roughly the mean.\n            out = categorical.sample()\n            check(\n                tf.reduce_mean(out)\n                if fw != \"torch\" else torch.mean(out.float()),\n                1.0,\n                decimals=0)\n\n            # Test log-likelihood outputs.\n            probs = softmax(inputs)\n            values = values_space.sample()\n\n            out = categorical.logp(values\n                                   if fw != \"torch\" else torch.Tensor(values))\n            expected = []\n            for i in range(batch_size):\n                expected.append(np.sum(np.log(np.array(probs[i][values[i]]))))\n            check(out, expected, decimals=4)\n\n            # Test entropy outputs.\n            out = categorical.entropy()\n            expected_entropy = -np.sum(probs * np.log(probs), -1)\n            check(out, expected_entropy) </s> remove         dist = distribution_cls(inputs, {})\n </s> add         dist = distribution_cls(inputs, {}, **(extra_kwargs or {})) </s> remove     # Prioritize on the worker side\n </s> add     # Prioritize on the worker side. </s> add                 # Make sure bounds make sense and are actually also being\n                # sampled.\n                if isinstance(bounds[0], int):\n                    assert isinstance(bounds[1], int)\n                    assert bounds[0] in sample_check\n                    assert bounds[1] in sample_check </s> remove     assert policy.config[\"n_step\"] == 1, \"TODO(hartikainen) n_step > 1\"\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8a891b3c304e4043dd27409d993582290da7c439", "file_name": "rllib/models/tests/test_distributions.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>           fcnet_hiddens: [256, 256]\n <mask>         tau: 0.005\n <mask>         target_entropy: auto\n <mask>         no_done_at_end: true\n <mask>         n_step: 1\n <mask>         rollout_fragment_length: 1\n <mask>         prioritized_replay: true\n <mask>         train_batch_size: 256\n <mask>         target_network_update_freq: 1\n <mask>         timesteps_per_iteration: 1000\n </s> [RLlib] SAC n_step > 1. (#10567) </s> add             # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1),\n                extra_kwargs={\"input_lens\": input_lengths})\n </s> remove         for fw in framework_iterator():\n </s> add         for fw, sess in framework_iterator(session=True): </s> remove         \"\"\"Tests the Categorical ActionDistribution (tf only).\"\"\"\n        num_samples = 100000\n        logits = tf1.placeholder(tf.float32, shape=(None, 10))\n        z = 8 * (np.random.rand(10) - 0.5)\n        data = np.tile(z, (num_samples, 1))\n        c = Categorical(logits, {})  # dummy config dict\n        sample_op = c.sample()\n        sess = tf1.Session()\n        sess.run(tf1.global_variables_initializer())\n        samples = sess.run(sample_op, feed_dict={logits: data})\n        counts = np.zeros(10)\n        for sample in samples:\n            counts[sample] += 1.0\n        probs = np.exp(z) / np.sum(np.exp(z))\n        self.assertTrue(np.sum(np.abs(probs - counts / num_samples)) <= 0.01)\n </s> add         batch_size = 10000\n        num_categories = 4\n        # Create categorical distribution with n categories.\n        inputs_space = Box(-1.0, 2.0, shape=(batch_size, num_categories))\n        values_space = Box(\n            0, num_categories - 1, shape=(batch_size, ), dtype=np.int32)\n\n        inputs = inputs_space.sample()\n\n        for fw, sess in framework_iterator(session=True):\n            # Create the correct distribution object.\n            cls = Categorical if fw != \"torch\" else TorchCategorical\n            categorical = cls(inputs, {})\n\n            # Do a stability test using extreme NN outputs to see whether\n            # sampling and logp'ing result in NaN or +/-inf values.\n            self._stability_test(\n                cls,\n                inputs_space.shape,\n                fw=fw,\n                sess=sess,\n                bounds=(0, num_categories - 1))\n\n            # Batch of size=3 and deterministic (True).\n            expected = np.transpose(np.argmax(inputs, axis=-1))\n            # Sample, expect always max value\n            # (max likelihood for deterministic draw).\n            out = categorical.deterministic_sample()\n            check(out, expected)\n\n            # Batch of size=3 and non-deterministic -> expect roughly the mean.\n            out = categorical.sample()\n            check(\n                tf.reduce_mean(out)\n                if fw != \"torch\" else torch.mean(out.float()),\n                1.0,\n                decimals=0)\n\n            # Test log-likelihood outputs.\n            probs = softmax(inputs)\n            values = values_space.sample()\n\n            out = categorical.logp(values\n                                   if fw != \"torch\" else torch.Tensor(values))\n            expected = []\n            for i in range(batch_size):\n                expected.append(np.sum(np.log(np.array(probs[i][values[i]]))))\n            check(out, expected, decimals=4)\n\n            # Test entropy outputs.\n            out = categorical.entropy()\n            expected_entropy = -np.sum(probs * np.log(probs), -1)\n            check(out, expected_entropy) </s> add                 # Make sure bounds make sense and are actually also being\n                # sampled.\n                if isinstance(bounds[0], int):\n                    assert isinstance(bounds[1], int)\n                    assert bounds[0] in sample_check\n                    assert bounds[1] in sample_check </s> remove         dist = distribution_cls(inputs, {})\n </s> add         dist = distribution_cls(inputs, {}, **(extra_kwargs or {})) </s> remove                         bounds=None):\n </s> add                         bounds=None,\n                        extra_kwargs=None):", "html_url": "https://github.com/ray-project/ray/commit/8a891b3c304e4043dd27409d993582290da7c439", "file_name": "rllib/tuned_examples/sac/pendulum-sac.yaml"}
{"docstring_tokens": "keep keep keep keep replace", "code_tokens": " <mask>         return state\n <mask> \n <mask>     def __setstate__(self, state):\n <mask>         self.__dict__.update(state)\n <mask>         self.delete = None\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         return self.checkpoint.value is not None\n </s> add         return self.checkpoint.dir_or_data is not None </s> remove         self.checkpoint.value = None\n </s> add         self.checkpoint.dir_or_data = None </s> remove         self.restored_checkpoint = checkpoint.value\n </s> add         if checkpoint.storage_mode == CheckpointStorage.MEMORY:\n            self.restored_checkpoint = checkpoint.dir_or_data[\"data\"]\n        else:\n            self.restored_checkpoint = checkpoint.dir_or_data </s> remove         trial.restored_checkpoint = checkpoint_obj.value\n </s> add         trial.restored_checkpoint = checkpoint_obj.dir_or_data </s> remove     def on_checkpoint(self, checkpoint: _TuneCheckpoint):\n </s> add     def on_checkpoint(self, checkpoint: _TrackedCheckpoint): </s> remove         self.last_result = self.restoring_from.result\n </s> add         self.last_result = self.restoring_from.metrics", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/checkpoint_manager.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         self.assertEqual(checkpoint_manager.best_checkpoints(), [])\n <mask> \n <mask>     def testSameCheckpoint(self):\n <mask>         checkpoint_manager = _CheckpointManager(\n <mask>             1, \"i\", delete_fn=lambda c: os.remove(c.value)\n <mask>         )\n <mask> \n <mask>         tmpfiles = []\n <mask>         for i in range(3):\n <mask>             _, tmpfile = tempfile.mkstemp()\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove     trainer.restore(trial.checkpoint.value)\n </s> add     trainer.restore(trial.checkpoint.dir_or_data) </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove             _CheckpointDeleter(self._trainable_name(), self.runner),\n </s> add             delete_fn=_CheckpointDeleter(self._trainable_name(), self.runner), </s> remove             self.assertTrue(os.path.exists(checkpoint.value))\n </s> add             self.assertTrue(os.path.exists(checkpoint.dir_or_data)) </s> remove             trial.on_checkpoint(tune_cp)\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_checkpoint_manager.py"}
{"docstring_tokens": "keep keep keep replace replace keep replace replace keep keep keep", "code_tokens": " <mask>             tmpfiles.append(tmpfile)\n <mask> \n <mask>         checkpoints = [\n <mask>             _TuneCheckpoint(\n <mask>                 _TuneCheckpoint.PERSISTENT, tmpfiles[0], self.mock_result(5, 5)\n <mask>             ),\n <mask>             _TuneCheckpoint(\n <mask>                 _TuneCheckpoint.PERSISTENT, tmpfiles[1], self.mock_result(10, 10)\n <mask>             ),\n <mask>             _TuneCheckpoint(\n <mask>                 _TuneCheckpoint.PERSISTENT, tmpfiles[2], self.mock_result(0, 0)\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove             _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, tmpfiles[2], self.mock_result(0, 0)\n </s> add             _TrackedCheckpoint(\n                dir_or_data=tmpfiles[2],\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=self.mock_result(0, 0), </s> remove             _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, tmpfiles[1], self.mock_result(20, 20)\n </s> add             _TrackedCheckpoint(\n                dir_or_data=tmpfiles[1],\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=self.mock_result(20, 20), </s> remove             trial.on_checkpoint(tune_cp)\n </s> add  </s> remove             tune_cp = _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, checkpoint_dir, result\n </s> add             tune_cp = _TrackedCheckpoint(\n                dir_or_data=checkpoint_dir,\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=result, </s> remove         cp = _TuneCheckpoint(\n            _TuneCheckpoint.PERSISTENT, value=ray.put(1), result={TRAINING_ITERATION: 0}\n </s> add         cp = _TrackedCheckpoint(\n            dir_or_data=ray.put(1),\n            storage_mode=CheckpointStorage.PERSISTENT,\n            metrics={TRAINING_ITERATION: 0},", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_checkpoint_manager.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>             ),\n <mask>             _TuneCheckpoint(\n <mask>                 _TuneCheckpoint.PERSISTENT, tmpfiles[1], self.mock_result(10, 10)\n <mask>             ),\n <mask>             _TuneCheckpoint(\n <mask>                 _TuneCheckpoint.PERSISTENT, tmpfiles[2], self.mock_result(0, 0)\n <mask>             ),\n <mask>             _TuneCheckpoint(\n <mask>                 _TuneCheckpoint.PERSISTENT, tmpfiles[1], self.mock_result(20, 20)\n <mask>             ),\n <mask>         ]\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove             _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, tmpfiles[1], self.mock_result(20, 20)\n </s> add             _TrackedCheckpoint(\n                dir_or_data=tmpfiles[1],\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=self.mock_result(20, 20), </s> remove             _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, tmpfiles[1], self.mock_result(10, 10)\n </s> add             _TrackedCheckpoint(\n                dir_or_data=tmpfiles[1],\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=self.mock_result(10, 10), </s> remove             _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, tmpfiles[0], self.mock_result(5, 5)\n </s> add             _TrackedCheckpoint(\n                dir_or_data=tmpfiles[0],\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=self.mock_result(5, 5), </s> remove             self.assertTrue(os.path.exists(checkpoint.value))\n </s> add             self.assertTrue(os.path.exists(checkpoint.dir_or_data)) </s> remove             trial.on_checkpoint(tune_cp)\n </s> add  </s> remove             tune_cp = _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, checkpoint_dir, result\n </s> add             tune_cp = _TrackedCheckpoint(\n                dir_or_data=checkpoint_dir,\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=result,", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_checkpoint_manager.py"}
{"docstring_tokens": "keep replace replace keep keep keep keep replace keep", "code_tokens": " <mask>             ),\n <mask>             _TuneCheckpoint(\n <mask>                 _TuneCheckpoint.PERSISTENT, tmpfiles[1], self.mock_result(20, 20)\n <mask>             ),\n <mask>         ]\n <mask>         for checkpoint in checkpoints:\n <mask>             checkpoint_manager.on_checkpoint(checkpoint)\n <mask>             self.assertTrue(os.path.exists(checkpoint.value))\n <mask> \n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove             _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, tmpfiles[2], self.mock_result(0, 0)\n </s> add             _TrackedCheckpoint(\n                dir_or_data=tmpfiles[2],\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=self.mock_result(0, 0), </s> remove             _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, tmpfiles[1], self.mock_result(10, 10)\n </s> add             _TrackedCheckpoint(\n                dir_or_data=tmpfiles[1],\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=self.mock_result(10, 10), </s> remove             _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, tmpfiles[0], self.mock_result(5, 5)\n </s> add             _TrackedCheckpoint(\n                dir_or_data=tmpfiles[0],\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=self.mock_result(5, 5), </s> remove             trial.on_checkpoint(tune_cp)\n </s> add  </s> remove         cp = _TuneCheckpoint(\n            _TuneCheckpoint.PERSISTENT, value=ray.put(1), result={TRAINING_ITERATION: 0}\n </s> add         cp = _TrackedCheckpoint(\n            dir_or_data=ray.put(1),\n            storage_mode=CheckpointStorage.PERSISTENT,\n            metrics={TRAINING_ITERATION: 0},", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_checkpoint_manager.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         cluster.add_node(num_cpus=1)\n <mask>         cluster.remove_node(node)\n <mask>         cluster.wait_for_nodes()\n <mask>         shutil.rmtree(os.path.dirname(t1.checkpoint.value))\n <mask>         while not runner.is_finished():\n <mask>             runner.step()\n <mask>     assert t1.status == Trial.TERMINATED, runner.debug_string()\n <mask> \n <mask> \n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         last_ckpt = analysis.trials[0].checkpoint.value\n </s> add         last_ckpt = analysis.trials[0].checkpoint.dir_or_data </s> remove         last_ckpt = trial.checkpoint.value\n        assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         last_ckpt = trial.checkpoint.dir_or_data\n        assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove         if storage_mode == CheckpointStorage.MEMORY and not isinstance(\n            dir_or_data, (dict, ray.ObjectRef)\n </s> add         if (\n            dir_or_data is not None\n            and storage_mode == CheckpointStorage.MEMORY\n            and not isinstance(dir_or_data, (dict, ray.ObjectRef)) </s> remove         return self.checkpoint.value is not None\n </s> add         return self.checkpoint.dir_or_data is not None </s> remove         if checkpoint.storage == _TuneCheckpoint.PERSISTENT and checkpoint.value:\n            checkpoint_path = checkpoint.value\n </s> add         if (\n            checkpoint.storage_mode == CheckpointStorage.PERSISTENT\n            and checkpoint.dir_or_data\n        ):\n            checkpoint_path = checkpoint.dir_or_data </s> remove         kwargs[\"restore_path\"] = trials[0].checkpoint.value\n </s> add         kwargs[\"restore_path\"] = trials[0].checkpoint.dir_or_data", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_cluster.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         self.assertEqual(ea.best_trial, trials[2])\n <mask>         self.assertEqual(ea.best_config, trials[2].config)\n <mask>         self.assertEqual(ea.best_logdir, trials[2].logdir)\n <mask>         self.assertEqual(ea.best_checkpoint._local_path, trials[2].checkpoint.value)\n <mask>         self.assertTrue(all(ea.best_dataframe[\"trial_id\"] == trials[2].trial_id))\n <mask>         self.assertEqual(ea.results_df.loc[trials[2].trial_id, \"res\"], 309)\n <mask>         self.assertEqual(ea.best_result[\"res\"], 309)\n <mask>         self.assertEqual(ea.best_result_df.loc[trials[2].trial_id, \"res\"], 309)\n <mask> \n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         if checkpoint.storage == _TuneCheckpoint.PERSISTENT and checkpoint.value:\n            checkpoint_path = checkpoint.value\n </s> add         if (\n            checkpoint.storage_mode == CheckpointStorage.PERSISTENT\n            and checkpoint.dir_or_data\n        ):\n            checkpoint_path = checkpoint.dir_or_data </s> remove         shutil.rmtree(os.path.dirname(t1.checkpoint.value))\n </s> add         shutil.rmtree(os.path.dirname(t1.checkpoint.dir_or_data)) </s> remove             f\"<TrackedCheckpoint storage='PERSISTENT' \"\n </s> add             f\"<_TrackedCheckpoint storage='PERSISTENT' \" </s> remove         if storage_mode == CheckpointStorage.MEMORY and not isinstance(\n            dir_or_data, (dict, ray.ObjectRef)\n </s> add         if (\n            dir_or_data is not None\n            and storage_mode == CheckpointStorage.MEMORY\n            and not isinstance(dir_or_data, (dict, ray.ObjectRef)) </s> remove             return f\"<TrackedCheckpoint storage='MEMORY' result={self.metrics}>\"\n </s> add             return f\"<_TrackedCheckpoint storage='MEMORY' result={self.metrics}>\" </s> remove         last_ckpt = analysis.trials[0].checkpoint.value\n </s> add         last_ckpt = analysis.trials[0].checkpoint.dir_or_data", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_experiment_analysis.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                 with open(checkpoint_path, \"w\") as f:\n <mask>                     f.write(\"hello\")\n <mask> \n <mask>         [trial] = tune.run(train).trials\n <mask>         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n <mask> \n <mask>     def testCheckpointFunctionAtEndContext(self):\n <mask>         def train(config, checkpoint_dir=False):\n <mask>             for i in range(10):\n <mask>                 tune.report(test=i)\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log2\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log2\")) </s> remove         last_ckpt = trial.checkpoint.value\n        assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         last_ckpt = trial.checkpoint.dir_or_data\n        assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove             tune_cp = _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, checkpoint_dir, result\n </s> add             tune_cp = _TrackedCheckpoint(\n                dir_or_data=checkpoint_dir,\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=result, </s> remove         last_ckpt = analysis.trials[0].checkpoint.value\n </s> add         last_ckpt = analysis.trials[0].checkpoint.dir_or_data </s> remove     trainer.restore(trial.checkpoint.value)\n </s> add     trainer.restore(trial.checkpoint.dir_or_data)", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_function_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                 with open(checkpoint_path, \"w\") as f:\n <mask>                     f.write(\"hello\")\n <mask> \n <mask>         [trial] = tune.run(train).trials\n <mask>         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n <mask> \n <mask>     def testVariousCheckpointFunctionAtEnd(self):\n <mask>         def train(config, checkpoint_dir=False):\n <mask>             for i in range(10):\n <mask>                 with tune.checkpoint_dir(step=i) as checkpoint_dir:\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log2\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log2\")) </s> remove             tune_cp = _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, checkpoint_dir, result\n </s> add             tune_cp = _TrackedCheckpoint(\n                dir_or_data=checkpoint_dir,\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=result, </s> remove         last_ckpt = trial.checkpoint.value\n        assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         last_ckpt = trial.checkpoint.dir_or_data\n        assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove         last_ckpt = analysis.trials[0].checkpoint.value\n </s> add         last_ckpt = analysis.trials[0].checkpoint.dir_or_data </s> remove     trainer.restore(trial.checkpoint.value)\n </s> add     trainer.restore(trial.checkpoint.dir_or_data)", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_function_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                 with open(checkpoint_path, \"w\") as f:\n <mask>                     f.write(\"goodbye\")\n <mask> \n <mask>         [trial] = tune.run(train, keep_checkpoints_num=3).trials\n <mask>         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log2\"))\n <mask> \n <mask>     def testReuseCheckpoint(self):\n <mask>         def train(config, checkpoint_dir=None):\n <mask>             itr = 0\n <mask>             if checkpoint_dir:\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove         last_ckpt = trial.checkpoint.value\n        assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         last_ckpt = trial.checkpoint.dir_or_data\n        assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove             tune_cp = _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, checkpoint_dir, result\n </s> add             tune_cp = _TrackedCheckpoint(\n                dir_or_data=checkpoint_dir,\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=result, </s> remove         last_ckpt = analysis.trials[0].checkpoint.value\n </s> add         last_ckpt = analysis.trials[0].checkpoint.dir_or_data </s> remove         return self.checkpoint.value is not None\n </s> add         return self.checkpoint.dir_or_data is not None", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_function_api.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         [trial] = tune.run(\n <mask>             train,\n <mask>             config={\"max_iter\": 5},\n <mask>         ).trials\n <mask>         last_ckpt = trial.checkpoint.value\n <mask>         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n <mask>         analysis = tune.run(train, config={\"max_iter\": 10}, restore=last_ckpt)\n <mask>         trial_dfs = list(analysis.trial_dataframes.values())\n <mask>         assert len(trial_dfs[0][\"training_iteration\"]) == 5\n <mask> \n <mask>     def testRetry(self):\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         last_ckpt = analysis.trials[0].checkpoint.value\n </s> add         last_ckpt = analysis.trials[0].checkpoint.dir_or_data </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log2\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log2\")) </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove         self.assertEqual(trial.checkpoint.value, None)\n </s> add         self.assertEqual(trial.checkpoint.dir_or_data, None) </s> remove         self.last_result = self.restoring_from.result\n </s> add         self.last_result = self.restoring_from.metrics", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_function_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                         f.write(str(i))\n <mask>                 tune.report(test=i, training_iteration=i)\n <mask> \n <mask>         analysis = tune.run(train, max_failures=3)\n <mask>         last_ckpt = analysis.trials[0].checkpoint.value\n <mask>         assert os.path.exists(os.path.join(last_ckpt, \"ckpt.log\"))\n <mask>         trial_dfs = list(analysis.trial_dataframes.values())\n <mask>         assert len(trial_dfs[0][\"training_iteration\"]) == 10\n <mask> \n <mask>     def testEnabled(self):\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         last_ckpt = trial.checkpoint.value\n        assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         last_ckpt = trial.checkpoint.dir_or_data\n        assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log2\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log2\")) </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove         self.assertEqual(trial.checkpoint.value, None)\n </s> add         self.assertEqual(trial.checkpoint.dir_or_data, None) </s> remove         self.last_result = self.restoring_from.result\n </s> add         self.last_result = self.restoring_from.metrics", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_function_api.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> )\n <mask> from ray.tune.registry import _global_registry, TRAINABLE_CLASS, register_trainable\n <mask> from ray.tune.result import PID, TRAINING_ITERATION, TRIAL_ID\n <mask> from ray.tune.suggest import BasicVariantGenerator\n <mask> from ray.tune.trial import Trial, _TuneCheckpoint\n <mask> from ray.tune.resources import Resources\n <mask> from ray.cluster_utils import Cluster\n <mask> from ray.tune.utils.placement_groups import (\n <mask>     PlacementGroupFactory,\n <mask>     _PlacementGroupManager,\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add  </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import _TuneCheckpoint, Trial\n </s> add from ray.tune.trial import Trial </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_ray_trial_executor.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>     _PlacementGroupManager,\n <mask> )\n <mask> from unittest.mock import patch\n <mask> \n <mask> \n <mask> class TrialExecutorInsufficientResourcesTest(unittest.TestCase):\n <mask>     def setUp(self):\n <mask>         os.environ[\"TUNE_WARN_INSUFFICENT_RESOURCE_THRESHOLD_S\"] = \"0\"\n <mask>         self.cluster = Cluster(\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         checkpoint.value = checkpoint_value\n </s> add         checkpoint.dir_or_data = checkpoint_value </s> remove         return _TuneCheckpoint(_TuneCheckpoint.MEMORY, \"None\", {})\n </s> add         return _TrackedCheckpoint(\n            dir_or_data=\"None\",\n            storage_mode=CheckpointStorage.MEMORY,\n            metrics={},\n        ) </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage </s> remove         return _TuneCheckpoint(_TuneCheckpoint.MEMORY, self.trainable_name, None)\n </s> add         return _TrackedCheckpoint(\n            dir_or_data={\"data\": self.trainable_name},\n            storage_mode=CheckpointStorage.MEMORY,\n            metrics=None,\n        ) </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_ray_trial_executor.py"}
{"docstring_tokens": "keep keep keep keep replace keep replace keep keep", "code_tokens": " <mask>         else:\n <mask>             trial.update_last_result(training_result)\n <mask> \n <mask>     def _simulate_saving(self, trial):\n <mask>         checkpoint = self.trial_executor.save(trial, _TuneCheckpoint.PERSISTENT)\n <mask>         self.assertEqual(checkpoint, trial.saving_to)\n <mask>         self.assertEqual(trial.checkpoint.value, None)\n <mask>         event = self.trial_executor.get_next_executor_event(\n <mask>             live_trials={trial}, next_trial_exists=False\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         self.trial_executor.save(trial, _TuneCheckpoint.MEMORY, None)\n </s> add         self.trial_executor.save(trial, CheckpointStorage.MEMORY, None) </s> remove         self.restored_checkpoint = checkpoint.value\n </s> add         if checkpoint.storage_mode == CheckpointStorage.MEMORY:\n            self.restored_checkpoint = checkpoint.dir_or_data[\"data\"]\n        else:\n            self.restored_checkpoint = checkpoint.dir_or_data </s> remove         if checkpoint.value is None:\n            checkpoint = _TuneCheckpoint(_TuneCheckpoint.PERSISTENT, self.restore_path)\n </s> add         if checkpoint.dir_or_data is None:\n            checkpoint = _TrackedCheckpoint(\n                dir_or_data=self.restore_path,\n                storage_mode=CheckpointStorage.PERSISTENT,\n            ) </s> remove         checkpoint.value = checkpoint_value\n </s> add         checkpoint.dir_or_data = checkpoint_value </s> remove         return _TuneCheckpoint(_TuneCheckpoint.MEMORY, self.trainable_name, None)\n </s> add         return _TrackedCheckpoint(\n            dir_or_data={\"data\": self.trainable_name},\n            storage_mode=CheckpointStorage.MEMORY,\n            metrics=None,\n        )", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_ray_trial_executor.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         # Pause\n <mask>         self.trial_executor.pause_trial(trial)\n <mask>         self.assertEqual(Trial.PAUSED, trial.status)\n <mask>         self.assertEqual(trial.checkpoint.storage, _TuneCheckpoint.MEMORY)\n <mask> \n <mask>         # Resume\n <mask>         self._simulate_starting_trial(trial)\n <mask> \n <mask>         # Error\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         self.checkpoint_manager.delete = _CheckpointDeleter(\n            self._trainable_name(), runner\n </s> add         self.checkpoint_manager.set_delete_fn(\n            _CheckpointDeleter(self._trainable_name(), runner) </s> remove         kwargs[\"restore_path\"] = trials[0].checkpoint.value\n </s> add         kwargs[\"restore_path\"] = trials[0].checkpoint.dir_or_data </s> remove     trainer.restore(trial.checkpoint.value)\n </s> add     trainer.restore(trial.checkpoint.dir_or_data) </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint, _CheckpointManager\n </s> add from ray.tune.checkpoint_manager import _CheckpointManager </s> remove         cp = _TuneCheckpoint(\n            _TuneCheckpoint.PERSISTENT, value=ray.put(1), result={TRAINING_ITERATION: 0}\n </s> add         cp = _TrackedCheckpoint(\n            dir_or_data=ray.put(1),\n            storage_mode=CheckpointStorage.PERSISTENT,\n            metrics={TRAINING_ITERATION: 0}, </s> remove         self.addCleanup(os.remove, trials[0].checkpoint.value)\n </s> add         self.addCleanup(os.remove, trials[0].checkpoint.dir_or_data)", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_ray_trial_executor.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     def process_trial_save(self, trial, checkpoint_value):\n <mask>         \"\"\"Simulates trial runner save.\"\"\"\n <mask>         checkpoint = trial.saving_to\n <mask>         checkpoint.value = checkpoint_value\n <mask>         trial.on_checkpoint(checkpoint)\n <mask> \n <mask> \n <mask> class RayExecutorPlacementGroupTest(unittest.TestCase):\n <mask>     def setUp(self):\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         return _TuneCheckpoint(_TuneCheckpoint.MEMORY, self.trainable_name, None)\n </s> add         return _TrackedCheckpoint(\n            dir_or_data={\"data\": self.trainable_name},\n            storage_mode=CheckpointStorage.MEMORY,\n            metrics=None,\n        ) </s> remove         return _TuneCheckpoint(_TuneCheckpoint.MEMORY, \"None\", {})\n </s> add         return _TrackedCheckpoint(\n            dir_or_data=\"None\",\n            storage_mode=CheckpointStorage.MEMORY,\n            metrics={},\n        ) </s> remove         trial.restored_checkpoint = checkpoint_obj.value\n </s> add         trial.restored_checkpoint = checkpoint_obj.dir_or_data </s> add from ray.util.ml_utils.checkpoint_manager import CheckpointStorage\n </s> remove             trial.saving_to.value = checkpoint_value\n </s> add             trial.saving_to.dir_or_data = checkpoint_value </s> remove         if checkpoint.value is None:\n            checkpoint = _TuneCheckpoint(_TuneCheckpoint.PERSISTENT, self.restore_path)\n </s> add         if checkpoint.dir_or_data is None:\n            checkpoint = _TrackedCheckpoint(\n                dir_or_data=self.restore_path,\n                storage_mode=CheckpointStorage.PERSISTENT,\n            )", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_ray_trial_executor.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> import ray\n <mask> from ray.rllib import _register_all\n <mask> \n <mask> from ray.tune import TuneError\n <mask> from ray.tune.checkpoint_manager import _TuneCheckpoint\n <mask> from ray.tune.schedulers import FIFOScheduler\n <mask> from ray.tune.result import DONE\n <mask> from ray.tune.registry import _global_registry, TRAINABLE_CLASS\n <mask> from ray.tune.trial import Trial\n <mask> from ray.tune.trial_runner import TrialRunner\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add  </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import _TuneCheckpoint, Trial\n </s> add from ray.tune.trial import Trial", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_runner_2.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask> from ray.tune.suggest import BasicVariantGenerator\n <mask> from ray.tune.tests.utils_for_test_trial_runner import TrialResultObserver\n <mask> from ray.tune.utils.trainable import TrainableUtil\n <mask> \n <mask> \n <mask> def create_mock_components():\n <mask>     class _MockScheduler(FIFOScheduler):\n <mask>         errored_trials = []\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage </s> remove from ray.tune.trial import _TuneCheckpoint, Trial\n </s> add from ray.tune.trial import Trial </s> add from ray.util.ml_utils.checkpoint_manager import CheckpointStorage\n </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_runner_2.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         self.assertEqual(trials[0].status, Trial.RUNNING)\n <mask>         self.assertEqual(ray.get(trials[0].runner.set_info.remote(1)), 1)\n <mask>         runner.step()  # Process result, dispatch save\n <mask>         runner.step()  # Process save, stop trial\n <mask>         kwargs[\"restore_path\"] = trials[0].checkpoint.value\n <mask>         self.assertEqual(trials[0].status, Trial.TERMINATED)\n <mask> \n <mask>         runner.add_trial(Trial(\"__fake\", **kwargs))\n <mask>         trials = runner.get_trials()\n <mask>         self.assertEqual(trials[1].status, Trial.PENDING)\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         kwargs[\"restore_path\"] = trials[0].checkpoint.value\n </s> add         kwargs[\"restore_path\"] = trials[0].checkpoint.dir_or_data </s> remove         self.addCleanup(os.remove, trials[0].checkpoint.value)\n </s> add         self.addCleanup(os.remove, trials[0].checkpoint.dir_or_data) </s> remove         cp = _TuneCheckpoint(\n            _TuneCheckpoint.PERSISTENT, value=ray.put(1), result={TRAINING_ITERATION: 0}\n </s> add         cp = _TrackedCheckpoint(\n            dir_or_data=ray.put(1),\n            storage_mode=CheckpointStorage.PERSISTENT,\n            metrics={TRAINING_ITERATION: 0}, </s> remove     trainer.restore(trial.checkpoint.value)\n </s> add     trainer.restore(trial.checkpoint.dir_or_data) </s> remove         trial.checkpoint_manager.delete = lambda cp: shutil.rmtree(cp.value)\n </s> add         trial.checkpoint_manager.set_delete_fn(lambda cp: shutil.rmtree(cp.dir_or_data)) </s> remove         shutil.rmtree(os.path.dirname(t1.checkpoint.value))\n </s> add         shutil.rmtree(os.path.dirname(t1.checkpoint.dir_or_data))", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_runner_2.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         runner.step()  # Process restore\n <mask>         self.assertEqual(trials[0].status, Trial.TERMINATED)\n <mask>         self.assertEqual(trials[1].status, Trial.RUNNING)\n <mask>         self.assertEqual(ray.get(trials[1].runner.get_info.remote()), 1)\n <mask>         self.addCleanup(os.remove, trials[0].checkpoint.value)\n <mask> \n <mask>     def testRestoreMetricsAfterCheckpointing(self):\n <mask>         ray.init(num_cpus=1, num_gpus=1)\n <mask> \n <mask>         observer = TrialResultObserver()\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         kwargs[\"restore_path\"] = trials[0].checkpoint.value\n </s> add         kwargs[\"restore_path\"] = trials[0].checkpoint.dir_or_data </s> remove         self.addCleanup(os.remove, trials[0].checkpoint.value)\n </s> add         self.addCleanup(os.remove, trials[0].checkpoint.dir_or_data) </s> remove         kwargs[\"restore_path\"] = trials[0].checkpoint.value\n </s> add         kwargs[\"restore_path\"] = trials[0].checkpoint.dir_or_data </s> remove         cp = _TuneCheckpoint(\n            _TuneCheckpoint.PERSISTENT, value=ray.put(1), result={TRAINING_ITERATION: 0}\n </s> add         cp = _TrackedCheckpoint(\n            dir_or_data=ray.put(1),\n            storage_mode=CheckpointStorage.PERSISTENT,\n            metrics={TRAINING_ITERATION: 0}, </s> remove         self.checkpoint_manager.delete = _CheckpointDeleter(\n            self._trainable_name(), runner\n </s> add         self.checkpoint_manager.set_delete_fn(\n            _CheckpointDeleter(self._trainable_name(), runner) </s> remove         trial.restored_checkpoint = checkpoint_obj.value\n </s> add         trial.restored_checkpoint = checkpoint_obj.dir_or_data", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_runner_2.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             runner.step()\n <mask> \n <mask>         self.assertEqual(trials[0].status, Trial.TERMINATED)\n <mask> \n <mask>         kwargs[\"restore_path\"] = trials[0].checkpoint.value\n <mask>         kwargs.pop(\"stopping_criterion\")\n <mask>         kwargs.pop(\"checkpoint_freq\")  # No checkpointing for next trial\n <mask>         runner.add_trial(Trial(\"__fake\", **kwargs))\n <mask>         trials = runner.get_trials()\n <mask> \n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         kwargs[\"restore_path\"] = trials[0].checkpoint.value\n </s> add         kwargs[\"restore_path\"] = trials[0].checkpoint.dir_or_data </s> remove         self.addCleanup(os.remove, trials[0].checkpoint.value)\n </s> add         self.addCleanup(os.remove, trials[0].checkpoint.dir_or_data) </s> remove     trainer.restore(trial.checkpoint.value)\n </s> add     trainer.restore(trial.checkpoint.dir_or_data) </s> remove         trial.restored_checkpoint = checkpoint_obj.value\n </s> add         trial.restored_checkpoint = checkpoint_obj.dir_or_data </s> remove         cp = _TuneCheckpoint(\n            _TuneCheckpoint.PERSISTENT, value=ray.put(1), result={TRAINING_ITERATION: 0}\n </s> add         cp = _TrackedCheckpoint(\n            dir_or_data=ray.put(1),\n            storage_mode=CheckpointStorage.PERSISTENT,\n            metrics={TRAINING_ITERATION: 0}, </s> remove         self.checkpoint_manager.delete = _CheckpointDeleter(\n            self._trainable_name(), runner\n </s> add         self.checkpoint_manager.set_delete_fn(\n            _CheckpointDeleter(self._trainable_name(), runner)", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_runner_2.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         self.assertEqual(trials[1].last_result[\"timesteps_since_restore\"], 20)\n <mask>         self.assertEqual(trials[1].last_result[\"iterations_since_restore\"], 2)\n <mask>         self.assertGreater(trials[1].last_result[\"time_since_restore\"], 0)\n <mask>         self.addCleanup(os.remove, trials[0].checkpoint.value)\n <mask> \n <mask>     def testCheckpointingAtEnd(self):\n <mask>         ray.init(num_cpus=1, num_gpus=1)\n <mask>         runner = TrialRunner()\n <mask>         kwargs = {\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         self.addCleanup(os.remove, trials[0].checkpoint.value)\n </s> add         self.addCleanup(os.remove, trials[0].checkpoint.dir_or_data) </s> remove         checkpoint.value = checkpoint_value\n </s> add         checkpoint.dir_or_data = checkpoint_value </s> remove     def __call__(self, checkpoint: _TuneCheckpoint):\n </s> add     def __call__(self, checkpoint: _TrackedCheckpoint): </s> remove         self.checkpoint_manager.delete = _CheckpointDeleter(\n            self._trainable_name(), runner\n </s> add         self.checkpoint_manager.set_delete_fn(\n            _CheckpointDeleter(self._trainable_name(), runner) </s> remove         return self.checkpoint.value is not None\n </s> add         return self.checkpoint.dir_or_data is not None </s> remove         trial.checkpoint_manager.delete = lambda cp: shutil.rmtree(cp.value)\n </s> add         trial.checkpoint_manager.set_delete_fn(lambda cp: shutil.rmtree(cp.dir_or_data))", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_runner_2.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         self.addCleanup(shutil.rmtree, tempdir)\n <mask> \n <mask>         trial = Trial(\"__fake\", keep_checkpoints_num=2)\n <mask>         trial.init_logdir()\n <mask>         trial.checkpoint_manager.delete = lambda cp: shutil.rmtree(cp.value)\n <mask> \n <mask>         def write_checkpoint(trial: Trial, index: int):\n <mask>             checkpoint_dir = TrainableUtil.make_checkpoint_dir(\n <mask>                 trial.logdir, index=index\n <mask>             )\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         trial.checkpoint_manager.delete = lambda cp: shutil.rmtree(cp.value)\n </s> add         trial.checkpoint_manager.set_delete_fn(lambda cp: shutil.rmtree(cp.dir_or_data)) </s> remove             trial.on_checkpoint(tune_cp)\n </s> add  </s> remove             tune_cp = _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, checkpoint_dir, result\n </s> add             tune_cp = _TrackedCheckpoint(\n                dir_or_data=checkpoint_dir,\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=result, </s> remove         checkpoint.value = checkpoint_value\n </s> add         checkpoint.dir_or_data = checkpoint_value </s> remove         self.last_result = self.restoring_from.result\n </s> add         self.last_result = self.restoring_from.metrics </s> remove         kwargs[\"restore_path\"] = trials[0].checkpoint.value\n </s> add         kwargs[\"restore_path\"] = trials[0].checkpoint.dir_or_data", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_runner_2.py"}
{"docstring_tokens": "keep replace replace keep keep replace", "code_tokens": " <mask> \n <mask>             tune_cp = _TuneCheckpoint(\n <mask>                 _TuneCheckpoint.PERSISTENT, checkpoint_dir, result\n <mask>             )\n <mask>             trial.saving_to = tune_cp\n <mask>             trial.on_checkpoint(tune_cp)\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove             _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, tmpfiles[1], self.mock_result(10, 10)\n </s> add             _TrackedCheckpoint(\n                dir_or_data=tmpfiles[1],\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=self.mock_result(10, 10), </s> remove             _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, tmpfiles[0], self.mock_result(5, 5)\n </s> add             _TrackedCheckpoint(\n                dir_or_data=tmpfiles[0],\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=self.mock_result(5, 5), </s> remove             _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, tmpfiles[2], self.mock_result(0, 0)\n </s> add             _TrackedCheckpoint(\n                dir_or_data=tmpfiles[2],\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=self.mock_result(0, 0), </s> remove         cp = _TuneCheckpoint(\n            _TuneCheckpoint.PERSISTENT, value=ray.put(1), result={TRAINING_ITERATION: 0}\n </s> add         cp = _TrackedCheckpoint(\n            dir_or_data=ray.put(1),\n            storage_mode=CheckpointStorage.PERSISTENT,\n            metrics={TRAINING_ITERATION: 0}, </s> remove         trial.checkpoint_manager.delete = lambda cp: shutil.rmtree(cp.value)\n </s> add         trial.checkpoint_manager.set_delete_fn(lambda cp: shutil.rmtree(cp.dir_or_data))", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_runner_2.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         runner = TrialRunner(local_checkpoint_dir=tempdir)\n <mask>         runner.resume()\n <mask> \n <mask>         trial = runner.get_trials()[0]\n <mask>         trial.checkpoint_manager.delete = lambda cp: shutil.rmtree(cp.value)\n <mask> \n <mask>         # Write fourth checkpoint\n <mask>         result = write_checkpoint(trial, 4)\n <mask>         runner._on_saving_result(trial, result)\n <mask> \n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         trial.checkpoint_manager.delete = lambda cp: shutil.rmtree(cp.value)\n </s> add         trial.checkpoint_manager.set_delete_fn(lambda cp: shutil.rmtree(cp.dir_or_data)) </s> remove         checkpoint.value = checkpoint_value\n </s> add         checkpoint.dir_or_data = checkpoint_value </s> remove     trainer.restore(trial.checkpoint.value)\n </s> add     trainer.restore(trial.checkpoint.dir_or_data) </s> remove             tune_cp = _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, checkpoint_dir, result\n </s> add             tune_cp = _TrackedCheckpoint(\n                dir_or_data=checkpoint_dir,\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=result, </s> remove         cp = _TuneCheckpoint(\n            _TuneCheckpoint.PERSISTENT, value=ray.put(1), result={TRAINING_ITERATION: 0}\n </s> add         cp = _TrackedCheckpoint(\n            dir_or_data=ray.put(1),\n            storage_mode=CheckpointStorage.PERSISTENT,\n            metrics={TRAINING_ITERATION: 0}, </s> remove         self.checkpoint_manager.delete = _CheckpointDeleter(\n            self._trainable_name(), runner\n </s> add         self.checkpoint_manager.set_delete_fn(\n            _CheckpointDeleter(self._trainable_name(), runner)", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_runner_2.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> import ray\n <mask> from ray import tune\n <mask> from ray.rllib import _register_all\n <mask> from ray.tune.checkpoint_manager import _TuneCheckpoint\n <mask> from ray.tune.logger import DEFAULT_LOGGERS, LoggerCallback, LegacyLoggerCallback\n <mask> from ray.tune.ray_trial_executor import (\n <mask>     _ExecutorEvent,\n <mask>     _ExecutorEventType,\n <mask>     RayTrialExecutor,\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add  </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint, _CheckpointManager\n </s> add from ray.tune.checkpoint_manager import _CheckpointManager", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_runner_callbacks.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask> from ray.tune.trial_runner import TrialRunner\n <mask> from ray.tune import Callback\n <mask> from ray.tune.utils.callback import create_default_callbacks\n <mask> from ray.tune.experiment import Experiment\n <mask> \n <mask> \n <mask> class TestCallback(Callback):\n <mask>     def __init__(self):\n <mask>         self.state = OrderedDict()\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add  </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> add from ray.util.ml_utils.checkpoint_manager import CheckpointStorage\n </s> remove from ray.tune.trial import _TuneCheckpoint, Trial\n </s> add from ray.tune.trial import Trial", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_runner_callbacks.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         self.assertEqual(self.callback.state[\"trial_start\"][\"iteration\"], 1)\n <mask>         self.assertEqual(self.callback.state[\"trial_start\"][\"trial\"].trial_id, \"two\")\n <mask> \n <mask>         # Just a placeholder object ref for cp.value.\n <mask>         cp = _TuneCheckpoint(\n <mask>             _TuneCheckpoint.PERSISTENT, value=ray.put(1), result={TRAINING_ITERATION: 0}\n <mask>         )\n <mask>         trials[0].saving_to = cp\n <mask> \n <mask>         # Let the first trial save a checkpoint\n <mask>         self.executor.next_future_result = _ExecutorEvent(\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         kwargs[\"restore_path\"] = trials[0].checkpoint.value\n </s> add         kwargs[\"restore_path\"] = trials[0].checkpoint.dir_or_data </s> remove     trainer.restore(trial.checkpoint.value)\n </s> add     trainer.restore(trial.checkpoint.dir_or_data) </s> remove         trial.checkpoint_manager.delete = lambda cp: shutil.rmtree(cp.value)\n </s> add         trial.checkpoint_manager.set_delete_fn(lambda cp: shutil.rmtree(cp.dir_or_data)) </s> remove         kwargs[\"restore_path\"] = trials[0].checkpoint.value\n </s> add         kwargs[\"restore_path\"] = trials[0].checkpoint.dir_or_data </s> remove             trial.on_checkpoint(tune_cp)\n </s> add  </s> remove             _TuneCheckpoint(\n                _TuneCheckpoint.PERSISTENT, tmpfiles[1], self.mock_result(10, 10)\n </s> add             _TrackedCheckpoint(\n                dir_or_data=tmpfiles[1],\n                storage_mode=CheckpointStorage.PERSISTENT,\n                metrics=self.mock_result(10, 10),", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_runner_callbacks.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> from ray.tune.schedulers.pbt import _explore, PopulationBasedTrainingReplay\n <mask> from ray.tune.suggest._mock import _MockSearcher\n <mask> from ray.tune.suggest.suggestion import ConcurrencyLimiter\n <mask> from ray.tune.trial import Trial, _TuneCheckpoint\n <mask> from ray.tune.resources import Resources\n <mask> \n <mask> from ray.rllib import _register_all\n <mask> \n <mask> _register_all()\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add  </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add  </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_scheduler.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask> \n <mask> from ray.rllib import _register_all\n <mask> \n <mask> _register_all()\n <mask> \n <mask> \n <mask> def result(t, rew):\n <mask>     return dict(time_total_s=t, episode_reward_mean=rew, training_iteration=int(t))\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add  </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add  </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_scheduler.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> # Only barebone impl for start/stop_trial. No internal state maintained.\n <mask> class _MockTrialExecutor(RayTrialExecutor):\n <mask>     def start_trial(self, trial, checkpoint_obj=None, train=True):\n <mask>         trial.logger_running = True\n <mask>         trial.restored_checkpoint = checkpoint_obj.value\n <mask>         trial.status = Trial.RUNNING\n <mask>         return True\n <mask> \n <mask>     def stop_trial(self, trial, error=False, error_msg=None):\n <mask>         trial.status = Trial.ERROR if error else Trial.TERMINATED\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         self.trial_executor.save(trial, _TuneCheckpoint.MEMORY, None)\n </s> add         self.trial_executor.save(trial, CheckpointStorage.MEMORY, None) </s> remove         checkpoint.value = checkpoint_value\n </s> add         checkpoint.dir_or_data = checkpoint_value </s> remove     def save(self, trial, type=_TuneCheckpoint.PERSISTENT, result=None):\n        return _TuneCheckpoint(_TuneCheckpoint.PERSISTENT, trial.trainable_name, result)\n </s> add     def save(self, trial, type=CheckpointStorage.PERSISTENT, result=None):\n        return _TrackedCheckpoint(\n            dir_or_data=trial.trainable_name,\n            storage_mode=CheckpointStorage.PERSISTENT,\n            metrics=result,\n        ) </s> remove         self.delete = None\n </s> add         self._delete_fn = None </s> remove         self.checkpoint_manager.delete = _CheckpointDeleter(\n            self._trainable_name(), runner\n </s> add         self.checkpoint_manager.set_delete_fn(\n            _CheckpointDeleter(self._trainable_name(), runner) </s> remove         kwargs[\"restore_path\"] = trials[0].checkpoint.value\n </s> add         kwargs[\"restore_path\"] = trials[0].checkpoint.dir_or_data", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_scheduler.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     def restore(self, trial, checkpoint=None, block=False):\n <mask>         pass\n <mask> \n <mask>     def save(self, trial, type=_TuneCheckpoint.PERSISTENT, result=None):\n <mask>         return _TuneCheckpoint(_TuneCheckpoint.PERSISTENT, trial.trainable_name, result)\n <mask> \n <mask>     def reset_trial(self, trial, new_config, new_experiment_tag):\n <mask>         return False\n <mask> \n <mask>     def debug_string(self):\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         trial.restored_checkpoint = checkpoint_obj.value\n </s> add         trial.restored_checkpoint = checkpoint_obj.dir_or_data </s> remove         checkpoint.value = checkpoint_value\n </s> add         checkpoint.dir_or_data = checkpoint_value </s> remove                 return _TuneCheckpoint(_TuneCheckpoint.MEMORY, \"None\", {})\n </s> add                 return _TrackedCheckpoint(\n                    dir_or_data={\"data\": \"None\"},\n                    storage_mode=CheckpointStorage.MEMORY,\n                    metrics={},\n                ) </s> remove         if checkpoint.value is None:\n            checkpoint = _TuneCheckpoint(_TuneCheckpoint.PERSISTENT, self.restore_path)\n </s> add         if checkpoint.dir_or_data is None:\n            checkpoint = _TrackedCheckpoint(\n                dir_or_data=self.restore_path,\n                storage_mode=CheckpointStorage.PERSISTENT,\n            ) </s> remove         return _TuneCheckpoint(_TuneCheckpoint.MEMORY, self.trainable_name, None)\n </s> add         return _TrackedCheckpoint(\n            dir_or_data={\"data\": self.trainable_name},\n            storage_mode=CheckpointStorage.MEMORY,\n            metrics=None,\n        ) </s> remove         return _TuneCheckpoint(_TuneCheckpoint.MEMORY, \"None\", {})\n </s> add         return _TrackedCheckpoint(\n            dir_or_data=\"None\",\n            storage_mode=CheckpointStorage.MEMORY,\n            metrics={},\n        )", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_scheduler.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     def get_live_trials(self):\n <mask>         return {t for t in self.trials if t.status != Trial.TERMINATED}\n <mask> \n <mask>     def _pause_trial(self, trial):\n <mask>         self.trial_executor.save(trial, _TuneCheckpoint.MEMORY, None)\n <mask>         trial.status = Trial.PAUSED\n <mask> \n <mask>     def _launch_trial(self, trial):\n <mask>         trial.status = Trial.RUNNING\n <mask> \n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         trial.restored_checkpoint = checkpoint_obj.value\n </s> add         trial.restored_checkpoint = checkpoint_obj.dir_or_data </s> remove         checkpoint = self.trial_executor.save(trial, _TuneCheckpoint.PERSISTENT)\n </s> add         checkpoint = self.trial_executor.save(trial, CheckpointStorage.PERSISTENT) </s> remove         self.assertEqual(trial.checkpoint.value, None)\n </s> add         self.assertEqual(trial.checkpoint.dir_or_data, None) </s> remove             trial.on_checkpoint(tune_cp)\n </s> add  </s> remove         self.restored_checkpoint = checkpoint.value\n </s> add         if checkpoint.storage_mode == CheckpointStorage.MEMORY:\n            self.restored_checkpoint = checkpoint.dir_or_data[\"data\"]\n        else:\n            self.restored_checkpoint = checkpoint.dir_or_data </s> remove         return _TuneCheckpoint(_TuneCheckpoint.MEMORY, self.trainable_name, None)\n </s> add         return _TrackedCheckpoint(\n            dir_or_data={\"data\": self.trainable_name},\n            storage_mode=CheckpointStorage.MEMORY,\n            metrics=None,\n        )", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_scheduler.py"}
{"docstring_tokens": "keep replace keep keep keep replace keep keep keep keep", "code_tokens": " <mask>     def on_checkpoint(self, checkpoint):\n <mask>         self.restored_checkpoint = checkpoint.value\n <mask> \n <mask>     @property\n <mask>     def checkpoint(self):\n <mask>         return _TuneCheckpoint(_TuneCheckpoint.MEMORY, self.trainable_name, None)\n <mask> \n <mask> \n <mask> class PopulationBasedTestingSuite(unittest.TestCase):\n <mask>     def setUp(self):\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         return _TuneCheckpoint(_TuneCheckpoint.MEMORY, \"None\", {})\n </s> add         return _TrackedCheckpoint(\n            dir_or_data=\"None\",\n            storage_mode=CheckpointStorage.MEMORY,\n            metrics={},\n        ) </s> remove                 return _TuneCheckpoint(_TuneCheckpoint.MEMORY, \"None\", {})\n </s> add                 return _TrackedCheckpoint(\n                    dir_or_data={\"data\": \"None\"},\n                    storage_mode=CheckpointStorage.MEMORY,\n                    metrics={},\n                ) </s> remove         checkpoint.value = checkpoint_value\n </s> add         checkpoint.dir_or_data = checkpoint_value </s> remove         self.trial_executor.save(trial, _TuneCheckpoint.MEMORY, None)\n </s> add         self.trial_executor.save(trial, CheckpointStorage.MEMORY, None) </s> remove         self.checkpoint.value = None\n </s> add         self.checkpoint.dir_or_data = None", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_scheduler.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> import ray\n <mask> from ray import tune\n <mask> from ray.tune import Trainable\n <mask> from ray.tune.trial import Trial, _TuneCheckpoint\n <mask> from ray.tune.trial_runner import TrialRunner\n <mask> from ray.tune.ray_trial_executor import RayTrialExecutor\n <mask> from ray.tune.schedulers import PopulationBasedTraining\n <mask> from ray._private.test_utils import object_memory_usage\n <mask> \n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add  </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add  </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_scheduler_pbt.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask> # Import psutil after ray so the packaged version is used.\n <mask> import psutil\n <mask> \n <mask> MB = 1024 ** 2\n <mask> \n <mask> \n <mask> class MockParam(object):\n <mask>     def __init__(self, params):\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage </s> remove         self.checkpoint.value = None\n </s> add         self.checkpoint.dir_or_data = None </s> remove         self.checkpoint_manager.delete = _CheckpointDeleter(\n            self._trainable_name(), runner\n </s> add         self.checkpoint_manager.set_delete_fn(\n            _CheckpointDeleter(self._trainable_name(), runner) </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint, _CheckpointManager\n </s> add from ray.tune.checkpoint_manager import _CheckpointManager </s> add from ray.util.ml_utils.checkpoint_manager import CheckpointStorage\n </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_scheduler_pbt.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         class MockTrial(Trial):\n <mask>             @property\n <mask>             def checkpoint(self):\n <mask>                 return _TuneCheckpoint(_TuneCheckpoint.MEMORY, \"None\", {})\n <mask> \n <mask>             @property\n <mask>             def status(self):\n <mask>                 return Trial.PAUSED\n <mask> \n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         return _TuneCheckpoint(_TuneCheckpoint.MEMORY, \"None\", {})\n </s> add         return _TrackedCheckpoint(\n            dir_or_data=\"None\",\n            storage_mode=CheckpointStorage.MEMORY,\n            metrics={},\n        ) </s> remove         return _TuneCheckpoint(_TuneCheckpoint.MEMORY, self.trainable_name, None)\n </s> add         return _TrackedCheckpoint(\n            dir_or_data={\"data\": self.trainable_name},\n            storage_mode=CheckpointStorage.MEMORY,\n            metrics=None,\n        ) </s> remove         self.restored_checkpoint = checkpoint.value\n </s> add         if checkpoint.storage_mode == CheckpointStorage.MEMORY:\n            self.restored_checkpoint = checkpoint.dir_or_data[\"data\"]\n        else:\n            self.restored_checkpoint = checkpoint.dir_or_data </s> remove         self.trial_executor.save(trial, _TuneCheckpoint.MEMORY, None)\n </s> add         self.trial_executor.save(trial, CheckpointStorage.MEMORY, None) </s> remove     def save(self, trial, type=_TuneCheckpoint.PERSISTENT, result=None):\n        return _TuneCheckpoint(_TuneCheckpoint.PERSISTENT, trial.trainable_name, result)\n </s> add     def save(self, trial, type=CheckpointStorage.PERSISTENT, result=None):\n        return _TrackedCheckpoint(\n            dir_or_data=trial.trainable_name,\n            storage_mode=CheckpointStorage.PERSISTENT,\n            metrics=result,\n        ) </s> remove         return self.checkpoint.value is not None\n </s> add         return self.checkpoint.dir_or_data is not None", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_scheduler_pbt.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> import unittest\n <mask> \n <mask> from ray.tune import PlacementGroupFactory\n <mask> from ray.tune.schedulers.trial_scheduler import TrialScheduler\n <mask> from ray.tune.trial import Trial, _TuneCheckpoint\n <mask> from ray.tune.schedulers.resource_changing_scheduler import (\n <mask>     ResourceChangingScheduler,\n <mask>     DistributeResources,\n <mask>     DistributeResourcesToTopJob,\n <mask> )\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import _TuneCheckpoint, Trial\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add  </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_scheduler_resource_changing.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask>     DistributeResourcesToTopJob,\n <mask> )\n <mask> \n <mask> \n <mask> class MockResourceUpdater:\n <mask>     def __init__(self, num_cpus, num_gpus):\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage </s> remove         return _TuneCheckpoint(_TuneCheckpoint.MEMORY, \"None\", {})\n </s> add         return _TrackedCheckpoint(\n            dir_or_data=\"None\",\n            storage_mode=CheckpointStorage.MEMORY,\n            metrics={},\n        ) </s> remove     def __call__(self, checkpoint: _TuneCheckpoint):\n </s> add     def __call__(self, checkpoint: _TrackedCheckpoint): </s> remove                 return _TuneCheckpoint(_TuneCheckpoint.MEMORY, \"None\", {})\n </s> add                 return _TrackedCheckpoint(\n                    dir_or_data={\"data\": \"None\"},\n                    storage_mode=CheckpointStorage.MEMORY,\n                    metrics={},\n                ) </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove         return _TuneCheckpoint(_TuneCheckpoint.MEMORY, self.trainable_name, None)\n </s> add         return _TrackedCheckpoint(\n            dir_or_data={\"data\": self.trainable_name},\n            storage_mode=CheckpointStorage.MEMORY,\n            metrics=None,\n        )", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_scheduler_resource_changing.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> class MockTrial(Trial):\n <mask>     @property\n <mask>     def checkpoint(self):\n <mask>         return _TuneCheckpoint(_TuneCheckpoint.MEMORY, \"None\", {})\n <mask> \n <mask> \n <mask> class TestUniformResourceAllocation(unittest.TestCase):\n <mask>     def setUp(self):\n <mask>         self.trial_runner = MockTrialRunner(8, 8)\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove                 return _TuneCheckpoint(_TuneCheckpoint.MEMORY, \"None\", {})\n </s> add                 return _TrackedCheckpoint(\n                    dir_or_data={\"data\": \"None\"},\n                    storage_mode=CheckpointStorage.MEMORY,\n                    metrics={},\n                ) </s> remove         return _TuneCheckpoint(_TuneCheckpoint.MEMORY, self.trainable_name, None)\n </s> add         return _TrackedCheckpoint(\n            dir_or_data={\"data\": self.trainable_name},\n            storage_mode=CheckpointStorage.MEMORY,\n            metrics=None,\n        ) </s> remove         self.restored_checkpoint = checkpoint.value\n </s> add         if checkpoint.storage_mode == CheckpointStorage.MEMORY:\n            self.restored_checkpoint = checkpoint.dir_or_data[\"data\"]\n        else:\n            self.restored_checkpoint = checkpoint.dir_or_data </s> remove         checkpoint.value = checkpoint_value\n </s> add         checkpoint.dir_or_data = checkpoint_value </s> add from ray.util.ml_utils.checkpoint_manager import CheckpointStorage\n </s> remove         trial.restored_checkpoint = checkpoint_obj.value\n </s> add         trial.restored_checkpoint = checkpoint_obj.dir_or_data", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/tests/test_trial_scheduler_resource_changing.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> import ray\n <mask> import ray.cloudpickle as cloudpickle\n <mask> from ray.exceptions import RayActorError, RayTaskError\n <mask> from ray.tune import TuneError\n <mask> from ray.tune.checkpoint_manager import _TuneCheckpoint, _CheckpointManager\n <mask> \n <mask> # NOTE(rkn): We import ray.tune.registry here instead of importing the names we\n <mask> # need because there are cyclic imports that may cause specific names to not\n <mask> # have been defined yet. See https://github.com/ray-project/ray/issues/1716.\n <mask> from ray.tune.registry import get_trainable_cls, validate_trainable\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add  </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add  </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import _TuneCheckpoint, Trial\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/trial.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask> from ray.util.annotations import DeveloperAPI\n <mask> from ray.util.debug import log_once\n <mask> from ray._private.utils import binary_to_hex, hex_to_binary\n <mask> \n <mask> DEBUG_PRINT_INTERVAL = 5\n <mask> logger = logging.getLogger(__name__)\n <mask> \n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> add from ray.util.ml_utils.checkpoint_manager import CheckpointStorage </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add  </s> remove from ray.tune.trial import _TuneCheckpoint, Trial\n </s> add from ray.tune.trial import Trial", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/trial.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     def __init__(self, trial_id, runner):\n <mask>         self.trial_id = trial_id\n <mask>         self.runner = runner\n <mask> \n <mask>     def __call__(self, checkpoint: _TuneCheckpoint):\n <mask>         \"\"\"Requests checkpoint deletion asynchronously.\n <mask> \n <mask>         Args:\n <mask>             checkpoint: Checkpoint to delete.\n <mask>         \"\"\"\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove     def on_checkpoint(self, checkpoint: _TuneCheckpoint):\n </s> add     def on_checkpoint(self, checkpoint: _TrackedCheckpoint): </s> remove         self.checkpoint.value = None\n </s> add         self.checkpoint.dir_or_data = None </s> remove         checkpoint.value = checkpoint_value\n </s> add         checkpoint.dir_or_data = checkpoint_value </s> remove         self.checkpoint_manager.delete = _CheckpointDeleter(\n            self._trainable_name(), runner\n </s> add         self.checkpoint_manager.set_delete_fn(\n            _CheckpointDeleter(self._trainable_name(), runner) </s> remove     ``TrackedCheckpoint.commit()`` API, which is only invoked if the checkpoint\n </s> add     ``_TrackedCheckpoint.commit()`` API, which is only invoked if the checkpoint </s> remove         self.addCleanup(os.remove, trials[0].checkpoint.value)\n </s> add         self.addCleanup(os.remove, trials[0].checkpoint.dir_or_data)", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/trial.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         \"\"\"\n <mask>         if not self.runner:\n <mask>             return\n <mask> \n <mask>         if checkpoint.storage == _TuneCheckpoint.PERSISTENT and checkpoint.value:\n <mask>             checkpoint_path = checkpoint.value\n <mask> \n <mask>             logger.debug(\n <mask>                 \"Trial %s: Deleting checkpoint %s\", self.trial_id, checkpoint_path\n <mask>             )\n <mask> \n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove             if trial.checkpoint.storage != _TuneCheckpoint.MEMORY:\n </s> add             if trial.checkpoint.storage_mode != CheckpointStorage.MEMORY: </s> remove         if checkpoint.value is None:\n            checkpoint = _TuneCheckpoint(_TuneCheckpoint.PERSISTENT, self.restore_path)\n </s> add         if checkpoint.dir_or_data is None:\n            checkpoint = _TrackedCheckpoint(\n                dir_or_data=self.restore_path,\n                storage_mode=CheckpointStorage.PERSISTENT,\n            ) </s> remove         if storage_mode == CheckpointStorage.MEMORY and not isinstance(\n            dir_or_data, (dict, ray.ObjectRef)\n </s> add         if (\n            dir_or_data is not None\n            and storage_mode == CheckpointStorage.MEMORY\n            and not isinstance(dir_or_data, (dict, ray.ObjectRef)) </s> remove         return self.checkpoint.value is not None\n </s> add         return self.checkpoint.dir_or_data is not None </s> remove         self.restored_checkpoint = checkpoint.value\n </s> add         if checkpoint.storage_mode == CheckpointStorage.MEMORY:\n            self.restored_checkpoint = checkpoint.dir_or_data[\"data\"]\n        else:\n            self.restored_checkpoint = checkpoint.dir_or_data </s> remove             trial.saving_to.value = checkpoint_value\n </s> add             trial.saving_to.dir_or_data = checkpoint_value", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/trial.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         self.sync_on_checkpoint = sync_on_checkpoint\n <mask>         self.checkpoint_manager = _CheckpointManager(\n <mask>             keep_checkpoints_num,\n <mask>             checkpoint_score_attr,\n <mask>             _CheckpointDeleter(self._trainable_name(), self.runner),\n <mask>         )\n <mask> \n <mask>         # Restoration fields\n <mask>         self.restore_path = restore_path\n <mask>         self.restoring_from = None\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         self.checkpoint.value = None\n </s> add         self.checkpoint.dir_or_data = None </s> remove         return self.checkpoint.value is not None\n </s> add         return self.checkpoint.dir_or_data is not None </s> remove         self.last_result = self.restoring_from.result\n </s> add         self.last_result = self.restoring_from.metrics </s> remove         self.checkpoint_manager.delete = _CheckpointDeleter(\n            self._trainable_name(), runner\n </s> add         self.checkpoint_manager.set_delete_fn(\n            _CheckpointDeleter(self._trainable_name(), runner) </s> remove     def on_checkpoint(self, checkpoint: _TuneCheckpoint):\n </s> add     def on_checkpoint(self, checkpoint: _TrackedCheckpoint): </s> remove         self.delete = None\n </s> add         self._delete_fn = None", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/trial.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         if self.status == Trial.ERROR:\n <mask>             checkpoint = self.checkpoint_manager.newest_persistent_checkpoint\n <mask>         else:\n <mask>             checkpoint = self.checkpoint_manager.newest_checkpoint\n <mask>         if checkpoint.value is None:\n <mask>             checkpoint = _TuneCheckpoint(_TuneCheckpoint.PERSISTENT, self.restore_path)\n <mask>         return checkpoint\n <mask> \n <mask>     @classmethod\n <mask>     def generate_id(cls):\n <mask>         return str(uuid.uuid1().hex)[:8]\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         if checkpoint.storage == _TuneCheckpoint.PERSISTENT and checkpoint.value:\n            checkpoint_path = checkpoint.value\n </s> add         if (\n            checkpoint.storage_mode == CheckpointStorage.PERSISTENT\n            and checkpoint.dir_or_data\n        ):\n            checkpoint_path = checkpoint.dir_or_data </s> remove         self.restored_checkpoint = checkpoint.value\n </s> add         if checkpoint.storage_mode == CheckpointStorage.MEMORY:\n            self.restored_checkpoint = checkpoint.dir_or_data[\"data\"]\n        else:\n            self.restored_checkpoint = checkpoint.dir_or_data </s> remove         checkpoint = self.trial_executor.save(trial, _TuneCheckpoint.PERSISTENT)\n </s> add         checkpoint = self.trial_executor.save(trial, CheckpointStorage.PERSISTENT) </s> remove         checkpoint.value = checkpoint_value\n </s> add         checkpoint.dir_or_data = checkpoint_value </s> remove         return self.checkpoint.value is not None\n </s> add         return self.checkpoint.dir_or_data is not None </s> remove         self.assertEqual(trial.checkpoint.value, None)\n </s> add         self.assertEqual(trial.checkpoint.dir_or_data, None)", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/trial.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>             # property is accessed\n <mask>             self._default_result_or_future = runner.get_auto_filled_metrics.remote(\n <mask>                 debug_metrics_only=True\n <mask>             )\n <mask>         self.checkpoint_manager.delete = _CheckpointDeleter(\n <mask>             self._trainable_name(), runner\n <mask>         )\n <mask>         # No need to invalidate state cache: runner is not stored in json\n <mask>         # self.invalidate_json_state()\n <mask> \n <mask>     def set_location(self, location):\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         return self.checkpoint.value is not None\n </s> add         return self.checkpoint.dir_or_data is not None </s> remove     trainer.restore(trial.checkpoint.value)\n </s> add     trainer.restore(trial.checkpoint.dir_or_data) </s> remove         trial.checkpoint_manager.delete = lambda cp: shutil.rmtree(cp.value)\n </s> add         trial.checkpoint_manager.set_delete_fn(lambda cp: shutil.rmtree(cp.dir_or_data)) </s> remove         self.checkpoint.value = None\n </s> add         self.checkpoint.dir_or_data = None </s> remove         trial.restored_checkpoint = checkpoint_obj.value\n </s> add         trial.restored_checkpoint = checkpoint_obj.dir_or_data </s> remove     def __call__(self, checkpoint: _TuneCheckpoint):\n </s> add     def __call__(self, checkpoint: _TrackedCheckpoint):", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/trial.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep replace keep keep keep", "code_tokens": " <mask>             and result.get(TRAINING_ITERATION, 0) % self.checkpoint_freq == 0\n <mask>         )\n <mask> \n <mask>     def has_checkpoint(self):\n <mask>         return self.checkpoint.value is not None\n <mask> \n <mask>     def clear_checkpoint(self):\n <mask>         self.checkpoint.value = None\n <mask>         self.restoring_from = None\n <mask>         self.invalidate_json_state()\n <mask> \n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove     def on_checkpoint(self, checkpoint: _TuneCheckpoint):\n </s> add     def on_checkpoint(self, checkpoint: _TrackedCheckpoint): </s> remove         self.delete = None\n </s> add         self._delete_fn = None </s> remove         self.last_result = self.restoring_from.result\n </s> add         self.last_result = self.restoring_from.metrics </s> remove         self.restored_checkpoint = checkpoint.value\n </s> add         if checkpoint.storage_mode == CheckpointStorage.MEMORY:\n            self.restored_checkpoint = checkpoint.dir_or_data[\"data\"]\n        else:\n            self.restored_checkpoint = checkpoint.dir_or_data </s> remove         if storage_mode == CheckpointStorage.MEMORY and not isinstance(\n            dir_or_data, (dict, ray.ObjectRef)\n </s> add         if (\n            dir_or_data is not None\n            and storage_mode == CheckpointStorage.MEMORY\n            and not isinstance(dir_or_data, (dict, ray.ObjectRef))", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/trial.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         self.checkpoint.value = None\n <mask>         self.restoring_from = None\n <mask>         self.invalidate_json_state()\n <mask> \n <mask>     def on_checkpoint(self, checkpoint: _TuneCheckpoint):\n <mask>         \"\"\"Hook for handling checkpoints taken by the Trainable.\n <mask> \n <mask>         Args:\n <mask>             checkpoint: Checkpoint taken.\n <mask>         \"\"\"\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         self.checkpoint.value = None\n </s> add         self.checkpoint.dir_or_data = None </s> remove     def __call__(self, checkpoint: _TuneCheckpoint):\n </s> add     def __call__(self, checkpoint: _TrackedCheckpoint): </s> remove         return self.checkpoint.value is not None\n </s> add         return self.checkpoint.dir_or_data is not None </s> remove         self.last_result = self.restoring_from.result\n </s> add         self.last_result = self.restoring_from.metrics </s> remove         self.restored_checkpoint = checkpoint.value\n </s> add         if checkpoint.storage_mode == CheckpointStorage.MEMORY:\n            self.restored_checkpoint = checkpoint.dir_or_data[\"data\"]\n        else:\n            self.restored_checkpoint = checkpoint.dir_or_data </s> remove         self.delete = None\n </s> add         self._delete_fn = None", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/trial.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     def on_restore(self):\n <mask>         \"\"\"Handles restoration completion.\"\"\"\n <mask>         assert self.is_restoring\n <mask>         self.last_result = self.restoring_from.result\n <mask>         self.restoring_from = None\n <mask>         self.invalidate_json_state()\n <mask> \n <mask>     def should_recover(self):\n <mask>         \"\"\"Returns whether the trial qualifies for retrying.\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         self.checkpoint.value = None\n </s> add         self.checkpoint.dir_or_data = None </s> remove     def on_checkpoint(self, checkpoint: _TuneCheckpoint):\n </s> add     def on_checkpoint(self, checkpoint: _TrackedCheckpoint): </s> remove         return self.checkpoint.value is not None\n </s> add         return self.checkpoint.dir_or_data is not None </s> remove         self.delete = None\n </s> add         self._delete_fn = None </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove         self.restored_checkpoint = checkpoint.value\n </s> add         if checkpoint.storage_mode == CheckpointStorage.MEMORY:\n            self.restored_checkpoint = checkpoint.dir_or_data[\"data\"]\n        else:\n            self.restored_checkpoint = checkpoint.dir_or_data", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/trial.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> from ray.tune.schedulers import FIFOScheduler, TrialScheduler\n <mask> from ray.tune.stopper import NoopStopper, Stopper\n <mask> from ray.tune.suggest import BasicVariantGenerator, SearchAlgorithm\n <mask> from ray.tune.syncer import CloudSyncer, get_cloud_syncer, SyncConfig\n <mask> from ray.tune.trial import _TuneCheckpoint, Trial\n <mask> from ray.tune.utils import warn_if_slow, flatten_dict\n <mask> from ray.tune.utils.log import Verbosity, has_verbosity\n <mask> from ray.tune.utils.placement_groups import PlacementGroupFactory\n <mask> from ray.tune.utils.serialization import TuneFunctionDecoder, TuneFunctionEncoder\n <mask> from ray.tune.web_server import TuneServer\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add  </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/trial_runner.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask> from ray.util.annotations import DeveloperAPI\n <mask> from ray.util.debug import log_once\n <mask> \n <mask> MAX_DEBUG_TRIALS = 20\n <mask> \n <mask> logger = logging.getLogger(__name__)\n <mask> \n <mask> \n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage </s> add from ray.util.ml_utils.checkpoint_manager import _TrackedCheckpoint, CheckpointStorage </s> add from ray.util.ml_utils.checkpoint_manager import CheckpointStorage\n </s> remove from ray.tune.checkpoint_manager import _TuneCheckpoint\n </s> add  </s> remove from ray.tune.trial import Trial, _TuneCheckpoint\n </s> add from ray.tune.trial import Trial", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/trial_runner.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         \"\"\"\n <mask>         logger.debug(\"Trial %s: Processing trial save.\", trial)\n <mask> \n <mask>         try:\n <mask>             trial.saving_to.value = checkpoint_value\n <mask>             self._callbacks.on_checkpoint(\n <mask>                 iteration=self._iteration,\n <mask>                 trials=self._trials,\n <mask>                 trial=trial,\n <mask>                 checkpoint=trial.saving_to,\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove             if trial.checkpoint.storage != _TuneCheckpoint.MEMORY:\n </s> add             if trial.checkpoint.storage_mode != CheckpointStorage.MEMORY: </s> remove         checkpoint.value = checkpoint_value\n </s> add         checkpoint.dir_or_data = checkpoint_value </s> remove         if checkpoint.storage == _TuneCheckpoint.PERSISTENT and checkpoint.value:\n            checkpoint_path = checkpoint.value\n </s> add         if (\n            checkpoint.storage_mode == CheckpointStorage.PERSISTENT\n            and checkpoint.dir_or_data\n        ):\n            checkpoint_path = checkpoint.dir_or_data </s> remove     def on_checkpoint(self, checkpoint: _TuneCheckpoint):\n </s> add     def on_checkpoint(self, checkpoint: _TrackedCheckpoint): </s> remove     def __call__(self, checkpoint: _TuneCheckpoint):\n </s> add     def __call__(self, checkpoint: _TrackedCheckpoint): </s> remove         kwargs[\"restore_path\"] = trials[0].checkpoint.value\n </s> add         kwargs[\"restore_path\"] = trials[0].checkpoint.dir_or_data", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/trial_runner.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                 trial=trial,\n <mask>                 checkpoint=trial.saving_to,\n <mask>             )\n <mask>             trial.on_checkpoint(trial.saving_to)\n <mask>             if trial.checkpoint.storage != _TuneCheckpoint.MEMORY:\n <mask>                 self.trial_executor.mark_trial_to_checkpoint(trial)\n <mask>         except Exception:\n <mask>             logger.exception(\n <mask>                 \"Trial %s: Error handling checkpoint %s\", trial, checkpoint_value\n <mask>             )\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove             trial.saving_to.value = checkpoint_value\n </s> add             trial.saving_to.dir_or_data = checkpoint_value </s> remove         if checkpoint.storage == _TuneCheckpoint.PERSISTENT and checkpoint.value:\n            checkpoint_path = checkpoint.value\n </s> add         if (\n            checkpoint.storage_mode == CheckpointStorage.PERSISTENT\n            and checkpoint.dir_or_data\n        ):\n            checkpoint_path = checkpoint.dir_or_data </s> remove         checkpoint.value = checkpoint_value\n </s> add         checkpoint.dir_or_data = checkpoint_value </s> remove     def save(self, trial, type=_TuneCheckpoint.PERSISTENT, result=None):\n        return _TuneCheckpoint(_TuneCheckpoint.PERSISTENT, trial.trainable_name, result)\n </s> add     def save(self, trial, type=CheckpointStorage.PERSISTENT, result=None):\n        return _TrackedCheckpoint(\n            dir_or_data=trial.trainable_name,\n            storage_mode=CheckpointStorage.PERSISTENT,\n            metrics=result,\n        ) </s> remove         if checkpoint.value is None:\n            checkpoint = _TuneCheckpoint(_TuneCheckpoint.PERSISTENT, self.restore_path)\n </s> add         if checkpoint.dir_or_data is None:\n            checkpoint = _TrackedCheckpoint(\n                dir_or_data=self.restore_path,\n                storage_mode=CheckpointStorage.PERSISTENT,\n            ) </s> remove         self.trial_executor.save(trial, _TuneCheckpoint.MEMORY, None)\n </s> add         self.trial_executor.save(trial, CheckpointStorage.MEMORY, None)", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/trial_runner.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         \"\"\"Checkpoints trial based off trial.last_result.\"\"\"\n <mask>         if trial.should_checkpoint() or force:\n <mask>             # Save trial runtime if possible.\n <mask>             if trial.runner:\n <mask>                 self.trial_executor.save(trial, storage=_TuneCheckpoint.PERSISTENT)\n <mask> \n <mask>     def _try_recover(self, trial: Trial, exc: Union[TuneError, RayTaskError]):\n <mask>         \"\"\"Tries to recover trial.\n <mask> \n <mask>         Notifies SearchAlgorithm and Scheduler if failure to recover.\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove     ``TrackedCheckpoint.commit()`` API, which is only invoked if the checkpoint\n </s> add     ``_TrackedCheckpoint.commit()`` API, which is only invoked if the checkpoint </s> remove         if storage_mode == CheckpointStorage.MEMORY and not isinstance(\n            dir_or_data, (dict, ray.ObjectRef)\n </s> add         if (\n            dir_or_data is not None\n            and storage_mode == CheckpointStorage.MEMORY\n            and not isinstance(dir_or_data, (dict, ray.ObjectRef)) </s> remove         if checkpoint.storage == _TuneCheckpoint.PERSISTENT and checkpoint.value:\n            checkpoint_path = checkpoint.value\n </s> add         if (\n            checkpoint.storage_mode == CheckpointStorage.PERSISTENT\n            and checkpoint.dir_or_data\n        ):\n            checkpoint_path = checkpoint.dir_or_data </s> remove         self.trial_executor.save(trial, _TuneCheckpoint.MEMORY, None)\n </s> add         self.trial_executor.save(trial, CheckpointStorage.MEMORY, None) </s> remove         if checkpoint.value is None:\n            checkpoint = _TuneCheckpoint(_TuneCheckpoint.PERSISTENT, self.restore_path)\n </s> add         if checkpoint.dir_or_data is None:\n            checkpoint = _TrackedCheckpoint(\n                dir_or_data=self.restore_path,\n                storage_mode=CheckpointStorage.PERSISTENT,\n            ) </s> remove     trainer.restore(trial.checkpoint.value)\n </s> add     trainer.restore(trial.checkpoint.dir_or_data)", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/tune/trial_runner.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         self.metrics = metrics or {}\n <mask>         self.node_ip = node_ip or self.metrics.get(NODE_IP, None)\n <mask> \n <mask>         if storage_mode == CheckpointStorage.MEMORY and not isinstance(\n <mask>             dir_or_data, (dict, ray.ObjectRef)\n <mask>         ):\n <mask>             raise ValueError(\n <mask>                 f\"Memory checkpoints only support Ray object references and dicts \"\n <mask>                 f\"as their data. Got: {dir_or_data}\"\n <mask>             )\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         if checkpoint.storage == _TuneCheckpoint.PERSISTENT and checkpoint.value:\n            checkpoint_path = checkpoint.value\n </s> add         if (\n            checkpoint.storage_mode == CheckpointStorage.PERSISTENT\n            and checkpoint.dir_or_data\n        ):\n            checkpoint_path = checkpoint.dir_or_data </s> remove         return self.checkpoint.value is not None\n </s> add         return self.checkpoint.dir_or_data is not None </s> remove     ``TrackedCheckpoint.commit()`` API, which is only invoked if the checkpoint\n </s> add     ``_TrackedCheckpoint.commit()`` API, which is only invoked if the checkpoint </s> remove                 self.trial_executor.save(trial, storage=_TuneCheckpoint.PERSISTENT)\n </s> add                 self.trial_executor.save(trial, storage=CheckpointStorage.PERSISTENT) </s> remove             f\"<TrackedCheckpoint storage='PERSISTENT' \"\n </s> add             f\"<_TrackedCheckpoint storage='PERSISTENT' \" </s> remove         self.restored_checkpoint = checkpoint.value\n </s> add         if checkpoint.storage_mode == CheckpointStorage.MEMORY:\n            self.restored_checkpoint = checkpoint.dir_or_data[\"data\"]\n        else:\n            self.restored_checkpoint = checkpoint.dir_or_data", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/util/ml_utils/checkpoint_manager.py"}
{"docstring_tokens": "keep replace keep keep replace", "code_tokens": " <mask>         if self.storage_mode == CheckpointStorage.MEMORY:\n <mask>             return f\"<TrackedCheckpoint storage='MEMORY' result={self.metrics}>\"\n <mask> \n <mask>         return (\n <mask>             f\"<TrackedCheckpoint storage='PERSISTENT' \"\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove         if checkpoint.storage == _TuneCheckpoint.PERSISTENT and checkpoint.value:\n            checkpoint_path = checkpoint.value\n </s> add         if (\n            checkpoint.storage_mode == CheckpointStorage.PERSISTENT\n            and checkpoint.dir_or_data\n        ):\n            checkpoint_path = checkpoint.dir_or_data </s> remove         self.restored_checkpoint = checkpoint.value\n </s> add         if checkpoint.storage_mode == CheckpointStorage.MEMORY:\n            self.restored_checkpoint = checkpoint.dir_or_data[\"data\"]\n        else:\n            self.restored_checkpoint = checkpoint.dir_or_data </s> remove         if storage_mode == CheckpointStorage.MEMORY and not isinstance(\n            dir_or_data, (dict, ray.ObjectRef)\n </s> add         if (\n            dir_or_data is not None\n            and storage_mode == CheckpointStorage.MEMORY\n            and not isinstance(dir_or_data, (dict, ray.ObjectRef)) </s> remove         if checkpoint.value is None:\n            checkpoint = _TuneCheckpoint(_TuneCheckpoint.PERSISTENT, self.restore_path)\n </s> add         if checkpoint.dir_or_data is None:\n            checkpoint = _TrackedCheckpoint(\n                dir_or_data=self.restore_path,\n                storage_mode=CheckpointStorage.PERSISTENT,\n            ) </s> remove             if trial.checkpoint.storage != _TuneCheckpoint.MEMORY:\n </s> add             if trial.checkpoint.storage_mode != CheckpointStorage.MEMORY:", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/util/ml_utils/checkpoint_manager.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     (both in-memory and on-disk checkpoints). For on-disk checkpoints, it\n <mask>     keeps a configured number of checkpoints according to specified metrics.\n <mask> \n <mask>     The manager supports lazy data writing by utilizing the\n <mask>     ``TrackedCheckpoint.commit()`` API, which is only invoked if the checkpoint\n <mask>     should be persisted to disk.\n <mask> \n <mask>     Args:\n <mask>         checkpoint_strategy: Checkpoint strategy defining how many and which\n <mask>             checkpoints to keep.\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove     def on_checkpoint(self, checkpoint: _TuneCheckpoint):\n </s> add     def on_checkpoint(self, checkpoint: _TrackedCheckpoint): </s> remove         if storage_mode == CheckpointStorage.MEMORY and not isinstance(\n            dir_or_data, (dict, ray.ObjectRef)\n </s> add         if (\n            dir_or_data is not None\n            and storage_mode == CheckpointStorage.MEMORY\n            and not isinstance(dir_or_data, (dict, ray.ObjectRef)) </s> remove                 self.trial_executor.save(trial, storage=_TuneCheckpoint.PERSISTENT)\n </s> add                 self.trial_executor.save(trial, storage=CheckpointStorage.PERSISTENT) </s> remove     def __call__(self, checkpoint: _TuneCheckpoint):\n </s> add     def __call__(self, checkpoint: _TrackedCheckpoint): </s> remove         self.checkpoint.value = None\n </s> add         self.checkpoint.dir_or_data = None </s> remove         if checkpoint.storage == _TuneCheckpoint.PERSISTENT and checkpoint.value:\n            checkpoint_path = checkpoint.value\n </s> add         if (\n            checkpoint.storage_mode == CheckpointStorage.PERSISTENT\n            and checkpoint.dir_or_data\n        ):\n            checkpoint_path = checkpoint.dir_or_data", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "python/ray/util/ml_utils/checkpoint_manager.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     # Restore trainer from checkpoint\n <mask>     trial = analysis.trials[0]\n <mask>     trainer = BanditLinTS(config=config)\n <mask>     trainer.restore(trial.checkpoint.value)\n <mask> \n <mask>     # Get model to plot arm weights distribution\n <mask>     model = trainer.get_policy().model\n <mask>     means = [model.arms[i].theta.numpy() for i in range(5)]\n <mask>     covs = [model.arms[i].covariance.numpy() for i in range(5)]\n </s> [tune/train] Consolidate checkpoint manager 3: Ray Tune (#24430)\n\n**Update**: This PR is now part 3 of a three PR group to consolidate the checkpoints.\r\n\r\n1. Part 1 adds the common checkpoint management class #24771 \r\n2. Part 2 adds the integration for Ray Train #24772\r\n3. This PR builds on #24772 and includes all changes. It moves the Ray Tune integration to use the new common checkpoint manager class.\r\n\r\nOld PR description:\r\n\r\nThis PR consolidates the Ray Train and Tune checkpoint managers. These concepts previously did something very similar but in different modules. To simplify maintenance in the future, we've consolidated the common core.\r\n\r\n- This PR keeps full compatibility with the previous interfaces and implementations. This means that for now, Train and Tune will have separate CheckpointManagers that both extend the common core\r\n- This PR prepares Tune to move to a CheckpointStrategy object\r\n- In follow-up PRs, we can further unify interfacing with the common core, possibly removing any train- or tune-specific adjustments (e.g. moving to setup on init rather on runtime for Ray Train)\r\n\r\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com> </s> remove             1, \"i\", delete_fn=lambda c: os.remove(c.value)\n </s> add             keep_checkpoints_num=1,\n            checkpoint_score_attr=\"i\",\n            delete_fn=lambda c: os.remove(c.dir_or_data), </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove         assert os.path.exists(os.path.join(trial.checkpoint.value, \"ckpt.log\"))\n </s> add         assert os.path.exists(os.path.join(trial.checkpoint.dir_or_data, \"ckpt.log\")) </s> remove         kwargs[\"restore_path\"] = trials[0].checkpoint.value\n </s> add         kwargs[\"restore_path\"] = trials[0].checkpoint.dir_or_data </s> remove         cp = _TuneCheckpoint(\n            _TuneCheckpoint.PERSISTENT, value=ray.put(1), result={TRAINING_ITERATION: 0}\n </s> add         cp = _TrackedCheckpoint(\n            dir_or_data=ray.put(1),\n            storage_mode=CheckpointStorage.PERSISTENT,\n            metrics={TRAINING_ITERATION: 0}, </s> remove         self.checkpoint_manager.delete = _CheckpointDeleter(\n            self._trainable_name(), runner\n </s> add         self.checkpoint_manager.set_delete_fn(\n            _CheckpointDeleter(self._trainable_name(), runner)", "html_url": "https://github.com/ray-project/ray/commit/8affbc7be6fdce169264b8db5b0276dbcc719f6d", "file_name": "rllib/examples/bandit/tune_lin_ts_train_wheel_env.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         \"\"\"\n <mask>         pending = []\n <mask>         infeasible = []\n <mask>         for bundle in unfulfilled:\n <mask>             placement_group = any(\"_group_\" in k for k in bundle)\n <mask>             if placement_group:\n <mask>                 continue\n <mask>             if self.resource_demand_scheduler.is_feasible(bundle):\n <mask>                 pending.append(bundle)\n <mask>             else:\n </s> Force disable placement_group for all dataset tasks (#19208) </s> remove         default_ray_remote_args = {\"retry_exceptions\": True}\n </s> add         default_ray_remote_args = {\n            \"retry_exceptions\": True,\n            \"placement_group\": None,\n        } </s> remove         if placement_group == \"default\":\n </s> add         if self._placement_group != \"default\":\n            if self._placement_group:\n                placement_group = self._placement_group\n            else:\n                placement_group = PlacementGroup.empty()\n        elif placement_group == \"default\": </s> remove         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\"\n </s> add         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\",\n        \"placement_group\" </s> add     placement_group = kwargs.get(\"placement_group\", \"default\") </s> add         self._placement_group = placement_group </s> remove @ray.remote\n </s> add @ray.remote(num_cpus=0, placement_group=None)", "html_url": "https://github.com/ray-project/ray/commit/8beabb283b53478448972da1ff94ecf829d51d5d", "file_name": "python/ray/autoscaler/_private/autoscaler.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         None,  # num_returns,\n <mask>         None,  # max_calls,\n <mask>         None,  # max_retries,\n <mask>         None,  # retry_exceptions,\n <mask>         None)  # runtime_env\n <mask> \n <mask> \n <mask> @PublicAPI(stability=\"beta\")\n <mask> def java_actor_class(class_name):\n <mask>     \"\"\"Define a Java actor class.\n </s> Force disable placement_group for all dataset tasks (#19208) </s> remove                 runtime_env)\n </s> add                 runtime_env, placement_group) </s> add                    placement_group=\"default\", </s> remove                  retry_exceptions, runtime_env):\n </s> add                  retry_exceptions, runtime_env, placement_group): </s> remove         default_ray_remote_args = {\"retry_exceptions\": True}\n </s> add         default_ray_remote_args = {\n            \"retry_exceptions\": True,\n            \"placement_group\": None,\n        } </s> remove         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\"\n </s> add         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\",\n        \"placement_group\" </s> add         self._placement_group = placement_group", "html_url": "https://github.com/ray-project/ray/commit/8beabb283b53478448972da1ff94ecf829d51d5d", "file_name": "python/ray/cross_language.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> if TYPE_CHECKING:\n <mask>     from ray.data.dataset_pipeline import DatasetPipeline\n <mask> \n <mask> \n <mask> @ray.remote\n <mask> def pipeline_stage(fn: Callable[[], Dataset[T]]) -> Dataset[T]:\n <mask>     try:\n <mask>         prev = set_progress_bars(False)\n <mask>         return fn()\n <mask>     finally:\n </s> Force disable placement_group for all dataset tasks (#19208) </s> remove @ray.remote\n </s> add @ray.remote(num_cpus=0, placement_group=None) </s> remove         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\"\n </s> add         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\",\n        \"placement_group\" </s> add     placement_group = kwargs.get(\"placement_group\", \"default\") </s> remove         default_ray_remote_args = {\"retry_exceptions\": True}\n </s> add         default_ray_remote_args = {\n            \"retry_exceptions\": True,\n            \"placement_group\": None,\n        } </s> remove     ray.get(run.options(placement_group=pg).remote())\n </s> add     ray.get(\n        run.options(\n            placement_group=pg,\n            placement_group_capture_child_tasks=True).remote()) </s> add         self._placement_group = placement_group", "html_url": "https://github.com/ray-project/ray/commit/8beabb283b53478448972da1ff94ecf829d51d5d", "file_name": "python/ray/data/impl/pipeline_executor.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         return output\n <mask> \n <mask> \n <mask> @ray.remote\n <mask> class PipelineSplitExecutorCoordinator:\n <mask>     def __init__(self, pipeline: \"DatasetPipeline[T]\", n: int,\n <mask>                  splitter: Callable[[Dataset], \"DatasetPipeline[T]\"]):\n <mask>         self.executor = PipelineExecutor(pipeline)\n <mask>         self.n = n\n </s> Force disable placement_group for all dataset tasks (#19208) </s> remove @ray.remote\n </s> add @ray.remote(num_cpus=0, placement_group=None) </s> add     placement_group = kwargs.get(\"placement_group\", \"default\") </s> add         self._placement_group = placement_group </s> remove         default_ray_remote_args = {\"retry_exceptions\": True}\n </s> add         default_ray_remote_args = {\n            \"retry_exceptions\": True,\n            \"placement_group\": None,\n        } </s> remove         if placement_group == \"default\":\n </s> add         if self._placement_group != \"default\":\n            if self._placement_group:\n                placement_group = self._placement_group\n            else:\n                placement_group = PlacementGroup.empty()\n        elif placement_group == \"default\": </s> remove         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\"\n </s> add         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\",\n        \"placement_group\"", "html_url": "https://github.com/ray-project/ray/commit/8beabb283b53478448972da1ff94ecf829d51d5d", "file_name": "python/ray/data/impl/pipeline_executor.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     (ray imports ray.data in order to allow ``ray.data.read_foo()`` to work,\n <mask>     which means ray.remote cannot be used top-level in ray.data).\n <mask>     \"\"\"\n <mask>     if fn not in CACHED_FUNCTIONS:\n <mask>         default_ray_remote_args = {\"retry_exceptions\": True}\n <mask>         CACHED_FUNCTIONS[fn] = ray.remote(**{\n <mask>             **default_ray_remote_args,\n <mask>             **ray_remote_args\n <mask>         })(fn)\n <mask>     return CACHED_FUNCTIONS[fn]\n </s> Force disable placement_group for all dataset tasks (#19208) </s> remove             placement_group = any(\"_group_\" in k for k in bundle)\n </s> add             placement_group = any(\n                \"_group_\" in k or k == \"bundle\" for k in bundle) </s> remove         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\"\n </s> add         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\",\n        \"placement_group\" </s> remove                  retry_exceptions, runtime_env):\n </s> add                  retry_exceptions, runtime_env, placement_group): </s> add     placement_group = kwargs.get(\"placement_group\", \"default\") </s> remove @ray.remote\n </s> add @ray.remote(num_cpus=0, placement_group=None) </s> remove                 runtime_env)\n </s> add                 runtime_env, placement_group)", "html_url": "https://github.com/ray-project/ray/commit/8beabb283b53478448972da1ff94ecf829d51d5d", "file_name": "python/ray/data/impl/remote_fn.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         ds = maybe_pipeline(ds0, pipelined)\n <mask>         assert sorted(ds.iter_rows()) == [0, 1, 2, 3, 4]\n <mask> \n <mask>     pg = ray.util.placement_group([{\"CPU\": 1}])\n <mask>     ray.get(run.options(placement_group=pg).remote())\n <mask> \n <mask> \n <mask> @pytest.mark.parametrize(\"pipelined\", [False, True])\n <mask> def test_equal_split(shutdown_only, pipelined):\n <mask>     ray.init(num_cpus=2)\n </s> Force disable placement_group for all dataset tasks (#19208) </s> remove         if placement_group == \"default\":\n </s> add         if self._placement_group != \"default\":\n            if self._placement_group:\n                placement_group = self._placement_group\n            else:\n                placement_group = PlacementGroup.empty()\n        elif placement_group == \"default\": </s> remove             placement_group = any(\"_group_\" in k for k in bundle)\n </s> add             placement_group = any(\n                \"_group_\" in k or k == \"bundle\" for k in bundle) </s> add         self._placement_group = placement_group </s> add     placement_group = kwargs.get(\"placement_group\", \"default\") </s> remove @ray.remote\n </s> add @ray.remote(num_cpus=0, placement_group=None) </s> remove         default_ray_remote_args = {\"retry_exceptions\": True}\n </s> add         default_ray_remote_args = {\n            \"retry_exceptions\": True,\n            \"placement_group\": None,\n        }", "html_url": "https://github.com/ray-project/ray/commit/8beabb283b53478448972da1ff94ecf829d51d5d", "file_name": "python/ray/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     def __init__(self, language, function, function_descriptor, num_cpus,\n <mask>                  num_gpus, memory, object_store_memory, resources,\n <mask>                  accelerator_type, num_returns, max_calls, max_retries,\n <mask>                  retry_exceptions, runtime_env):\n <mask>         if inspect.iscoroutinefunction(function):\n <mask>             raise ValueError(\"'async def' should not be used for remote \"\n <mask>                              \"tasks. You can wrap the async function with \"\n <mask>                              \"`asyncio.get_event_loop.run_until(f())`. \"\n <mask>                              \"See more at docs.ray.io/async_api.html\")\n </s> Force disable placement_group for all dataset tasks (#19208) </s> remove                 runtime_env)\n </s> add                 runtime_env, placement_group) </s> remove         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\"\n </s> add         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\",\n        \"placement_group\" </s> add                    placement_group=\"default\", </s> remove         None)  # runtime_env\n </s> add         None,  # runtime_env\n        None)  # placement_group </s> remove         default_ray_remote_args = {\"retry_exceptions\": True}\n </s> add         default_ray_remote_args = {\n            \"retry_exceptions\": True,\n            \"placement_group\": None,\n        } </s> remove @ray.remote\n </s> add @ray.remote(num_cpus=0, placement_group=None)", "html_url": "https://github.com/ray-project/ray/commit/8beabb283b53478448972da1ff94ecf829d51d5d", "file_name": "python/ray/remote_function.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>         self._runtime_env = ParsedRuntimeEnv(\n <mask>             runtime_env or {}, is_task_or_actor=True)\n <mask>         self._decorator = getattr(function, \"__ray_invocation_decorator__\",\n <mask>                                   None)\n <mask>         self._function_signature = ray._private.signature.extract_signature(\n <mask>             self._function)\n <mask> \n <mask>         self._last_export_session_and_job = None\n </s> Force disable placement_group for all dataset tasks (#19208) </s> add     placement_group = kwargs.get(\"placement_group\", \"default\") </s> remove             placement_group = any(\"_group_\" in k for k in bundle)\n </s> add             placement_group = any(\n                \"_group_\" in k or k == \"bundle\" for k in bundle) </s> remove         None)  # runtime_env\n </s> add         None,  # runtime_env\n        None)  # placement_group </s> remove         if placement_group == \"default\":\n </s> add         if self._placement_group != \"default\":\n            if self._placement_group:\n                placement_group = self._placement_group\n            else:\n                placement_group = PlacementGroup.empty()\n        elif placement_group == \"default\": </s> remove @ray.remote\n </s> add @ray.remote(num_cpus=0, placement_group=None) </s> remove         default_ray_remote_args = {\"retry_exceptions\": True}\n </s> add         default_ray_remote_args = {\n            \"retry_exceptions\": True,\n            \"placement_group\": None,\n        }", "html_url": "https://github.com/ray-project/ray/commit/8beabb283b53478448972da1ff94ecf829d51d5d", "file_name": "python/ray/remote_function.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         if placement_group_capture_child_tasks is None:\n <mask>             placement_group_capture_child_tasks = (\n <mask>                 worker.should_capture_child_tasks_in_placement_group)\n <mask> \n <mask>         if placement_group == \"default\":\n <mask>             if placement_group_capture_child_tasks:\n <mask>                 placement_group = get_current_placement_group()\n <mask>             else:\n <mask>                 placement_group = PlacementGroup.empty()\n <mask> \n </s> Force disable placement_group for all dataset tasks (#19208) </s> remove             placement_group = any(\"_group_\" in k for k in bundle)\n </s> add             placement_group = any(\n                \"_group_\" in k or k == \"bundle\" for k in bundle) </s> add     placement_group = kwargs.get(\"placement_group\", \"default\") </s> add         self._placement_group = placement_group </s> remove                 runtime_env)\n </s> add                 runtime_env, placement_group) </s> remove     ray.get(run.options(placement_group=pg).remote())\n </s> add     ray.get(\n        run.options(\n            placement_group=pg,\n            placement_group_capture_child_tasks=True).remote()) </s> remove @ray.remote\n </s> add @ray.remote(num_cpus=0, placement_group=None)", "html_url": "https://github.com/ray-project/ray/commit/8beabb283b53478448972da1ff94ecf829d51d5d", "file_name": "python/ray/remote_function.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>                    max_task_retries=None,\n <mask>                    runtime_env=None,\n <mask>                    worker=None,\n <mask>                    retry_exceptions=None):\n <mask>     def decorator(function_or_class):\n <mask>         if (inspect.isfunction(function_or_class)\n <mask>                 or is_cython(function_or_class)):\n <mask>             # Set the remote function default resources.\n </s> Force disable placement_group for all dataset tasks (#19208) </s> remove                  retry_exceptions, runtime_env):\n </s> add                  retry_exceptions, runtime_env, placement_group): </s> remove         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\"\n </s> add         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\",\n        \"placement_group\" </s> remove         None)  # runtime_env\n </s> add         None,  # runtime_env\n        None)  # placement_group </s> add         self._placement_group = placement_group </s> remove             placement_group = any(\"_group_\" in k for k in bundle)\n </s> add             placement_group = any(\n                \"_group_\" in k or k == \"bundle\" for k in bundle) </s> remove @ray.remote\n </s> add @ray.remote(num_cpus=0, placement_group=None)", "html_url": "https://github.com/ray-project/ray/commit/8beabb283b53478448972da1ff94ecf829d51d5d", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             return ray.remote_function.RemoteFunction(\n <mask>                 Language.PYTHON, function_or_class, None, num_cpus, num_gpus,\n <mask>                 memory, object_store_memory, resources, accelerator_type,\n <mask>                 num_returns, max_calls, max_retries, retry_exceptions,\n <mask>                 runtime_env)\n <mask> \n <mask>         if inspect.isclass(function_or_class):\n <mask>             if num_returns is not None:\n <mask>                 raise TypeError(\"The keyword 'num_returns' is not \"\n <mask>                                 \"allowed for actors.\")\n </s> Force disable placement_group for all dataset tasks (#19208) </s> remove                  retry_exceptions, runtime_env):\n </s> add                  retry_exceptions, runtime_env, placement_group): </s> remove         None)  # runtime_env\n </s> add         None,  # runtime_env\n        None)  # placement_group </s> remove         if placement_group == \"default\":\n </s> add         if self._placement_group != \"default\":\n            if self._placement_group:\n                placement_group = self._placement_group\n            else:\n                placement_group = PlacementGroup.empty()\n        elif placement_group == \"default\": </s> remove         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\"\n </s> add         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\",\n        \"placement_group\" </s> remove         default_ray_remote_args = {\"retry_exceptions\": True}\n </s> add         default_ray_remote_args = {\n            \"retry_exceptions\": True,\n            \"placement_group\": None,\n        } </s> remove             placement_group = any(\"_group_\" in k for k in bundle)\n </s> add             placement_group = any(\n                \"_group_\" in k or k == \"bundle\" for k in bundle)", "html_url": "https://github.com/ray-project/ray/commit/8beabb283b53478448972da1ff94ecf829d51d5d", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     # Parse the keyword arguments from the decorator.\n <mask>     valid_kwargs = [\n <mask>         \"num_returns\", \"num_cpus\", \"num_gpus\", \"memory\", \"object_store_memory\",\n <mask>         \"resources\", \"accelerator_type\", \"max_calls\", \"max_restarts\",\n <mask>         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\"\n <mask>     ]\n <mask>     error_string = (\"The @ray.remote decorator must be applied either \"\n <mask>                     \"with no arguments and no parentheses, for example \"\n <mask>                     \"'@ray.remote', or it must be applied using some of \"\n <mask>                     f\"the arguments in the list {valid_kwargs}, for example \"\n </s> Force disable placement_group for all dataset tasks (#19208) </s> remove                  retry_exceptions, runtime_env):\n </s> add                  retry_exceptions, runtime_env, placement_group): </s> remove                 runtime_env)\n </s> add                 runtime_env, placement_group) </s> remove             placement_group = any(\"_group_\" in k for k in bundle)\n </s> add             placement_group = any(\n                \"_group_\" in k or k == \"bundle\" for k in bundle) </s> add                    placement_group=\"default\", </s> remove         default_ray_remote_args = {\"retry_exceptions\": True}\n </s> add         default_ray_remote_args = {\n            \"retry_exceptions\": True,\n            \"placement_group\": None,\n        } </s> remove @ray.remote\n </s> add @ray.remote(num_cpus=0, placement_group=None)", "html_url": "https://github.com/ray-project/ray/commit/8beabb283b53478448972da1ff94ecf829d51d5d", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask>     max_retries = kwargs.get(\"max_retries\")\n <mask>     runtime_env = kwargs.get(\"runtime_env\")\n <mask>     retry_exceptions = kwargs.get(\"retry_exceptions\")\n <mask> \n <mask>     return make_decorator(\n <mask>         num_returns=num_returns,\n </s> Force disable placement_group for all dataset tasks (#19208) </s> add         self._placement_group = placement_group </s> remove @ray.remote\n </s> add @ray.remote(num_cpus=0, placement_group=None) </s> remove         if placement_group == \"default\":\n </s> add         if self._placement_group != \"default\":\n            if self._placement_group:\n                placement_group = self._placement_group\n            else:\n                placement_group = PlacementGroup.empty()\n        elif placement_group == \"default\": </s> remove         default_ray_remote_args = {\"retry_exceptions\": True}\n </s> add         default_ray_remote_args = {\n            \"retry_exceptions\": True,\n            \"placement_group\": None,\n        } </s> remove             placement_group = any(\"_group_\" in k for k in bundle)\n </s> add             placement_group = any(\n                \"_group_\" in k or k == \"bundle\" for k in bundle) </s> remove @ray.remote\n </s> add @ray.remote(num_cpus=0, placement_group=None)", "html_url": "https://github.com/ray-project/ray/commit/8beabb283b53478448972da1ff94ecf829d51d5d", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep add keep keep", "code_tokens": " <mask>         max_restarts=max_restarts,\n <mask>         max_task_retries=max_task_retries,\n <mask>         max_retries=max_retries,\n <mask>         runtime_env=runtime_env,\n <mask>         worker=worker,\n <mask>         retry_exceptions=retry_exceptions)\n </s> Force disable placement_group for all dataset tasks (#19208) </s> add     placement_group = kwargs.get(\"placement_group\", \"default\") </s> remove         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\"\n </s> add         \"max_task_retries\", \"max_retries\", \"runtime_env\", \"retry_exceptions\",\n        \"placement_group\" </s> remove                 runtime_env)\n </s> add                 runtime_env, placement_group) </s> add                    placement_group=\"default\", </s> remove         if placement_group == \"default\":\n </s> add         if self._placement_group != \"default\":\n            if self._placement_group:\n                placement_group = self._placement_group\n            else:\n                placement_group = PlacementGroup.empty()\n        elif placement_group == \"default\": </s> add         self._placement_group = placement_group", "html_url": "https://github.com/ray-project/ray/commit/8beabb283b53478448972da1ff94ecf829d51d5d", "file_name": "python/ray/worker.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                 num_cpus=3,\n <mask>                 num_gpus=resource_quantity,\n <mask>                 resources=custom_resources))\n <mask>     cluster.wait_for_nodes()\n <mask>     num_nodes = len(nodes)\n <mask> \n <mask>     ray.init(address=cluster.address)\n <mask>     while not ray.is_initialized():\n <mask>         time.sleep(0.1)\n <mask>     bundles = [{\"GPU\": 1, \"pg_custom\": 1}] * num_nodes\n </s> Revert \"[Placement Group] Fix the high load bug from the placement group (#19277)\" (#19327)\n\nThis reverts commit 4360b998031d2f71109ddf0bc66a056a7190ce36. </s> remove             for i in range(num_nodes):\n                tasks.append(\n                    mock_task.options(\n                        placement_group=pg,\n                        placement_group_bundle_index=i).remote())\n </s> add             tasks.append(mock_task.options(placement_group=pg).remote()) </s> remove   const auto &resources = bundle_spec.GetFormattedResources();\n  for (const auto &resource : resources) {\n </s> add   for (const auto &resource : bundle_spec.GetFormattedResources()) { </s> remove     if (resource_map_filter.count(resource_name) == 0u) {\n      continue;\n    }\n\n </s> add  </s> remove     if (resource_map_filter.count(resource_name) == 0u) {\n      continue;\n    }\n\n </s> add  </s> remove ClusterResourceScheduler::GetResourceTotals(\n    const std::unordered_map<std::string, double> &resource_map_filter) const {\n </s> add ClusterResourceScheduler::GetResourceTotals() const { </s> remove   update_resources_(\n      cluster_resource_scheduler_->GetResourceTotals(/*resource_name_filter*/ resources));\n </s> add   update_resources_(cluster_resource_scheduler_->GetResourceTotals());", "html_url": "https://github.com/ray-project/ray/commit/8c152bd17ce78589f16a6fb26bfdb5fcf54df026", "file_name": "python/ray/tests/test_placement_group_mini_integration.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         tasks = []\n <mask>         # Randomly schedule tasks or actors on placement groups that\n <mask>         # are not removed.\n <mask>         for pg in pgs_unremoved:\n <mask>             for i in range(num_nodes):\n <mask>                 tasks.append(\n <mask>                     mock_task.options(\n <mask>                         placement_group=pg,\n <mask>                         placement_group_bundle_index=i).remote())\n <mask>         # Remove the rest of placement groups.\n <mask>         for pg in pgs_removed:\n <mask>             remove_placement_group(pg)\n <mask>         ray.get(tasks)\n <mask>         # Since placement groups are scheduled, remove them.\n </s> Revert \"[Placement Group] Fix the high load bug from the placement group (#19277)\" (#19327)\n\nThis reverts commit 4360b998031d2f71109ddf0bc66a056a7190ce36. </s> remove   /// Populate a UpdateResourcesRequest. This is inteneded to update the\n  /// resource totals on a node when a custom resource is created or deleted\n  /// (e.g. during the placement group lifecycle).\n  ///\n  /// \\param resource_map_filter When returning the resource map, the returned result will\n  /// only contain the keys in the filter. Note that only the key of the map is used.\n </s> add  </s> remove   /// \\param resource_map_filter When returning the resource map, the returned result will\n  /// only contain the keys in the filter. Note that only the key of the map is used.\n  /// \\return The total resource capacity of the node.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals(\n      const std::unordered_map<std::string, double> &resource_map_filter) const = 0;\n </s> add   /// \\param Output parameter. Fills out all fields.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals() const = 0; </s> remove   ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals(\n      const std::unordered_map<std::string, double> &resource_map_filter) const override;\n </s> add   ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals() const override; </s> remove ClusterResourceScheduler::GetResourceTotals(\n    const std::unordered_map<std::string, double> &resource_map_filter) const {\n </s> add ClusterResourceScheduler::GetResourceTotals() const { </s> remove     if (resource_map_filter.count(resource_name) == 0u) {\n      continue;\n    }\n\n </s> add  </s> remove     num_nodes = len(nodes)\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8c152bd17ce78589f16a6fb26bfdb5fcf54df026", "file_name": "python/ray/tests/test_placement_group_mini_integration.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>               (const std::string &node_name, const std::string &resource_name),\n <mask>               (override));\n <mask>   MOCK_METHOD(std::string, GetLocalResourceViewString, (), (const, override));\n <mask>   MOCK_METHOD(void, FillResourceUsage, (rpc::ResourcesData & resources_data), (override));\n <mask>   MOCK_METHOD(ray::gcs::NodeResourceInfoAccessor::ResourceMap, GetResourceTotals,\n <mask>               (const std::unordered_map<std::string, double> &resource_map_filter),\n <mask>               (const, override));\n <mask>   MOCK_METHOD(void, UpdateLastResourceUsage,\n <mask>               (const std::shared_ptr<SchedulingResources> gcs_resources), (override));\n <mask>   MOCK_METHOD(double, GetLocalAvailableCpus, (), (const, override));\n <mask> };\n </s> Revert \"[Placement Group] Fix the high load bug from the placement group (#19277)\" (#19327)\n\nThis reverts commit 4360b998031d2f71109ddf0bc66a056a7190ce36. </s> remove   MOCK_METHOD(ray::gcs::NodeResourceInfoAccessor::ResourceMap, GetResourceTotals,\n              (const std::unordered_map<std::string, double> &resource_map_filter),\n </s> add   MOCK_METHOD(ray::gcs::NodeResourceInfoAccessor::ResourceMap, GetResourceTotals, (), </s> remove   const auto &resources = bundle_spec.GetFormattedResources();\n  for (const auto &resource : resources) {\n </s> add   for (const auto &resource : bundle_spec.GetFormattedResources()) { </s> remove   /// \\param resource_map_filter When returning the resource map, the returned result will\n  /// only contain the keys in the filter. Note that only the key of the map is used.\n  /// \\return The total resource capacity of the node.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals(\n      const std::unordered_map<std::string, double> &resource_map_filter) const = 0;\n </s> add   /// \\param Output parameter. Fills out all fields.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals() const = 0; </s> remove     if (resource_map_filter.count(resource_name) == 0u) {\n      continue;\n    }\n\n </s> add  </s> remove     if (resource_map_filter.count(resource_name) == 0u) {\n      continue;\n    }\n\n </s> add  </s> remove ClusterResourceScheduler::GetResourceTotals(\n    const std::unordered_map<std::string, double> &resource_map_filter) const {\n </s> add ClusterResourceScheduler::GetResourceTotals() const {", "html_url": "https://github.com/ray-project/ray/commit/8c152bd17ce78589f16a6fb26bfdb5fcf54df026", "file_name": "src/mock/ray/raylet/scheduling/cluster_resource_scheduler.h"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>   MOCK_METHOD(void, UpdateLastResourceUsage,\n <mask>               (const std::shared_ptr<SchedulingResources> gcs_resources), (override));\n <mask>   MOCK_METHOD(void, FillResourceUsage, (rpc::ResourcesData & data), (override));\n <mask>   MOCK_METHOD(double, GetLocalAvailableCpus, (), (const, override));\n <mask>   MOCK_METHOD(ray::gcs::NodeResourceInfoAccessor::ResourceMap, GetResourceTotals,\n <mask>               (const std::unordered_map<std::string, double> &resource_map_filter),\n <mask>               (const, override));\n <mask>   MOCK_METHOD(std::string, GetLocalResourceViewString, (), (const, override));\n <mask> };\n <mask> \n <mask> }  // namespace ray\n </s> Revert \"[Placement Group] Fix the high load bug from the placement group (#19277)\" (#19327)\n\nThis reverts commit 4360b998031d2f71109ddf0bc66a056a7190ce36. </s> remove   MOCK_METHOD(ray::gcs::NodeResourceInfoAccessor::ResourceMap, GetResourceTotals,\n              (const std::unordered_map<std::string, double> &resource_map_filter),\n </s> add   MOCK_METHOD(ray::gcs::NodeResourceInfoAccessor::ResourceMap, GetResourceTotals, (), </s> remove   /// \\param resource_map_filter When returning the resource map, the returned result will\n  /// only contain the keys in the filter. Note that only the key of the map is used.\n  /// \\return The total resource capacity of the node.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals(\n      const std::unordered_map<std::string, double> &resource_map_filter) const = 0;\n </s> add   /// \\param Output parameter. Fills out all fields.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals() const = 0; </s> remove   const auto &resources = bundle_spec.GetFormattedResources();\n  for (const auto &resource : resources) {\n </s> add   for (const auto &resource : bundle_spec.GetFormattedResources()) { </s> remove   ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals(\n      const std::unordered_map<std::string, double> &resource_map_filter) const override;\n </s> add   ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals() const override; </s> remove   /// Populate a UpdateResourcesRequest. This is inteneded to update the\n  /// resource totals on a node when a custom resource is created or deleted\n  /// (e.g. during the placement group lifecycle).\n  ///\n  /// \\param resource_map_filter When returning the resource map, the returned result will\n  /// only contain the keys in the filter. Note that only the key of the map is used.\n </s> add  </s> remove     if (resource_map_filter.count(resource_name) == 0u) {\n      continue;\n    }\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8c152bd17ce78589f16a6fb26bfdb5fcf54df026", "file_name": "src/mock/ray/raylet/scheduling/cluster_resource_scheduler_interface.h"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>   const auto &string_id_map = cluster_resource_scheduler_->GetStringIdMap();\n <mask>   const auto &task_resource_instances = *bundle_state->resources_;\n <mask> \n <mask>   const auto &resources = bundle_spec.GetFormattedResources();\n <mask>   for (const auto &resource : resources) {\n <mask>     const auto &resource_name = resource.first;\n <mask>     const auto &original_resource_name = GetOriginalResourceName(resource_name);\n <mask>     if (original_resource_name != kBundle_ResourceLabel) {\n <mask>       const auto &instances =\n <mask>           task_resource_instances.Get(original_resource_name, string_id_map);\n </s> Revert \"[Placement Group] Fix the high load bug from the placement group (#19277)\" (#19327)\n\nThis reverts commit 4360b998031d2f71109ddf0bc66a056a7190ce36. </s> remove ClusterResourceScheduler::GetResourceTotals(\n    const std::unordered_map<std::string, double> &resource_map_filter) const {\n </s> add ClusterResourceScheduler::GetResourceTotals() const { </s> remove     if (resource_map_filter.count(resource_name) == 0u) {\n      continue;\n    }\n\n </s> add  </s> remove     if (resource_map_filter.count(resource_name) == 0u) {\n      continue;\n    }\n\n </s> add  </s> remove   update_resources_(\n      cluster_resource_scheduler_->GetResourceTotals(/*resource_name_filter*/ resources));\n </s> add   update_resources_(cluster_resource_scheduler_->GetResourceTotals()); </s> remove   /// \\param resource_map_filter When returning the resource map, the returned result will\n  /// only contain the keys in the filter. Note that only the key of the map is used.\n  /// \\return The total resource capacity of the node.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals(\n      const std::unordered_map<std::string, double> &resource_map_filter) const = 0;\n </s> add   /// \\param Output parameter. Fills out all fields.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals() const = 0; </s> remove   MOCK_METHOD(ray::gcs::NodeResourceInfoAccessor::ResourceMap, GetResourceTotals,\n              (const std::unordered_map<std::string, double> &resource_map_filter),\n </s> add   MOCK_METHOD(ray::gcs::NodeResourceInfoAccessor::ResourceMap, GetResourceTotals, (),", "html_url": "https://github.com/ray-project/ray/commit/8c152bd17ce78589f16a6fb26bfdb5fcf54df026", "file_name": "src/ray/raylet/placement_group_resource_manager.cc"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>                                                              {resource.second});\n <mask>     }\n <mask>   }\n <mask>   cluster_resource_scheduler_->UpdateLocalAvailableResourcesFromResourceInstances();\n <mask>   update_resources_(\n <mask>       cluster_resource_scheduler_->GetResourceTotals(/*resource_name_filter*/ resources));\n <mask> }\n <mask> \n <mask> void NewPlacementGroupResourceManager::ReturnBundle(\n <mask>     const BundleSpecification &bundle_spec) {\n <mask>   auto it = pg_bundles_.find(bundle_spec.BundleId());\n </s> Revert \"[Placement Group] Fix the high load bug from the placement group (#19277)\" (#19327)\n\nThis reverts commit 4360b998031d2f71109ddf0bc66a056a7190ce36. </s> remove ClusterResourceScheduler::GetResourceTotals(\n    const std::unordered_map<std::string, double> &resource_map_filter) const {\n </s> add ClusterResourceScheduler::GetResourceTotals() const { </s> remove   const auto &resources = bundle_spec.GetFormattedResources();\n  for (const auto &resource : resources) {\n </s> add   for (const auto &resource : bundle_spec.GetFormattedResources()) { </s> remove     if (resource_map_filter.count(resource_name) == 0u) {\n      continue;\n    }\n\n </s> add  </s> remove     if (resource_map_filter.count(resource_name) == 0u) {\n      continue;\n    }\n\n </s> add  </s> remove   /// Populate a UpdateResourcesRequest. This is inteneded to update the\n  /// resource totals on a node when a custom resource is created or deleted\n  /// (e.g. during the placement group lifecycle).\n  ///\n  /// \\param resource_map_filter When returning the resource map, the returned result will\n  /// only contain the keys in the filter. Note that only the key of the map is used.\n </s> add  </s> remove   /// \\param resource_map_filter When returning the resource map, the returned result will\n  /// only contain the keys in the filter. Note that only the key of the map is used.\n  /// \\return The total resource capacity of the node.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals(\n      const std::unordered_map<std::string, double> &resource_map_filter) const = 0;\n </s> add   /// \\param Output parameter. Fills out all fields.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals() const = 0;", "html_url": "https://github.com/ray-project/ray/commit/8c152bd17ce78589f16a6fb26bfdb5fcf54df026", "file_name": "src/ray/raylet/placement_group_resource_manager.cc"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>   return capacity.available.Double();\n <mask> }\n <mask> \n <mask> ray::gcs::NodeResourceInfoAccessor::ResourceMap\n <mask> ClusterResourceScheduler::GetResourceTotals(\n <mask>     const std::unordered_map<std::string, double> &resource_map_filter) const {\n <mask>   ray::gcs::NodeResourceInfoAccessor::ResourceMap map;\n <mask>   auto it = nodes_.find(local_node_id_);\n <mask>   RAY_CHECK(it != nodes_.end());\n <mask>   const auto &local_resources = it->second.GetLocalView();\n <mask>   for (size_t i = 0; i < local_resources.predefined_resources.size(); i++) {\n </s> Revert \"[Placement Group] Fix the high load bug from the placement group (#19277)\" (#19327)\n\nThis reverts commit 4360b998031d2f71109ddf0bc66a056a7190ce36. </s> remove     if (resource_map_filter.count(resource_name) == 0u) {\n      continue;\n    }\n\n </s> add  </s> remove   const auto &resources = bundle_spec.GetFormattedResources();\n  for (const auto &resource : resources) {\n </s> add   for (const auto &resource : bundle_spec.GetFormattedResources()) { </s> remove   update_resources_(\n      cluster_resource_scheduler_->GetResourceTotals(/*resource_name_filter*/ resources));\n </s> add   update_resources_(cluster_resource_scheduler_->GetResourceTotals()); </s> remove   /// \\param resource_map_filter When returning the resource map, the returned result will\n  /// only contain the keys in the filter. Note that only the key of the map is used.\n  /// \\return The total resource capacity of the node.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals(\n      const std::unordered_map<std::string, double> &resource_map_filter) const = 0;\n </s> add   /// \\param Output parameter. Fills out all fields.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals() const = 0; </s> remove     if (resource_map_filter.count(resource_name) == 0u) {\n      continue;\n    }\n\n </s> add  </s> remove   ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals(\n      const std::unordered_map<std::string, double> &resource_map_filter) const override;\n </s> add   ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals() const override;", "html_url": "https://github.com/ray-project/ray/commit/8c152bd17ce78589f16a6fb26bfdb5fcf54df026", "file_name": "src/ray/raylet/scheduling/cluster_resource_scheduler.cc"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>   const auto &local_resources = it->second.GetLocalView();\n <mask>   for (size_t i = 0; i < local_resources.predefined_resources.size(); i++) {\n <mask>     std::string resource_name = ResourceEnumToString(static_cast<PredefinedResources>(i));\n <mask>     double resource_total = local_resources.predefined_resources[i].total.Double();\n <mask>     if (resource_map_filter.count(resource_name) == 0u) {\n <mask>       continue;\n <mask>     }\n <mask> \n <mask>     if (resource_total > 0) {\n <mask>       auto data = std::make_shared<rpc::ResourceTableData>();\n <mask>       data->set_resource_capacity(resource_total);\n <mask>       map.emplace(resource_name, std::move(data));\n <mask>     }\n </s> Revert \"[Placement Group] Fix the high load bug from the placement group (#19277)\" (#19327)\n\nThis reverts commit 4360b998031d2f71109ddf0bc66a056a7190ce36. </s> remove     if (resource_map_filter.count(resource_name) == 0u) {\n      continue;\n    }\n\n </s> add  </s> remove ClusterResourceScheduler::GetResourceTotals(\n    const std::unordered_map<std::string, double> &resource_map_filter) const {\n </s> add ClusterResourceScheduler::GetResourceTotals() const { </s> remove   const auto &resources = bundle_spec.GetFormattedResources();\n  for (const auto &resource : resources) {\n </s> add   for (const auto &resource : bundle_spec.GetFormattedResources()) { </s> remove   update_resources_(\n      cluster_resource_scheduler_->GetResourceTotals(/*resource_name_filter*/ resources));\n </s> add   update_resources_(cluster_resource_scheduler_->GetResourceTotals()); </s> remove   /// \\param resource_map_filter When returning the resource map, the returned result will\n  /// only contain the keys in the filter. Note that only the key of the map is used.\n  /// \\return The total resource capacity of the node.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals(\n      const std::unordered_map<std::string, double> &resource_map_filter) const = 0;\n </s> add   /// \\param Output parameter. Fills out all fields.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals() const = 0; </s> remove             for i in range(num_nodes):\n                tasks.append(\n                    mock_task.options(\n                        placement_group=pg,\n                        placement_group_bundle_index=i).remote())\n </s> add             tasks.append(mock_task.options(placement_group=pg).remote())", "html_url": "https://github.com/ray-project/ray/commit/8c152bd17ce78589f16a6fb26bfdb5fcf54df026", "file_name": "src/ray/raylet/scheduling/cluster_resource_scheduler.cc"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>   for (auto entry : local_resources.custom_resources) {\n <mask>     std::string resource_name = string_to_int_map_.Get(entry.first);\n <mask>     double resource_total = entry.second.total.Double();\n <mask>     if (resource_map_filter.count(resource_name) == 0u) {\n <mask>       continue;\n <mask>     }\n <mask> \n <mask>     if (resource_total > 0) {\n <mask>       auto data = std::make_shared<rpc::ResourceTableData>();\n <mask>       data->set_resource_capacity(resource_total);\n <mask>       map.emplace(resource_name, std::move(data));\n <mask>     }\n </s> Revert \"[Placement Group] Fix the high load bug from the placement group (#19277)\" (#19327)\n\nThis reverts commit 4360b998031d2f71109ddf0bc66a056a7190ce36. </s> remove     if (resource_map_filter.count(resource_name) == 0u) {\n      continue;\n    }\n\n </s> add  </s> remove   const auto &resources = bundle_spec.GetFormattedResources();\n  for (const auto &resource : resources) {\n </s> add   for (const auto &resource : bundle_spec.GetFormattedResources()) { </s> remove ClusterResourceScheduler::GetResourceTotals(\n    const std::unordered_map<std::string, double> &resource_map_filter) const {\n </s> add ClusterResourceScheduler::GetResourceTotals() const { </s> remove   update_resources_(\n      cluster_resource_scheduler_->GetResourceTotals(/*resource_name_filter*/ resources));\n </s> add   update_resources_(cluster_resource_scheduler_->GetResourceTotals()); </s> remove     num_nodes = len(nodes)\n </s> add  </s> remove   /// \\param resource_map_filter When returning the resource map, the returned result will\n  /// only contain the keys in the filter. Note that only the key of the map is used.\n  /// \\return The total resource capacity of the node.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals(\n      const std::unordered_map<std::string, double> &resource_map_filter) const = 0;\n </s> add   /// \\param Output parameter. Fills out all fields.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals() const = 0;", "html_url": "https://github.com/ray-project/ray/commit/8c152bd17ce78589f16a6fb26bfdb5fcf54df026", "file_name": "src/ray/raylet/scheduling/cluster_resource_scheduler.cc"}
{"docstring_tokens": "keep keep replace replace replace replace replace replace keep replace replace keep", "code_tokens": " <mask>   void FillResourceUsage(rpc::ResourcesData &resources_data) override;\n <mask> \n <mask>   /// Populate a UpdateResourcesRequest. This is inteneded to update the\n <mask>   /// resource totals on a node when a custom resource is created or deleted\n <mask>   /// (e.g. during the placement group lifecycle).\n <mask>   ///\n <mask>   /// \\param resource_map_filter When returning the resource map, the returned result will\n <mask>   /// only contain the keys in the filter. Note that only the key of the map is used.\n <mask>   /// \\return The total resource capacity of the node.\n <mask>   ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals(\n <mask>       const std::unordered_map<std::string, double> &resource_map_filter) const override;\n <mask> \n </s> Revert \"[Placement Group] Fix the high load bug from the placement group (#19277)\" (#19327)\n\nThis reverts commit 4360b998031d2f71109ddf0bc66a056a7190ce36. </s> remove   /// \\param resource_map_filter When returning the resource map, the returned result will\n  /// only contain the keys in the filter. Note that only the key of the map is used.\n  /// \\return The total resource capacity of the node.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals(\n      const std::unordered_map<std::string, double> &resource_map_filter) const = 0;\n </s> add   /// \\param Output parameter. Fills out all fields.\n  virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals() const = 0; </s> remove             for i in range(num_nodes):\n                tasks.append(\n                    mock_task.options(\n                        placement_group=pg,\n                        placement_group_bundle_index=i).remote())\n </s> add             tasks.append(mock_task.options(placement_group=pg).remote()) </s> remove ClusterResourceScheduler::GetResourceTotals(\n    const std::unordered_map<std::string, double> &resource_map_filter) const {\n </s> add ClusterResourceScheduler::GetResourceTotals() const { </s> remove   update_resources_(\n      cluster_resource_scheduler_->GetResourceTotals(/*resource_name_filter*/ resources));\n </s> add   update_resources_(cluster_resource_scheduler_->GetResourceTotals()); </s> remove   const auto &resources = bundle_spec.GetFormattedResources();\n  for (const auto &resource : resources) {\n </s> add   for (const auto &resource : bundle_spec.GetFormattedResources()) {", "html_url": "https://github.com/ray-project/ray/commit/8c152bd17ce78589f16a6fb26bfdb5fcf54df026", "file_name": "src/ray/raylet/scheduling/cluster_resource_scheduler.h"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>   /// Populate a UpdateResourcesRequest. This is inteneded to update the\n <mask>   /// resource totals on a node when a custom resource is created or deleted\n <mask>   /// (e.g. during the placement group lifecycle).\n <mask>   ///\n <mask>   /// \\param resource_map_filter When returning the resource map, the returned result will\n <mask>   /// only contain the keys in the filter. Note that only the key of the map is used.\n <mask>   /// \\return The total resource capacity of the node.\n <mask>   virtual ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals(\n <mask>       const std::unordered_map<std::string, double> &resource_map_filter) const = 0;\n <mask> \n <mask>   /// Return local resources in human-readable string form.\n <mask>   virtual std::string GetLocalResourceViewString() const = 0;\n <mask> };\n <mask> }  // namespace ray\n </s> Revert \"[Placement Group] Fix the high load bug from the placement group (#19277)\" (#19327)\n\nThis reverts commit 4360b998031d2f71109ddf0bc66a056a7190ce36. </s> remove   /// Populate a UpdateResourcesRequest. This is inteneded to update the\n  /// resource totals on a node when a custom resource is created or deleted\n  /// (e.g. during the placement group lifecycle).\n  ///\n  /// \\param resource_map_filter When returning the resource map, the returned result will\n  /// only contain the keys in the filter. Note that only the key of the map is used.\n </s> add  </s> remove   ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals(\n      const std::unordered_map<std::string, double> &resource_map_filter) const override;\n </s> add   ray::gcs::NodeResourceInfoAccessor::ResourceMap GetResourceTotals() const override; </s> remove             for i in range(num_nodes):\n                tasks.append(\n                    mock_task.options(\n                        placement_group=pg,\n                        placement_group_bundle_index=i).remote())\n </s> add             tasks.append(mock_task.options(placement_group=pg).remote()) </s> remove ClusterResourceScheduler::GetResourceTotals(\n    const std::unordered_map<std::string, double> &resource_map_filter) const {\n </s> add ClusterResourceScheduler::GetResourceTotals() const { </s> remove   MOCK_METHOD(ray::gcs::NodeResourceInfoAccessor::ResourceMap, GetResourceTotals,\n              (const std::unordered_map<std::string, double> &resource_map_filter),\n </s> add   MOCK_METHOD(ray::gcs::NodeResourceInfoAccessor::ResourceMap, GetResourceTotals, (), </s> remove     if (resource_map_filter.count(resource_name) == 0u) {\n      continue;\n    }\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8c152bd17ce78589f16a6fb26bfdb5fcf54df026", "file_name": "src/ray/raylet/scheduling/cluster_resource_scheduler_interface.h"}
{"docstring_tokens": "keep replace keep replace keep keep", "code_tokens": " <mask>     - cleanup() { if [ \"${BUILDKITE_PULL_REQUEST}\" = \"false\" ]; then ./ci/build/upload_build_info.sh; fi }; trap cleanup EXIT\n <mask>     # Set up local kind cluster.\n <mask>     - ./ci/k8s/prep-k8s-environment.sh\n <mask>     # Build py37-cpu Ray image for the test.\n <mask>     - LINUX_WHEELS=1 ./ci/ci.sh build\n <mask>     - pip install -q docker\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> add     - echo \"--- Running the test.\" </s> remove         ray_cr_config_str = open(EXAMPLE_CLUSTER_PATH).read()\n </s> add         with open(EXAMPLE_CLUSTER_PATH) as example_cluster_file:\n            ray_cr_config_str = example_cluster_file.read() </s> remove         - Two Worker pods\n </s> add         - Three Worker pods </s> remove logger.info(f\"Using image {RAY_IMAGE} for autoscaler and Ray nodes.\")\n </s> add # Set to IfNotPresent in kind CI.\nPULL_POLICY = os.environ.get(\"PULL_POLICY\", \"Always\")\nlogger.info(f\"Using image `{RAY_IMAGE}` for autoscaler and Ray nodes.\")\nlogger.info(f\"Using pull policy `{PULL_POLICY}` for all images.\") </s> remove         # Set pull policies to IfNotPresent to ensure no issues using a local test\n        # image on kind.\n        ray_cr_config_str = ray_cr_config_str.replace(\"Always\", \"IfNotPresent\")\n </s> add         # CI should set pull policies to IfNotPresent to ensure no issues using a local\n        # test image on kind.\n        ray_cr_config_str = ray_cr_config_str.replace(\"Always\", PULL_POLICY)", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": ".buildkite/pipeline.yml"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>     # Tag the image built in the last step. We want to be sure to distinguish the image from the real Ray nightly.\n <mask>     - docker tag rayproject/ray:nightly-py37-cpu ray-ci:kuberay-test\n <mask>     # Load the image into the kind node.\n <mask>     - kind load docker-image ray-ci:kuberay-test\n <mask>     - bazel test --config=ci $(./ci/run/bazel_export_options)\n <mask>       --test_tag_filters=kuberay_operator\n <mask>       --test_env=RAY_IMAGE=docker.io/library/ray-ci:kuberay-test\n <mask>       --test_env=PULL_POLICY=IfNotPresent\n <mask>       --test_env=KUBECONFIG=/root/.kube/config\n <mask>       python/ray/tests/...\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove     # Build py37-cpu Ray image for the test.\n </s> add     - echo \"--- Building py37-cpu Ray image for the test.\" </s> remove logger.info(f\"Using image {RAY_IMAGE} for autoscaler and Ray nodes.\")\n </s> add # Set to IfNotPresent in kind CI.\nPULL_POLICY = os.environ.get(\"PULL_POLICY\", \"Always\")\nlogger.info(f\"Using image `{RAY_IMAGE}` for autoscaler and Ray nodes.\")\nlogger.info(f\"Using pull policy `{PULL_POLICY}` for all images.\") </s> remove     # Set up local kind cluster.\n </s> add     - echo \"--- Setting up local kind cluster.\" </s> add       --test_env=PULL_POLICY=IfNotPresent </s> add       # Moreover, \"CPU\" and \"GPU\" should NOT be included in the `resources` arg.\n      # (Use `num-cpus` and `num-gpus` rayStartParams instead.) </s> remove         # Set pull policies to IfNotPresent to ensure no issues using a local test\n        # image on kind.\n        ray_cr_config_str = ray_cr_config_str.replace(\"Always\", \"IfNotPresent\")\n </s> add         # CI should set pull policies to IfNotPresent to ensure no issues using a local\n        # test image on kind.\n        ray_cr_config_str = ray_cr_config_str.replace(\"Always\", PULL_POLICY)", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": ".buildkite/pipeline.yml"}
{"docstring_tokens": "keep add keep keep", "code_tokens": " <mask>       --test_tag_filters=kuberay_operator\n <mask>       --test_env=RAY_IMAGE=docker.io/library/ray-ci:kuberay-test\n <mask>       --test_env=KUBECONFIG=/root/.kube/config\n <mask>       python/ray/tests/...\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> add     - echo \"--- Running the test.\" </s> remove \n    subprocess.check_call(\n </s> add     kubectl_exec_command = ( </s> remove         ray_cr_config_str = open(EXAMPLE_CLUSTER_PATH).read()\n </s> add         with open(EXAMPLE_CLUSTER_PATH) as example_cluster_file:\n            ray_cr_config_str = example_cluster_file.read() </s> remove         wait_for_ray_health(head_pod, namespace=\"default\")\n </s> add         wait_for_ray_health(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        ) </s> remove         wait_for_pod_to_start(head_pod, namespace=\"default\")\n </s> add         wait_for_pod_to_start(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        ) </s> remove         head_pod = get_pod(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        )\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": ".buildkite/pipeline.yml"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>     Potential TODO: Read GPU info from the container spec, here and in the\n <mask>     Ray Operator.\n <mask>     \"\"\"\n <mask> \n <mask>     if \"num_gpus\" in ray_start_params:\n <mask>         return int(ray_start_params[\"num_gpus\"])\n <mask> \n <mask>     # Issue a warning if GPUs are present in the container spec but not in the\n <mask>     # ray start params.\n <mask>     # TODO: Consider reading GPU info from container spec.\n <mask>     for key in k8s_resource_limits:\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> add     Prints and return kubectl's output as a string. </s> remove         kubectl_exec(\n            command=[\"python\", \"-c\", scale_down_script],\n </s> add         # 2. Trigger GPU upscaling by requesting placement of a GPU actor.\n        logger.info(\"Scheduling an Actor with GPU demands.\")\n        kubectl_exec_python_script(\n            script_name=\"gpu_actor_placement.py\",\n            pod=head_pod,\n            container=\"ray-head\",\n            namespace=\"default\",\n        )\n        # 3. Confirm new pod number and presence of fake GPU worker.\n        logger.info(\"Confirming fake GPU worker up-scaling.\")\n        wait_for_pods(goal_num_pods=4, namespace=\"default\")\n        gpu_workers = [\n            pod_name\n            for pod_name in get_pod_names(namespace=\"default\")\n            if \"gpu\" in pod_name\n        ]\n        assert len(gpu_workers) == 1\n        # 4. Confirm that the GPU actor is up and that Ray believes\n        # the node the actor is on has a GPU.\n        logger.info(\"Confirming GPU actor placement.\")\n        out = kubectl_exec_python_script(\n            script_name=\"gpu_actor_validation.py\",\n            pod=head_pod,\n            container=\"ray-head\",\n            namespace=\"default\",\n        )\n        # Confirms the actor was placed on a GPU-annotated node.\n        # (See gpu_actor_validation.py for details.)\n        assert \"on-a-gpu-node\" in out\n\n        # Scale-down\n        logger.info(\"Removing resource demands.\")\n        kubectl_exec_python_script(\n            script_name=\"scale_down.py\", </s> remove \n    subprocess.check_call(\n </s> add     kubectl_exec_command = ( </s> remove     command: List[str], pod: str, namespace: str, container: Optional[str] = None\n) -> None:\n </s> add     command: List[str],\n    pod: str,\n    namespace: str,\n    container: Optional[str] = None,\n) -> str: </s> add       # Use `resources` to optionally specify custom resource annotations for the Ray node.\n      # The value of `resources` is a string-integer mapping.\n      # Currently, `resources` must be provided in the unfortunate format demonstrated below.\n      # Moreover, \"CPU\" and \"GPU\" should NOT be included in the `resources` arg.\n      # (Use `num-cpus` and `num-gpus` rayStartParams instead.)\n      resources: '\"{\\\"Custom2\\\": 5, \\\"Custom3\\\": 1}\"' </s> add       # Moreover, \"CPU\" and \"GPU\" should NOT be included in the `resources` arg.\n      # (Use `num-cpus` and `num-gpus` rayStartParams instead.)", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/autoscaler/_private/kuberay/autoscaling_config.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>       num-cpus: '1' # can be auto-completed from the limits\n <mask>       # Use `resources` to optionally specify custom resource annotations for the Ray node.\n <mask>       # The value of `resources` is a string-integer mapping.\n <mask>       # Currently, `resources` must be provided in the unfortunate format demonstrated below.\n <mask>       resources: '\"{\\\"Custom1\\\": 1, \\\"Custom2\\\": 5}\"'\n <mask>     #pod template\n <mask>     template:\n <mask>       metadata:\n <mask>         labels:\n <mask>           # custom labels. NOTE: do not define custom labels start with `raycluster.`, they may be used in controller.\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> add       # Use `resources` to optionally specify custom resource annotations for the Ray node.\n      # The value of `resources` is a string-integer mapping.\n      # Currently, `resources` must be provided in the unfortunate format demonstrated below.\n      # Moreover, \"CPU\" and \"GPU\" should NOT be included in the `resources` arg.\n      # (Use `num-cpus` and `num-gpus` rayStartParams instead.)\n      resources: '\"{\\\"Custom2\\\": 5, \\\"Custom3\\\": 1}\"' </s> remove logger.info(f\"Using image {RAY_IMAGE} for autoscaler and Ray nodes.\")\n </s> add # Set to IfNotPresent in kind CI.\nPULL_POLICY = os.environ.get(\"PULL_POLICY\", \"Always\")\nlogger.info(f\"Using image `{RAY_IMAGE}` for autoscaler and Ray nodes.\")\nlogger.info(f\"Using pull policy `{PULL_POLICY}` for all images.\") </s> add     - echo \"--- Running the test.\" </s> remove     if \"num_gpus\" in ray_start_params:\n        return int(ray_start_params[\"num_gpus\"])\n </s> add     if \"num-gpus\" in ray_start_params:\n        return int(ray_start_params[\"num-gpus\"]) </s> remove         # Cluster-creation\n </s> add         # Cluster creation </s> remove     # Build py37-cpu Ray image for the test.\n </s> add     - echo \"--- Building py37-cpu Ray image for the test.\"", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/autoscaler/kuberay/ray-cluster.complete.yaml"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>     # the following params are used to complete the ray start: ray start --block --node-ip-address= ...\n <mask>     rayStartParams:\n <mask>       node-ip-address: $MY_POD_IP\n <mask>       block: 'true'\n <mask>     #pod template\n <mask>     template:\n <mask>       metadata:\n <mask>         labels:\n <mask>           key: value\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> add       # Moreover, \"CPU\" and \"GPU\" should NOT be included in the `resources` arg.\n      # (Use `num-cpus` and `num-gpus` rayStartParams instead.) </s> remove     if \"num_gpus\" in ray_start_params:\n        return int(ray_start_params[\"num_gpus\"])\n </s> add     if \"num-gpus\" in ray_start_params:\n        return int(ray_start_params[\"num-gpus\"]) </s> remove             logger.info(f\"Failed ray health check for pod {ray_pod}.\")\n </s> add             logger.info(f\"Failed ray health check for pod {pod}.\") </s> remove logger.info(f\"Using image {RAY_IMAGE} for autoscaler and Ray nodes.\")\n </s> add # Set to IfNotPresent in kind CI.\nPULL_POLICY = os.environ.get(\"PULL_POLICY\", \"Always\")\nlogger.info(f\"Using image `{RAY_IMAGE}` for autoscaler and Ray nodes.\")\nlogger.info(f\"Using pull policy `{PULL_POLICY}` for all images.\") </s> remove                 [\"ray\", \"health-check\"], ray_pod, namespace, container=ray_container\n </s> add                 [\"ray\", \"health-check\"], pod, namespace, container=ray_container </s> remove     substring of its name. Raises an assertion error if there are no matches.\n </s> add     substring of its name. Returns None if there are no matches.", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/autoscaler/kuberay/ray-cluster.complete.yaml"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             \"small-group\": {\n <mask>                 \"max_workers\": 300,\n <mask>                 \"min_workers\": 1,\n <mask>                 \"node_config\": {},\n <mask>                 \"resources\": {\"CPU\": 1},\n <mask>             },\n <mask>         },\n <mask>         \"auth\": {},\n <mask>         \"cluster_synced_files\": [],\n <mask>         \"file_mounts\": {},\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove     # Set up local kind cluster.\n </s> add     - echo \"--- Setting up local kind cluster.\" </s> add       # Moreover, \"CPU\" and \"GPU\" should NOT be included in the `resources` arg.\n      # (Use `num-cpus` and `num-gpus` rayStartParams instead.) </s> remove     # Build py37-cpu Ray image for the test.\n </s> add     - echo \"--- Building py37-cpu Ray image for the test.\" </s> remove         # Set pull policies to IfNotPresent to ensure no issues using a local test\n        # image on kind.\n        ray_cr_config_str = ray_cr_config_str.replace(\"Always\", \"IfNotPresent\")\n </s> add         # CI should set pull policies to IfNotPresent to ensure no issues using a local\n        # test image on kind.\n        ray_cr_config_str = ray_cr_config_str.replace(\"Always\", PULL_POLICY) </s> remove         wait_for_ray_health(head_pod, namespace=\"default\")\n </s> add         wait_for_ray_health(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        ) </s> remove         wait_for_pod_to_start(head_pod, namespace=\"default\")\n </s> add         wait_for_pod_to_start(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        )", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_config.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> def _get_ray_cr_memory_and_gpu() -> dict:\n <mask>     \"\"\"CR with memory and gpu rayStartParams.\"\"\"\n <mask>     cr = _get_basic_ray_cr()\n <mask>     cr[\"spec\"][\"workerGroupSpecs\"][0][\"rayStartParams\"][\"memory\"] = \"300000000\"\n <mask>     cr[\"spec\"][\"workerGroupSpecs\"][0][\"rayStartParams\"][\"num_gpus\"] = \"1\"\n <mask>     return cr\n <mask> \n <mask> \n <mask> def _get_autoscaling_config_memory_and_gpu() -> dict:\n <mask>     \"\"\"Autoscaling config with memory and gpu annotations.\"\"\"\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove         \"\"\"Get Ray CR config yaml, with configurable replica fields for the single\n        workerGroup.\"\"\"\n        config = yaml.safe_load(open(self._get_ray_cr_config_file()).read())\n        config[\"spec\"][\"workerGroupSpecs\"][0][\"replicas\"] = replicas\n        config[\"spec\"][\"workerGroupSpecs\"][0][\"minReplicas\"] = min_replicas\n        config[\"spec\"][\"workerGroupSpecs\"][0][\"maxReplicas\"] = max_replicas\n </s> add         \"\"\"Get Ray CR config yaml.\n\n        Use configurable replica fields for a CPU workerGroup.\n\n        Also add a GPU-annotated group for testing GPU upscaling.\n        \"\"\"\n        with open(self._get_ray_cr_config_file()) as ray_config_file:\n            ray_config_str = ray_config_file.read()\n        config = yaml.safe_load(ray_config_str)\n        cpu_group = config[\"spec\"][\"workerGroupSpecs\"][0]\n        cpu_group[\"replicas\"] = replicas\n        cpu_group[\"minReplicas\"] = min_replicas\n        cpu_group[\"maxReplicas\"] = max_replicas\n\n        # Add a GPU-annotated group.\n        # (We're not using real GPUs, just adding a GPU annotation for the autoscaler\n        # and Ray scheduler.)\n        gpu_group = copy.deepcopy(cpu_group)\n        gpu_group[\"rayStartParams\"][\"num-gpus\"] = \"1\"\n        gpu_group[\"replicas\"] = 0\n        gpu_group[\"minReplicas\"] = 0\n        gpu_group[\"maxReplicas\"] = 1\n        gpu_group[\"groupName\"] = \"fake-gpu-group\"\n        config[\"spec\"][\"workerGroupSpecs\"].append(gpu_group) </s> remove def wait_for_pod_to_start(pod: str, namespace: str, tries=60, backoff_s=5) -> None:\n    \"\"\"Waits for the pod to enter running Running status.phase.\"\"\"\n </s> add def wait_for_pod_to_start(\n    pod_name_filter: str, namespace: str, tries=60, backoff_s=5\n) -> None:\n    \"\"\"Waits for a pod to have Running status.phase.\n\n    More precisely, waits until there is a pod with name containing `pod_name_filter`\n    and the pod has Running status.phase.\"\"\" </s> remove         \"\"\"Apply Ray CR config yaml, with configurable replica fields for the single\n </s> add         \"\"\"Apply Ray CR config yaml, with configurable replica fields for the cpu </s> remove     get_pod_output = (\n </s> add     return len(get_pod_names(namespace))\n\n\ndef get_pod_names(namespace: str) -> List[str]:\n    \"\"\"Get the list of pod names in the namespace.\"\"\"\n    get_pods_output = ( </s> remove         - Two Worker pods\n </s> add         - Three Worker pods </s> remove     command: List[str], pod: str, namespace: str, container: Optional[str] = None\n) -> None:\n </s> add     command: List[str],\n    pod: str,\n    namespace: str,\n    container: Optional[str] = None,\n) -> str:", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_config.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask> \n <mask> from ray.tests.kuberay.utils import (\n <mask>     get_pod,\n <mask>     get_raycluster,\n <mask>     kubectl_exec_python_script,\n <mask>     wait_for_pods,\n <mask>     wait_for_pod_to_start,\n <mask>     wait_for_ray_health,\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove     kubectl_exec,\n </s> add     kubectl_exec_python_script, </s> add from pathlib import Path </s> remove     if \"num_gpus\" in ray_start_params:\n        return int(ray_start_params[\"num_gpus\"])\n </s> add     if \"num-gpus\" in ray_start_params:\n        return int(ray_start_params[\"num-gpus\"]) </s> remove     get_pod_output = (\n </s> add     return len(get_pod_names(namespace))\n\n\ndef get_pod_names(namespace: str) -> List[str]:\n    \"\"\"Get the list of pod names in the namespace.\"\"\"\n    get_pods_output = ( </s> remove def get_pod(pod_name_filter: str, namespace: str) -> str:\n </s> add def get_pod(pod_name_filter: str, namespace: str) -> Optional[str]: </s> add     - echo \"--- Running the test.\"", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_e2e.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> from ray.tests.kuberay.utils import (\n <mask>     get_pod,\n <mask>     get_raycluster,\n <mask>     kubectl_exec,\n <mask>     wait_for_pods,\n <mask>     wait_for_pod_to_start,\n <mask>     wait_for_ray_health,\n <mask>     wait_for_crd,\n <mask> )\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> add     get_pod_names, </s> add from pathlib import Path </s> remove     get_pod_output = (\n </s> add     return len(get_pod_names(namespace))\n\n\ndef get_pod_names(namespace: str) -> List[str]:\n    \"\"\"Get the list of pod names in the namespace.\"\"\"\n    get_pods_output = ( </s> remove             [\"kubectl\", \"-n\", namespace, \"get\", \"pods\", \"--no-headers\"]\n </s> add             [\n                \"kubectl\",\n                \"-n\",\n                namespace,\n                \"get\",\n                \"pods\",\n                \"-o\",\n                \"custom-columns=POD:metadata.name\",\n                \"--no-headers\",\n            ] </s> remove         # Scale-down\n        logger.info(\"Removing resource request.\")\n        scale_down_script = (\n            \"import ray;\"\n            'ray.init(\"auto\");'\n            \"ray.autoscaler.sdk.request_resources(num_cpus=0)\"\n </s> add         # GPU upscaling.\n        # 1. Check we haven't spuriously already started a fake GPU node.\n        assert not any(\n            \"gpu\" in pod_name for pod_name in get_pod_names(namespace=\"default\") </s> remove     if \"num_gpus\" in ray_start_params:\n        return int(ray_start_params[\"num_gpus\"])\n </s> add     if \"num-gpus\" in ray_start_params:\n        return int(ray_start_params[\"num-gpus\"])", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_e2e.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> # This image will be used for both the Ray nodes and the autoscaler.\n <mask> # The CI should pass an image built from the test branch.\n <mask> RAY_IMAGE = os.environ.get(\"RAY_IMAGE\", \"rayproject/ray:413fe0\")\n <mask> logger.info(f\"Using image {RAY_IMAGE} for autoscaler and Ray nodes.\")\n <mask> # The default \"rayproject/ray:413fe0\" is the currently pinned autoscaler image\n <mask> # (to be replaced with rayproject/ray:1.12.0 upon 1.12.0 release).\n <mask> \n <mask> # Parent directory of Ray repository\n <mask> RAY_PARENT = str(pathlib.Path(__file__).resolve().parents[5])\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> add     - echo \"--- Running the test.\" </s> remove     # Build py37-cpu Ray image for the test.\n </s> add     - echo \"--- Building py37-cpu Ray image for the test.\" </s> remove         # Set pull policies to IfNotPresent to ensure no issues using a local test\n        # image on kind.\n        ray_cr_config_str = ray_cr_config_str.replace(\"Always\", \"IfNotPresent\")\n </s> add         # CI should set pull policies to IfNotPresent to ensure no issues using a local\n        # test image on kind.\n        ray_cr_config_str = ray_cr_config_str.replace(\"Always\", PULL_POLICY) </s> add       # Moreover, \"CPU\" and \"GPU\" should NOT be included in the `resources` arg.\n      # (Use `num-cpus` and `num-gpus` rayStartParams instead.) </s> add       # Use `resources` to optionally specify custom resource annotations for the Ray node.\n      # The value of `resources` is a string-integer mapping.\n      # Currently, `resources` must be provided in the unfortunate format demonstrated below.\n      # Moreover, \"CPU\" and \"GPU\" should NOT be included in the `resources` arg.\n      # (Use `num-cpus` and `num-gpus` rayStartParams instead.)\n      resources: '\"{\\\"Custom2\\\": 5, \\\"Custom3\\\": 1}\"' </s> remove         \"\"\"Get Ray CR config yaml, with configurable replica fields for the single\n        workerGroup.\"\"\"\n        config = yaml.safe_load(open(self._get_ray_cr_config_file()).read())\n        config[\"spec\"][\"workerGroupSpecs\"][0][\"replicas\"] = replicas\n        config[\"spec\"][\"workerGroupSpecs\"][0][\"minReplicas\"] = min_replicas\n        config[\"spec\"][\"workerGroupSpecs\"][0][\"maxReplicas\"] = max_replicas\n </s> add         \"\"\"Get Ray CR config yaml.\n\n        Use configurable replica fields for a CPU workerGroup.\n\n        Also add a GPU-annotated group for testing GPU upscaling.\n        \"\"\"\n        with open(self._get_ray_cr_config_file()) as ray_config_file:\n            ray_config_str = ray_config_file.read()\n        config = yaml.safe_load(ray_config_str)\n        cpu_group = config[\"spec\"][\"workerGroupSpecs\"][0]\n        cpu_group[\"replicas\"] = replicas\n        cpu_group[\"minReplicas\"] = min_replicas\n        cpu_group[\"maxReplicas\"] = max_replicas\n\n        # Add a GPU-annotated group.\n        # (We're not using real GPUs, just adding a GPU annotation for the autoscaler\n        # and Ray scheduler.)\n        gpu_group = copy.deepcopy(cpu_group)\n        gpu_group[\"rayStartParams\"][\"num-gpus\"] = \"1\"\n        gpu_group[\"replicas\"] = 0\n        gpu_group[\"minReplicas\"] = 0\n        gpu_group[\"maxReplicas\"] = 1\n        gpu_group[\"groupName\"] = \"fake-gpu-group\"\n        config[\"spec\"][\"workerGroupSpecs\"].append(gpu_group)", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_e2e.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         - Writes modified CR to temp file.\n <mask>         - Returns temp file's name.\n <mask>         \"\"\"\n <mask>         # Set Ray and autoscaler images.\n <mask>         ray_cr_config_str = open(EXAMPLE_CLUSTER_PATH).read()\n <mask>         ray_images = [\n <mask>             word for word in ray_cr_config_str.split() if \"rayproject/ray:\" in word\n <mask>         ]\n <mask>         for ray_image in ray_images:\n <mask>             ray_cr_config_str = ray_cr_config_str.replace(ray_image, RAY_IMAGE)\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove         # Set pull policies to IfNotPresent to ensure no issues using a local test\n        # image on kind.\n        ray_cr_config_str = ray_cr_config_str.replace(\"Always\", \"IfNotPresent\")\n </s> add         # CI should set pull policies to IfNotPresent to ensure no issues using a local\n        # test image on kind.\n        ray_cr_config_str = ray_cr_config_str.replace(\"Always\", PULL_POLICY) </s> remove     get_pods_output = (\n        subprocess.check_output(\n            [\n                \"kubectl\",\n                \"-n\",\n                namespace,\n                \"get\",\n                \"pods\",\n                \"-o\",\n                \"custom-columns=POD:metadata.name\",\n                \"--no-headers\",\n            ]\n        )\n        .decode()\n        .split()\n    )\n    matches = [item for item in get_pods_output if pod_name_filter in item]\n    assert matches, f\"No match for `{pod_name_filter}` in namespace `{namespace}`.\"\n </s> add     pod_names = get_pod_names(namespace)\n    matches = [pod_name for pod_name in pod_names if pod_name_filter in pod_name]\n    if not matches:\n        logger.warning(f\"No match for `{pod_name_filter}` in namespace `{namespace}`.\")\n        return None </s> remove     # Build py37-cpu Ray image for the test.\n </s> add     - echo \"--- Building py37-cpu Ray image for the test.\" </s> remove logger.info(f\"Using image {RAY_IMAGE} for autoscaler and Ray nodes.\")\n </s> add # Set to IfNotPresent in kind CI.\nPULL_POLICY = os.environ.get(\"PULL_POLICY\", \"Always\")\nlogger.info(f\"Using image `{RAY_IMAGE}` for autoscaler and Ray nodes.\")\nlogger.info(f\"Using pull policy `{PULL_POLICY}` for all images.\") </s> remove     substring of its name. Raises an assertion error if there are no matches.\n </s> add     substring of its name. Returns None if there are no matches. </s> remove     # Set up local kind cluster.\n </s> add     - echo \"--- Setting up local kind cluster.\"", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_e2e.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         ]\n <mask>         for ray_image in ray_images:\n <mask>             ray_cr_config_str = ray_cr_config_str.replace(ray_image, RAY_IMAGE)\n <mask> \n <mask>         # Set pull policies to IfNotPresent to ensure no issues using a local test\n <mask>         # image on kind.\n <mask>         ray_cr_config_str = ray_cr_config_str.replace(\"Always\", \"IfNotPresent\")\n <mask> \n <mask>         raycluster_cr_file = tempfile.NamedTemporaryFile(delete=False)\n <mask>         raycluster_cr_file.write(ray_cr_config_str.encode())\n <mask>         raycluster_cr_file.close()\n <mask>         return raycluster_cr_file.name\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove         ray_cr_config_str = open(EXAMPLE_CLUSTER_PATH).read()\n </s> add         with open(EXAMPLE_CLUSTER_PATH) as example_cluster_file:\n            ray_cr_config_str = example_cluster_file.read() </s> remove logger.info(f\"Using image {RAY_IMAGE} for autoscaler and Ray nodes.\")\n </s> add # Set to IfNotPresent in kind CI.\nPULL_POLICY = os.environ.get(\"PULL_POLICY\", \"Always\")\nlogger.info(f\"Using image `{RAY_IMAGE}` for autoscaler and Ray nodes.\")\nlogger.info(f\"Using pull policy `{PULL_POLICY}` for all images.\") </s> remove     # Build py37-cpu Ray image for the test.\n </s> add     - echo \"--- Building py37-cpu Ray image for the test.\" </s> remove     # Set up local kind cluster.\n </s> add     - echo \"--- Setting up local kind cluster.\" </s> remove     get_pods_output = (\n        subprocess.check_output(\n            [\n                \"kubectl\",\n                \"-n\",\n                namespace,\n                \"get\",\n                \"pods\",\n                \"-o\",\n                \"custom-columns=POD:metadata.name\",\n                \"--no-headers\",\n            ]\n        )\n        .decode()\n        .split()\n    )\n    matches = [item for item in get_pods_output if pod_name_filter in item]\n    assert matches, f\"No match for `{pod_name_filter}` in namespace `{namespace}`.\"\n </s> add     pod_names = get_pod_names(namespace)\n    matches = [pod_name for pod_name in pod_names if pod_name_filter in pod_name]\n    if not matches:\n        logger.warning(f\"No match for `{pod_name_filter}` in namespace `{namespace}`.\")\n        return None </s> remove         \"\"\"Get Ray CR config yaml, with configurable replica fields for the single\n        workerGroup.\"\"\"\n        config = yaml.safe_load(open(self._get_ray_cr_config_file()).read())\n        config[\"spec\"][\"workerGroupSpecs\"][0][\"replicas\"] = replicas\n        config[\"spec\"][\"workerGroupSpecs\"][0][\"minReplicas\"] = min_replicas\n        config[\"spec\"][\"workerGroupSpecs\"][0][\"maxReplicas\"] = max_replicas\n </s> add         \"\"\"Get Ray CR config yaml.\n\n        Use configurable replica fields for a CPU workerGroup.\n\n        Also add a GPU-annotated group for testing GPU upscaling.\n        \"\"\"\n        with open(self._get_ray_cr_config_file()) as ray_config_file:\n            ray_config_str = ray_config_file.read()\n        config = yaml.safe_load(ray_config_str)\n        cpu_group = config[\"spec\"][\"workerGroupSpecs\"][0]\n        cpu_group[\"replicas\"] = replicas\n        cpu_group[\"minReplicas\"] = min_replicas\n        cpu_group[\"maxReplicas\"] = max_replicas\n\n        # Add a GPU-annotated group.\n        # (We're not using real GPUs, just adding a GPU annotation for the autoscaler\n        # and Ray scheduler.)\n        gpu_group = copy.deepcopy(cpu_group)\n        gpu_group[\"rayStartParams\"][\"num-gpus\"] = \"1\"\n        gpu_group[\"replicas\"] = 0\n        gpu_group[\"minReplicas\"] = 0\n        gpu_group[\"maxReplicas\"] = 1\n        gpu_group[\"groupName\"] = \"fake-gpu-group\"\n        config[\"spec\"][\"workerGroupSpecs\"].append(gpu_group)", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_e2e.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     def _get_ray_cr_config(\n <mask>         self, min_replicas=0, max_replicas=300, replicas=0\n <mask>     ) -> Dict[str, Any]:\n <mask>         \"\"\"Get Ray CR config yaml, with configurable replica fields for the single\n <mask>         workerGroup.\"\"\"\n <mask>         config = yaml.safe_load(open(self._get_ray_cr_config_file()).read())\n <mask>         config[\"spec\"][\"workerGroupSpecs\"][0][\"replicas\"] = replicas\n <mask>         config[\"spec\"][\"workerGroupSpecs\"][0][\"minReplicas\"] = min_replicas\n <mask>         config[\"spec\"][\"workerGroupSpecs\"][0][\"maxReplicas\"] = max_replicas\n <mask> \n <mask>         return config\n <mask> \n <mask>     def _apply_ray_cr(\n <mask>         self,\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove         \"\"\"Apply Ray CR config yaml, with configurable replica fields for the single\n </s> add         \"\"\"Apply Ray CR config yaml, with configurable replica fields for the cpu </s> remove     cr[\"spec\"][\"workerGroupSpecs\"][0][\"rayStartParams\"][\"num_gpus\"] = \"1\"\n </s> add     cr[\"spec\"][\"workerGroupSpecs\"][0][\"rayStartParams\"][\"num-gpus\"] = \"1\" </s> remove     get_pod_output = (\n </s> add     return len(get_pod_names(namespace))\n\n\ndef get_pod_names(namespace: str) -> List[str]:\n    \"\"\"Get the list of pod names in the namespace.\"\"\"\n    get_pods_output = ( </s> remove \n    subprocess.check_call(\n </s> add     kubectl_exec_command = ( </s> remove     command: List[str], pod: str, namespace: str, container: Optional[str] = None\n) -> None:\n </s> add     command: List[str],\n    pod: str,\n    namespace: str,\n    container: Optional[str] = None,\n) -> str: </s> remove def wait_for_pod_to_start(pod: str, namespace: str, tries=60, backoff_s=5) -> None:\n    \"\"\"Waits for the pod to enter running Running status.phase.\"\"\"\n </s> add def wait_for_pod_to_start(\n    pod_name_filter: str, namespace: str, tries=60, backoff_s=5\n) -> None:\n    \"\"\"Waits for a pod to have Running status.phase.\n\n    More precisely, waits until there is a pod with name containing `pod_name_filter`\n    and the pod has Running status.phase.\"\"\"", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_e2e.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         max_replicas=300,\n <mask>         replicas=0,\n <mask>         validate_replicas: bool = False,\n <mask>     ) -> None:\n <mask>         \"\"\"Apply Ray CR config yaml, with configurable replica fields for the single\n <mask>         workerGroup.\n <mask> \n <mask>         If the CR does not yet exist, `replicas` can be set as desired.\n <mask>         If the CR does already exist, the recommended usage is this:\n <mask>             (1) Set `replicas` to what we currently expect it to be.\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove         \"\"\"Get Ray CR config yaml, with configurable replica fields for the single\n        workerGroup.\"\"\"\n        config = yaml.safe_load(open(self._get_ray_cr_config_file()).read())\n        config[\"spec\"][\"workerGroupSpecs\"][0][\"replicas\"] = replicas\n        config[\"spec\"][\"workerGroupSpecs\"][0][\"minReplicas\"] = min_replicas\n        config[\"spec\"][\"workerGroupSpecs\"][0][\"maxReplicas\"] = max_replicas\n </s> add         \"\"\"Get Ray CR config yaml.\n\n        Use configurable replica fields for a CPU workerGroup.\n\n        Also add a GPU-annotated group for testing GPU upscaling.\n        \"\"\"\n        with open(self._get_ray_cr_config_file()) as ray_config_file:\n            ray_config_str = ray_config_file.read()\n        config = yaml.safe_load(ray_config_str)\n        cpu_group = config[\"spec\"][\"workerGroupSpecs\"][0]\n        cpu_group[\"replicas\"] = replicas\n        cpu_group[\"minReplicas\"] = min_replicas\n        cpu_group[\"maxReplicas\"] = max_replicas\n\n        # Add a GPU-annotated group.\n        # (We're not using real GPUs, just adding a GPU annotation for the autoscaler\n        # and Ray scheduler.)\n        gpu_group = copy.deepcopy(cpu_group)\n        gpu_group[\"rayStartParams\"][\"num-gpus\"] = \"1\"\n        gpu_group[\"replicas\"] = 0\n        gpu_group[\"minReplicas\"] = 0\n        gpu_group[\"maxReplicas\"] = 1\n        gpu_group[\"groupName\"] = \"fake-gpu-group\"\n        config[\"spec\"][\"workerGroupSpecs\"].append(gpu_group) </s> remove         ray_cr_config_str = open(EXAMPLE_CLUSTER_PATH).read()\n </s> add         with open(EXAMPLE_CLUSTER_PATH) as example_cluster_file:\n            ray_cr_config_str = example_cluster_file.read() </s> remove         return len(get_pod_output.split(\"\\n\"))\n </s> add         return get_pods_output.split(\"\\n\")\n    pass </s> remove logger.info(f\"Using image {RAY_IMAGE} for autoscaler and Ray nodes.\")\n </s> add # Set to IfNotPresent in kind CI.\nPULL_POLICY = os.environ.get(\"PULL_POLICY\", \"Always\")\nlogger.info(f\"Using image `{RAY_IMAGE}` for autoscaler and Ray nodes.\")\nlogger.info(f\"Using pull policy `{PULL_POLICY}` for all images.\") </s> remove     if not get_pod_output:\n        return 0\n </s> add     if not get_pods_output:\n        return [] </s> remove     command: List[str], pod: str, namespace: str, container: Optional[str] = None\n) -> None:\n </s> add     command: List[str],\n    pod: str,\n    namespace: str,\n    container: Optional[str] = None,\n) -> str:", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_e2e.py"}
{"docstring_tokens": "keep keep replace keep replace keep keep", "code_tokens": " <mask>             - Autoscaler: .25 CPU, .5 Gi memory\n <mask>             - Ray node: .5 CPU, .5 Gi memeory\n <mask>         - Two Worker pods\n <mask>             - Ray node: .5 CPU, .5 Gi memory\n <mask>         Total: 1.75 CPU, 2 Gi memory.\n <mask> \n <mask>         Including operator and system pods, the total CPU requested is around 3.\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove     # Build py37-cpu Ray image for the test.\n </s> add     - echo \"--- Building py37-cpu Ray image for the test.\" </s> remove     # Set up local kind cluster.\n </s> add     - echo \"--- Setting up local kind cluster.\" </s> add     - echo \"--- Running the test.\" </s> remove         ray_cr_config_str = open(EXAMPLE_CLUSTER_PATH).read()\n </s> add         with open(EXAMPLE_CLUSTER_PATH) as example_cluster_file:\n            ray_cr_config_str = example_cluster_file.read() </s> remove     cr[\"spec\"][\"workerGroupSpecs\"][0][\"rayStartParams\"][\"num_gpus\"] = \"1\"\n </s> add     cr[\"spec\"][\"workerGroupSpecs\"][0][\"rayStartParams\"][\"num-gpus\"] = \"1\"", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_e2e.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         The cpu LIMIT of each Ray container is 1.\n <mask>         The `num-cpus` arg to Ray start is 1 for each Ray container; thus Ray accounts\n <mask>         1 CPU for each Ray node in the test.\n <mask>         \"\"\"\n <mask>         # Cluster-creation\n <mask>         logger.info(\"Creating a RayCluster with no worker pods.\")\n <mask>         self._apply_ray_cr(min_replicas=0, replicas=0)\n <mask> \n <mask>         logger.info(\"Confirming presence of head.\")\n <mask>         wait_for_pods(goal_num_pods=1, namespace=\"default\")\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove         Total: 1.75 CPU, 2 Gi memory.\n </s> add         Total: 2.25 CPU, 2.5 Gi memory. </s> remove         head_pod = get_pod(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        )\n </s> add  </s> remove         kubectl_exec(\n            command=[\"python\", \"-c\", scale_down_script],\n </s> add         # 2. Trigger GPU upscaling by requesting placement of a GPU actor.\n        logger.info(\"Scheduling an Actor with GPU demands.\")\n        kubectl_exec_python_script(\n            script_name=\"gpu_actor_placement.py\",\n            pod=head_pod,\n            container=\"ray-head\",\n            namespace=\"default\",\n        )\n        # 3. Confirm new pod number and presence of fake GPU worker.\n        logger.info(\"Confirming fake GPU worker up-scaling.\")\n        wait_for_pods(goal_num_pods=4, namespace=\"default\")\n        gpu_workers = [\n            pod_name\n            for pod_name in get_pod_names(namespace=\"default\")\n            if \"gpu\" in pod_name\n        ]\n        assert len(gpu_workers) == 1\n        # 4. Confirm that the GPU actor is up and that Ray believes\n        # the node the actor is on has a GPU.\n        logger.info(\"Confirming GPU actor placement.\")\n        out = kubectl_exec_python_script(\n            script_name=\"gpu_actor_validation.py\",\n            pod=head_pod,\n            container=\"ray-head\",\n            namespace=\"default\",\n        )\n        # Confirms the actor was placed on a GPU-annotated node.\n        # (See gpu_actor_validation.py for details.)\n        assert \"on-a-gpu-node\" in out\n\n        # Scale-down\n        logger.info(\"Removing resource demands.\")\n        kubectl_exec_python_script(\n            script_name=\"scale_down.py\", </s> add       # Moreover, \"CPU\" and \"GPU\" should NOT be included in the `resources` arg.\n      # (Use `num-cpus` and `num-gpus` rayStartParams instead.) </s> remove         wait_for_pod_to_start(head_pod, namespace=\"default\")\n </s> add         wait_for_pod_to_start(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        ) </s> remove         wait_for_ray_health(head_pod, namespace=\"default\")\n </s> add         wait_for_ray_health(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        )", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_e2e.py"}
{"docstring_tokens": "keep keep replace replace replace keep keep replace keep keep keep", "code_tokens": " <mask>         logger.info(\"Confirming presence of head.\")\n <mask>         wait_for_pods(goal_num_pods=1, namespace=\"default\")\n <mask>         head_pod = get_pod(\n <mask>             pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n <mask>         )\n <mask> \n <mask>         logger.info(\"Waiting for head pod to start Running.\")\n <mask>         wait_for_pod_to_start(head_pod, namespace=\"default\")\n <mask>         logger.info(\"Confirming Ray is up on the head pod.\")\n <mask>         wait_for_ray_health(head_pod, namespace=\"default\")\n <mask> \n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove         wait_for_ray_health(head_pod, namespace=\"default\")\n </s> add         wait_for_ray_health(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        ) </s> add         head_pod = get_pod(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        )\n        assert head_pod, \"Could not find the Ray head pod.\" </s> remove         # Cluster-creation\n </s> add         # Cluster creation </s> remove         kubectl_exec(\n            command=[\"python\", \"-c\", scale_down_script],\n </s> add         # 2. Trigger GPU upscaling by requesting placement of a GPU actor.\n        logger.info(\"Scheduling an Actor with GPU demands.\")\n        kubectl_exec_python_script(\n            script_name=\"gpu_actor_placement.py\",\n            pod=head_pod,\n            container=\"ray-head\",\n            namespace=\"default\",\n        )\n        # 3. Confirm new pod number and presence of fake GPU worker.\n        logger.info(\"Confirming fake GPU worker up-scaling.\")\n        wait_for_pods(goal_num_pods=4, namespace=\"default\")\n        gpu_workers = [\n            pod_name\n            for pod_name in get_pod_names(namespace=\"default\")\n            if \"gpu\" in pod_name\n        ]\n        assert len(gpu_workers) == 1\n        # 4. Confirm that the GPU actor is up and that Ray believes\n        # the node the actor is on has a GPU.\n        logger.info(\"Confirming GPU actor placement.\")\n        out = kubectl_exec_python_script(\n            script_name=\"gpu_actor_validation.py\",\n            pod=head_pod,\n            container=\"ray-head\",\n            namespace=\"default\",\n        )\n        # Confirms the actor was placed on a GPU-annotated node.\n        # (See gpu_actor_validation.py for details.)\n        assert \"on-a-gpu-node\" in out\n\n        # Scale-down\n        logger.info(\"Removing resource demands.\")\n        kubectl_exec_python_script(\n            script_name=\"scale_down.py\", </s> remove         scale_script = (\n            \"import ray;\"\n            'ray.init(\"auto\");'\n            \"ray.autoscaler.sdk.request_resources(num_cpus=2)\"\n        )\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_e2e.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         logger.info(\"Waiting for head pod to start Running.\")\n <mask>         wait_for_pod_to_start(head_pod, namespace=\"default\")\n <mask>         logger.info(\"Confirming Ray is up on the head pod.\")\n <mask>         wait_for_ray_health(head_pod, namespace=\"default\")\n <mask> \n <mask>         # Scale-up\n <mask>         logger.info(\"Scaling up to one worker via Ray resource request.\")\n <mask>         scale_script = (\n <mask>             \"import ray;\"\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove         wait_for_pod_to_start(head_pod, namespace=\"default\")\n </s> add         wait_for_pod_to_start(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        ) </s> remove         head_pod = get_pod(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        )\n </s> add  </s> remove         scale_script = (\n            \"import ray;\"\n            'ray.init(\"auto\");'\n            \"ray.autoscaler.sdk.request_resources(num_cpus=2)\"\n        )\n </s> add  </s> add         head_pod = get_pod(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        )\n        assert head_pod, \"Could not find the Ray head pod.\" </s> remove         # Scale-down\n        logger.info(\"Removing resource request.\")\n        scale_down_script = (\n            \"import ray;\"\n            'ray.init(\"auto\");'\n            \"ray.autoscaler.sdk.request_resources(num_cpus=0)\"\n </s> add         # GPU upscaling.\n        # 1. Check we haven't spuriously already started a fake GPU node.\n        assert not any(\n            \"gpu\" in pod_name for pod_name in get_pod_names(namespace=\"default\") </s> remove         kubectl_exec(\n            command=[\"python\", \"-c\", scale_down_script],\n </s> add         # 2. Trigger GPU upscaling by requesting placement of a GPU actor.\n        logger.info(\"Scheduling an Actor with GPU demands.\")\n        kubectl_exec_python_script(\n            script_name=\"gpu_actor_placement.py\",\n            pod=head_pod,\n            container=\"ray-head\",\n            namespace=\"default\",\n        )\n        # 3. Confirm new pod number and presence of fake GPU worker.\n        logger.info(\"Confirming fake GPU worker up-scaling.\")\n        wait_for_pods(goal_num_pods=4, namespace=\"default\")\n        gpu_workers = [\n            pod_name\n            for pod_name in get_pod_names(namespace=\"default\")\n            if \"gpu\" in pod_name\n        ]\n        assert len(gpu_workers) == 1\n        # 4. Confirm that the GPU actor is up and that Ray believes\n        # the node the actor is on has a GPU.\n        logger.info(\"Confirming GPU actor placement.\")\n        out = kubectl_exec_python_script(\n            script_name=\"gpu_actor_validation.py\",\n            pod=head_pod,\n            container=\"ray-head\",\n            namespace=\"default\",\n        )\n        # Confirms the actor was placed on a GPU-annotated node.\n        # (See gpu_actor_validation.py for details.)\n        assert \"on-a-gpu-node\" in out\n\n        # Scale-down\n        logger.info(\"Removing resource demands.\")\n        kubectl_exec_python_script(\n            script_name=\"scale_down.py\",", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_e2e.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>         wait_for_ray_health(\n <mask>             pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n <mask>         )\n <mask> \n <mask>         # Scale-up\n <mask>         logger.info(\"Scaling up to one worker via Ray resource request.\")\n <mask>         # The request for 2 cpus should give us a 1-cpu head (already present) and a\n <mask>         # 1-cpu worker (will await scale-up).\n <mask>         kubectl_exec_python_script(\n <mask>             script_name=\"scale_up.py\",\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove         scale_script = (\n            \"import ray;\"\n            'ray.init(\"auto\");'\n            \"ray.autoscaler.sdk.request_resources(num_cpus=2)\"\n        )\n </s> add  </s> remove         kubectl_exec(\n            command=[\"python\", \"-c\", scale_script],\n </s> add         kubectl_exec_python_script(\n            script_name=\"scale_up.py\", </s> remove         wait_for_pod_to_start(head_pod, namespace=\"default\")\n </s> add         wait_for_pod_to_start(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        ) </s> remove         wait_for_ray_health(head_pod, namespace=\"default\")\n </s> add         wait_for_ray_health(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        ) </s> remove         kubectl_exec(\n            command=[\"python\", \"-c\", scale_down_script],\n </s> add         # 2. Trigger GPU upscaling by requesting placement of a GPU actor.\n        logger.info(\"Scheduling an Actor with GPU demands.\")\n        kubectl_exec_python_script(\n            script_name=\"gpu_actor_placement.py\",\n            pod=head_pod,\n            container=\"ray-head\",\n            namespace=\"default\",\n        )\n        # 3. Confirm new pod number and presence of fake GPU worker.\n        logger.info(\"Confirming fake GPU worker up-scaling.\")\n        wait_for_pods(goal_num_pods=4, namespace=\"default\")\n        gpu_workers = [\n            pod_name\n            for pod_name in get_pod_names(namespace=\"default\")\n            if \"gpu\" in pod_name\n        ]\n        assert len(gpu_workers) == 1\n        # 4. Confirm that the GPU actor is up and that Ray believes\n        # the node the actor is on has a GPU.\n        logger.info(\"Confirming GPU actor placement.\")\n        out = kubectl_exec_python_script(\n            script_name=\"gpu_actor_validation.py\",\n            pod=head_pod,\n            container=\"ray-head\",\n            namespace=\"default\",\n        )\n        # Confirms the actor was placed on a GPU-annotated node.\n        # (See gpu_actor_validation.py for details.)\n        assert \"on-a-gpu-node\" in out\n\n        # Scale-down\n        logger.info(\"Removing resource demands.\")\n        kubectl_exec_python_script(\n            script_name=\"scale_down.py\", </s> remove         head_pod = get_pod(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        )\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_e2e.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace keep keep replace replace keep keep keep keep", "code_tokens": " <mask>         wait_for_ray_health(head_pod, namespace=\"default\")\n <mask> \n <mask>         # Scale-up\n <mask>         logger.info(\"Scaling up to one worker via Ray resource request.\")\n <mask>         scale_script = (\n <mask>             \"import ray;\"\n <mask>             'ray.init(\"auto\");'\n <mask>             \"ray.autoscaler.sdk.request_resources(num_cpus=2)\"\n <mask>         )\n <mask>         # The request for 2 cpus should give us a 1-cpu head (already present) and a\n <mask>         # 1-cpu worker (will await scale-up).\n <mask>         kubectl_exec(\n <mask>             command=[\"python\", \"-c\", scale_script],\n <mask>             pod=head_pod,\n <mask>             container=\"ray-head\",\n <mask>             namespace=\"default\",\n <mask>         )\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> add         head_pod = get_pod(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        )\n        assert head_pod, \"Could not find the Ray head pod.\" </s> remove         wait_for_ray_health(head_pod, namespace=\"default\")\n </s> add         wait_for_ray_health(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        ) </s> remove         wait_for_pod_to_start(head_pod, namespace=\"default\")\n </s> add         wait_for_pod_to_start(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        ) </s> remove         # Scale-down\n        logger.info(\"Removing resource request.\")\n        scale_down_script = (\n            \"import ray;\"\n            'ray.init(\"auto\");'\n            \"ray.autoscaler.sdk.request_resources(num_cpus=0)\"\n </s> add         # GPU upscaling.\n        # 1. Check we haven't spuriously already started a fake GPU node.\n        assert not any(\n            \"gpu\" in pod_name for pod_name in get_pod_names(namespace=\"default\") </s> remove         kubectl_exec(\n            command=[\"python\", \"-c\", scale_down_script],\n </s> add         # 2. Trigger GPU upscaling by requesting placement of a GPU actor.\n        logger.info(\"Scheduling an Actor with GPU demands.\")\n        kubectl_exec_python_script(\n            script_name=\"gpu_actor_placement.py\",\n            pod=head_pod,\n            container=\"ray-head\",\n            namespace=\"default\",\n        )\n        # 3. Confirm new pod number and presence of fake GPU worker.\n        logger.info(\"Confirming fake GPU worker up-scaling.\")\n        wait_for_pods(goal_num_pods=4, namespace=\"default\")\n        gpu_workers = [\n            pod_name\n            for pod_name in get_pod_names(namespace=\"default\")\n            if \"gpu\" in pod_name\n        ]\n        assert len(gpu_workers) == 1\n        # 4. Confirm that the GPU actor is up and that Ray believes\n        # the node the actor is on has a GPU.\n        logger.info(\"Confirming GPU actor placement.\")\n        out = kubectl_exec_python_script(\n            script_name=\"gpu_actor_validation.py\",\n            pod=head_pod,\n            container=\"ray-head\",\n            namespace=\"default\",\n        )\n        # Confirms the actor was placed on a GPU-annotated node.\n        # (See gpu_actor_validation.py for details.)\n        assert \"on-a-gpu-node\" in out\n\n        # Scale-down\n        logger.info(\"Removing resource demands.\")\n        kubectl_exec_python_script(\n            script_name=\"scale_down.py\",", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_e2e.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask>             pod=head_pod,\n <mask>             container=\"ray-head\",\n <mask>             namespace=\"default\",\n <mask>         )\n <mask>         logger.info(\"Confirming number of workers.\")\n <mask>         wait_for_pods(goal_num_pods=2, namespace=\"default\")\n <mask> \n <mask>         logger.info(\"Scaling up to two workers by editing minReplicas.\")\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove         kubectl_exec(\n            command=[\"python\", \"-c\", scale_down_script],\n </s> add         # 2. Trigger GPU upscaling by requesting placement of a GPU actor.\n        logger.info(\"Scheduling an Actor with GPU demands.\")\n        kubectl_exec_python_script(\n            script_name=\"gpu_actor_placement.py\",\n            pod=head_pod,\n            container=\"ray-head\",\n            namespace=\"default\",\n        )\n        # 3. Confirm new pod number and presence of fake GPU worker.\n        logger.info(\"Confirming fake GPU worker up-scaling.\")\n        wait_for_pods(goal_num_pods=4, namespace=\"default\")\n        gpu_workers = [\n            pod_name\n            for pod_name in get_pod_names(namespace=\"default\")\n            if \"gpu\" in pod_name\n        ]\n        assert len(gpu_workers) == 1\n        # 4. Confirm that the GPU actor is up and that Ray believes\n        # the node the actor is on has a GPU.\n        logger.info(\"Confirming GPU actor placement.\")\n        out = kubectl_exec_python_script(\n            script_name=\"gpu_actor_validation.py\",\n            pod=head_pod,\n            container=\"ray-head\",\n            namespace=\"default\",\n        )\n        # Confirms the actor was placed on a GPU-annotated node.\n        # (See gpu_actor_validation.py for details.)\n        assert \"on-a-gpu-node\" in out\n\n        # Scale-down\n        logger.info(\"Removing resource demands.\")\n        kubectl_exec_python_script(\n            script_name=\"scale_down.py\", </s> remove         kubectl_exec(\n            command=[\"python\", \"-c\", scale_script],\n </s> add         kubectl_exec_python_script(\n            script_name=\"scale_up.py\", </s> add         # TODO (Dmitri) Expose worker idleTimeout in KubeRay CRD, set it low,\n        # and validate autoscaler-initiated idle timeout, instead of modifying the CR. </s> remove         # Scale-down\n        logger.info(\"Removing resource request.\")\n        scale_down_script = (\n            \"import ray;\"\n            'ray.init(\"auto\");'\n            \"ray.autoscaler.sdk.request_resources(num_cpus=0)\"\n </s> add         # GPU upscaling.\n        # 1. Check we haven't spuriously already started a fake GPU node.\n        assert not any(\n            \"gpu\" in pod_name for pod_name in get_pod_names(namespace=\"default\") </s> remove         wait_for_pod_to_start(head_pod, namespace=\"default\")\n </s> add         wait_for_pod_to_start(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        ) </s> remove         wait_for_ray_health(head_pod, namespace=\"default\")\n </s> add         wait_for_ray_health(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        )", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_e2e.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace keep replace replace keep keep", "code_tokens": " <mask>         )\n <mask>         logger.info(\"Confirming number of workers.\")\n <mask>         wait_for_pods(goal_num_pods=3, namespace=\"default\")\n <mask> \n <mask>         # Scale-down\n <mask>         logger.info(\"Removing resource request.\")\n <mask>         scale_down_script = (\n <mask>             \"import ray;\"\n <mask>             'ray.init(\"auto\");'\n <mask>             \"ray.autoscaler.sdk.request_resources(num_cpus=0)\"\n <mask>         )\n <mask>         kubectl_exec(\n <mask>             command=[\"python\", \"-c\", scale_down_script],\n <mask>             pod=head_pod,\n <mask>             container=\"ray-head\",\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove         scale_script = (\n            \"import ray;\"\n            'ray.init(\"auto\");'\n            \"ray.autoscaler.sdk.request_resources(num_cpus=2)\"\n        )\n </s> add  </s> remove         kubectl_exec(\n            command=[\"python\", \"-c\", scale_script],\n </s> add         kubectl_exec_python_script(\n            script_name=\"scale_up.py\", </s> add         # TODO (Dmitri) Use Ray Client and/or Ray Job submission API to submit\n        # instead of `kubectl exec`. </s> remove         wait_for_ray_health(head_pod, namespace=\"default\")\n </s> add         wait_for_ray_health(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        ) </s> remove         wait_for_pod_to_start(head_pod, namespace=\"default\")\n </s> add         wait_for_pod_to_start(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        )", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_e2e.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask>         )\n <mask>         logger.info(\"Scaling down all workers by editing maxReplicas.\")\n <mask>         # (replicas=2 reflects the current number of workers)\n <mask>         self._apply_ray_cr(\n <mask>             min_replicas=0,\n <mask>             max_replicas=0,\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> add         # TODO (Dmitri) Use Ray Client and/or Ray Job submission API to submit\n        # instead of `kubectl exec`. </s> remove         kubectl_exec(\n            command=[\"python\", \"-c\", scale_down_script],\n </s> add         # 2. Trigger GPU upscaling by requesting placement of a GPU actor.\n        logger.info(\"Scheduling an Actor with GPU demands.\")\n        kubectl_exec_python_script(\n            script_name=\"gpu_actor_placement.py\",\n            pod=head_pod,\n            container=\"ray-head\",\n            namespace=\"default\",\n        )\n        # 3. Confirm new pod number and presence of fake GPU worker.\n        logger.info(\"Confirming fake GPU worker up-scaling.\")\n        wait_for_pods(goal_num_pods=4, namespace=\"default\")\n        gpu_workers = [\n            pod_name\n            for pod_name in get_pod_names(namespace=\"default\")\n            if \"gpu\" in pod_name\n        ]\n        assert len(gpu_workers) == 1\n        # 4. Confirm that the GPU actor is up and that Ray believes\n        # the node the actor is on has a GPU.\n        logger.info(\"Confirming GPU actor placement.\")\n        out = kubectl_exec_python_script(\n            script_name=\"gpu_actor_validation.py\",\n            pod=head_pod,\n            container=\"ray-head\",\n            namespace=\"default\",\n        )\n        # Confirms the actor was placed on a GPU-annotated node.\n        # (See gpu_actor_validation.py for details.)\n        assert \"on-a-gpu-node\" in out\n\n        # Scale-down\n        logger.info(\"Removing resource demands.\")\n        kubectl_exec_python_script(\n            script_name=\"scale_down.py\", </s> remove         kubectl_exec(\n            command=[\"python\", \"-c\", scale_script],\n </s> add         kubectl_exec_python_script(\n            script_name=\"scale_up.py\", </s> remove         # Scale-down\n        logger.info(\"Removing resource request.\")\n        scale_down_script = (\n            \"import ray;\"\n            'ray.init(\"auto\");'\n            \"ray.autoscaler.sdk.request_resources(num_cpus=0)\"\n </s> add         # GPU upscaling.\n        # 1. Check we haven't spuriously already started a fake GPU node.\n        assert not any(\n            \"gpu\" in pod_name for pod_name in get_pod_names(namespace=\"default\") </s> remove         wait_for_pod_to_start(head_pod, namespace=\"default\")\n </s> add         wait_for_pod_to_start(\n            pod_name_filter=\"raycluster-complete-head\", namespace=\"default\"\n        ) </s> remove logger.info(f\"Using image {RAY_IMAGE} for autoscaler and Ray nodes.\")\n </s> add # Set to IfNotPresent in kind CI.\nPULL_POLICY = os.environ.get(\"PULL_POLICY\", \"Always\")\nlogger.info(f\"Using image `{RAY_IMAGE}` for autoscaler and Ray nodes.\")\nlogger.info(f\"Using pull policy `{PULL_POLICY}` for all images.\")", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/test_autoscaling_e2e.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask> For consistency, all K8s interactions use kubectl through subprocess calls.\n <mask> \"\"\"\n <mask> import logging\n <mask> import subprocess\n <mask> import time\n <mask> from typing import Any, Dict, List, Optional\n <mask> import yaml\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> add     get_pod_names, </s> remove     kubectl_exec,\n </s> add     kubectl_exec_python_script, </s> remove logger.info(f\"Using image {RAY_IMAGE} for autoscaler and Ray nodes.\")\n </s> add # Set to IfNotPresent in kind CI.\nPULL_POLICY = os.environ.get(\"PULL_POLICY\", \"Always\")\nlogger.info(f\"Using image `{RAY_IMAGE}` for autoscaler and Ray nodes.\")\nlogger.info(f\"Using pull policy `{PULL_POLICY}` for all images.\") </s> remove     if \"num_gpus\" in ray_start_params:\n        return int(ray_start_params[\"num_gpus\"])\n </s> add     if \"num-gpus\" in ray_start_params:\n        return int(ray_start_params[\"num-gpus\"]) </s> add         # TODO (Dmitri) Expose worker idleTimeout in KubeRay CRD, set it low,\n        # and validate autoscaler-initiated idle timeout, instead of modifying the CR. </s> remove def get_pod(pod_name_filter: str, namespace: str) -> str:\n </s> add def get_pod(pod_name_filter: str, namespace: str) -> Optional[str]:", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/utils.py"}
{"docstring_tokens": "keep keep keep keep replace keep replace keep", "code_tokens": " <mask>             )\n <mask> \n <mask> \n <mask> def _get_num_pods(namespace: str) -> int:\n <mask>     get_pod_output = (\n <mask>         subprocess.check_output(\n <mask>             [\"kubectl\", \"-n\", namespace, \"get\", \"pods\", \"--no-headers\"]\n <mask>         )\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove     get_pods_output = (\n        subprocess.check_output(\n            [\n                \"kubectl\",\n                \"-n\",\n                namespace,\n                \"get\",\n                \"pods\",\n                \"-o\",\n                \"custom-columns=POD:metadata.name\",\n                \"--no-headers\",\n            ]\n        )\n        .decode()\n        .split()\n    )\n    matches = [item for item in get_pods_output if pod_name_filter in item]\n    assert matches, f\"No match for `{pod_name_filter}` in namespace `{namespace}`.\"\n </s> add     pod_names = get_pod_names(namespace)\n    matches = [pod_name for pod_name in pod_names if pod_name_filter in pod_name]\n    if not matches:\n        logger.warning(f\"No match for `{pod_name_filter}` in namespace `{namespace}`.\")\n        return None </s> remove \n    subprocess.check_call(\n </s> add     kubectl_exec_command = ( </s> add         pod = get_pod(pod_name_filter=pod_name_filter, namespace=namespace)\n        if not pod:\n            # We didn't get a matching pod.\n            continue </s> remove     substring of its name. Raises an assertion error if there are no matches.\n </s> add     substring of its name. Returns None if there are no matches. </s> remove def wait_for_pod_to_start(pod: str, namespace: str, tries=60, backoff_s=5) -> None:\n    \"\"\"Waits for the pod to enter running Running status.phase.\"\"\"\n </s> add def wait_for_pod_to_start(\n    pod_name_filter: str, namespace: str, tries=60, backoff_s=5\n) -> None:\n    \"\"\"Waits for a pod to have Running status.phase.\n\n    More precisely, waits until there is a pod with name containing `pod_name_filter`\n    and the pod has Running status.phase.\"\"\"", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/utils.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep replace keep keep keep", "code_tokens": " <mask>         .strip()\n <mask>     )\n <mask> \n <mask>     # If there aren't any pods, the output is any empty string.\n <mask>     if not get_pod_output:\n <mask>         return 0\n <mask>     else:\n <mask>         return len(get_pod_output.split(\"\\n\"))\n <mask> \n <mask> \n <mask> def wait_for_pod_to_start(pod: str, namespace: str, tries=60, backoff_s=5) -> None:\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove def wait_for_pod_to_start(pod: str, namespace: str, tries=60, backoff_s=5) -> None:\n    \"\"\"Waits for the pod to enter running Running status.phase.\"\"\"\n </s> add def wait_for_pod_to_start(\n    pod_name_filter: str, namespace: str, tries=60, backoff_s=5\n) -> None:\n    \"\"\"Waits for a pod to have Running status.phase.\n\n    More precisely, waits until there is a pod with name containing `pod_name_filter`\n    and the pod has Running status.phase.\"\"\" </s> remove     command: List[str], pod: str, namespace: str, container: Optional[str] = None\n) -> None:\n </s> add     command: List[str],\n    pod: str,\n    namespace: str,\n    container: Optional[str] = None,\n) -> str: </s> remove     ray_pod: str, namespace: str, tries=60, backoff_s=5, ray_container=\"ray-head\"\n </s> add     pod_name_filter: str,\n    namespace: str,\n    tries=60,\n    backoff_s=5,\n    ray_container=\"ray-head\", </s> remove     \"\"\"Waits for the Ray pod to pass `ray health-check`.\n </s> add     \"\"\"Waits until a Ray pod passes `ray health-check`.\n\n    More precisely, waits until a Ray pod whose name includes the string\n    `pod_name_filter` passes `ray health-check`. </s> add     Prints and return kubectl's output as a string.", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/utils.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>     else:\n <mask>         return len(get_pod_output.split(\"\\n\"))\n <mask> \n <mask> \n <mask> def wait_for_pod_to_start(pod: str, namespace: str, tries=60, backoff_s=5) -> None:\n <mask>     \"\"\"Waits for the pod to enter running Running status.phase.\"\"\"\n <mask>     for i in range(tries):\n <mask>         pod_status = (\n <mask>             subprocess.check_output(\n <mask>                 [\n <mask>                     \"kubectl\",\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove         return len(get_pod_output.split(\"\\n\"))\n </s> add         return get_pods_output.split(\"\\n\")\n    pass </s> remove     ray_pod: str, namespace: str, tries=60, backoff_s=5, ray_container=\"ray-head\"\n </s> add     pod_name_filter: str,\n    namespace: str,\n    tries=60,\n    backoff_s=5,\n    ray_container=\"ray-head\", </s> remove     if not get_pod_output:\n        return 0\n </s> add     if not get_pods_output:\n        return [] </s> add         pod = get_pod(pod_name_filter=pod_name_filter, namespace=namespace)\n        if not pod:\n            # We didn't get a matching pod.\n            continue </s> remove     \"\"\"Waits for the Ray pod to pass `ray health-check`.\n </s> add     \"\"\"Waits until a Ray pod passes `ray health-check`.\n\n    More precisely, waits until a Ray pod whose name includes the string\n    `pod_name_filter` passes `ray health-check`. </s> remove     get_pods_output = (\n        subprocess.check_output(\n            [\n                \"kubectl\",\n                \"-n\",\n                namespace,\n                \"get\",\n                \"pods\",\n                \"-o\",\n                \"custom-columns=POD:metadata.name\",\n                \"--no-headers\",\n            ]\n        )\n        .decode()\n        .split()\n    )\n    matches = [item for item in get_pods_output if pod_name_filter in item]\n    assert matches, f\"No match for `{pod_name_filter}` in namespace `{namespace}`.\"\n </s> add     pod_names = get_pod_names(namespace)\n    matches = [pod_name for pod_name in pod_names if pod_name_filter in pod_name]\n    if not matches:\n        logger.warning(f\"No match for `{pod_name_filter}` in namespace `{namespace}`.\")\n        return None", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/utils.py"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask>     and the pod has Running status.phase.\"\"\"\n <mask>     for i in range(tries):\n <mask>         pod_status = (\n <mask>             subprocess.check_output(\n <mask>                 [\n <mask>                     \"kubectl\",\n <mask>                     \"-n\",\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove def wait_for_pod_to_start(pod: str, namespace: str, tries=60, backoff_s=5) -> None:\n    \"\"\"Waits for the pod to enter running Running status.phase.\"\"\"\n </s> add def wait_for_pod_to_start(\n    pod_name_filter: str, namespace: str, tries=60, backoff_s=5\n) -> None:\n    \"\"\"Waits for a pod to have Running status.phase.\n\n    More precisely, waits until there is a pod with name containing `pod_name_filter`\n    and the pod has Running status.phase.\"\"\" </s> remove             [\"kubectl\", \"-n\", namespace, \"get\", \"pods\", \"--no-headers\"]\n </s> add             [\n                \"kubectl\",\n                \"-n\",\n                namespace,\n                \"get\",\n                \"pods\",\n                \"-o\",\n                \"custom-columns=POD:metadata.name\",\n                \"--no-headers\",\n            ] </s> remove     get_pods_output = (\n        subprocess.check_output(\n            [\n                \"kubectl\",\n                \"-n\",\n                namespace,\n                \"get\",\n                \"pods\",\n                \"-o\",\n                \"custom-columns=POD:metadata.name\",\n                \"--no-headers\",\n            ]\n        )\n        .decode()\n        .split()\n    )\n    matches = [item for item in get_pods_output if pod_name_filter in item]\n    assert matches, f\"No match for `{pod_name_filter}` in namespace `{namespace}`.\"\n </s> add     pod_names = get_pod_names(namespace)\n    matches = [pod_name for pod_name in pod_names if pod_name_filter in pod_name]\n    if not matches:\n        logger.warning(f\"No match for `{pod_name_filter}` in namespace `{namespace}`.\")\n        return None </s> remove         return len(get_pod_output.split(\"\\n\"))\n </s> add         return get_pods_output.split(\"\\n\")\n    pass </s> remove     substring of its name. Raises an assertion error if there are no matches.\n </s> add     substring of its name. Returns None if there are no matches. </s> remove     ray_pod: str, namespace: str, tries=60, backoff_s=5, ray_container=\"ray-head\"\n </s> add     pod_name_filter: str,\n    namespace: str,\n    tries=60,\n    backoff_s=5,\n    ray_container=\"ray-head\",", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/utils.py"}
{"docstring_tokens": "keep keep keep keep replace keep replace keep keep", "code_tokens": " <mask>             raise Exception(f\"Timed out waiting for pod {pod} to enter Running status.\")\n <mask> \n <mask> \n <mask> def wait_for_ray_health(\n <mask>     ray_pod: str, namespace: str, tries=60, backoff_s=5, ray_container=\"ray-head\"\n <mask> ) -> None:\n <mask>     \"\"\"Waits for the Ray pod to pass `ray health-check`.\n <mask>     (Ensures Ray has completely started in the pod.)\n <mask>     \"\"\"\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove def wait_for_pod_to_start(pod: str, namespace: str, tries=60, backoff_s=5) -> None:\n    \"\"\"Waits for the pod to enter running Running status.phase.\"\"\"\n </s> add def wait_for_pod_to_start(\n    pod_name_filter: str, namespace: str, tries=60, backoff_s=5\n) -> None:\n    \"\"\"Waits for a pod to have Running status.phase.\n\n    More precisely, waits until there is a pod with name containing `pod_name_filter`\n    and the pod has Running status.phase.\"\"\" </s> remove         return len(get_pod_output.split(\"\\n\"))\n </s> add         return get_pods_output.split(\"\\n\")\n    pass </s> add     Use case: Wait until there is a Ray head pod with Ray running on it. </s> remove def get_pod(pod_name_filter: str, namespace: str) -> str:\n </s> add def get_pod(pod_name_filter: str, namespace: str) -> Optional[str]: </s> remove     command: List[str], pod: str, namespace: str, container: Optional[str] = None\n) -> None:\n </s> add     command: List[str],\n    pod: str,\n    namespace: str,\n    container: Optional[str] = None,\n) -> str:", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/utils.py"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>     More precisely, waits until a Ray pod whose name includes the string\n <mask>     `pod_name_filter` passes `ray health-check`.\n <mask>     (Ensures Ray has completely started in the pod.)\n <mask>     \"\"\"\n <mask>     for i in range(tries):\n <mask>         try:\n <mask>             pod = get_pod(pod_name_filter=pod_name_filter, namespace=\"default\")\n <mask>             assert pod, f\"Couldn't find a pod matching {pod_name_filter}.\"\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove     \"\"\"Waits for the Ray pod to pass `ray health-check`.\n </s> add     \"\"\"Waits until a Ray pod passes `ray health-check`.\n\n    More precisely, waits until a Ray pod whose name includes the string\n    `pod_name_filter` passes `ray health-check`. </s> add             pod = get_pod(pod_name_filter=pod_name_filter, namespace=\"default\")\n            assert pod, f\"Couldn't find a pod matching {pod_name_filter}.\" </s> remove     ray_pod: str, namespace: str, tries=60, backoff_s=5, ray_container=\"ray-head\"\n </s> add     pod_name_filter: str,\n    namespace: str,\n    tries=60,\n    backoff_s=5,\n    ray_container=\"ray-head\", </s> remove def wait_for_pod_to_start(pod: str, namespace: str, tries=60, backoff_s=5) -> None:\n    \"\"\"Waits for the pod to enter running Running status.phase.\"\"\"\n </s> add def wait_for_pod_to_start(\n    pod_name_filter: str, namespace: str, tries=60, backoff_s=5\n) -> None:\n    \"\"\"Waits for a pod to have Running status.phase.\n\n    More precisely, waits until there is a pod with name containing `pod_name_filter`\n    and the pod has Running status.phase.\"\"\" </s> add         pod = get_pod(pod_name_filter=pod_name_filter, namespace=namespace)\n        if not pod:\n            # We didn't get a matching pod.\n            continue </s> remove                 [\"ray\", \"health-check\"], ray_pod, namespace, container=ray_container\n </s> add                 [\"ray\", \"health-check\"], pod, namespace, container=ray_container", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/utils.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>     Use case: Wait until there is a Ray head pod with Ray running on it.\n <mask>     \"\"\"\n <mask>     for i in range(tries):\n <mask>         try:\n <mask>             # `ray health-check` yields 0 exit status iff it succeeds\n <mask>             kubectl_exec(\n <mask>                 [\"ray\", \"health-check\"], pod, namespace, container=ray_container\n <mask>             )\n <mask>             logger.info(f\"ray health check passes for pod {pod}\")\n <mask>             return\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove                 [\"ray\", \"health-check\"], ray_pod, namespace, container=ray_container\n </s> add                 [\"ray\", \"health-check\"], pod, namespace, container=ray_container </s> remove             logger.info(f\"ray health check passes for pod {ray_pod}\")\n </s> add             logger.info(f\"ray health check passes for pod {pod}\") </s> add     Use case: Wait until there is a Ray head pod with Ray running on it. </s> remove     \"\"\"Waits for the Ray pod to pass `ray health-check`.\n </s> add     \"\"\"Waits until a Ray pod passes `ray health-check`.\n\n    More precisely, waits until a Ray pod whose name includes the string\n    `pod_name_filter` passes `ray health-check`. </s> remove             logger.info(f\"Failed ray health check for pod {ray_pod}.\")\n </s> add             logger.info(f\"Failed ray health check for pod {pod}.\") </s> remove         return len(get_pod_output.split(\"\\n\"))\n </s> add         return get_pods_output.split(\"\\n\")\n    pass", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/utils.py"}
{"docstring_tokens": "keep keep replace keep replace keep", "code_tokens": " <mask>             # `ray health-check` yields 0 exit status iff it succeeds\n <mask>             kubectl_exec(\n <mask>                 [\"ray\", \"health-check\"], ray_pod, namespace, container=ray_container\n <mask>             )\n <mask>             logger.info(f\"ray health check passes for pod {ray_pod}\")\n <mask>             return\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> add             pod = get_pod(pod_name_filter=pod_name_filter, namespace=\"default\")\n            assert pod, f\"Couldn't find a pod matching {pod_name_filter}.\" </s> remove     \"\"\"Waits for the Ray pod to pass `ray health-check`.\n </s> add     \"\"\"Waits until a Ray pod passes `ray health-check`.\n\n    More precisely, waits until a Ray pod whose name includes the string\n    `pod_name_filter` passes `ray health-check`. </s> remove             logger.info(f\"Failed ray health check for pod {ray_pod}.\")\n </s> add             logger.info(f\"Failed ray health check for pod {pod}.\") </s> remove         return len(get_pod_output.split(\"\\n\"))\n </s> add         return get_pods_output.split(\"\\n\")\n    pass </s> remove     get_pods_output = (\n        subprocess.check_output(\n            [\n                \"kubectl\",\n                \"-n\",\n                namespace,\n                \"get\",\n                \"pods\",\n                \"-o\",\n                \"custom-columns=POD:metadata.name\",\n                \"--no-headers\",\n            ]\n        )\n        .decode()\n        .split()\n    )\n    matches = [item for item in get_pods_output if pod_name_filter in item]\n    assert matches, f\"No match for `{pod_name_filter}` in namespace `{namespace}`.\"\n </s> add     pod_names = get_pod_names(namespace)\n    matches = [pod_name for pod_name in pod_names if pod_name_filter in pod_name]\n    if not matches:\n        logger.warning(f\"No match for `{pod_name_filter}` in namespace `{namespace}`.\")\n        return None", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/utils.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             )\n <mask>             logger.info(f\"ray health check passes for pod {ray_pod}\")\n <mask>             return\n <mask>         except subprocess.CalledProcessError as e:\n <mask>             logger.info(f\"Failed ray health check for pod {ray_pod}.\")\n <mask>             if i < tries - 1:\n <mask>                 logger.info(\"Trying again.\")\n <mask>                 time.sleep(backoff_s)\n <mask>             else:\n <mask>                 logger.info(\"Giving up.\")\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove             logger.info(f\"ray health check passes for pod {ray_pod}\")\n </s> add             logger.info(f\"ray health check passes for pod {pod}\") </s> remove                 [\"ray\", \"health-check\"], ray_pod, namespace, container=ray_container\n </s> add                 [\"ray\", \"health-check\"], pod, namespace, container=ray_container </s> add             pod = get_pod(pod_name_filter=pod_name_filter, namespace=\"default\")\n            assert pod, f\"Couldn't find a pod matching {pod_name_filter}.\" </s> remove         return len(get_pod_output.split(\"\\n\"))\n </s> add         return get_pods_output.split(\"\\n\")\n    pass </s> remove def get_pod(pod_name_filter: str, namespace: str) -> str:\n </s> add def get_pod(pod_name_filter: str, namespace: str) -> Optional[str]: </s> remove def wait_for_pod_to_start(pod: str, namespace: str, tries=60, backoff_s=5) -> None:\n    \"\"\"Waits for the pod to enter running Running status.phase.\"\"\"\n </s> add def wait_for_pod_to_start(\n    pod_name_filter: str, namespace: str, tries=60, backoff_s=5\n) -> None:\n    \"\"\"Waits for a pod to have Running status.phase.\n\n    More precisely, waits until there is a pod with name containing `pod_name_filter`\n    and the pod has Running status.phase.\"\"\"", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/utils.py"}
{"docstring_tokens": "keep keep keep replace keep keep keep replace keep keep keep keep", "code_tokens": " <mask>                 raise e from None\n <mask> \n <mask> \n <mask> def get_pod(pod_name_filter: str, namespace: str) -> str:\n <mask>     \"\"\"Gets pods in the `namespace`.\n <mask> \n <mask>     Returns the first pod that has `pod_name_filter` as a\n <mask>     substring of its name. Raises an assertion error if there are no matches.\n <mask>     \"\"\"\n <mask>     get_pods_output = (\n <mask>         subprocess.check_output(\n <mask>             [\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove     get_pods_output = (\n        subprocess.check_output(\n            [\n                \"kubectl\",\n                \"-n\",\n                namespace,\n                \"get\",\n                \"pods\",\n                \"-o\",\n                \"custom-columns=POD:metadata.name\",\n                \"--no-headers\",\n            ]\n        )\n        .decode()\n        .split()\n    )\n    matches = [item for item in get_pods_output if pod_name_filter in item]\n    assert matches, f\"No match for `{pod_name_filter}` in namespace `{namespace}`.\"\n </s> add     pod_names = get_pod_names(namespace)\n    matches = [pod_name for pod_name in pod_names if pod_name_filter in pod_name]\n    if not matches:\n        logger.warning(f\"No match for `{pod_name_filter}` in namespace `{namespace}`.\")\n        return None </s> remove     command: List[str], pod: str, namespace: str, container: Optional[str] = None\n) -> None:\n </s> add     command: List[str],\n    pod: str,\n    namespace: str,\n    container: Optional[str] = None,\n) -> str: </s> remove def wait_for_pod_to_start(pod: str, namespace: str, tries=60, backoff_s=5) -> None:\n    \"\"\"Waits for the pod to enter running Running status.phase.\"\"\"\n </s> add def wait_for_pod_to_start(\n    pod_name_filter: str, namespace: str, tries=60, backoff_s=5\n) -> None:\n    \"\"\"Waits for a pod to have Running status.phase.\n\n    More precisely, waits until there is a pod with name containing `pod_name_filter`\n    and the pod has Running status.phase.\"\"\" </s> remove     get_pod_output = (\n </s> add     return len(get_pod_names(namespace))\n\n\ndef get_pod_names(namespace: str) -> List[str]:\n    \"\"\"Get the list of pod names in the namespace.\"\"\"\n    get_pods_output = ( </s> remove \n    subprocess.check_call(\n </s> add     kubectl_exec_command = (", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/utils.py"}
{"docstring_tokens": "keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep replace replace keep keep keep keep", "code_tokens": " <mask>     \"\"\"\n <mask>     get_pods_output = (\n <mask>         subprocess.check_output(\n <mask>             [\n <mask>                 \"kubectl\",\n <mask>                 \"-n\",\n <mask>                 namespace,\n <mask>                 \"get\",\n <mask>                 \"pods\",\n <mask>                 \"-o\",\n <mask>                 \"custom-columns=POD:metadata.name\",\n <mask>                 \"--no-headers\",\n <mask>             ]\n <mask>         )\n <mask>         .decode()\n <mask>         .split()\n <mask>     )\n <mask>     matches = [item for item in get_pods_output if pod_name_filter in item]\n <mask>     assert matches, f\"No match for `{pod_name_filter}` in namespace `{namespace}`.\"\n <mask>     return matches[0]\n <mask> \n <mask> \n <mask> def kubectl_exec(\n <mask>     command: List[str], pod: str, namespace: str, container: Optional[str] = None\n <mask> ) -> None:\n <mask>     \"\"\"kubectl exec the `command` in the given `pod` in the given `namespace`.\n <mask>     If a `container` is specified, will specify that container for kubectl.\n <mask>     \"\"\"\n <mask>     container_option = [\"-c\", container] if container else []\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove \n    subprocess.check_call(\n </s> add     kubectl_exec_command = ( </s> add     Prints and return kubectl's output as a string. </s> remove             [\"kubectl\", \"-n\", namespace, \"get\", \"pods\", \"--no-headers\"]\n </s> add             [\n                \"kubectl\",\n                \"-n\",\n                namespace,\n                \"get\",\n                \"pods\",\n                \"-o\",\n                \"custom-columns=POD:metadata.name\",\n                \"--no-headers\",\n            ] </s> remove     get_pod_output = (\n </s> add     return len(get_pod_names(namespace))\n\n\ndef get_pod_names(namespace: str) -> List[str]:\n    \"\"\"Get the list of pod names in the namespace.\"\"\"\n    get_pods_output = ( </s> remove     substring of its name. Raises an assertion error if there are no matches.\n </s> add     substring of its name. Returns None if there are no matches.", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/utils.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask> ) -> str:\n <mask>     \"\"\"kubectl exec the `command` in the given `pod` in the given `namespace`.\n <mask>     If a `container` is specified, will specify that container for kubectl.\n <mask>     \"\"\"\n <mask>     container_option = [\"-c\", container] if container else []\n <mask>     kubectl_exec_command = (\n <mask>         [\"kubectl\", \"exec\", \"-it\", pod] + container_option + [\"--\"] + command\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> remove \n    subprocess.check_call(\n </s> add     kubectl_exec_command = ( </s> remove     command: List[str], pod: str, namespace: str, container: Optional[str] = None\n) -> None:\n </s> add     command: List[str],\n    pod: str,\n    namespace: str,\n    container: Optional[str] = None,\n) -> str: </s> remove     substring of its name. Raises an assertion error if there are no matches.\n </s> add     substring of its name. Returns None if there are no matches. </s> remove     if \"num_gpus\" in ray_start_params:\n        return int(ray_start_params[\"num_gpus\"])\n </s> add     if \"num-gpus\" in ray_start_params:\n        return int(ray_start_params[\"num-gpus\"]) </s> remove def get_pod(pod_name_filter: str, namespace: str) -> str:\n </s> add def get_pod(pod_name_filter: str, namespace: str) -> Optional[str]: </s> remove     get_pod_output = (\n </s> add     return len(get_pod_names(namespace))\n\n\ndef get_pod_names(namespace: str) -> List[str]:\n    \"\"\"Get the list of pod names in the namespace.\"\"\"\n    get_pods_output = (", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/utils.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>     \"\"\"kubectl exec the `command` in the given `pod` in the given `namespace`.\n <mask>     If a `container` is specified, will specify that container for kubectl.\n <mask>     \"\"\"\n <mask>     container_option = [\"-c\", container] if container else []\n <mask> \n <mask>     subprocess.check_call(\n <mask>         [\"kubectl\", \"exec\", \"-it\", pod] + container_option + [\"--\"] + command\n <mask>     )\n <mask> \n <mask> \n <mask> def get_raycluster(raycluster: str, namespace: str) -> Dict[str, Any]:\n </s> [KubeRay] Fix autoscaling with GPUs and custom resources, with e2e tests (#23883)\n\n- Closes #23874 by fixing a typo (\"num_gpus\" -> \"num-gpus\").\r\n- Adds end-to-end test logic confirming the fix.\r\n- Adds end-to-end test logic confirming autoscaling with custom resources works.\r\n- Slightly refines developer instructions.\r\n- Deflakes test logic a bit by allowing for the event that the head pod changes its identity as the Ray cluster starts up. </s> add     Prints and return kubectl's output as a string. </s> remove     command: List[str], pod: str, namespace: str, container: Optional[str] = None\n) -> None:\n </s> add     command: List[str],\n    pod: str,\n    namespace: str,\n    container: Optional[str] = None,\n) -> str: </s> remove def get_pod(pod_name_filter: str, namespace: str) -> str:\n </s> add def get_pod(pod_name_filter: str, namespace: str) -> Optional[str]: </s> remove     substring of its name. Raises an assertion error if there are no matches.\n </s> add     substring of its name. Returns None if there are no matches. </s> remove     get_pod_output = (\n </s> add     return len(get_pod_names(namespace))\n\n\ndef get_pod_names(namespace: str) -> List[str]:\n    \"\"\"Get the list of pod names in the namespace.\"\"\"\n    get_pods_output = ( </s> remove     if not get_pod_output:\n        return 0\n </s> add     if not get_pods_output:\n        return []", "html_url": "https://github.com/ray-project/ray/commit/8c5fe44542668c76d94557be06493a551b5ea77e", "file_name": "python/ray/tests/kuberay/utils.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         self.args = args\n <mask>         self.kwargs = kwargs\n <mask> \n <mask>     def run(self):\n <mask>         os.environ[\"WANDB_START_METHOD\"] = \"fork\"\n <mask>         wandb.init(*self.args, **self.kwargs)\n <mask>         while True:\n <mask>             result = self.queue.get()\n <mask>             if result == _WANDB_QUEUE_END:\n <mask>                 break\n </s> [tune/wandb] Use `resume=False` per default (#21892)\n\nThe WandbLoggingCallback is run on the driver side, with the experiment directory was the cwd. Using resume=True will pick up state from other trials (as the file name is global), and thus lead to warning messages. Thus, we should default to resume=False when using the callback.\r\nThis PR also incorporates changes from #20966.\r\n\r\nCo-authored by: Queimo <queimo@gmx.net>\r\nCo-authored by: Karim <karim.ben.hicham@rwth-aachen.de> </s> remove         os.environ[\"WANDB_START_METHOD\"] = \"fork\"\n </s> add         # On windows, we can't fork\n        if os.name == \"nt\":\n            os.environ[\"WANDB_START_METHOD\"] = \"thread\"\n        else:\n            os.environ[\"WANDB_START_METHOD\"] = \"fork\"\n </s> remove             resume=True,\n </s> add             resume=False,", "html_url": "https://github.com/ray-project/ray/commit/8dcd4a99ef4d389bb5d3db63e5f39f0e07c978b5", "file_name": "python/ray/tune/integration/wandb.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         wandb_init_kwargs = dict(\n <mask>             id=trial_id,\n <mask>             name=trial_name,\n <mask>             resume=True,\n <mask>             reinit=True,\n <mask>             allow_val_change=True,\n <mask>             group=wandb_group,\n <mask>             project=wandb_project,\n <mask>             config=config)\n </s> [tune/wandb] Use `resume=False` per default (#21892)\n\nThe WandbLoggingCallback is run on the driver side, with the experiment directory was the cwd. Using resume=True will pick up state from other trials (as the file name is global), and thus lead to warning messages. Thus, we should default to resume=False when using the callback.\r\nThis PR also incorporates changes from #20966.\r\n\r\nCo-authored by: Queimo <queimo@gmx.net>\r\nCo-authored by: Karim <karim.ben.hicham@rwth-aachen.de> </s> remove         os.environ[\"WANDB_START_METHOD\"] = \"fork\"\n </s> add         # On windows, we can't fork\n        if os.name == \"nt\":\n            os.environ[\"WANDB_START_METHOD\"] = \"thread\"\n        else:\n            os.environ[\"WANDB_START_METHOD\"] = \"fork\"\n </s> remove         os.environ[\"WANDB_START_METHOD\"] = \"fork\"\n </s> add         # Since we're running in a separate process already, use threads.\n        os.environ[\"WANDB_START_METHOD\"] = \"thread\"", "html_url": "https://github.com/ray-project/ray/commit/8dcd4a99ef4d389bb5d3db63e5f39f0e07c978b5", "file_name": "python/ray/tune/integration/wandb.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             project=wandb_project,\n <mask>             config=_config)\n <mask>         wandb_init_kwargs.update(wandb_config)\n <mask> \n <mask>         os.environ[\"WANDB_START_METHOD\"] = \"fork\"\n <mask>         self.wandb = self._wandb.init(**wandb_init_kwargs)\n <mask> \n <mask>     def stop(self):\n <mask>         self._wandb.join()\n <mask>         if hasattr(super(), \"stop\"):\n </s> [tune/wandb] Use `resume=False` per default (#21892)\n\nThe WandbLoggingCallback is run on the driver side, with the experiment directory was the cwd. Using resume=True will pick up state from other trials (as the file name is global), and thus lead to warning messages. Thus, we should default to resume=False when using the callback.\r\nThis PR also incorporates changes from #20966.\r\n\r\nCo-authored by: Queimo <queimo@gmx.net>\r\nCo-authored by: Karim <karim.ben.hicham@rwth-aachen.de> </s> remove         os.environ[\"WANDB_START_METHOD\"] = \"fork\"\n </s> add         # Since we're running in a separate process already, use threads.\n        os.environ[\"WANDB_START_METHOD\"] = \"thread\" </s> remove             resume=True,\n </s> add             resume=False,", "html_url": "https://github.com/ray-project/ray/commit/8dcd4a99ef4d389bb5d3db63e5f39f0e07c978b5", "file_name": "python/ray/tune/integration/wandb.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         splits.append(rest)\n <mask> \n <mask>         return splits\n <mask> \n <mask>     def union(self, *other: List[\"Dataset[T]\"],\n <mask>               preserve_order: bool = False) -> \"Dataset[T]\":\n <mask>         \"\"\"Combine this dataset with others of the same type.\n <mask> \n <mask>         Args:\n <mask>             other: List of datasets to combine with this one. The datasets\n <mask>                 must have the same schema as this dataset, otherwise the\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> add         The order of the blocks in the datasets is preserved, as is the\n        relative ordering between the datasets passed in the argument list.\n </s> remove             preserve_order: Whether to preserve the order of the data blocks.\n                This may trigger eager loading of data from disk.\n </s> add  </s> remove                 if preserve_order:\n                    # Force evaluation of blocks, which preserves order since\n                    # then we don't need to move evaluated blocks to the front\n                    # of LazyBlockList.\n                    list(bl)\n                for block, meta in zip(bl._blocks, bl._metadata):\n                    blocks.append(block)\n                    metadata.append(meta)\n                lim = len(bl._blocks)\n                for call, meta in zip(bl._calls[lim:], bl._metadata[lim:]):\n                    pending_blocks.append(call)\n                    pending_metadata.append(meta)\n </s> add                 calls.extend(bl._calls) </s> remove         blocks: List[ObjectRef[Block]] = []\n </s> add         calls: List[Callable[[], ObjectRef[Block]]] = [] </s> remove                 assert isinstance(bl, BlockList), bl\n                blocks.extend(list(bl._blocks))\n                metadata.extend(bl.get_metadata())\n\n        result = LazyBlockList([], [])\n        result._calls = ([None] * len(blocks)) + pending_blocks\n        result._blocks = blocks\n        result._metadata = metadata + pending_metadata\n\n        assert len(result._calls) == len(result._metadata), result\n        assert len(result._blocks) <= len(result._calls), result\n        return Dataset(result)\n </s> add                 calls.extend([None] * len(bl))\n            metadata.extend(bl._metadata)\n            blocks.extend(bl._blocks)\n\n        return Dataset(LazyBlockList(calls, metadata, blocks)) </s> remove         if i >= len(self._blocks):\n            start = len(self._blocks)\n </s> add         if not self._blocks[i]:", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/dataset.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask> \n <mask>     def union(self, *other: List[\"Dataset[T]\"]) -> \"Dataset[T]\":\n <mask>         \"\"\"Combine this dataset with others of the same type.\n <mask> \n <mask>         Args:\n <mask>             other: List of datasets to combine with this one. The datasets\n <mask>                 must have the same schema as this dataset, otherwise the\n <mask>                 behavior is undefined.\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove     def union(self, *other: List[\"Dataset[T]\"],\n              preserve_order: bool = False) -> \"Dataset[T]\":\n </s> add     def union(self, *other: List[\"Dataset[T]\"]) -> \"Dataset[T]\": </s> remove             preserve_order: Whether to preserve the order of the data blocks.\n                This may trigger eager loading of data from disk.\n </s> add  </s> remove         blocks: List[ObjectRef[Block]] = []\n </s> add         calls: List[Callable[[], ObjectRef[Block]]] = [] </s> remove                 if preserve_order:\n                    # Force evaluation of blocks, which preserves order since\n                    # then we don't need to move evaluated blocks to the front\n                    # of LazyBlockList.\n                    list(bl)\n                for block, meta in zip(bl._blocks, bl._metadata):\n                    blocks.append(block)\n                    metadata.append(meta)\n                lim = len(bl._blocks)\n                for call, meta in zip(bl._calls[lim:], bl._metadata[lim:]):\n                    pending_blocks.append(call)\n                    pending_metadata.append(meta)\n </s> add                 calls.extend(bl._calls) </s> remove         if i >= len(self._blocks):\n            start = len(self._blocks)\n </s> add         if not self._blocks[i]: </s> remove         # TODO(ekl) isn't this not copying already computed blocks?\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/dataset.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>         Args:\n <mask>             other: List of datasets to combine with this one. The datasets\n <mask>                 must have the same schema as this dataset, otherwise the\n <mask>                 behavior is undefined.\n <mask>             preserve_order: Whether to preserve the order of the data blocks.\n <mask>                 This may trigger eager loading of data from disk.\n <mask> \n <mask>         Returns:\n <mask>             A new dataset holding the union of their data.\n <mask>         \"\"\"\n <mask> \n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> add         The order of the blocks in the datasets is preserved, as is the\n        relative ordering between the datasets passed in the argument list.\n </s> remove     def union(self, *other: List[\"Dataset[T]\"],\n              preserve_order: bool = False) -> \"Dataset[T]\":\n </s> add     def union(self, *other: List[\"Dataset[T]\"]) -> \"Dataset[T]\": </s> remove         blocks: List[ObjectRef[Block]] = []\n </s> add         calls: List[Callable[[], ObjectRef[Block]]] = [] </s> remove                 if preserve_order:\n                    # Force evaluation of blocks, which preserves order since\n                    # then we don't need to move evaluated blocks to the front\n                    # of LazyBlockList.\n                    list(bl)\n                for block, meta in zip(bl._blocks, bl._metadata):\n                    blocks.append(block)\n                    metadata.append(meta)\n                lim = len(bl._blocks)\n                for call, meta in zip(bl._calls[lim:], bl._metadata[lim:]):\n                    pending_blocks.append(call)\n                    pending_metadata.append(meta)\n </s> add                 calls.extend(bl._calls) </s> remove         if i >= len(self._blocks):\n            start = len(self._blocks)\n </s> add         if not self._blocks[i]: </s> remove             for c in self._calls[start:max(i + 1, start * 2)]:\n                self._blocks.append(c())\n </s> add             for j in range(max(i + 1, i * 2)):\n                if j >= len(self._blocks):\n                    break\n                if not self._blocks[j]:\n                    self._blocks[j] = self._calls[j]()\n            assert self._blocks[i], self._blocks", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep replace replace keep keep keep", "code_tokens": " <mask>         Returns:\n <mask>             A new dataset holding the union of their data.\n <mask>         \"\"\"\n <mask> \n <mask>         blocks: List[ObjectRef[Block]] = []\n <mask>         metadata: List[BlockMetadata] = []\n <mask>         pending_blocks: List[Callable[[], ObjectRef[Block]]] = []\n <mask>         pending_metadata: List[BlockMetadata] = []\n <mask> \n <mask>         datasets = [self] + list(other)\n <mask>         for ds in datasets:\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove             preserve_order: Whether to preserve the order of the data blocks.\n                This may trigger eager loading of data from disk.\n </s> add  </s> remove     def __init__(self, calls: Callable[[], ObjectRef[Block]],\n                 metadata: List[BlockMetadata]):\n        assert len(calls) == len(metadata), (calls, metadata)\n </s> add     def __init__(self,\n                 calls: Callable[[], ObjectRef[Block]],\n                 metadata: List[BlockMetadata],\n                 blocks: List[ObjectRef[Block]] = None): </s> remove                 if preserve_order:\n                    # Force evaluation of blocks, which preserves order since\n                    # then we don't need to move evaluated blocks to the front\n                    # of LazyBlockList.\n                    list(bl)\n                for block, meta in zip(bl._blocks, bl._metadata):\n                    blocks.append(block)\n                    metadata.append(meta)\n                lim = len(bl._blocks)\n                for call, meta in zip(bl._calls[lim:], bl._metadata[lim:]):\n                    pending_blocks.append(call)\n                    pending_metadata.append(meta)\n </s> add                 calls.extend(bl._calls) </s> remove         self._blocks = [calls[0]()] if calls else []\n </s> add  </s> add         blocks = np.array_split(self._blocks, num_splits)", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/dataset.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace keep replace replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep", "code_tokens": " <mask>         datasets = [self] + list(other)\n <mask>         for ds in datasets:\n <mask>             bl = ds._blocks\n <mask>             if isinstance(bl, LazyBlockList):\n <mask>                 if preserve_order:\n <mask>                     # Force evaluation of blocks, which preserves order since\n <mask>                     # then we don't need to move evaluated blocks to the front\n <mask>                     # of LazyBlockList.\n <mask>                     list(bl)\n <mask>                 for block, meta in zip(bl._blocks, bl._metadata):\n <mask>                     blocks.append(block)\n <mask>                     metadata.append(meta)\n <mask>                 lim = len(bl._blocks)\n <mask>                 for call, meta in zip(bl._calls[lim:], bl._metadata[lim:]):\n <mask>                     pending_blocks.append(call)\n <mask>                     pending_metadata.append(meta)\n <mask>             else:\n <mask>                 assert isinstance(bl, BlockList), bl\n <mask>                 blocks.extend(list(bl._blocks))\n <mask>                 metadata.extend(bl.get_metadata())\n <mask> \n <mask>         result = LazyBlockList([], [])\n <mask>         result._calls = ([None] * len(blocks)) + pending_blocks\n <mask>         result._blocks = blocks\n <mask>         result._metadata = metadata + pending_metadata\n <mask> \n <mask>         assert len(result._calls) == len(result._metadata), result\n <mask>         assert len(result._blocks) <= len(result._calls), result\n <mask>         return Dataset(result)\n <mask> \n <mask>     def sort(self,\n <mask>              key: Union[None, str, List[str], Callable[[T], Any]] = None,\n <mask>              descending: bool = False) -> \"Dataset[T]\":\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove         pending_blocks: List[Callable[[], ObjectRef[Block]]] = []\n        pending_metadata: List[BlockMetadata] = []\n </s> add         blocks: List[ObjectRef[Block]] = [] </s> remove             for c in self._calls[start:max(i + 1, start * 2)]:\n                self._blocks.append(c())\n </s> add             for j in range(max(i + 1, i * 2)):\n                if j >= len(self._blocks):\n                    break\n                if not self._blocks[j]:\n                    self._blocks[j] = self._calls[j]()\n            assert self._blocks[i], self._blocks </s> remove         if i >= len(self._blocks):\n            start = len(self._blocks)\n </s> add         if not self._blocks[i]: </s> add         if blocks:\n            self._blocks = blocks\n        else:\n            self._blocks = [None] * len(calls)\n            # Immediately compute the first block at least.\n            if calls:\n                self._blocks[0] = calls[0]()\n        assert len(calls) == len(metadata), (calls, metadata)\n        assert len(calls) == len(self._blocks), (calls, self._blocks) </s> remove     def union(self, *other: List[\"Dataset[T]\"],\n              preserve_order: bool = False) -> \"Dataset[T]\":\n </s> add     def union(self, *other: List[\"Dataset[T]\"]) -> \"Dataset[T]\":", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>                     if self._buffer is None:\n <mask>                         self._buffer = next(self._original_iter)\n <mask>                     while self._buffer.num_blocks() < blocks_per_window:\n <mask>                         self._buffer = self._buffer.union(\n <mask>                             next(self._original_iter), preserve_order=True)\n <mask>                     # Slice off the left-most chunk and return it.\n <mask>                     res, self._buffer = self._buffer._divide(blocks_per_window)\n <mask>                     assert res.num_blocks() <= blocks_per_window, res\n <mask>                     return lambda: res\n <mask>                 except StopIteration:\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> add     def _num_computed(self):\n        i = 0\n        for b in self._blocks:\n            if b is not None:\n                i += 1\n        return i </s> remove                 assert isinstance(bl, BlockList), bl\n                blocks.extend(list(bl._blocks))\n                metadata.extend(bl.get_metadata())\n\n        result = LazyBlockList([], [])\n        result._calls = ([None] * len(blocks)) + pending_blocks\n        result._blocks = blocks\n        result._metadata = metadata + pending_metadata\n\n        assert len(result._calls) == len(result._metadata), result\n        assert len(result._blocks) <= len(result._calls), result\n        return Dataset(result)\n </s> add                 calls.extend([None] * len(bl))\n            metadata.extend(bl._metadata)\n            blocks.extend(bl._blocks)\n\n        return Dataset(LazyBlockList(calls, metadata, blocks)) </s> remove         if i >= len(self._blocks):\n            start = len(self._blocks)\n </s> add         if not self._blocks[i]: </s> add         if blocks:\n            self._blocks = blocks\n        else:\n            self._blocks = [None] * len(calls)\n            # Immediately compute the first block at least.\n            if calls:\n                self._blocks[0] = calls[0]()\n        assert len(calls) == len(metadata), (calls, metadata)\n        assert len(calls) == len(self._blocks), (calls, self._blocks) </s> remove         new_list = LazyBlockList.__new__(LazyBlockList)\n        new_list._calls = self._calls\n        new_list._blocks = self._blocks\n        new_list._metadata = self._metadata\n        return new_list\n </s> add         return LazyBlockList(self._calls.copy(), self._metadata.copy(),\n                             self._blocks.copy()) </s> remove             for c in self._calls[start:max(i + 1, start * 2)]:\n                self._blocks.append(c())\n </s> add             for j in range(max(i + 1, i * 2)):\n                if j >= len(self._blocks):\n                    break\n                if not self._blocks[j]:\n                    self._blocks[j] = self._calls[j]()\n            assert self._blocks[i], self._blocks", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/dataset_pipeline.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep replace keep", "code_tokens": " <mask> from ray.data.impl.block_list import BlockList\n <mask> \n <mask> \n <mask> class LazyBlockList(BlockList[T]):\n <mask>     def __init__(self, calls: Callable[[], ObjectRef[Block]],\n <mask>                  metadata: List[BlockMetadata]):\n <mask>         assert len(calls) == len(metadata), (calls, metadata)\n <mask>         self._calls = calls\n <mask>         self._blocks = [calls[0]()] if calls else []\n <mask>         self._metadata = metadata\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> add         if blocks:\n            self._blocks = blocks\n        else:\n            self._blocks = [None] * len(calls)\n            # Immediately compute the first block at least.\n            if calls:\n                self._blocks[0] = calls[0]()\n        assert len(calls) == len(metadata), (calls, metadata)\n        assert len(calls) == len(self._blocks), (calls, self._blocks) </s> remove         new_list = LazyBlockList.__new__(LazyBlockList)\n        new_list._calls = self._calls\n        new_list._blocks = self._blocks\n        new_list._metadata = self._metadata\n        return new_list\n </s> add         return LazyBlockList(self._calls.copy(), self._metadata.copy(),\n                             self._blocks.copy()) </s> remove         blocks: List[ObjectRef[Block]] = []\n </s> add         calls: List[Callable[[], ObjectRef[Block]]] = [] </s> remove         # TODO(ekl) isn't this not copying already computed blocks?\n </s> add  </s> remove         pending_blocks: List[Callable[[], ObjectRef[Block]]] = []\n        pending_metadata: List[BlockMetadata] = []\n </s> add         blocks: List[ObjectRef[Block]] = []", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/impl/lazy_block_list.py"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>                  blocks: List[ObjectRef[Block]] = None):\n <mask>         self._calls = calls\n <mask>         self._metadata = metadata\n <mask> \n <mask>     def copy(self) -> \"LazyBlockList\":\n <mask>         return LazyBlockList(self._calls.copy(), self._metadata.copy(),\n <mask>                              self._blocks.copy())\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove         new_list = LazyBlockList.__new__(LazyBlockList)\n        new_list._calls = self._calls\n        new_list._blocks = self._blocks\n        new_list._metadata = self._metadata\n        return new_list\n </s> add         return LazyBlockList(self._calls.copy(), self._metadata.copy(),\n                             self._blocks.copy()) </s> remove     def __init__(self, calls: Callable[[], ObjectRef[Block]],\n                 metadata: List[BlockMetadata]):\n        assert len(calls) == len(metadata), (calls, metadata)\n </s> add     def __init__(self,\n                 calls: Callable[[], ObjectRef[Block]],\n                 metadata: List[BlockMetadata],\n                 blocks: List[ObjectRef[Block]] = None): </s> remove         self._blocks = [calls[0]()] if calls else []\n </s> add  </s> remove         pending_blocks: List[Callable[[], ObjectRef[Block]]] = []\n        pending_metadata: List[BlockMetadata] = []\n </s> add         blocks: List[ObjectRef[Block]] = [] </s> remove         blocks: List[ObjectRef[Block]] = []\n </s> add         calls: List[Callable[[], ObjectRef[Block]]] = [] </s> remove         # TODO(ekl) isn't this not copying already computed blocks?\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/impl/lazy_block_list.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         self._blocks = [calls[0]()] if calls else []\n <mask>         self._metadata = metadata\n <mask> \n <mask>     def copy(self) -> \"LazyBlockList\":\n <mask>         new_list = LazyBlockList.__new__(LazyBlockList)\n <mask>         new_list._calls = self._calls\n <mask>         new_list._blocks = self._blocks\n <mask>         new_list._metadata = self._metadata\n <mask>         return new_list\n <mask> \n <mask>     def clear(self):\n <mask>         super().clear()\n <mask>         self._calls = None\n <mask> \n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove         self._blocks = [calls[0]()] if calls else []\n </s> add  </s> remove     def __init__(self, calls: Callable[[], ObjectRef[Block]],\n                 metadata: List[BlockMetadata]):\n        assert len(calls) == len(metadata), (calls, metadata)\n </s> add     def __init__(self,\n                 calls: Callable[[], ObjectRef[Block]],\n                 metadata: List[BlockMetadata],\n                 blocks: List[ObjectRef[Block]] = None): </s> add         if blocks:\n            self._blocks = blocks\n        else:\n            self._blocks = [None] * len(calls)\n            # Immediately compute the first block at least.\n            if calls:\n                self._blocks[0] = calls[0]()\n        assert len(calls) == len(metadata), (calls, metadata)\n        assert len(calls) == len(self._blocks), (calls, self._blocks) </s> remove         # TODO(ekl) isn't this not copying already computed blocks?\n </s> add  </s> add     def _num_computed(self):\n        i = 0\n        for b in self._blocks:\n            if b is not None:\n                i += 1\n        return i </s> add         blocks = np.array_split(self._blocks, num_splits)", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/impl/lazy_block_list.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         super().clear()\n <mask>         self._calls = None\n <mask> \n <mask>     def split(self, split_size: int) -> List[\"LazyBlockList\"]:\n <mask>         # TODO(ekl) isn't this not copying already computed blocks?\n <mask>         self._check_if_cleared()\n <mask>         num_splits = math.ceil(len(self._calls) / split_size)\n <mask>         calls = np.array_split(self._calls, num_splits)\n <mask>         meta = np.array_split(self._metadata, num_splits)\n <mask>         output = []\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove         for c, m in zip(calls, meta):\n            output.append(LazyBlockList(c.tolist(), m.tolist()))\n </s> add         for c, m, b in zip(calls, meta, blocks):\n            output.append(LazyBlockList(c.tolist(), m.tolist(), b.tolist())) </s> add         blocks = np.array_split(self._blocks, num_splits) </s> remove         new_list = LazyBlockList.__new__(LazyBlockList)\n        new_list._calls = self._calls\n        new_list._blocks = self._blocks\n        new_list._metadata = self._metadata\n        return new_list\n </s> add         return LazyBlockList(self._calls.copy(), self._metadata.copy(),\n                             self._blocks.copy()) </s> remove         left = self.copy()\n        right = self.copy()\n        left._calls = left._calls[:block_idx]\n        left._blocks = left._blocks[:block_idx]\n        left._metadata = left._metadata[:block_idx]\n        right._calls = right._calls[block_idx:]\n        right._blocks = right._blocks[block_idx:]\n        right._metadata = right._metadata[block_idx:]\n </s> add         left = LazyBlockList(self._calls[:block_idx],\n                             self._metadata[:block_idx],\n                             self._blocks[:block_idx])\n        right = LazyBlockList(self._calls[block_idx:],\n                              self._metadata[block_idx:],\n                              self._blocks[block_idx:]) </s> remove         self._blocks = [calls[0]()] if calls else []\n </s> add  </s> remove     def __init__(self, calls: Callable[[], ObjectRef[Block]],\n                 metadata: List[BlockMetadata]):\n        assert len(calls) == len(metadata), (calls, metadata)\n </s> add     def __init__(self,\n                 calls: Callable[[], ObjectRef[Block]],\n                 metadata: List[BlockMetadata],\n                 blocks: List[ObjectRef[Block]] = None):", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/impl/lazy_block_list.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>         calls = np.array_split(self._calls, num_splits)\n <mask>         meta = np.array_split(self._metadata, num_splits)\n <mask>         output = []\n <mask>         for c, m, b in zip(calls, meta, blocks):\n <mask>             output.append(LazyBlockList(c.tolist(), m.tolist(), b.tolist()))\n <mask>         return output\n <mask> \n <mask>     def divide(self, block_idx: int) -> (\"BlockList\", \"BlockList\"):\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove         for c, m in zip(calls, meta):\n            output.append(LazyBlockList(c.tolist(), m.tolist()))\n </s> add         for c, m, b in zip(calls, meta, blocks):\n            output.append(LazyBlockList(c.tolist(), m.tolist(), b.tolist())) </s> remove         # TODO(ekl) isn't this not copying already computed blocks?\n </s> add  </s> remove         left = self.copy()\n        right = self.copy()\n        left._calls = left._calls[:block_idx]\n        left._blocks = left._blocks[:block_idx]\n        left._metadata = left._metadata[:block_idx]\n        right._calls = right._calls[block_idx:]\n        right._blocks = right._blocks[block_idx:]\n        right._metadata = right._metadata[block_idx:]\n </s> add         left = LazyBlockList(self._calls[:block_idx],\n                             self._metadata[:block_idx],\n                             self._blocks[:block_idx])\n        right = LazyBlockList(self._calls[block_idx:],\n                              self._metadata[block_idx:],\n                              self._blocks[block_idx:]) </s> remove         new_list = LazyBlockList.__new__(LazyBlockList)\n        new_list._calls = self._calls\n        new_list._blocks = self._blocks\n        new_list._metadata = self._metadata\n        return new_list\n </s> add         return LazyBlockList(self._calls.copy(), self._metadata.copy(),\n                             self._blocks.copy()) </s> add     def _num_computed(self):\n        i = 0\n        for b in self._blocks:\n            if b is not None:\n                i += 1\n        return i </s> remove         self._blocks = [calls[0]()] if calls else []\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/impl/lazy_block_list.py"}
{"docstring_tokens": "keep replace replace keep keep keep keep replace replace replace replace replace replace replace replace keep keep", "code_tokens": " <mask>         output = []\n <mask>         for c, m in zip(calls, meta):\n <mask>             output.append(LazyBlockList(c.tolist(), m.tolist()))\n <mask>         return output\n <mask> \n <mask>     def divide(self, block_idx: int) -> (\"BlockList\", \"BlockList\"):\n <mask>         self._check_if_cleared()\n <mask>         left = self.copy()\n <mask>         right = self.copy()\n <mask>         left._calls = left._calls[:block_idx]\n <mask>         left._blocks = left._blocks[:block_idx]\n <mask>         left._metadata = left._metadata[:block_idx]\n <mask>         right._calls = right._calls[block_idx:]\n <mask>         right._blocks = right._blocks[block_idx:]\n <mask>         right._metadata = right._metadata[block_idx:]\n <mask>         return left, right\n <mask> \n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> add         blocks = np.array_split(self._blocks, num_splits) </s> remove         # TODO(ekl) isn't this not copying already computed blocks?\n </s> add  </s> remove         new_list = LazyBlockList.__new__(LazyBlockList)\n        new_list._calls = self._calls\n        new_list._blocks = self._blocks\n        new_list._metadata = self._metadata\n        return new_list\n </s> add         return LazyBlockList(self._calls.copy(), self._metadata.copy(),\n                             self._blocks.copy()) </s> remove         pending_blocks: List[Callable[[], ObjectRef[Block]]] = []\n        pending_metadata: List[BlockMetadata] = []\n </s> add         blocks: List[ObjectRef[Block]] = [] </s> remove         blocks: List[ObjectRef[Block]] = []\n </s> add         calls: List[Callable[[], ObjectRef[Block]]] = []", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/impl/lazy_block_list.py"}
{"docstring_tokens": "keep replace replace keep replace replace", "code_tokens": " <mask>         # Check if we need to compute more blocks.\n <mask>         if i >= len(self._blocks):\n <mask>             start = len(self._blocks)\n <mask>             # Exponentially increase the number of blocks computed per batch.\n <mask>             for c in self._calls[start:max(i + 1, start * 2)]:\n <mask>                 self._blocks.append(c())\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove                 if preserve_order:\n                    # Force evaluation of blocks, which preserves order since\n                    # then we don't need to move evaluated blocks to the front\n                    # of LazyBlockList.\n                    list(bl)\n                for block, meta in zip(bl._blocks, bl._metadata):\n                    blocks.append(block)\n                    metadata.append(meta)\n                lim = len(bl._blocks)\n                for call, meta in zip(bl._calls[lim:], bl._metadata[lim:]):\n                    pending_blocks.append(call)\n                    pending_metadata.append(meta)\n </s> add                 calls.extend(bl._calls) </s> add         if blocks:\n            self._blocks = blocks\n        else:\n            self._blocks = [None] * len(calls)\n            # Immediately compute the first block at least.\n            if calls:\n                self._blocks[0] = calls[0]()\n        assert len(calls) == len(metadata), (calls, metadata)\n        assert len(calls) == len(self._blocks), (calls, self._blocks) </s> add     def _num_computed(self):\n        i = 0\n        for b in self._blocks:\n            if b is not None:\n                i += 1\n        return i </s> remove             preserve_order: Whether to preserve the order of the data blocks.\n                This may trigger eager loading of data from disk.\n </s> add  </s> add         The order of the blocks in the datasets is preserved, as is the\n        relative ordering between the datasets passed in the argument list.\n", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/impl/lazy_block_list.py"}
{"docstring_tokens": "keep keep keep add", "code_tokens": " <mask>                 if not self._blocks[j]:\n <mask>                     self._blocks[j] = self._calls[j]()\n <mask>             assert self._blocks[i], self._blocks\n <mask>         return self._blocks[i]\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove             for c in self._calls[start:max(i + 1, start * 2)]:\n                self._blocks.append(c())\n </s> add             for j in range(max(i + 1, i * 2)):\n                if j >= len(self._blocks):\n                    break\n                if not self._blocks[j]:\n                    self._blocks[j] = self._calls[j]()\n            assert self._blocks[i], self._blocks </s> remove         if i >= len(self._blocks):\n            start = len(self._blocks)\n </s> add         if not self._blocks[i]: </s> add         if blocks:\n            self._blocks = blocks\n        else:\n            self._blocks = [None] * len(calls)\n            # Immediately compute the first block at least.\n            if calls:\n                self._blocks[0] = calls[0]()\n        assert len(calls) == len(metadata), (calls, metadata)\n        assert len(calls) == len(self._blocks), (calls, self._blocks) </s> remove         new_list = LazyBlockList.__new__(LazyBlockList)\n        new_list._calls = self._calls\n        new_list._blocks = self._blocks\n        new_list._metadata = self._metadata\n        return new_list\n </s> add         return LazyBlockList(self._calls.copy(), self._metadata.copy(),\n                             self._blocks.copy()) </s> remove         self._blocks = [calls[0]()] if calls else []\n </s> add  </s> remove     def __init__(self, calls: Callable[[], ObjectRef[Block]],\n                 metadata: List[BlockMetadata]):\n        assert len(calls) == len(metadata), (calls, metadata)\n </s> add     def __init__(self,\n                 calls: Callable[[], ObjectRef[Block]],\n                 metadata: List[BlockMetadata],\n                 blocks: List[ObjectRef[Block]] = None):", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/impl/lazy_block_list.py"}
{"docstring_tokens": "keep replace keep replace keep keep", "code_tokens": " <mask>     ds = ray.data.range(100, parallelism=20)\n <mask>     assert len(ds._blocks._blocks) == 1\n <mask>     assert ds.take(10) == list(range(10))\n <mask>     assert len(ds._blocks._blocks) == 2\n <mask>     assert ds.take(20) == list(range(20))\n <mask>     assert len(ds._blocks._blocks) == 4\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove     assert len(ds._blocks._blocks) == 4\n </s> add     assert ds._blocks._num_computed() == 4 </s> remove     assert len(ds._blocks._blocks) == 8\n </s> add     assert ds._blocks._num_computed() == 8 </s> remove     assert len(ds._blocks._blocks) == 20\n </s> add     assert ds._blocks._num_computed() == 20 </s> remove     assert len(ds._blocks._blocks) == 16\n </s> add     assert ds._blocks._num_computed() == 16 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep replace keep keep keep", "code_tokens": " <mask>     assert len(ds._blocks._blocks) == 1\n <mask>     assert ds.take(10) == list(range(10))\n <mask>     assert len(ds._blocks._blocks) == 2\n <mask>     assert ds.take(20) == list(range(20))\n <mask>     assert len(ds._blocks._blocks) == 4\n <mask>     assert ds.take(30) == list(range(30))\n <mask>     assert len(ds._blocks._blocks) == 8\n <mask>     assert ds.take(50) == list(range(50))\n <mask>     assert len(ds._blocks._blocks) == 16\n <mask>     assert ds.take(100) == list(range(100))\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 16\n </s> add     assert ds._blocks._num_computed() == 16 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 20\n </s> add     assert ds._blocks._num_computed() == 20 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep replace", "code_tokens": " <mask>     assert len(ds._blocks._blocks) == 4\n <mask>     assert ds.take(30) == list(range(30))\n <mask>     assert len(ds._blocks._blocks) == 8\n <mask>     assert ds.take(50) == list(range(50))\n <mask>     assert len(ds._blocks._blocks) == 16\n <mask>     assert ds.take(100) == list(range(100))\n <mask>     assert len(ds._blocks._blocks) == 20\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove     assert len(ds._blocks._blocks) == 8\n </s> add     assert ds._blocks._num_computed() == 8 </s> remove     assert len(ds._blocks._blocks) == 4\n </s> add     assert ds._blocks._num_computed() == 4 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     ds = ray.data.read_parquet([path1, path2], filesystem=fs)\n <mask> \n <mask>     # Test metadata-only parquet ops.\n <mask>     assert len(ds._blocks._blocks) == 1\n <mask>     assert ds.count() == 6\n <mask> \n <mask>     out_path = os.path.join(tmp_path, \"out\")\n <mask>     os.mkdir(out_path)\n <mask> \n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove         assert len(ds._blocks._blocks) == expected\n </s> add         assert ds._blocks._num_computed() == expected </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     ds = ray.data.read_parquet(data_path, filesystem=fs)\n <mask> \n <mask>     # Test metadata-only parquet ops.\n <mask>     assert len(ds._blocks._blocks) == 1\n <mask>     assert ds.count() == 6\n <mask>     assert ds.size_bytes() > 0\n <mask>     assert ds.schema() is not None\n <mask>     input_files = ds.input_files()\n <mask>     assert len(input_files) == 2, input_files\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove         assert len(ds._blocks._blocks) == expected\n </s> add         assert ds._blocks._num_computed() == expected </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep replace keep keep keep replace keep keep", "code_tokens": " <mask>     assert repr(ds) == \\\n <mask>         \"Dataset(num_blocks=2, num_rows=6, \" \\\n <mask>         \"schema={one: int64, two: string})\", ds\n <mask>     assert len(ds._blocks._blocks) == 1\n <mask> \n <mask>     # Forces a data read.\n <mask>     values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\n <mask>     assert len(ds._blocks._blocks) == 2\n <mask>     assert sorted(values) == [[1, \"a\"], [2, \"b\"], [3, \"c\"], [4, \"e\"], [5, \"f\"],\n <mask>                               [6, \"g\"]]\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     ds = ray.data.read_parquet(data_path, filesystem=fs)\n <mask> \n <mask>     # Test metadata-only parquet ops.\n <mask>     assert len(ds._blocks._blocks) == 1\n <mask>     assert ds.count() == 6\n <mask>     assert ds.size_bytes() > 0\n <mask>     assert ds.schema() is not None\n <mask>     input_files = ds.input_files()\n <mask>     assert len(input_files) == 2, input_files\n </s> Refactor LazyBlockList to simplify union of lists (#19214)", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep replace keep keep keep replace keep keep keep keep", "code_tokens": " <mask>         \"schema={two: string, \" \\\n <mask>         \"one: dictionary<values=int32, indices=int32, ordered=0>})\", ds\n <mask>     assert len(ds._blocks._blocks) == 1\n <mask> \n <mask>     # Forces a data read.\n <mask>     values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\n <mask>     assert len(ds._blocks._blocks) == 2\n <mask>     assert sorted(values) == [[1, \"a\"], [1, \"b\"], [1, \"c\"], [3, \"e\"], [3, \"f\"],\n <mask>                               [3, \"g\"]]\n <mask> \n <mask>     # Test column selection.\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove         assert len(ds._blocks._blocks) == expected\n </s> add         assert ds._blocks._num_computed() == expected", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     ds = ray.data.read_parquet(\n <mask>         str(tmp_path), parallelism=1, filter=(pa.dataset.field(\"two\") == \"a\"))\n <mask> \n <mask>     values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\n <mask>     assert len(ds._blocks._blocks) == 1\n <mask>     assert sorted(values) == [[1, \"a\"], [1, \"a\"]]\n <mask> \n <mask>     # 2 partitions, 1 empty partition, 2 block/read tasks, 1 empty block\n <mask> \n <mask>     ds = ray.data.read_parquet(\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     ds = ray.data.read_parquet(\n <mask>         str(tmp_path), parallelism=2, filter=(pa.dataset.field(\"two\") == \"a\"))\n <mask> \n <mask>     values = [[s[\"one\"], s[\"two\"]] for s in ds.take()]\n <mask>     assert len(ds._blocks._blocks) == 2\n <mask>     assert sorted(values) == [[1, \"a\"], [1, \"a\"]]\n <mask> \n <mask> \n <mask> def test_parquet_read_with_udf(ray_start_regular_shared, tmp_path):\n <mask>     one_data = list(range(6))\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     ds = ray.data.read_parquet(\n <mask>         str(tmp_path), parallelism=1, _block_udf=_block_udf)\n <mask> \n <mask>     ones, twos = zip(*[[s[\"one\"], s[\"two\"]] for s in ds.take()])\n <mask>     assert len(ds._blocks._blocks) == 1\n <mask>     np.testing.assert_array_equal(sorted(ones), np.array(one_data) + 1)\n <mask> \n <mask>     # 2 blocks/read tasks\n <mask> \n <mask>     ds = ray.data.read_parquet(\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     ds = ray.data.read_parquet(\n <mask>         str(tmp_path), parallelism=2, _block_udf=_block_udf)\n <mask> \n <mask>     ones, twos = zip(*[[s[\"one\"], s[\"two\"]] for s in ds.take()])\n <mask>     assert len(ds._blocks._blocks) == 2\n <mask>     np.testing.assert_array_equal(sorted(ones), np.array(one_data) + 1)\n <mask> \n <mask>     # 2 blocks/read tasks, 1 empty block\n <mask> \n <mask>     ds = ray.data.read_parquet(\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         filter=(pa.dataset.field(\"two\") == \"a\"),\n <mask>         _block_udf=_block_udf)\n <mask> \n <mask>     ones, twos = zip(*[[s[\"one\"], s[\"two\"]] for s in ds.take()])\n <mask>     assert len(ds._blocks._blocks) == 2\n <mask>     np.testing.assert_array_equal(sorted(ones), np.array(one_data[:2]) + 1)\n <mask> \n <mask> \n <mask> @pytest.mark.parametrize(\"fs,data_path,endpoint_url\", [\n <mask>     (None, lazy_fixture(\"local_path\"), None),\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>         ray_start_regular_shared):\n <mask>     ds = ray.data.range(32, parallelism=8)\n <mask>     expected_num_blocks = [1, 2, 4, 4, 8, 8, 8, 8]\n <mask>     for _, expected in zip(ds.iter_batches(), expected_num_blocks):\n <mask>         assert len(ds._blocks._blocks) == expected\n <mask> \n <mask> \n <mask> def test_map_batch(ray_start_regular_shared, tmp_path):\n <mask>     # Test input validation\n <mask>     ds = ray.data.range(5)\n </s> Refactor LazyBlockList to simplify union of lists (#19214) </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 2\n </s> add     assert ds._blocks._num_computed() == 2 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1 </s> remove     assert len(ds._blocks._blocks) == 1\n </s> add     assert ds._blocks._num_computed() == 1", "html_url": "https://github.com/ray-project/ray/commit/8dded147987d1c89d4c4ae8ca7a6348306192c49", "file_name": "python/ray/data/tests/test_dataset.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     DEFAULT_COMMAND_TIMEOUT,\n <mask>     DEFAULT_WAIT_FOR_NODES_TIMEOUT,\n <mask>     RELEASE_PACKAGE_DIR,\n <mask>     DEFAULT_AUTOSUSPEND_MINS,\n <mask>     validate_test,\n <mask> )\n <mask> from ray_release.template import load_test_cluster_env, load_test_cluster_compute\n <mask> from ray_release.exception import (\n <mask>     ReleaseTestConfigError,\n <mask>     ReleaseTestSetupError,\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove from ray_release.config import (\n    Test,\n    DEFAULT_COMMAND_TIMEOUT,\n    DEFAULT_WAIT_FOR_NODES_TIMEOUT,\n)\n </s> add from ray_release.config import Test </s> remove     LocalEnvSetupError,\n    ClusterComputeCreateError,\n </s> add  </s> remove     TIMEOUT_BUFFER_MINUTES,\n </s> add  </s> remove     LocalEnvSetupError,\n </s> add  </s> remove from ray_release.util import (\n    run_bash_script,\n    get_pip_packages,\n    reinstall_anyscale_dependencies,\n)\n </s> add  </s> remove def _setup_local_environment(\n    test: Test,\n    command_runner: CommandRunner,\n    ray_wheels_url: str,\n) -> None:\n    driver_setup_script = test.get(\"driver_setup\", None)\n    if driver_setup_script:\n        try:\n            run_bash_script(driver_setup_script)\n        except Exception as e:\n            raise LocalEnvSetupError(f\"Driver setup script failed: {e}\") from e\n\n    # Install local dependencies\n    command_runner.prepare_local_env(ray_wheels_url)\n\n    # Re-install anyscale package as local dependencies might have changed\n    # from local env setup\n    reinstall_anyscale_dependencies()\n\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/ray_release/glue.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     CommandTimeout,\n <mask>     PrepareCommandTimeout,\n <mask>     TestCommandError,\n <mask>     TestCommandTimeout,\n <mask>     LocalEnvSetupError,\n <mask>     ClusterEnvCreateError,\n <mask> )\n <mask> from ray_release.file_manager.job_file_manager import JobFileManager\n <mask> from ray_release.logger import logger\n <mask> from ray_release.reporter.reporter import Reporter\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove     TIMEOUT_BUFFER_MINUTES,\n </s> add  </s> remove from ray_release.config import (\n    Test,\n    DEFAULT_COMMAND_TIMEOUT,\n    DEFAULT_WAIT_FOR_NODES_TIMEOUT,\n)\n </s> add from ray_release.config import Test </s> remove     LocalEnvSetupError,\n    ClusterComputeCreateError,\n </s> add  </s> remove     validate_test,\n </s> add  </s> remove from ray_release.util import (\n    run_bash_script,\n    get_pip_packages,\n    reinstall_anyscale_dependencies,\n)\n </s> add  </s> remove def _setup_local_environment(\n    test: Test,\n    command_runner: CommandRunner,\n    ray_wheels_url: str,\n) -> None:\n    driver_setup_script = test.get(\"driver_setup\", None)\n    if driver_setup_script:\n        try:\n            run_bash_script(driver_setup_script)\n        except Exception as e:\n            raise LocalEnvSetupError(f\"Driver setup script failed: {e}\") from e\n\n    # Install local dependencies\n    command_runner.prepare_local_env(ray_wheels_url)\n\n    # Re-install anyscale package as local dependencies might have changed\n    # from local env setup\n    reinstall_anyscale_dependencies()\n\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/ray_release/glue.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     setup_signal_handling,\n <mask>     reset_signal_handling,\n <mask>     register_handler,\n <mask> )\n <mask> from ray_release.util import (\n <mask>     run_bash_script,\n <mask>     get_pip_packages,\n <mask>     reinstall_anyscale_dependencies,\n <mask> )\n <mask> \n <mask> type_str_to_command_runner = {\n <mask>     \"job\": JobRunner,\n <mask>     \"anyscale_job\": AnyscaleJobRunner,\n <mask> }\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove from ray_release.config import (\n    Test,\n    DEFAULT_COMMAND_TIMEOUT,\n    DEFAULT_WAIT_FOR_NODES_TIMEOUT,\n)\n </s> add from ray_release.config import Test </s> remove     TIMEOUT_BUFFER_MINUTES,\n </s> add  </s> remove     validate_test,\n </s> add  </s> remove     LocalEnvSetupError,\n    ClusterComputeCreateError,\n </s> add  </s> remove \t\t\t\t\"driver_setup\": {\n\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t},\n </s> add  </s> remove \t\t\t\t\"driver_setup\": {\n\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t},\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/ray_release/glue.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     ray_wheels_url: str,\n <mask>     smoke_test: bool = False,\n <mask>     no_terminate: bool = False,\n <mask> ) -> Tuple[ClusterManager, CommandRunner, str]:\n <mask>     validate_test(test)\n <mask>     logger.info(f\"Test config: {test}\")\n <mask> \n <mask>     # Populate result paramaters\n <mask>     result.wheels_url = ray_wheels_url\n <mask>     result.stable = test.get(\"stable\", True)\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove     def testAutomaticClusterEnvVariables(self):\n        result = Result()\n\n        self._succeed_until(\"local_env\")\n\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n\n        cluster_manager = self.instances[\"cluster_manager\"]\n\n        command_timeout = self.test[\"run\"].get(\"timeout\", DEFAULT_COMMAND_TIMEOUT)\n        prepare_cmd = self.test[\"run\"].get(\"prepare\", None)\n        if prepare_cmd:\n            prepare_timeout = self.test[\"run\"].get(\"prepare_timeout\", command_timeout)\n        else:\n            prepare_timeout = 0\n        command_and_prepare_timeout = command_timeout + prepare_timeout\n\n        wait_timeout = self.test[\"run\"][\"wait_for_nodes\"].get(\n            \"timeout\", DEFAULT_WAIT_FOR_NODES_TIMEOUT\n        )\n\n        expected_idle_termination_minutes = int(\n            command_and_prepare_timeout / 60 + TIMEOUT_BUFFER_MINUTES\n        )\n        expected_maximum_uptime_minutes = int(\n            expected_idle_termination_minutes + wait_timeout + TIMEOUT_BUFFER_MINUTES\n        )\n\n        self.assertEqual(\n            cluster_manager.cluster_compute[\"idle_termination_minutes\"],\n            expected_idle_termination_minutes,\n        )\n        self.assertEqual(\n            cluster_manager.cluster_compute[\"maximum_uptime_minutes\"],\n            expected_maximum_uptime_minutes,\n        )\n\n    def testInvalidPrepareLocalEnv(self):\n        result = Result()\n\n        self.command_runner_return[\"prepare_local_env\"] = _fail_on_call(\n            LocalEnvSetupError\n        )\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n\n    def testDriverSetupFails(self):\n        result = Result()\n\n        self._succeed_until(\"local_env\")\n\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n\n    def testInvalidClusterIdOverride(self):\n        result = Result()\n\n        self._succeed_until(\"driver_setup\")\n\n        self.sdk.returns[\"get_cluster_environment\"] = None\n\n        with self.assertRaises(ClusterEnvCreateError):\n            self._run(result, cluster_env_id=\"existing\")\n\n        self.sdk.returns[\"get_cluster_environment\"] = APIDict(\n            result=APIDict(config_json={\"overridden\": True})\n        )\n\n        with self.assertRaises(Exception) as cm:  # Fail somewhere else\n            self._run(result, cluster_env_id=\"existing\")\n            self.assertNotIsInstance(cm.exception, ClusterEnvCreateError)\n\n    def testBuildConfigFailsClusterCompute(self):\n        result = Result()\n\n        self._succeed_until(\"driver_setup\")\n\n        # These commands should succeed\n        self.command_runner_return[\"prepare_local_env\"] = None\n\n        # Fails because API response faulty\n        with self.assertRaisesRegex(ClusterComputeCreateError, \"Unexpected\"):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n\n        # Fails for random cluster compute reason\n        self.cluster_manager_return[\"create_cluster_compute\"] = _fail_on_call(\n            ClusterComputeCreateError, \"Known\"\n        )\n        with self.assertRaisesRegex(ClusterComputeCreateError, \"Known\"):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n\n </s> add  </s> remove def _setup_local_environment(\n    test: Test,\n    command_runner: CommandRunner,\n    ray_wheels_url: str,\n) -> None:\n    driver_setup_script = test.get(\"driver_setup\", None)\n    if driver_setup_script:\n        try:\n            run_bash_script(driver_setup_script)\n        except Exception as e:\n            raise LocalEnvSetupError(f\"Driver setup script failed: {e}\") from e\n\n    # Install local dependencies\n    command_runner.prepare_local_env(ray_wheels_url)\n\n    # Re-install anyscale package as local dependencies might have changed\n    # from local env setup\n    reinstall_anyscale_dependencies()\n\n\n </s> add  </s> remove     pip_packages = get_pip_packages()\n    pip_package_string = \"\\n\".join(pip_packages)\n    logger.info(f\"Installed python packages:\\n{pip_package_string}\")\n\n </s> add  </s> remove             driver_setup=\"driver_fail.sh\",\n </s> add  </s> remove         self.command_runner_return[\"prepare_local_env\"] = None\n\n        if until == \"local_env\":\n            return\n\n        self.test[\"driver_setup\"] = \"driver_succeed.sh\"\n\n        if until == \"driver_setup\":\n            return\n\n </s> add  </s> remove from ray_release.util import (\n    run_bash_script,\n    get_pip_packages,\n    reinstall_anyscale_dependencies,\n)\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/ray_release/glue.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>     return prepare_cmd, prepare_timeout, build_timeout, cluster_timeout, command_timeout\n <mask> \n <mask> \n <mask> def _setup_local_environment(\n <mask>     test: Test,\n <mask>     command_runner: CommandRunner,\n <mask>     ray_wheels_url: str,\n <mask> ) -> None:\n <mask>     driver_setup_script = test.get(\"driver_setup\", None)\n <mask>     if driver_setup_script:\n <mask>         try:\n <mask>             run_bash_script(driver_setup_script)\n <mask>         except Exception as e:\n <mask>             raise LocalEnvSetupError(f\"Driver setup script failed: {e}\") from e\n <mask> \n <mask>     # Install local dependencies\n <mask>     command_runner.prepare_local_env(ray_wheels_url)\n <mask> \n <mask>     # Re-install anyscale package as local dependencies might have changed\n <mask>     # from local env setup\n <mask>     reinstall_anyscale_dependencies()\n <mask> \n <mask> \n <mask> def _local_environment_information(\n <mask>     result: Result,\n <mask>     cluster_manager: ClusterManager,\n <mask>     command_runner: CommandRunner,\n <mask>     build_timeout: int,\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove     validate_test(test)\n </s> add  </s> remove #  # Optional location of a bash setup script to run on the driver\n#  # when setting up the local environment. Relative to working_dir\n#  driver_setup: setup_driver.sh\n#\n </s> add  </s> remove         buildkite_group(\":nut_and_bolt: Setting up local environment\")\n        _setup_local_environment(test, command_runner, ray_wheels_url)\n\n        # Print installed pip packages\n </s> add  </s> remove         self.command_runner_return[\"prepare_local_env\"] = None\n\n        if until == \"local_env\":\n            return\n\n        self.test[\"driver_setup\"] = \"driver_succeed.sh\"\n\n        if until == \"driver_setup\":\n            return\n\n </s> add  </s> remove @patch(\"ray_release.glue.reinstall_anyscale_dependencies\", lambda: None)\n@patch(\"ray_release.glue.get_pip_packages\", lambda: [\"pip-packages\"])\n </s> add  </s> remove     def testAutomaticClusterEnvVariables(self):\n        result = Result()\n\n        self._succeed_until(\"local_env\")\n\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n\n        cluster_manager = self.instances[\"cluster_manager\"]\n\n        command_timeout = self.test[\"run\"].get(\"timeout\", DEFAULT_COMMAND_TIMEOUT)\n        prepare_cmd = self.test[\"run\"].get(\"prepare\", None)\n        if prepare_cmd:\n            prepare_timeout = self.test[\"run\"].get(\"prepare_timeout\", command_timeout)\n        else:\n            prepare_timeout = 0\n        command_and_prepare_timeout = command_timeout + prepare_timeout\n\n        wait_timeout = self.test[\"run\"][\"wait_for_nodes\"].get(\n            \"timeout\", DEFAULT_WAIT_FOR_NODES_TIMEOUT\n        )\n\n        expected_idle_termination_minutes = int(\n            command_and_prepare_timeout / 60 + TIMEOUT_BUFFER_MINUTES\n        )\n        expected_maximum_uptime_minutes = int(\n            expected_idle_termination_minutes + wait_timeout + TIMEOUT_BUFFER_MINUTES\n        )\n\n        self.assertEqual(\n            cluster_manager.cluster_compute[\"idle_termination_minutes\"],\n            expected_idle_termination_minutes,\n        )\n        self.assertEqual(\n            cluster_manager.cluster_compute[\"maximum_uptime_minutes\"],\n            expected_maximum_uptime_minutes,\n        )\n\n    def testInvalidPrepareLocalEnv(self):\n        result = Result()\n\n        self.command_runner_return[\"prepare_local_env\"] = _fail_on_call(\n            LocalEnvSetupError\n        )\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n\n    def testDriverSetupFails(self):\n        result = Result()\n\n        self._succeed_until(\"local_env\")\n\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n\n    def testInvalidClusterIdOverride(self):\n        result = Result()\n\n        self._succeed_until(\"driver_setup\")\n\n        self.sdk.returns[\"get_cluster_environment\"] = None\n\n        with self.assertRaises(ClusterEnvCreateError):\n            self._run(result, cluster_env_id=\"existing\")\n\n        self.sdk.returns[\"get_cluster_environment\"] = APIDict(\n            result=APIDict(config_json={\"overridden\": True})\n        )\n\n        with self.assertRaises(Exception) as cm:  # Fail somewhere else\n            self._run(result, cluster_env_id=\"existing\")\n            self.assertNotIsInstance(cm.exception, ClusterEnvCreateError)\n\n    def testBuildConfigFailsClusterCompute(self):\n        result = Result()\n\n        self._succeed_until(\"driver_setup\")\n\n        # These commands should succeed\n        self.command_runner_return[\"prepare_local_env\"] = None\n\n        # Fails because API response faulty\n        with self.assertRaisesRegex(ClusterComputeCreateError, \"Unexpected\"):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n\n        # Fails for random cluster compute reason\n        self.cluster_manager_return[\"create_cluster_compute\"] = _fail_on_call(\n            ClusterComputeCreateError, \"Known\"\n        )\n        with self.assertRaisesRegex(ClusterComputeCreateError, \"Known\"):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/ray_release/glue.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>     no_terminate: bool,\n <mask>     cluster_id: Optional[str],\n <mask>     cluster_env_id: Optional[str],\n <mask> ) -> None:\n <mask>     pip_packages = get_pip_packages()\n <mask>     pip_package_string = \"\\n\".join(pip_packages)\n <mask>     logger.info(f\"Installed python packages:\\n{pip_package_string}\")\n <mask> \n <mask>     if isinstance(cluster_manager, FullClusterManager):\n <mask>         if not no_terminate:\n <mask>             register_handler(\n <mask>                 lambda sig, frame: cluster_manager.terminate_cluster(wait=True)\n <mask>             )\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove     validate_test(test)\n </s> add  </s> remove         self.command_runner_return[\"prepare_local_env\"] = None\n\n        if until == \"local_env\":\n            return\n\n        self.test[\"driver_setup\"] = \"driver_succeed.sh\"\n\n        if until == \"driver_setup\":\n            return\n\n </s> add  </s> remove             driver_setup=\"driver_fail.sh\",\n </s> add  </s> remove def _setup_local_environment(\n    test: Test,\n    command_runner: CommandRunner,\n    ray_wheels_url: str,\n) -> None:\n    driver_setup_script = test.get(\"driver_setup\", None)\n    if driver_setup_script:\n        try:\n            run_bash_script(driver_setup_script)\n        except Exception as e:\n            raise LocalEnvSetupError(f\"Driver setup script failed: {e}\") from e\n\n    # Install local dependencies\n    command_runner.prepare_local_env(ray_wheels_url)\n\n    # Re-install anyscale package as local dependencies might have changed\n    # from local env setup\n    reinstall_anyscale_dependencies()\n\n\n </s> add  </s> remove     def testAutomaticClusterEnvVariables(self):\n        result = Result()\n\n        self._succeed_until(\"local_env\")\n\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n\n        cluster_manager = self.instances[\"cluster_manager\"]\n\n        command_timeout = self.test[\"run\"].get(\"timeout\", DEFAULT_COMMAND_TIMEOUT)\n        prepare_cmd = self.test[\"run\"].get(\"prepare\", None)\n        if prepare_cmd:\n            prepare_timeout = self.test[\"run\"].get(\"prepare_timeout\", command_timeout)\n        else:\n            prepare_timeout = 0\n        command_and_prepare_timeout = command_timeout + prepare_timeout\n\n        wait_timeout = self.test[\"run\"][\"wait_for_nodes\"].get(\n            \"timeout\", DEFAULT_WAIT_FOR_NODES_TIMEOUT\n        )\n\n        expected_idle_termination_minutes = int(\n            command_and_prepare_timeout / 60 + TIMEOUT_BUFFER_MINUTES\n        )\n        expected_maximum_uptime_minutes = int(\n            expected_idle_termination_minutes + wait_timeout + TIMEOUT_BUFFER_MINUTES\n        )\n\n        self.assertEqual(\n            cluster_manager.cluster_compute[\"idle_termination_minutes\"],\n            expected_idle_termination_minutes,\n        )\n        self.assertEqual(\n            cluster_manager.cluster_compute[\"maximum_uptime_minutes\"],\n            expected_maximum_uptime_minutes,\n        )\n\n    def testInvalidPrepareLocalEnv(self):\n        result = Result()\n\n        self.command_runner_return[\"prepare_local_env\"] = _fail_on_call(\n            LocalEnvSetupError\n        )\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n\n    def testDriverSetupFails(self):\n        result = Result()\n\n        self._succeed_until(\"local_env\")\n\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n\n    def testInvalidClusterIdOverride(self):\n        result = Result()\n\n        self._succeed_until(\"driver_setup\")\n\n        self.sdk.returns[\"get_cluster_environment\"] = None\n\n        with self.assertRaises(ClusterEnvCreateError):\n            self._run(result, cluster_env_id=\"existing\")\n\n        self.sdk.returns[\"get_cluster_environment\"] = APIDict(\n            result=APIDict(config_json={\"overridden\": True})\n        )\n\n        with self.assertRaises(Exception) as cm:  # Fail somewhere else\n            self._run(result, cluster_env_id=\"existing\")\n            self.assertNotIsInstance(cm.exception, ClusterEnvCreateError)\n\n    def testBuildConfigFailsClusterCompute(self):\n        result = Result()\n\n        self._succeed_until(\"driver_setup\")\n\n        # These commands should succeed\n        self.command_runner_return[\"prepare_local_env\"] = None\n\n        # Fails because API response faulty\n        with self.assertRaisesRegex(ClusterComputeCreateError, \"Unexpected\"):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n\n        # Fails for random cluster compute reason\n        self.cluster_manager_return[\"create_cluster_compute\"] = _fail_on_call(\n            ClusterComputeCreateError, \"Known\"\n        )\n        with self.assertRaisesRegex(ClusterComputeCreateError, \"Known\"):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n\n </s> add  </s> remove @patch(\"ray_release.glue.reinstall_anyscale_dependencies\", lambda: None)\n@patch(\"ray_release.glue.get_pip_packages\", lambda: [\"pip-packages\"])\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/ray_release/glue.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>             ray_wheels_url,\n <mask>             cluster_env_id,\n <mask>         )\n <mask> \n <mask>         buildkite_group(\":nut_and_bolt: Setting up local environment\")\n <mask>         _setup_local_environment(test, command_runner, ray_wheels_url)\n <mask> \n <mask>         # Print installed pip packages\n <mask>         buildkite_group(\":bulb: Local environment information\")\n <mask>         _local_environment_information(\n <mask>             result,\n <mask>             cluster_manager,\n <mask>             command_runner,\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove def _setup_local_environment(\n    test: Test,\n    command_runner: CommandRunner,\n    ray_wheels_url: str,\n) -> None:\n    driver_setup_script = test.get(\"driver_setup\", None)\n    if driver_setup_script:\n        try:\n            run_bash_script(driver_setup_script)\n        except Exception as e:\n            raise LocalEnvSetupError(f\"Driver setup script failed: {e}\") from e\n\n    # Install local dependencies\n    command_runner.prepare_local_env(ray_wheels_url)\n\n    # Re-install anyscale package as local dependencies might have changed\n    # from local env setup\n    reinstall_anyscale_dependencies()\n\n\n </s> add  </s> remove #  # Optional location of a bash setup script to run on the driver\n#  # when setting up the local environment. Relative to working_dir\n#  driver_setup: setup_driver.sh\n#\n </s> add  </s> remove     validate_test(test)\n </s> add  </s> remove     def testAutomaticClusterEnvVariables(self):\n        result = Result()\n\n        self._succeed_until(\"local_env\")\n\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n\n        cluster_manager = self.instances[\"cluster_manager\"]\n\n        command_timeout = self.test[\"run\"].get(\"timeout\", DEFAULT_COMMAND_TIMEOUT)\n        prepare_cmd = self.test[\"run\"].get(\"prepare\", None)\n        if prepare_cmd:\n            prepare_timeout = self.test[\"run\"].get(\"prepare_timeout\", command_timeout)\n        else:\n            prepare_timeout = 0\n        command_and_prepare_timeout = command_timeout + prepare_timeout\n\n        wait_timeout = self.test[\"run\"][\"wait_for_nodes\"].get(\n            \"timeout\", DEFAULT_WAIT_FOR_NODES_TIMEOUT\n        )\n\n        expected_idle_termination_minutes = int(\n            command_and_prepare_timeout / 60 + TIMEOUT_BUFFER_MINUTES\n        )\n        expected_maximum_uptime_minutes = int(\n            expected_idle_termination_minutes + wait_timeout + TIMEOUT_BUFFER_MINUTES\n        )\n\n        self.assertEqual(\n            cluster_manager.cluster_compute[\"idle_termination_minutes\"],\n            expected_idle_termination_minutes,\n        )\n        self.assertEqual(\n            cluster_manager.cluster_compute[\"maximum_uptime_minutes\"],\n            expected_maximum_uptime_minutes,\n        )\n\n    def testInvalidPrepareLocalEnv(self):\n        result = Result()\n\n        self.command_runner_return[\"prepare_local_env\"] = _fail_on_call(\n            LocalEnvSetupError\n        )\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n\n    def testDriverSetupFails(self):\n        result = Result()\n\n        self._succeed_until(\"local_env\")\n\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n\n    def testInvalidClusterIdOverride(self):\n        result = Result()\n\n        self._succeed_until(\"driver_setup\")\n\n        self.sdk.returns[\"get_cluster_environment\"] = None\n\n        with self.assertRaises(ClusterEnvCreateError):\n            self._run(result, cluster_env_id=\"existing\")\n\n        self.sdk.returns[\"get_cluster_environment\"] = APIDict(\n            result=APIDict(config_json={\"overridden\": True})\n        )\n\n        with self.assertRaises(Exception) as cm:  # Fail somewhere else\n            self._run(result, cluster_env_id=\"existing\")\n            self.assertNotIsInstance(cm.exception, ClusterEnvCreateError)\n\n    def testBuildConfigFailsClusterCompute(self):\n        result = Result()\n\n        self._succeed_until(\"driver_setup\")\n\n        # These commands should succeed\n        self.command_runner_return[\"prepare_local_env\"] = None\n\n        # Fails because API response faulty\n        with self.assertRaisesRegex(ClusterComputeCreateError, \"Unexpected\"):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n\n        # Fails for random cluster compute reason\n        self.cluster_manager_return[\"create_cluster_compute\"] = _fail_on_call(\n            ClusterComputeCreateError, \"Known\"\n        )\n        with self.assertRaisesRegex(ClusterComputeCreateError, \"Known\"):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n\n </s> add  </s> remove         self.command_runner_return[\"prepare_local_env\"] = None\n\n        if until == \"local_env\":\n            return\n\n        self.test[\"driver_setup\"] = \"driver_succeed.sh\"\n\n        if until == \"driver_setup\":\n            return\n\n </s> add  </s> remove   driver_setup: tune_rllib/driver_setup.sh\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/ray_release/glue.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask> \t\t\t\t},\n <mask> \t\t\t\t\"team\": {\n <mask> \t\t\t\t\t\"type\": \"string\"\n <mask> \t\t\t\t},\n <mask> \t\t\t\t\"driver_setup\": {\n <mask> \t\t\t\t\t\"type\": \"string\"\n <mask> \t\t\t\t},\n <mask> \t\t\t\t\"cluster\": {\n <mask> \t\t\t\t\t\"$ref\": \"#/definitions/Cluster\"\n <mask> \t\t\t\t},\n <mask> \t\t\t\t\"run\": {\n <mask> \t\t\t\t\t\"$ref\": \"#/definitions/Run\"\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove \t\t\t\t\"driver_setup\": {\n\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t},\n </s> add  </s> remove from ray_release.util import (\n    run_bash_script,\n    get_pip_packages,\n    reinstall_anyscale_dependencies,\n)\n </s> add  </s> remove         self.command_runner_return[\"prepare_local_env\"] = None\n\n        if until == \"local_env\":\n            return\n\n        self.test[\"driver_setup\"] = \"driver_succeed.sh\"\n\n        if until == \"driver_setup\":\n            return\n\n </s> add  </s> remove   driver_setup: tune_rllib/driver_setup.sh\n </s> add  </s> remove     LocalEnvSetupError,\n    ClusterComputeCreateError,\n </s> add  </s> remove     LocalEnvSetupError,\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/ray_release/schema.json"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask> \t\t\t\t},\n <mask> \t\t\t\t\"env\": {\n <mask> \t\t\t\t\t\"type\": \"string\"\n <mask> \t\t\t\t},\n <mask> \t\t\t\t\"driver_setup\": {\n <mask> \t\t\t\t\t\"type\": \"string\"\n <mask> \t\t\t\t},\n <mask> \t\t\t\t\"cluster\": {\n <mask> \t\t\t\t\t\"type\": \"object\"\n <mask> \t\t\t\t},\n <mask> \t\t\t\t\"run\": {\n <mask> \t\t\t\t\t\"type\": \"object\"\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove \t\t\t\t\"driver_setup\": {\n\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t},\n </s> add  </s> remove from ray_release.util import (\n    run_bash_script,\n    get_pip_packages,\n    reinstall_anyscale_dependencies,\n)\n </s> add  </s> remove         self.command_runner_return[\"prepare_local_env\"] = None\n\n        if until == \"local_env\":\n            return\n\n        self.test[\"driver_setup\"] = \"driver_succeed.sh\"\n\n        if until == \"driver_setup\":\n            return\n\n </s> add  </s> remove   driver_setup: tune_rllib/driver_setup.sh\n </s> add  </s> remove     LocalEnvSetupError,\n    ClusterComputeCreateError,\n </s> add  </s> remove     LocalEnvSetupError,\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/ray_release/schema.json"}
{"docstring_tokens": "keep keep keep replace replace replace replace replace keep keep replace replace", "code_tokens": " <mask> from ray_release.cluster_manager.cluster_manager import ClusterManager\n <mask> from ray_release.cluster_manager.full import FullClusterManager\n <mask> from ray_release.command_runner.command_runner import CommandRunner\n <mask> from ray_release.config import (\n <mask>     Test,\n <mask>     DEFAULT_COMMAND_TIMEOUT,\n <mask>     DEFAULT_WAIT_FOR_NODES_TIMEOUT,\n <mask> )\n <mask> from ray_release.exception import (\n <mask>     ReleaseTestConfigError,\n <mask>     LocalEnvSetupError,\n <mask>     ClusterComputeCreateError,\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove     validate_test,\n </s> add  </s> remove     TIMEOUT_BUFFER_MINUTES,\n </s> add  </s> remove     LocalEnvSetupError,\n </s> add  </s> remove from ray_release.util import (\n    run_bash_script,\n    get_pip_packages,\n    reinstall_anyscale_dependencies,\n)\n </s> add  </s> remove def _setup_local_environment(\n    test: Test,\n    command_runner: CommandRunner,\n    ray_wheels_url: str,\n) -> None:\n    driver_setup_script = test.get(\"driver_setup\", None)\n    if driver_setup_script:\n        try:\n            run_bash_script(driver_setup_script)\n        except Exception as e:\n            raise LocalEnvSetupError(f\"Driver setup script failed: {e}\") from e\n\n    # Install local dependencies\n    command_runner.prepare_local_env(ray_wheels_url)\n\n    # Re-install anyscale package as local dependencies might have changed\n    # from local env setup\n    reinstall_anyscale_dependencies()\n\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/ray_release/tests/test_glue.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> from ray_release.glue import (\n <mask>     run_release_test,\n <mask>     type_str_to_command_runner,\n <mask>     command_runner_to_cluster_manager,\n <mask>     TIMEOUT_BUFFER_MINUTES,\n <mask> )\n <mask> from ray_release.logger import logger\n <mask> from ray_release.reporter.reporter import Reporter\n <mask> from ray_release.result import Result, ExitCode\n <mask> from ray_release.tests.utils import MockSDK, APIDict\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove     LocalEnvSetupError,\n </s> add  </s> remove from ray_release.config import (\n    Test,\n    DEFAULT_COMMAND_TIMEOUT,\n    DEFAULT_WAIT_FOR_NODES_TIMEOUT,\n)\n </s> add from ray_release.config import Test </s> remove     validate_test,\n </s> add  </s> remove     LocalEnvSetupError,\n    ClusterComputeCreateError,\n </s> add  </s> remove from ray_release.util import (\n    run_bash_script,\n    get_pip_packages,\n    reinstall_anyscale_dependencies,\n)\n </s> add  </s> remove def _setup_local_environment(\n    test: Test,\n    command_runner: CommandRunner,\n    ray_wheels_url: str,\n) -> None:\n    driver_setup_script = test.get(\"driver_setup\", None)\n    if driver_setup_script:\n        try:\n            run_bash_script(driver_setup_script)\n        except Exception as e:\n            raise LocalEnvSetupError(f\"Driver setup script failed: {e}\") from e\n\n    # Install local dependencies\n    command_runner.prepare_local_env(ray_wheels_url)\n\n    # Re-install anyscale package as local dependencies might have changed\n    # from local env setup\n    reinstall_anyscale_dependencies()\n\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/ray_release/tests/test_glue.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>                 return lambda *a, **kw: mocked\n <mask>         return object.__getattribute__(self, item)\n <mask> \n <mask> \n <mask> @patch(\"ray_release.glue.reinstall_anyscale_dependencies\", lambda: None)\n <mask> @patch(\"ray_release.glue.get_pip_packages\", lambda: [\"pip-packages\"])\n <mask> class GlueTest(unittest.TestCase):\n <mask>     def writeClusterEnv(self, content: str):\n <mask>         with open(os.path.join(self.tempdir, \"cluster_env.yaml\"), \"wt\") as fp:\n <mask>             fp.write(content)\n <mask> \n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove         self.command_runner_return[\"prepare_local_env\"] = None\n\n        if until == \"local_env\":\n            return\n\n        self.test[\"driver_setup\"] = \"driver_succeed.sh\"\n\n        if until == \"driver_setup\":\n            return\n\n </s> add  </s> remove def _setup_local_environment(\n    test: Test,\n    command_runner: CommandRunner,\n    ray_wheels_url: str,\n) -> None:\n    driver_setup_script = test.get(\"driver_setup\", None)\n    if driver_setup_script:\n        try:\n            run_bash_script(driver_setup_script)\n        except Exception as e:\n            raise LocalEnvSetupError(f\"Driver setup script failed: {e}\") from e\n\n    # Install local dependencies\n    command_runner.prepare_local_env(ray_wheels_url)\n\n    # Re-install anyscale package as local dependencies might have changed\n    # from local env setup\n    reinstall_anyscale_dependencies()\n\n\n </s> add  </s> remove     def testAutomaticClusterEnvVariables(self):\n        result = Result()\n\n        self._succeed_until(\"local_env\")\n\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n\n        cluster_manager = self.instances[\"cluster_manager\"]\n\n        command_timeout = self.test[\"run\"].get(\"timeout\", DEFAULT_COMMAND_TIMEOUT)\n        prepare_cmd = self.test[\"run\"].get(\"prepare\", None)\n        if prepare_cmd:\n            prepare_timeout = self.test[\"run\"].get(\"prepare_timeout\", command_timeout)\n        else:\n            prepare_timeout = 0\n        command_and_prepare_timeout = command_timeout + prepare_timeout\n\n        wait_timeout = self.test[\"run\"][\"wait_for_nodes\"].get(\n            \"timeout\", DEFAULT_WAIT_FOR_NODES_TIMEOUT\n        )\n\n        expected_idle_termination_minutes = int(\n            command_and_prepare_timeout / 60 + TIMEOUT_BUFFER_MINUTES\n        )\n        expected_maximum_uptime_minutes = int(\n            expected_idle_termination_minutes + wait_timeout + TIMEOUT_BUFFER_MINUTES\n        )\n\n        self.assertEqual(\n            cluster_manager.cluster_compute[\"idle_termination_minutes\"],\n            expected_idle_termination_minutes,\n        )\n        self.assertEqual(\n            cluster_manager.cluster_compute[\"maximum_uptime_minutes\"],\n            expected_maximum_uptime_minutes,\n        )\n\n    def testInvalidPrepareLocalEnv(self):\n        result = Result()\n\n        self.command_runner_return[\"prepare_local_env\"] = _fail_on_call(\n            LocalEnvSetupError\n        )\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n\n    def testDriverSetupFails(self):\n        result = Result()\n\n        self._succeed_until(\"local_env\")\n\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n\n    def testInvalidClusterIdOverride(self):\n        result = Result()\n\n        self._succeed_until(\"driver_setup\")\n\n        self.sdk.returns[\"get_cluster_environment\"] = None\n\n        with self.assertRaises(ClusterEnvCreateError):\n            self._run(result, cluster_env_id=\"existing\")\n\n        self.sdk.returns[\"get_cluster_environment\"] = APIDict(\n            result=APIDict(config_json={\"overridden\": True})\n        )\n\n        with self.assertRaises(Exception) as cm:  # Fail somewhere else\n            self._run(result, cluster_env_id=\"existing\")\n            self.assertNotIsInstance(cm.exception, ClusterEnvCreateError)\n\n    def testBuildConfigFailsClusterCompute(self):\n        result = Result()\n\n        self._succeed_until(\"driver_setup\")\n\n        # These commands should succeed\n        self.command_runner_return[\"prepare_local_env\"] = None\n\n        # Fails because API response faulty\n        with self.assertRaisesRegex(ClusterComputeCreateError, \"Unexpected\"):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n\n        # Fails for random cluster compute reason\n        self.cluster_manager_return[\"create_cluster_compute\"] = _fail_on_call(\n            ClusterComputeCreateError, \"Known\"\n        )\n        with self.assertRaisesRegex(ClusterComputeCreateError, \"Known\"):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n\n </s> add  </s> remove     pip_packages = get_pip_packages()\n    pip_package_string = \"\\n\".join(pip_packages)\n    logger.info(f\"Installed python packages:\\n{pip_package_string}\")\n\n </s> add  </s> remove             driver_setup=\"driver_fail.sh\",\n </s> add  </s> remove   driver_setup: tune_rllib/driver_setup.sh\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/ray_release/tests/test_glue.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>             cluster=dict(\n <mask>                 cluster_env=\"cluster_env.yaml\", cluster_compute=\"cluster_compute.yaml\"\n <mask>             ),\n <mask>             alert=\"unit_test_alerter\",\n <mask>             driver_setup=\"driver_fail.sh\",\n <mask>         )\n <mask>         self.anyscale_project = \"prj_unit12345678\"\n <mask>         self.ray_wheels_url = \"http://mock.wheels/\"\n <mask> \n <mask>     def tearDown(self) -> None:\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove     pip_packages = get_pip_packages()\n    pip_package_string = \"\\n\".join(pip_packages)\n    logger.info(f\"Installed python packages:\\n{pip_package_string}\")\n\n </s> add  </s> remove     def testAutomaticClusterEnvVariables(self):\n        result = Result()\n\n        self._succeed_until(\"local_env\")\n\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n\n        cluster_manager = self.instances[\"cluster_manager\"]\n\n        command_timeout = self.test[\"run\"].get(\"timeout\", DEFAULT_COMMAND_TIMEOUT)\n        prepare_cmd = self.test[\"run\"].get(\"prepare\", None)\n        if prepare_cmd:\n            prepare_timeout = self.test[\"run\"].get(\"prepare_timeout\", command_timeout)\n        else:\n            prepare_timeout = 0\n        command_and_prepare_timeout = command_timeout + prepare_timeout\n\n        wait_timeout = self.test[\"run\"][\"wait_for_nodes\"].get(\n            \"timeout\", DEFAULT_WAIT_FOR_NODES_TIMEOUT\n        )\n\n        expected_idle_termination_minutes = int(\n            command_and_prepare_timeout / 60 + TIMEOUT_BUFFER_MINUTES\n        )\n        expected_maximum_uptime_minutes = int(\n            expected_idle_termination_minutes + wait_timeout + TIMEOUT_BUFFER_MINUTES\n        )\n\n        self.assertEqual(\n            cluster_manager.cluster_compute[\"idle_termination_minutes\"],\n            expected_idle_termination_minutes,\n        )\n        self.assertEqual(\n            cluster_manager.cluster_compute[\"maximum_uptime_minutes\"],\n            expected_maximum_uptime_minutes,\n        )\n\n    def testInvalidPrepareLocalEnv(self):\n        result = Result()\n\n        self.command_runner_return[\"prepare_local_env\"] = _fail_on_call(\n            LocalEnvSetupError\n        )\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n\n    def testDriverSetupFails(self):\n        result = Result()\n\n        self._succeed_until(\"local_env\")\n\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n\n    def testInvalidClusterIdOverride(self):\n        result = Result()\n\n        self._succeed_until(\"driver_setup\")\n\n        self.sdk.returns[\"get_cluster_environment\"] = None\n\n        with self.assertRaises(ClusterEnvCreateError):\n            self._run(result, cluster_env_id=\"existing\")\n\n        self.sdk.returns[\"get_cluster_environment\"] = APIDict(\n            result=APIDict(config_json={\"overridden\": True})\n        )\n\n        with self.assertRaises(Exception) as cm:  # Fail somewhere else\n            self._run(result, cluster_env_id=\"existing\")\n            self.assertNotIsInstance(cm.exception, ClusterEnvCreateError)\n\n    def testBuildConfigFailsClusterCompute(self):\n        result = Result()\n\n        self._succeed_until(\"driver_setup\")\n\n        # These commands should succeed\n        self.command_runner_return[\"prepare_local_env\"] = None\n\n        # Fails because API response faulty\n        with self.assertRaisesRegex(ClusterComputeCreateError, \"Unexpected\"):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n\n        # Fails for random cluster compute reason\n        self.cluster_manager_return[\"create_cluster_compute\"] = _fail_on_call(\n            ClusterComputeCreateError, \"Known\"\n        )\n        with self.assertRaisesRegex(ClusterComputeCreateError, \"Known\"):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n\n </s> add  </s> remove def _setup_local_environment(\n    test: Test,\n    command_runner: CommandRunner,\n    ray_wheels_url: str,\n) -> None:\n    driver_setup_script = test.get(\"driver_setup\", None)\n    if driver_setup_script:\n        try:\n            run_bash_script(driver_setup_script)\n        except Exception as e:\n            raise LocalEnvSetupError(f\"Driver setup script failed: {e}\") from e\n\n    # Install local dependencies\n    command_runner.prepare_local_env(ray_wheels_url)\n\n    # Re-install anyscale package as local dependencies might have changed\n    # from local env setup\n    reinstall_anyscale_dependencies()\n\n\n </s> add  </s> remove     validate_test(test)\n </s> add  </s> remove         self.command_runner_return[\"prepare_local_env\"] = None\n\n        if until == \"local_env\":\n            return\n\n        self.test[\"driver_setup\"] = \"driver_succeed.sh\"\n\n        if until == \"driver_setup\":\n            return\n\n </s> add  </s> remove from ray_release.util import (\n    run_bash_script,\n    get_pip_packages,\n    reinstall_anyscale_dependencies,\n)\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/ray_release/tests/test_glue.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>         shutil.rmtree(self.tempdir)\n <mask> \n <mask>     def _succeed_until(self, until: str):\n <mask>         # These commands should succeed\n <mask>         self.command_runner_return[\"prepare_local_env\"] = None\n <mask> \n <mask>         if until == \"local_env\":\n <mask>             return\n <mask> \n <mask>         self.test[\"driver_setup\"] = \"driver_succeed.sh\"\n <mask> \n <mask>         if until == \"driver_setup\":\n <mask>             return\n <mask> \n <mask>         self.cluster_manager_return[\"cluster_compute_id\"] = \"valid\"\n <mask>         self.cluster_manager_return[\"create_cluster_compute\"] = None\n <mask> \n <mask>         if until == \"cluster_compute\":\n <mask>             return\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove     def testAutomaticClusterEnvVariables(self):\n        result = Result()\n\n        self._succeed_until(\"local_env\")\n\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n\n        cluster_manager = self.instances[\"cluster_manager\"]\n\n        command_timeout = self.test[\"run\"].get(\"timeout\", DEFAULT_COMMAND_TIMEOUT)\n        prepare_cmd = self.test[\"run\"].get(\"prepare\", None)\n        if prepare_cmd:\n            prepare_timeout = self.test[\"run\"].get(\"prepare_timeout\", command_timeout)\n        else:\n            prepare_timeout = 0\n        command_and_prepare_timeout = command_timeout + prepare_timeout\n\n        wait_timeout = self.test[\"run\"][\"wait_for_nodes\"].get(\n            \"timeout\", DEFAULT_WAIT_FOR_NODES_TIMEOUT\n        )\n\n        expected_idle_termination_minutes = int(\n            command_and_prepare_timeout / 60 + TIMEOUT_BUFFER_MINUTES\n        )\n        expected_maximum_uptime_minutes = int(\n            expected_idle_termination_minutes + wait_timeout + TIMEOUT_BUFFER_MINUTES\n        )\n\n        self.assertEqual(\n            cluster_manager.cluster_compute[\"idle_termination_minutes\"],\n            expected_idle_termination_minutes,\n        )\n        self.assertEqual(\n            cluster_manager.cluster_compute[\"maximum_uptime_minutes\"],\n            expected_maximum_uptime_minutes,\n        )\n\n    def testInvalidPrepareLocalEnv(self):\n        result = Result()\n\n        self.command_runner_return[\"prepare_local_env\"] = _fail_on_call(\n            LocalEnvSetupError\n        )\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n\n    def testDriverSetupFails(self):\n        result = Result()\n\n        self._succeed_until(\"local_env\")\n\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n\n    def testInvalidClusterIdOverride(self):\n        result = Result()\n\n        self._succeed_until(\"driver_setup\")\n\n        self.sdk.returns[\"get_cluster_environment\"] = None\n\n        with self.assertRaises(ClusterEnvCreateError):\n            self._run(result, cluster_env_id=\"existing\")\n\n        self.sdk.returns[\"get_cluster_environment\"] = APIDict(\n            result=APIDict(config_json={\"overridden\": True})\n        )\n\n        with self.assertRaises(Exception) as cm:  # Fail somewhere else\n            self._run(result, cluster_env_id=\"existing\")\n            self.assertNotIsInstance(cm.exception, ClusterEnvCreateError)\n\n    def testBuildConfigFailsClusterCompute(self):\n        result = Result()\n\n        self._succeed_until(\"driver_setup\")\n\n        # These commands should succeed\n        self.command_runner_return[\"prepare_local_env\"] = None\n\n        # Fails because API response faulty\n        with self.assertRaisesRegex(ClusterComputeCreateError, \"Unexpected\"):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n\n        # Fails for random cluster compute reason\n        self.cluster_manager_return[\"create_cluster_compute\"] = _fail_on_call(\n            ClusterComputeCreateError, \"Known\"\n        )\n        with self.assertRaisesRegex(ClusterComputeCreateError, \"Known\"):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n\n </s> add  </s> remove @patch(\"ray_release.glue.reinstall_anyscale_dependencies\", lambda: None)\n@patch(\"ray_release.glue.get_pip_packages\", lambda: [\"pip-packages\"])\n </s> add  </s> remove def _setup_local_environment(\n    test: Test,\n    command_runner: CommandRunner,\n    ray_wheels_url: str,\n) -> None:\n    driver_setup_script = test.get(\"driver_setup\", None)\n    if driver_setup_script:\n        try:\n            run_bash_script(driver_setup_script)\n        except Exception as e:\n            raise LocalEnvSetupError(f\"Driver setup script failed: {e}\") from e\n\n    # Install local dependencies\n    command_runner.prepare_local_env(ray_wheels_url)\n\n    # Re-install anyscale package as local dependencies might have changed\n    # from local env setup\n    reinstall_anyscale_dependencies()\n\n\n </s> add  </s> remove     pip_packages = get_pip_packages()\n    pip_package_string = \"\\n\".join(pip_packages)\n    logger.info(f\"Installed python packages:\\n{pip_package_string}\")\n\n </s> add  </s> remove     validate_test(test)\n </s> add  </s> remove             driver_setup=\"driver_fail.sh\",\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/ray_release/tests/test_glue.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>             self._run(result)\n <mask> \n <mask>         self.assertEqual(result.return_code, ExitCode.CONFIG_ERROR.value)\n <mask> \n <mask>     def testAutomaticClusterEnvVariables(self):\n <mask>         result = Result()\n <mask> \n <mask>         self._succeed_until(\"local_env\")\n <mask> \n <mask>         with self.assertRaises(LocalEnvSetupError):\n <mask>             self._run(result)\n <mask> \n <mask>         cluster_manager = self.instances[\"cluster_manager\"]\n <mask> \n <mask>         command_timeout = self.test[\"run\"].get(\"timeout\", DEFAULT_COMMAND_TIMEOUT)\n <mask>         prepare_cmd = self.test[\"run\"].get(\"prepare\", None)\n <mask>         if prepare_cmd:\n <mask>             prepare_timeout = self.test[\"run\"].get(\"prepare_timeout\", command_timeout)\n <mask>         else:\n <mask>             prepare_timeout = 0\n <mask>         command_and_prepare_timeout = command_timeout + prepare_timeout\n <mask> \n <mask>         wait_timeout = self.test[\"run\"][\"wait_for_nodes\"].get(\n <mask>             \"timeout\", DEFAULT_WAIT_FOR_NODES_TIMEOUT\n <mask>         )\n <mask> \n <mask>         expected_idle_termination_minutes = int(\n <mask>             command_and_prepare_timeout / 60 + TIMEOUT_BUFFER_MINUTES\n <mask>         )\n <mask>         expected_maximum_uptime_minutes = int(\n <mask>             expected_idle_termination_minutes + wait_timeout + TIMEOUT_BUFFER_MINUTES\n <mask>         )\n <mask> \n <mask>         self.assertEqual(\n <mask>             cluster_manager.cluster_compute[\"idle_termination_minutes\"],\n <mask>             expected_idle_termination_minutes,\n <mask>         )\n <mask>         self.assertEqual(\n <mask>             cluster_manager.cluster_compute[\"maximum_uptime_minutes\"],\n <mask>             expected_maximum_uptime_minutes,\n <mask>         )\n <mask> \n <mask>     def testInvalidPrepareLocalEnv(self):\n <mask>         result = Result()\n <mask> \n <mask>         self.command_runner_return[\"prepare_local_env\"] = _fail_on_call(\n <mask>             LocalEnvSetupError\n <mask>         )\n <mask>         with self.assertRaises(LocalEnvSetupError):\n <mask>             self._run(result)\n <mask>         self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n <mask> \n <mask>     def testDriverSetupFails(self):\n <mask>         result = Result()\n <mask> \n <mask>         self._succeed_until(\"local_env\")\n <mask> \n <mask>         with self.assertRaises(LocalEnvSetupError):\n <mask>             self._run(result)\n <mask>         self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n <mask> \n <mask>     def testInvalidClusterIdOverride(self):\n <mask>         result = Result()\n <mask> \n <mask>         self._succeed_until(\"driver_setup\")\n <mask> \n <mask>         self.sdk.returns[\"get_cluster_environment\"] = None\n <mask> \n <mask>         with self.assertRaises(ClusterEnvCreateError):\n <mask>             self._run(result, cluster_env_id=\"existing\")\n <mask> \n <mask>         self.sdk.returns[\"get_cluster_environment\"] = APIDict(\n <mask>             result=APIDict(config_json={\"overridden\": True})\n <mask>         )\n <mask> \n <mask>         with self.assertRaises(Exception) as cm:  # Fail somewhere else\n <mask>             self._run(result, cluster_env_id=\"existing\")\n <mask>             self.assertNotIsInstance(cm.exception, ClusterEnvCreateError)\n <mask> \n <mask>     def testBuildConfigFailsClusterCompute(self):\n <mask>         result = Result()\n <mask> \n <mask>         self._succeed_until(\"driver_setup\")\n <mask> \n <mask>         # These commands should succeed\n <mask>         self.command_runner_return[\"prepare_local_env\"] = None\n <mask> \n <mask>         # Fails because API response faulty\n <mask>         with self.assertRaisesRegex(ClusterComputeCreateError, \"Unexpected\"):\n <mask>             self._run(result)\n <mask>         self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n <mask> \n <mask>         # Fails for random cluster compute reason\n <mask>         self.cluster_manager_return[\"create_cluster_compute\"] = _fail_on_call(\n <mask>             ClusterComputeCreateError, \"Known\"\n <mask>         )\n <mask>         with self.assertRaisesRegex(ClusterComputeCreateError, \"Known\"):\n <mask>             self._run(result)\n <mask>         self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n <mask> \n <mask>     def testBuildConfigFailsClusterEnv(self):\n <mask>         result = Result()\n <mask> \n <mask>         self._succeed_until(\"cluster_compute\")\n <mask> \n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove         self.command_runner_return[\"prepare_local_env\"] = None\n\n        if until == \"local_env\":\n            return\n\n        self.test[\"driver_setup\"] = \"driver_succeed.sh\"\n\n        if until == \"driver_setup\":\n            return\n\n </s> add  </s> remove     validate_test(test)\n </s> add  </s> remove             driver_setup=\"driver_fail.sh\",\n </s> add  </s> remove def _setup_local_environment(\n    test: Test,\n    command_runner: CommandRunner,\n    ray_wheels_url: str,\n) -> None:\n    driver_setup_script = test.get(\"driver_setup\", None)\n    if driver_setup_script:\n        try:\n            run_bash_script(driver_setup_script)\n        except Exception as e:\n            raise LocalEnvSetupError(f\"Driver setup script failed: {e}\") from e\n\n    # Install local dependencies\n    command_runner.prepare_local_env(ray_wheels_url)\n\n    # Re-install anyscale package as local dependencies might have changed\n    # from local env setup\n    reinstall_anyscale_dependencies()\n\n\n </s> add  </s> remove @patch(\"ray_release.glue.reinstall_anyscale_dependencies\", lambda: None)\n@patch(\"ray_release.glue.get_pip_packages\", lambda: [\"pip-packages\"])\n </s> add  </s> remove     pip_packages = get_pip_packages()\n    pip_package_string = \"\\n\".join(pip_packages)\n    logger.info(f\"Installed python packages:\\n{pip_package_string}\")\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/ray_release/tests/test_glue.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask> #  # Python version. This optional field determines which Python version to run tests\n <mask> #  # on. This must be a string!\n <mask> #  python: \"3.7\"\n <mask> #\n <mask> #  # Optional location of a bash setup script to run on the driver\n <mask> #  # when setting up the local environment. Relative to working_dir\n <mask> #  driver_setup: setup_driver.sh\n <mask> #\n <mask> #  # Cluster information\n <mask> #  cluster:\n <mask> #    # Location of cluster env, relative to working_dir\n <mask> #    cluster_env: cluster_env.yaml\n <mask> #    # Location of cluster compute, relative to working_dir\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove def _setup_local_environment(\n    test: Test,\n    command_runner: CommandRunner,\n    ray_wheels_url: str,\n) -> None:\n    driver_setup_script = test.get(\"driver_setup\", None)\n    if driver_setup_script:\n        try:\n            run_bash_script(driver_setup_script)\n        except Exception as e:\n            raise LocalEnvSetupError(f\"Driver setup script failed: {e}\") from e\n\n    # Install local dependencies\n    command_runner.prepare_local_env(ray_wheels_url)\n\n    # Re-install anyscale package as local dependencies might have changed\n    # from local env setup\n    reinstall_anyscale_dependencies()\n\n\n </s> add  </s> remove         buildkite_group(\":nut_and_bolt: Setting up local environment\")\n        _setup_local_environment(test, command_runner, ray_wheels_url)\n\n        # Print installed pip packages\n </s> add  </s> remove     validate_test(test)\n </s> add  </s> remove     def testAutomaticClusterEnvVariables(self):\n        result = Result()\n\n        self._succeed_until(\"local_env\")\n\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n\n        cluster_manager = self.instances[\"cluster_manager\"]\n\n        command_timeout = self.test[\"run\"].get(\"timeout\", DEFAULT_COMMAND_TIMEOUT)\n        prepare_cmd = self.test[\"run\"].get(\"prepare\", None)\n        if prepare_cmd:\n            prepare_timeout = self.test[\"run\"].get(\"prepare_timeout\", command_timeout)\n        else:\n            prepare_timeout = 0\n        command_and_prepare_timeout = command_timeout + prepare_timeout\n\n        wait_timeout = self.test[\"run\"][\"wait_for_nodes\"].get(\n            \"timeout\", DEFAULT_WAIT_FOR_NODES_TIMEOUT\n        )\n\n        expected_idle_termination_minutes = int(\n            command_and_prepare_timeout / 60 + TIMEOUT_BUFFER_MINUTES\n        )\n        expected_maximum_uptime_minutes = int(\n            expected_idle_termination_minutes + wait_timeout + TIMEOUT_BUFFER_MINUTES\n        )\n\n        self.assertEqual(\n            cluster_manager.cluster_compute[\"idle_termination_minutes\"],\n            expected_idle_termination_minutes,\n        )\n        self.assertEqual(\n            cluster_manager.cluster_compute[\"maximum_uptime_minutes\"],\n            expected_maximum_uptime_minutes,\n        )\n\n    def testInvalidPrepareLocalEnv(self):\n        result = Result()\n\n        self.command_runner_return[\"prepare_local_env\"] = _fail_on_call(\n            LocalEnvSetupError\n        )\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n\n    def testDriverSetupFails(self):\n        result = Result()\n\n        self._succeed_until(\"local_env\")\n\n        with self.assertRaises(LocalEnvSetupError):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.LOCAL_ENV_SETUP_ERROR.value)\n\n    def testInvalidClusterIdOverride(self):\n        result = Result()\n\n        self._succeed_until(\"driver_setup\")\n\n        self.sdk.returns[\"get_cluster_environment\"] = None\n\n        with self.assertRaises(ClusterEnvCreateError):\n            self._run(result, cluster_env_id=\"existing\")\n\n        self.sdk.returns[\"get_cluster_environment\"] = APIDict(\n            result=APIDict(config_json={\"overridden\": True})\n        )\n\n        with self.assertRaises(Exception) as cm:  # Fail somewhere else\n            self._run(result, cluster_env_id=\"existing\")\n            self.assertNotIsInstance(cm.exception, ClusterEnvCreateError)\n\n    def testBuildConfigFailsClusterCompute(self):\n        result = Result()\n\n        self._succeed_until(\"driver_setup\")\n\n        # These commands should succeed\n        self.command_runner_return[\"prepare_local_env\"] = None\n\n        # Fails because API response faulty\n        with self.assertRaisesRegex(ClusterComputeCreateError, \"Unexpected\"):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n\n        # Fails for random cluster compute reason\n        self.cluster_manager_return[\"create_cluster_compute\"] = _fail_on_call(\n            ClusterComputeCreateError, \"Known\"\n        )\n        with self.assertRaisesRegex(ClusterComputeCreateError, \"Known\"):\n            self._run(result)\n        self.assertEqual(result.return_code, ExitCode.CLUSTER_RESOURCE_ERROR.value)\n\n </s> add  </s> remove         self.command_runner_return[\"prepare_local_env\"] = None\n\n        if until == \"local_env\":\n            return\n\n        self.test[\"driver_setup\"] = \"driver_succeed.sh\"\n\n        if until == \"driver_setup\":\n            return\n\n </s> add  </s> remove   driver_setup: tune_rllib/driver_setup.sh\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/release_tests.yaml"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   cluster:\n <mask>     cluster_env: horovod/app_config.yaml\n <mask>     cluster_compute: horovod/compute_tpl_aws.yaml\n <mask> \n <mask>   driver_setup: horovod/driver_setup_latest.sh\n <mask>   run:\n <mask>     timeout: 1200\n <mask>     script: python horovod/horovod_user_test.py\n <mask>     wait_for_nodes:\n <mask>       num_nodes: 4\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove   driver_setup: horovod/driver_setup_master.sh\n </s> add  </s> remove   driver_setup: ray-lightning/driver_setup.sh\n </s> add  </s> remove   driver_setup: ray-lightning/driver_setup.sh\n </s> add  </s> remove   driver_setup: tune_rllib/driver_setup.sh\n </s> add  </s> remove   driver_setup: train/driver_setup.sh\n </s> add  </s> remove   driver_setup: train/driver_setup.sh\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/release_tests.yaml"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   cluster:\n <mask>     cluster_env: horovod/app_config_master.yaml\n <mask>     cluster_compute: horovod/compute_tpl_aws.yaml\n <mask> \n <mask>   driver_setup: horovod/driver_setup_master.sh\n <mask>   run:\n <mask>     timeout: 1200\n <mask>     script: python horovod/horovod_user_test.py\n <mask>     wait_for_nodes:\n <mask>       num_nodes: 4\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove   driver_setup: horovod/driver_setup_latest.sh\n </s> add  </s> remove   driver_setup: ray-lightning/driver_setup.sh\n </s> add  </s> remove   driver_setup: ray-lightning/driver_setup.sh\n </s> add  </s> remove   driver_setup: tune_rllib/driver_setup.sh\n </s> add  </s> remove   driver_setup: train/driver_setup.sh\n </s> add  </s> remove   driver_setup: train/driver_setup.sh\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/release_tests.yaml"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   cluster:\n <mask>     cluster_env: train/app_config.yaml\n <mask>     cluster_compute: train/compute_tpl_aws.yaml\n <mask> \n <mask>   driver_setup: train/driver_setup.sh\n <mask>   run:\n <mask>     timeout: 36000\n <mask>     script: python train/train_tensorflow_mnist_test.py\n <mask>     wait_for_nodes:\n <mask>       num_nodes: 3\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove   driver_setup: train/driver_setup.sh\n </s> add  </s> remove   driver_setup: ray-lightning/driver_setup.sh\n </s> add  </s> remove   driver_setup: ray-lightning/driver_setup.sh\n </s> add  </s> remove   driver_setup: tune_rllib/driver_setup.sh\n </s> add  </s> remove   driver_setup: horovod/driver_setup_master.sh\n </s> add  </s> remove   driver_setup: horovod/driver_setup_latest.sh\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/release_tests.yaml"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   cluster:\n <mask>     cluster_env: train/app_config.yaml\n <mask>     cluster_compute: train/compute_tpl_aws.yaml\n <mask> \n <mask>   driver_setup: train/driver_setup.sh\n <mask>   run:\n <mask>     timeout: 36000\n <mask>     script: python train/train_torch_linear_test.py\n <mask>     wait_for_nodes:\n <mask>       num_nodes: 3\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove   driver_setup: train/driver_setup.sh\n </s> add  </s> remove   driver_setup: ray-lightning/driver_setup.sh\n </s> add  </s> remove   driver_setup: ray-lightning/driver_setup.sh\n </s> add  </s> remove   driver_setup: tune_rllib/driver_setup.sh\n </s> add  </s> remove   driver_setup: horovod/driver_setup_master.sh\n </s> add  </s> remove   driver_setup: horovod/driver_setup_latest.sh\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/release_tests.yaml"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   cluster:\n <mask>     cluster_env: ray-lightning/app_config.yaml\n <mask>     cluster_compute: ray-lightning/compute_tpl_aws.yaml\n <mask> \n <mask>   driver_setup: ray-lightning/driver_setup.sh\n <mask>   run:\n <mask>     timeout: 1200\n <mask>     script: python ray-lightning/ray_lightning_user_test.py\n <mask>     wait_for_nodes:\n <mask>       num_nodes: 3\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove   driver_setup: ray-lightning/driver_setup.sh\n </s> add  </s> remove   driver_setup: train/driver_setup.sh\n </s> add  </s> remove   driver_setup: train/driver_setup.sh\n </s> add  </s> remove   driver_setup: horovod/driver_setup_master.sh\n </s> add  </s> remove   driver_setup: horovod/driver_setup_latest.sh\n </s> add  </s> remove   driver_setup: tune_rllib/driver_setup.sh\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/release_tests.yaml"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   cluster:\n <mask>     cluster_env: ray-lightning/app_config_master.yaml\n <mask>     cluster_compute: ray-lightning/compute_tpl_aws.yaml\n <mask> \n <mask>   driver_setup: ray-lightning/driver_setup.sh\n <mask>   run:\n <mask>     timeout: 1200\n <mask>     script: python ray-lightning/ray_lightning_user_test.py\n <mask>     wait_for_nodes:\n <mask>       num_nodes: 3\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove   driver_setup: ray-lightning/driver_setup.sh\n </s> add  </s> remove   driver_setup: train/driver_setup.sh\n </s> add  </s> remove   driver_setup: train/driver_setup.sh\n </s> add  </s> remove   driver_setup: horovod/driver_setup_master.sh\n </s> add  </s> remove   driver_setup: horovod/driver_setup_latest.sh\n </s> add  </s> remove   driver_setup: tune_rllib/driver_setup.sh\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/release_tests.yaml"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   cluster:\n <mask>     cluster_env: ../rllib_tests/app_config.yaml\n <mask>     cluster_compute: tune_rllib/compute_tpl_aws.yaml\n <mask> \n <mask>   driver_setup: tune_rllib/driver_setup.sh\n <mask>   run:\n <mask>     timeout: 2000\n <mask>     script: python tune_rllib/run_connect_tests.py\n <mask>     wait_for_nodes:\n <mask>       num_nodes: 9\n </s> [ci][byod/1] clean up local environment setup for release tests (#35355)\n\nAll release tests are using remote execution via anyscale at this point. Remove code path for local environment setup. In particular, the driver_setup is used to install packages on buildkite host, which is no longer neccessary/used.\r\n\r\nSigned-off-by: Cuong Nguyen <can@anyscale.com> </s> remove   driver_setup: ray-lightning/driver_setup.sh\n </s> add  </s> remove   driver_setup: train/driver_setup.sh\n </s> add  </s> remove   driver_setup: train/driver_setup.sh\n </s> add  </s> remove   driver_setup: horovod/driver_setup_master.sh\n </s> add  </s> remove   driver_setup: horovod/driver_setup_latest.sh\n </s> add  </s> remove   driver_setup: ray-lightning/driver_setup.sh\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8f4a0e4c92ccfe232535cfa2b7daae266d3f57bb", "file_name": "release/release_tests.yaml"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask>         def ready(self):\n <mask>             return\n <mask> \n <mask>     actor = RestartableActor.remote()\n <mask>     ray.get(actor.ready.remote())\n <mask>     results = [actor.increase.remote() for _ in range(100)]\n <mask>     # Kill actor node, while the above task is still being executed.\n <mask>     cluster.remove_node(actor_node)\n <mask>     cluster.add_node(num_cpus=1, _internal_config=config)\n </s> GCS server error handling for actor creation (#8899) </s> remove     @ray.remote\n    def increase(x):\n        return x + 1\n </s> add @ray.remote\ndef increase(x):\n    return x + 1 </s> remove     ray.shutdown()\n </s> add def test_gcs_server_restart_during_actor_creation(ray_start_regular):\n    ids = []\n    for i in range(0, 100):\n        actor = Increase.remote()\n        ids.append(actor.method.remote(1))\n\n    ray.worker._global_node.kill_gcs_server()\n    ray.worker._global_node.start_gcs_server()\n\n    ready, unready = ray.wait(ids, 100, 240)\n    print(\"Ready objects is {}.\".format(ready))\n    print(\"Unready objects is {}.\".format(unready))\n    assert len(unready) == 0 </s> add def test_gcs_server_restart(ray_start_regular): </s> add   // If GCS server is restarted after sending an actor creation task to this core worker,\n  // the restarted GCS server will send the same actor creation task to the core worker\n  // again. We just need to ignore it and reply ok.\n  if (task_spec.IsActorCreationTask() &&\n      worker_context_.GetCurrentActorID() == task_spec.ActorCreationId()) {\n    send_reply_callback(Status::OK(), nullptr, nullptr);\n    RAY_LOG(INFO) << \"Ignoring duplicate actor creation task for actor \"\n                  << task_spec.ActorCreationId()\n                  << \". This is likely due to a GCS server restart.\";\n    return;\n  }\n </s> remove   // The backend storage is reliable in the future, so the status must be ok.\n  RAY_CHECK_OK(gcs_actor_table_.Put(\n      actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n        RAY_CHECK_OK(status);\n        RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n                                           actor->GetActorTableData().SerializeAsString(),\n                                           nullptr));\n        // There is no promise that the node the\n        // actor tied to is still alive as the\n        // flush is asynchronously, so just\n        // invoke `Schedule` which will lease\n        // worker directly if the node is still\n        // available or select a new one if not.\n        Schedule(actor);\n      }));\n </s> add   RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                .emplace(actor->GetActorID())\n                .second);\n\n  // Lease worker directly from the node.\n  LeaseWorkerFromNode(actor, node);\n}\n\nvoid GcsActorScheduler::Reschedule(std::shared_ptr<GcsActor> actor) {\n  if (!actor->GetWorkerID().IsNil()) {\n    RAY_LOG(INFO)\n        << \"Actor \" << actor->GetActorID()\n        << \" is already tied to a leased worker. Create actor directly on worker.\";\n    auto leased_worker = std::make_shared<GcsLeasedWorker>(\n        actor->GetAddress(),\n        VectorFromProtobuf(actor->GetMutableActorTableData()->resource_mapping()),\n        actor->GetActorID());\n    auto iter_node = node_to_workers_when_creating_.find(actor->GetNodeID());\n    if (iter_node != node_to_workers_when_creating_.end()) {\n      if (0 == iter_node->second.count(leased_worker->GetWorkerID())) {\n        iter_node->second.emplace(leased_worker->GetWorkerID(), leased_worker);\n      }\n    } else {\n      node_to_workers_when_creating_[actor->GetNodeID()].emplace(\n          leased_worker->GetWorkerID(), leased_worker);\n    }\n    CreateActorOnWorker(actor, leased_worker);\n  } else {\n    Schedule(actor);\n  } </s> remove     @ray.remote\n    class Increase:\n        def method(self, x):\n            return x + 2\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "python/ray/tests/test_actor_failures.py"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> import ray\n <mask> \n <mask> \n <mask> def test_gcs_server_restart():\n <mask>     ray.init()\n <mask> \n <mask>     @ray.remote\n <mask>     class Increase:\n <mask>         def method(self, x):\n <mask>             return x + 2\n </s> GCS server error handling for actor creation (#8899) </s> remove     @ray.remote\n    class Increase:\n        def method(self, x):\n            return x + 2\n </s> add  </s> remove     @ray.remote\n    def increase(x):\n        return x + 1\n </s> add @ray.remote\ndef increase(x):\n    return x + 1 </s> add def test_gcs_server_restart(ray_start_regular): </s> remove     ray.shutdown()\n </s> add def test_gcs_server_restart_during_actor_creation(ray_start_regular):\n    ids = []\n    for i in range(0, 100):\n        actor = Increase.remote()\n        ids.append(actor.method.remote(1))\n\n    ray.worker._global_node.kill_gcs_server()\n    ray.worker._global_node.start_gcs_server()\n\n    ready, unready = ray.wait(ids, 100, 240)\n    print(\"Ready objects is {}.\".format(ready))\n    print(\"Unready objects is {}.\".format(unready))\n    assert len(unready) == 0 </s> remove     actor = RestartableActor.remote()\n </s> add     actor = RestartableActor.options(detached=True).remote() </s> add const absl::flat_hash_map<ClientID, absl::flat_hash_map<WorkerID, ActorID>>\n    &GcsActorManager::GetCreatedActors() const {\n  return created_actors_;\n}\n", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "python/ray/tests/test_gcs_fault_tolerance.py"}
{"docstring_tokens": "keep keep replace replace replace replace keep replace replace replace keep keep keep", "code_tokens": " <mask>     ray.init()\n <mask> \n <mask>     @ray.remote\n <mask>     class Increase:\n <mask>         def method(self, x):\n <mask>             return x + 2\n <mask> \n <mask>     @ray.remote\n <mask>     def increase(x):\n <mask>         return x + 1\n <mask> \n <mask>     actor1 = Increase.remote()\n <mask>     result = ray.get(actor1.method.remote(1))\n </s> GCS server error handling for actor creation (#8899) </s> remove def test_gcs_server_restart():\n    ray.init()\n </s> add @ray.remote\nclass Increase:\n    def method(self, x):\n        return x + 2 </s> add def test_gcs_server_restart(ray_start_regular): </s> remove     ray.shutdown()\n </s> add def test_gcs_server_restart_during_actor_creation(ray_start_regular):\n    ids = []\n    for i in range(0, 100):\n        actor = Increase.remote()\n        ids.append(actor.method.remote(1))\n\n    ray.worker._global_node.kill_gcs_server()\n    ray.worker._global_node.start_gcs_server()\n\n    ready, unready = ray.wait(ids, 100, 240)\n    print(\"Ready objects is {}.\".format(ready))\n    print(\"Unready objects is {}.\".format(unready))\n    assert len(unready) == 0 </s> remove     actor = RestartableActor.remote()\n </s> add     actor = RestartableActor.options(detached=True).remote() </s> add     std::promise<bool> promise;\n    thread_io_service_.reset(new std::thread([this, &promise] {\n      std::unique_ptr<boost::asio::io_service::work> work(\n          new boost::asio::io_service::work(io_service_));\n      promise.set_value(true);\n      io_service_.run();\n    }));\n    promise.get_future().get();\n", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "python/ray/tests/test_gcs_fault_tolerance.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask> @ray.remote\n <mask> def increase(x):\n <mask>     return x + 1\n <mask> \n <mask>     actor1 = Increase.remote()\n <mask>     result = ray.get(actor1.method.remote(1))\n <mask>     assert result == 3\n <mask> \n <mask>     ray.worker._global_node.kill_gcs_server()\n </s> GCS server error handling for actor creation (#8899) </s> remove     @ray.remote\n    def increase(x):\n        return x + 1\n </s> add @ray.remote\ndef increase(x):\n    return x + 1 </s> remove     @ray.remote\n    class Increase:\n        def method(self, x):\n            return x + 2\n </s> add  </s> remove     ray.shutdown()\n </s> add def test_gcs_server_restart_during_actor_creation(ray_start_regular):\n    ids = []\n    for i in range(0, 100):\n        actor = Increase.remote()\n        ids.append(actor.method.remote(1))\n\n    ray.worker._global_node.kill_gcs_server()\n    ray.worker._global_node.start_gcs_server()\n\n    ready, unready = ray.wait(ids, 100, 240)\n    print(\"Ready objects is {}.\".format(ready))\n    print(\"Unready objects is {}.\".format(unready))\n    assert len(unready) == 0 </s> remove def test_gcs_server_restart():\n    ray.init()\n </s> add @ray.remote\nclass Increase:\n    def method(self, x):\n        return x + 2 </s> remove     actor = RestartableActor.remote()\n </s> add     actor = RestartableActor.options(detached=True).remote() </s> remove   int load_count = 3;\n </s> add   int load_count = 2;", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "python/ray/tests/test_gcs_fault_tolerance.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     assert result == 4\n <mask> \n <mask>     result = ray.get(increase.remote(1))\n <mask>     assert result == 2\n <mask>     ray.shutdown()\n <mask> \n <mask> \n <mask> if __name__ == \"__main__\":\n <mask>     import pytest\n <mask>     sys.exit(pytest.main([\"-v\", __file__]))\n </s> GCS server error handling for actor creation (#8899) </s> add def test_gcs_server_restart(ray_start_regular): </s> remove     @ray.remote\n    def increase(x):\n        return x + 1\n </s> add @ray.remote\ndef increase(x):\n    return x + 1 </s> remove def test_gcs_server_restart():\n    ray.init()\n </s> add @ray.remote\nclass Increase:\n    def method(self, x):\n        return x + 2 </s> remove   int load_count = 3;\n </s> add   int load_count = 2; </s> add     // We will reschedule the unfinished actors, so we have to load the actor data at the\n    // end to make sure the other table data is loaded. </s> remove       // Start RPC server when all tables have finished loading initial data.\n      rpc_server_.Run();\n\n      // Store gcs rpc server address in redis.\n      StoreGcsServerAddressInRedis();\n      is_started_ = true;\n </s> add       auto actor_manager_load_initial_data_callback = [this]() {\n        // Start RPC server when all tables have finished loading initial data.\n        rpc_server_.Run();\n\n        // Store gcs rpc server address in redis.\n        StoreGcsServerAddressInRedis();\n        is_started_ = true;\n      };\n      gcs_actor_manager_->LoadInitialData(actor_manager_load_initial_data_callback);", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "python/ray/tests/test_gcs_fault_tolerance.py"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>                                std::vector<std::shared_ptr<RayObject>> *return_objects,\n <mask>                                ReferenceCounter::ReferenceTableProto *borrowed_refs) {\n <mask>   task_queue_length_ -= 1;\n <mask>   num_executed_tasks_ += 1;\n <mask> \n <mask>   if (!options_.is_local_mode) {\n <mask>     worker_context_.SetCurrentTask(task_spec);\n <mask>     SetCurrentTaskId(task_spec.TaskId());\n </s> GCS server error handling for actor creation (#8899) </s> add   RAY_LOG(INFO) << \"Removing node, node id = \" << node_id; </s> remove   // The backend storage is reliable in the future, so the status must be ok.\n  RAY_CHECK_OK(gcs_actor_table_.Put(\n      actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n        RAY_CHECK_OK(status);\n        RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n                                           actor->GetActorTableData().SerializeAsString(),\n                                           nullptr));\n        // There is no promise that the node the\n        // actor tied to is still alive as the\n        // flush is asynchronously, so just\n        // invoke `Schedule` which will lease\n        // worker directly if the node is still\n        // available or select a new one if not.\n        Schedule(actor);\n      }));\n </s> add   RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                .emplace(actor->GetActorID())\n                .second);\n\n  // Lease worker directly from the node.\n  LeaseWorkerFromNode(actor, node);\n}\n\nvoid GcsActorScheduler::Reschedule(std::shared_ptr<GcsActor> actor) {\n  if (!actor->GetWorkerID().IsNil()) {\n    RAY_LOG(INFO)\n        << \"Actor \" << actor->GetActorID()\n        << \" is already tied to a leased worker. Create actor directly on worker.\";\n    auto leased_worker = std::make_shared<GcsLeasedWorker>(\n        actor->GetAddress(),\n        VectorFromProtobuf(actor->GetMutableActorTableData()->resource_mapping()),\n        actor->GetActorID());\n    auto iter_node = node_to_workers_when_creating_.find(actor->GetNodeID());\n    if (iter_node != node_to_workers_when_creating_.end()) {\n      if (0 == iter_node->second.count(leased_worker->GetWorkerID())) {\n        iter_node->second.emplace(leased_worker->GetWorkerID(), leased_worker);\n      }\n    } else {\n      node_to_workers_when_creating_[actor->GetNodeID()].emplace(\n          leased_worker->GetWorkerID(), leased_worker);\n    }\n    CreateActorOnWorker(actor, leased_worker);\n  } else {\n    Schedule(actor);\n  } </s> remove   auto worker_id = actor->GetWorkerID();\n  auto node_id = actor->GetNodeID();\n  RAY_CHECK(!worker_id.IsNil());\n  RAY_CHECK(!node_id.IsNil());\n  RAY_CHECK(created_actors_[node_id].emplace(worker_id, actor_id).second);\n </s> add         auto worker_id = actor->GetWorkerID();\n        auto node_id = actor->GetNodeID();\n        RAY_CHECK(!worker_id.IsNil());\n        RAY_CHECK(!node_id.IsNil());\n        RAY_CHECK(created_actors_[node_id].emplace(worker_id, actor_id).second);\n      })); </s> remove     actor->UpdateAddress(retry_at_raylet_address);\n    // The backend storage is reliable in the future, so the status must be ok.\n    RAY_CHECK_OK(gcs_actor_table_.Put(\n        actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n          RAY_CHECK_OK(status);\n          RAY_CHECK_OK(gcs_pub_sub_->Publish(\n              ACTOR_CHANNEL, actor->GetActorID().Hex(),\n              actor->GetActorTableData().SerializeAsString(), nullptr));\n          Schedule(actor);\n        }));\n </s> add     auto spill_back_node_id = ClientID::FromBinary(retry_at_raylet_address.raylet_id());\n    if (auto spill_back_node = gcs_node_manager_.GetNode(spill_back_node_id)) {\n      actor->UpdateAddress(retry_at_raylet_address);\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, spill_back_node);\n    } else {\n      // If the spill back node is dead, we need to schedule again.\n      actor->UpdateAddress(rpc::Address());\n      actor->GetMutableActorTableData()->clear_resource_mapping();\n      Schedule(actor);\n    } </s> remove   int load_count = 3;\n </s> add   int load_count = 2; </s> remove   // Invoke all callbacks for all registration requests of this actor (duplicated\n  // requests are included) and remove all of them from actor_to_register_callbacks_.\n  auto iter = actor_to_register_callbacks_.find(actor_id);\n  if (iter != actor_to_register_callbacks_.end()) {\n    for (auto &callback : iter->second) {\n      callback(actor);\n    }\n    actor_to_register_callbacks_.erase(iter);\n  }\n </s> add         // Invoke all callbacks for all registration requests of this actor (duplicated\n        // requests are included) and remove all of them from\n        // actor_to_register_callbacks_.\n        auto iter = actor_to_register_callbacks_.find(actor_id);\n        if (iter != actor_to_register_callbacks_.end()) {\n          for (auto &callback : iter->second) {\n            callback(actor);\n          }\n          actor_to_register_callbacks_.erase(iter);\n        }", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/core_worker/core_worker.cc"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>     rpc::SendReplyCallback send_reply_callback) {\n <mask>   RAY_CHECK(waiter_ != nullptr) << \"Must call init() prior to use\";\n <mask>   const TaskSpecification task_spec(request.task_spec());\n <mask>   std::vector<ObjectID> dependencies;\n <mask>   for (size_t i = 0; i < task_spec.NumArgs(); ++i) {\n <mask>     int count = task_spec.ArgIdCount(i);\n <mask>     for (int j = 0; j < count; j++) {\n <mask>       dependencies.push_back(task_spec.ArgId(i, j));\n </s> GCS server error handling for actor creation (#8899) </s> add   RAY_LOG(INFO) << \"Removing node, node id = \" << node_id; </s> remove   RAY_LOG(INFO) << \"Node \" << node_id << \" failed, reconstructing actors\";\n </s> add   RAY_LOG(WARNING) << \"Node \" << node_id << \" failed, reconstructing actors.\"; </s> add     RAY_LOG(DEBUG) << \"The number of registered actors is \" << registered_actors_.size()\n                   << \", and the number of created actors is \" << created_actors_.size();\n    for (auto &item : registered_actors_) {\n      auto &actor = item.second;\n      if (actor->GetState() != ray::rpc::ActorTableData::ALIVE) {\n        RAY_LOG(DEBUG) << \"Rescheduling a non-alive actor, actor id = \"\n                       << actor->GetActorID() << \", state = \" << actor->GetState();\n        gcs_actor_scheduler_->Reschedule(actor);\n      }\n    }\n </s> remove   // Invoke all callbacks for all registration requests of this actor (duplicated\n  // requests are included) and remove all of them from actor_to_register_callbacks_.\n  auto iter = actor_to_register_callbacks_.find(actor_id);\n  if (iter != actor_to_register_callbacks_.end()) {\n    for (auto &callback : iter->second) {\n      callback(actor);\n    }\n    actor_to_register_callbacks_.erase(iter);\n  }\n </s> add         // Invoke all callbacks for all registration requests of this actor (duplicated\n        // requests are included) and remove all of them from\n        // actor_to_register_callbacks_.\n        auto iter = actor_to_register_callbacks_.find(actor_id);\n        if (iter != actor_to_register_callbacks_.end()) {\n          for (auto &callback : iter->second) {\n            callback(actor);\n          }\n          actor_to_register_callbacks_.erase(iter);\n        } </s> remove   auto node_id = actor->GetNodeID();\n  if (!node_id.IsNil()) {\n    if (auto node = gcs_node_manager_.GetNode(node_id)) {\n      // If the actor is already tied to a node and the node is available, then record\n      // the relationship of the node and actor and then lease worker directly from the\n      // node.\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, node);\n      return;\n    }\n\n    // The actor is already tied to a node which is unavailable now, so we should reset\n    // the address.\n    actor->UpdateAddress(rpc::Address());\n  }\n </s> add   RAY_CHECK(actor->GetNodeID().IsNil() && actor->GetWorkerID().IsNil()); </s> remove     ray.shutdown()\n </s> add def test_gcs_server_restart_during_actor_creation(ray_start_regular):\n    ids = []\n    for i in range(0, 100):\n        actor = Increase.remote()\n        ids.append(actor.method.remote(1))\n\n    ray.worker._global_node.kill_gcs_server()\n    ray.worker._global_node.start_gcs_server()\n\n    ready, unready = ray.wait(ids, 100, 240)\n    print(\"Ready objects is {}.\".format(ready))\n    print(\"Unready objects is {}.\".format(unready))\n    assert len(unready) == 0", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/core_worker/transport/direct_actor_transport.cc"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>     actor_id = gcs_actor_scheduler_->CancelOnWorker(node_id, worker_id);\n <mask>   }\n <mask> \n <mask>   if (!actor_id.IsNil()) {\n <mask>     RAY_LOG(INFO) << \"Worker \" << worker_id << \" on node \" << node_id\n <mask>                   << \" failed, restarting actor \" << actor_id;\n <mask>     // Reconstruct the actor.\n <mask>     ReconstructActor(actor_id, /*need_reschedule=*/!intentional_exit);\n <mask>   }\n <mask> }\n <mask> \n </s> GCS server error handling for actor creation (#8899) </s> remove   RAY_LOG(INFO) << \"Node \" << node_id << \" failed, reconstructing actors\";\n </s> add   RAY_LOG(WARNING) << \"Node \" << node_id << \" failed, reconstructing actors.\"; </s> add     RAY_LOG(DEBUG) << \"The number of registered actors is \" << registered_actors_.size()\n                   << \", and the number of created actors is \" << created_actors_.size();\n    for (auto &item : registered_actors_) {\n      auto &actor = item.second;\n      if (actor->GetState() != ray::rpc::ActorTableData::ALIVE) {\n        RAY_LOG(DEBUG) << \"Rescheduling a non-alive actor, actor id = \"\n                       << actor->GetActorID() << \", state = \" << actor->GetState();\n        gcs_actor_scheduler_->Reschedule(actor);\n      }\n    }\n </s> remove     RAY_LOG(WARNING) << \"Submitted task \" << task_id\n                     << \" is already queued and will not be restarted. This is most \"\n                        \"likely due to spurious reconstruction.\";\n    return;\n </s> add     if (spec.IsActorCreationTask()) {\n      RAY_LOG(WARNING) << \"Submitted actor creation task \" << task_id\n                       << \" is already queued. This is most likely due to a GCS restart. \"\n                          \"We will remove \"\n                          \"the old one from the queue, and enqueue the new one.\";\n      std::unordered_set<TaskID> task_ids{task_id};\n      local_queues_.RemoveTasks(task_ids);\n    } else {\n      RAY_LOG(WARNING) << \"Submitted task \" << task_id\n                       << \" is already queued and will not be restarted. This is most \"\n                          \"likely due to spurious reconstruction.\";\n      return;\n    } </s> add   RAY_LOG(DEBUG) << \"Actor created successfully, actor id = \" << actor_id; </s> add   RAY_LOG(INFO) << \"Removing node, node id = \" << node_id; </s> add   RAY_LOG(DEBUG) << \"Executing task, task info = \" << task_spec.DebugString();", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_actor_manager.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   }\n <mask> }\n <mask> \n <mask> void GcsActorManager::OnNodeDead(const ClientID &node_id) {\n <mask>   RAY_LOG(INFO) << \"Node \" << node_id << \" failed, reconstructing actors\";\n <mask>   const auto it = owners_.find(node_id);\n <mask>   if (it != owners_.end()) {\n <mask>     std::vector<ActorID> children_ids;\n <mask>     // Make a copy of all the actor IDs owned by workers on the dead node.\n <mask>     for (const auto &owner : it->second) {\n </s> GCS server error handling for actor creation (#8899) </s> remove     RAY_LOG(INFO) << \"Worker \" << worker_id << \" on node \" << node_id\n                  << \" failed, restarting actor \" << actor_id;\n </s> add     RAY_LOG(WARNING) << \"Worker \" << worker_id << \" on node \" << node_id\n                     << \" failed, restarting actor \" << actor_id; </s> add     RAY_LOG(DEBUG) << \"The number of registered actors is \" << registered_actors_.size()\n                   << \", and the number of created actors is \" << created_actors_.size();\n    for (auto &item : registered_actors_) {\n      auto &actor = item.second;\n      if (actor->GetState() != ray::rpc::ActorTableData::ALIVE) {\n        RAY_LOG(DEBUG) << \"Rescheduling a non-alive actor, actor id = \"\n                       << actor->GetActorID() << \", state = \" << actor->GetState();\n        gcs_actor_scheduler_->Reschedule(actor);\n      }\n    }\n </s> remove   // The backend storage is reliable in the future, so the status must be ok.\n  RAY_CHECK_OK(gcs_actor_table_.Put(\n      actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n        RAY_CHECK_OK(status);\n        RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n                                           actor->GetActorTableData().SerializeAsString(),\n                                           nullptr));\n        // There is no promise that the node the\n        // actor tied to is still alive as the\n        // flush is asynchronously, so just\n        // invoke `Schedule` which will lease\n        // worker directly if the node is still\n        // available or select a new one if not.\n        Schedule(actor);\n      }));\n </s> add   RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                .emplace(actor->GetActorID())\n                .second);\n\n  // Lease worker directly from the node.\n  LeaseWorkerFromNode(actor, node);\n}\n\nvoid GcsActorScheduler::Reschedule(std::shared_ptr<GcsActor> actor) {\n  if (!actor->GetWorkerID().IsNil()) {\n    RAY_LOG(INFO)\n        << \"Actor \" << actor->GetActorID()\n        << \" is already tied to a leased worker. Create actor directly on worker.\";\n    auto leased_worker = std::make_shared<GcsLeasedWorker>(\n        actor->GetAddress(),\n        VectorFromProtobuf(actor->GetMutableActorTableData()->resource_mapping()),\n        actor->GetActorID());\n    auto iter_node = node_to_workers_when_creating_.find(actor->GetNodeID());\n    if (iter_node != node_to_workers_when_creating_.end()) {\n      if (0 == iter_node->second.count(leased_worker->GetWorkerID())) {\n        iter_node->second.emplace(leased_worker->GetWorkerID(), leased_worker);\n      }\n    } else {\n      node_to_workers_when_creating_[actor->GetNodeID()].emplace(\n          leased_worker->GetWorkerID(), leased_worker);\n    }\n    CreateActorOnWorker(actor, leased_worker);\n  } else {\n    Schedule(actor);\n  } </s> remove     RAY_LOG(WARNING) << \"Submitted task \" << task_id\n                     << \" is already queued and will not be restarted. This is most \"\n                        \"likely due to spurious reconstruction.\";\n    return;\n </s> add     if (spec.IsActorCreationTask()) {\n      RAY_LOG(WARNING) << \"Submitted actor creation task \" << task_id\n                       << \" is already queued. This is most likely due to a GCS restart. \"\n                          \"We will remove \"\n                          \"the old one from the queue, and enqueue the new one.\";\n      std::unordered_set<TaskID> task_ids{task_id};\n      local_queues_.RemoveTasks(task_ids);\n    } else {\n      RAY_LOG(WARNING) << \"Submitted task \" << task_id\n                       << \" is already queued and will not be restarted. This is most \"\n                          \"likely due to spurious reconstruction.\";\n      return;\n    } </s> add   RAY_LOG(INFO) << \"Removing node, node id = \" << node_id; </s> add   // If GCS server is restarted after sending an actor creation task to this core worker,\n  // the restarted GCS server will send the same actor creation task to the core worker\n  // again. We just need to ignore it and reply ok.\n  if (task_spec.IsActorCreationTask() &&\n      worker_context_.GetCurrentActorID() == task_spec.ActorCreationId()) {\n    send_reply_callback(Status::OK(), nullptr, nullptr);\n    RAY_LOG(INFO) << \"Ignoring duplicate actor creation task for actor \"\n                  << task_spec.ActorCreationId()\n                  << \". This is likely due to a GCS server restart.\";\n    return;\n  }\n", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_actor_manager.cc"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask>     mutable_actor_table_data->set_num_restarts(++num_restarts);\n <mask>     mutable_actor_table_data->set_state(rpc::ActorTableData::RESTARTING);\n <mask>     // The backend storage is reliable in the future, so the status must be ok.\n <mask>     RAY_CHECK_OK(gcs_table_storage_->ActorTable().Put(\n <mask>         actor_id, *mutable_actor_table_data,\n <mask>         [this, actor_id, mutable_actor_table_data](Status status) {\n </s> GCS server error handling for actor creation (#8899) </s> remove       actor_id, actor_table_data, [this, actor_id, actor_table_data](Status status) {\n </s> add       actor_id, actor_table_data,\n      [this, actor_id, actor_table_data, actor](Status status) { </s> add   RAY_LOG(DEBUG) << \"Actor created successfully, actor id = \" << actor_id; </s> remove   // Update the address of the actor as it is tied to a new node.\n </s> add   // Update the address of the actor as it is tied to a node. </s> remove     actor->UpdateAddress(retry_at_raylet_address);\n    // The backend storage is reliable in the future, so the status must be ok.\n    RAY_CHECK_OK(gcs_actor_table_.Put(\n        actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n          RAY_CHECK_OK(status);\n          RAY_CHECK_OK(gcs_pub_sub_->Publish(\n              ACTOR_CHANNEL, actor->GetActorID().Hex(),\n              actor->GetActorTableData().SerializeAsString(), nullptr));\n          Schedule(actor);\n        }));\n </s> add     auto spill_back_node_id = ClientID::FromBinary(retry_at_raylet_address.raylet_id());\n    if (auto spill_back_node = gcs_node_manager_.GetNode(spill_back_node_id)) {\n      actor->UpdateAddress(retry_at_raylet_address);\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, spill_back_node);\n    } else {\n      // If the spill back node is dead, we need to schedule again.\n      actor->UpdateAddress(rpc::Address());\n      actor->GetMutableActorTableData()->clear_resource_mapping();\n      Schedule(actor);\n    } </s> remove   // The backend storage is reliable in the future, so the status must be ok.\n  RAY_CHECK_OK(gcs_actor_table_.Put(\n      actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n        RAY_CHECK_OK(status);\n        RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n                                           actor->GetActorTableData().SerializeAsString(),\n                                           nullptr));\n        // There is no promise that the node the\n        // actor tied to is still alive as the\n        // flush is asynchronously, so just\n        // invoke `Schedule` which will lease\n        // worker directly if the node is still\n        // available or select a new one if not.\n        Schedule(actor);\n      }));\n </s> add   RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                .emplace(actor->GetActorID())\n                .second);\n\n  // Lease worker directly from the node.\n  LeaseWorkerFromNode(actor, node);\n}\n\nvoid GcsActorScheduler::Reschedule(std::shared_ptr<GcsActor> actor) {\n  if (!actor->GetWorkerID().IsNil()) {\n    RAY_LOG(INFO)\n        << \"Actor \" << actor->GetActorID()\n        << \" is already tied to a leased worker. Create actor directly on worker.\";\n    auto leased_worker = std::make_shared<GcsLeasedWorker>(\n        actor->GetAddress(),\n        VectorFromProtobuf(actor->GetMutableActorTableData()->resource_mapping()),\n        actor->GetActorID());\n    auto iter_node = node_to_workers_when_creating_.find(actor->GetNodeID());\n    if (iter_node != node_to_workers_when_creating_.end()) {\n      if (0 == iter_node->second.count(leased_worker->GetWorkerID())) {\n        iter_node->second.emplace(leased_worker->GetWorkerID(), leased_worker);\n      }\n    } else {\n      node_to_workers_when_creating_[actor->GetNodeID()].emplace(\n          leased_worker->GetWorkerID(), leased_worker);\n    }\n    CreateActorOnWorker(actor, leased_worker);\n  } else {\n    Schedule(actor);\n  } </s> remove       }));\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_actor_manager.cc"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask> }\n <mask> \n <mask> void GcsActorManager::OnActorCreationSuccess(const std::shared_ptr<GcsActor> &actor) {\n <mask>   auto actor_id = actor->GetActorID();\n <mask>   RAY_CHECK(registered_actors_.count(actor_id) > 0);\n <mask>   actor->UpdateState(rpc::ActorTableData::ALIVE);\n <mask>   auto actor_table_data = actor->GetActorTableData();\n <mask>   // The backend storage is reliable in the future, so the status must be ok.\n <mask>   RAY_CHECK_OK(gcs_table_storage_->ActorTable().Put(\n <mask>       actor_id, actor_table_data,\n </s> GCS server error handling for actor creation (#8899) </s> remove       actor_id, actor_table_data, [this, actor_id, actor_table_data](Status status) {\n </s> add       actor_id, actor_table_data,\n      [this, actor_id, actor_table_data, actor](Status status) { </s> add     mutable_actor_table_data->clear_resource_mapping(); </s> remove   // Update the address of the actor as it is tied to a new node.\n </s> add   // Update the address of the actor as it is tied to a node. </s> remove     actor->UpdateAddress(retry_at_raylet_address);\n    // The backend storage is reliable in the future, so the status must be ok.\n    RAY_CHECK_OK(gcs_actor_table_.Put(\n        actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n          RAY_CHECK_OK(status);\n          RAY_CHECK_OK(gcs_pub_sub_->Publish(\n              ACTOR_CHANNEL, actor->GetActorID().Hex(),\n              actor->GetActorTableData().SerializeAsString(), nullptr));\n          Schedule(actor);\n        }));\n </s> add     auto spill_back_node_id = ClientID::FromBinary(retry_at_raylet_address.raylet_id());\n    if (auto spill_back_node = gcs_node_manager_.GetNode(spill_back_node_id)) {\n      actor->UpdateAddress(retry_at_raylet_address);\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, spill_back_node);\n    } else {\n      // If the spill back node is dead, we need to schedule again.\n      actor->UpdateAddress(rpc::Address());\n      actor->GetMutableActorTableData()->clear_resource_mapping();\n      Schedule(actor);\n    } </s> remove   // The backend storage is reliable in the future, so the status must be ok.\n  RAY_CHECK_OK(gcs_actor_table_.Put(\n      actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n        RAY_CHECK_OK(status);\n        RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n                                           actor->GetActorTableData().SerializeAsString(),\n                                           nullptr));\n        // There is no promise that the node the\n        // actor tied to is still alive as the\n        // flush is asynchronously, so just\n        // invoke `Schedule` which will lease\n        // worker directly if the node is still\n        // available or select a new one if not.\n        Schedule(actor);\n      }));\n </s> add   RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                .emplace(actor->GetActorID())\n                .second);\n\n  // Lease worker directly from the node.\n  LeaseWorkerFromNode(actor, node);\n}\n\nvoid GcsActorScheduler::Reschedule(std::shared_ptr<GcsActor> actor) {\n  if (!actor->GetWorkerID().IsNil()) {\n    RAY_LOG(INFO)\n        << \"Actor \" << actor->GetActorID()\n        << \" is already tied to a leased worker. Create actor directly on worker.\";\n    auto leased_worker = std::make_shared<GcsLeasedWorker>(\n        actor->GetAddress(),\n        VectorFromProtobuf(actor->GetMutableActorTableData()->resource_mapping()),\n        actor->GetActorID());\n    auto iter_node = node_to_workers_when_creating_.find(actor->GetNodeID());\n    if (iter_node != node_to_workers_when_creating_.end()) {\n      if (0 == iter_node->second.count(leased_worker->GetWorkerID())) {\n        iter_node->second.emplace(leased_worker->GetWorkerID(), leased_worker);\n      }\n    } else {\n      node_to_workers_when_creating_[actor->GetNodeID()].emplace(\n          leased_worker->GetWorkerID(), leased_worker);\n    }\n    CreateActorOnWorker(actor, leased_worker);\n  } else {\n    Schedule(actor);\n  } </s> remove       create_actor_request, [&finished_actors](std::shared_ptr<gcs::GcsActor> actor) {\n </s> add       create_actor_request,\n      [&finished_actors](const std::shared_ptr<gcs::GcsActor> &actor) {", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_actor_manager.cc"}
{"docstring_tokens": "keep keep replace keep keep keep replace keep", "code_tokens": " <mask>   // The backend storage is reliable in the future, so the status must be ok.\n <mask>   RAY_CHECK_OK(gcs_table_storage_->ActorTable().Put(\n <mask>       actor_id, actor_table_data, [this, actor_id, actor_table_data](Status status) {\n <mask>         RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor_id.Hex(),\n <mask>                                            actor_table_data.SerializeAsString(),\n <mask>                                            nullptr));\n <mask>       }));\n <mask> \n </s> GCS server error handling for actor creation (#8899) </s> add     mutable_actor_table_data->clear_resource_mapping(); </s> add   RAY_LOG(DEBUG) << \"Actor created successfully, actor id = \" << actor_id; </s> remove   // Update the address of the actor as it is tied to a new node.\n </s> add   // Update the address of the actor as it is tied to a node. </s> remove     actor->UpdateAddress(retry_at_raylet_address);\n    // The backend storage is reliable in the future, so the status must be ok.\n    RAY_CHECK_OK(gcs_actor_table_.Put(\n        actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n          RAY_CHECK_OK(status);\n          RAY_CHECK_OK(gcs_pub_sub_->Publish(\n              ACTOR_CHANNEL, actor->GetActorID().Hex(),\n              actor->GetActorTableData().SerializeAsString(), nullptr));\n          Schedule(actor);\n        }));\n </s> add     auto spill_back_node_id = ClientID::FromBinary(retry_at_raylet_address.raylet_id());\n    if (auto spill_back_node = gcs_node_manager_.GetNode(spill_back_node_id)) {\n      actor->UpdateAddress(retry_at_raylet_address);\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, spill_back_node);\n    } else {\n      // If the spill back node is dead, we need to schedule again.\n      actor->UpdateAddress(rpc::Address());\n      actor->GetMutableActorTableData()->clear_resource_mapping();\n      Schedule(actor);\n    } </s> remove   // The backend storage is reliable in the future, so the status must be ok.\n  RAY_CHECK_OK(gcs_actor_table_.Put(\n      actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n        RAY_CHECK_OK(status);\n        RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n                                           actor->GetActorTableData().SerializeAsString(),\n                                           nullptr));\n        // There is no promise that the node the\n        // actor tied to is still alive as the\n        // flush is asynchronously, so just\n        // invoke `Schedule` which will lease\n        // worker directly if the node is still\n        // available or select a new one if not.\n        Schedule(actor);\n      }));\n </s> add   RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                .emplace(actor->GetActorID())\n                .second);\n\n  // Lease worker directly from the node.\n  LeaseWorkerFromNode(actor, node);\n}\n\nvoid GcsActorScheduler::Reschedule(std::shared_ptr<GcsActor> actor) {\n  if (!actor->GetWorkerID().IsNil()) {\n    RAY_LOG(INFO)\n        << \"Actor \" << actor->GetActorID()\n        << \" is already tied to a leased worker. Create actor directly on worker.\";\n    auto leased_worker = std::make_shared<GcsLeasedWorker>(\n        actor->GetAddress(),\n        VectorFromProtobuf(actor->GetMutableActorTableData()->resource_mapping()),\n        actor->GetActorID());\n    auto iter_node = node_to_workers_when_creating_.find(actor->GetNodeID());\n    if (iter_node != node_to_workers_when_creating_.end()) {\n      if (0 == iter_node->second.count(leased_worker->GetWorkerID())) {\n        iter_node->second.emplace(leased_worker->GetWorkerID(), leased_worker);\n      }\n    } else {\n      node_to_workers_when_creating_[actor->GetNodeID()].emplace(\n          leased_worker->GetWorkerID(), leased_worker);\n    }\n    CreateActorOnWorker(actor, leased_worker);\n  } else {\n    Schedule(actor);\n  }", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_actor_manager.cc"}
{"docstring_tokens": "keep keep keep replace replace replace replace replace replace replace replace replace keep replace replace replace replace replace keep", "code_tokens": " <mask>                                            nullptr));\n <mask>       }));\n <mask> \n <mask>   // Invoke all callbacks for all registration requests of this actor (duplicated\n <mask>   // requests are included) and remove all of them from actor_to_register_callbacks_.\n <mask>   auto iter = actor_to_register_callbacks_.find(actor_id);\n <mask>   if (iter != actor_to_register_callbacks_.end()) {\n <mask>     for (auto &callback : iter->second) {\n <mask>       callback(actor);\n <mask>     }\n <mask>     actor_to_register_callbacks_.erase(iter);\n <mask>   }\n <mask> \n <mask>   auto worker_id = actor->GetWorkerID();\n <mask>   auto node_id = actor->GetNodeID();\n <mask>   RAY_CHECK(!worker_id.IsNil());\n <mask>   RAY_CHECK(!node_id.IsNil());\n <mask>   RAY_CHECK(created_actors_[node_id].emplace(worker_id, actor_id).second);\n <mask> }\n </s> GCS server error handling for actor creation (#8899) </s> remove       }));\n </s> add  </s> remove   RAY_LOG(INFO) << \"Node \" << node_id << \" failed, reconstructing actors\";\n </s> add   RAY_LOG(WARNING) << \"Node \" << node_id << \" failed, reconstructing actors.\"; </s> remove   auto node_id = actor->GetNodeID();\n  if (!node_id.IsNil()) {\n    if (auto node = gcs_node_manager_.GetNode(node_id)) {\n      // If the actor is already tied to a node and the node is available, then record\n      // the relationship of the node and actor and then lease worker directly from the\n      // node.\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, node);\n      return;\n    }\n\n    // The actor is already tied to a node which is unavailable now, so we should reset\n    // the address.\n    actor->UpdateAddress(rpc::Address());\n  }\n </s> add   RAY_CHECK(actor->GetNodeID().IsNil() && actor->GetWorkerID().IsNil()); </s> add     RAY_LOG(DEBUG) << \"The number of registered actors is \" << registered_actors_.size()\n                   << \", and the number of created actors is \" << created_actors_.size();\n    for (auto &item : registered_actors_) {\n      auto &actor = item.second;\n      if (actor->GetState() != ray::rpc::ActorTableData::ALIVE) {\n        RAY_LOG(DEBUG) << \"Rescheduling a non-alive actor, actor id = \"\n                       << actor->GetActorID() << \", state = \" << actor->GetState();\n        gcs_actor_scheduler_->Reschedule(actor);\n      }\n    }\n </s> add       actor->GetMutableActorTableData()->add_resource_mapping()->CopyFrom(resource);", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_actor_manager.cc"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>       }\n <mask>     }\n <mask>     RAY_LOG(INFO) << \"Finished loading initial data.\";\n <mask>     done();\n <mask>   };\n <mask>   RAY_CHECK_OK(gcs_table_storage_->ActorTable().GetAll(callback));\n <mask> }\n <mask> \n </s> GCS server error handling for actor creation (#8899) </s> remove       // Start RPC server when all tables have finished loading initial data.\n      rpc_server_.Run();\n\n      // Store gcs rpc server address in redis.\n      StoreGcsServerAddressInRedis();\n      is_started_ = true;\n </s> add       auto actor_manager_load_initial_data_callback = [this]() {\n        // Start RPC server when all tables have finished loading initial data.\n        rpc_server_.Run();\n\n        // Store gcs rpc server address in redis.\n        StoreGcsServerAddressInRedis();\n        is_started_ = true;\n      };\n      gcs_actor_manager_->LoadInitialData(actor_manager_load_initial_data_callback); </s> remove     RAY_LOG(INFO) << \"Worker \" << worker_id << \" on node \" << node_id\n                  << \" failed, restarting actor \" << actor_id;\n </s> add     RAY_LOG(WARNING) << \"Worker \" << worker_id << \" on node \" << node_id\n                     << \" failed, restarting actor \" << actor_id; </s> remove   RAY_LOG(INFO) << \"Node \" << node_id << \" failed, reconstructing actors\";\n </s> add   RAY_LOG(WARNING) << \"Node \" << node_id << \" failed, reconstructing actors.\"; </s> remove   gcs_actor_manager_->LoadInitialData(on_done);\n </s> add  </s> remove   int load_count = 3;\n </s> add   int load_count = 2; </s> remove     RAY_LOG(WARNING) << \"Submitted task \" << task_id\n                     << \" is already queued and will not be restarted. This is most \"\n                        \"likely due to spurious reconstruction.\";\n    return;\n </s> add     if (spec.IsActorCreationTask()) {\n      RAY_LOG(WARNING) << \"Submitted actor creation task \" << task_id\n                       << \" is already queued. This is most likely due to a GCS restart. \"\n                          \"We will remove \"\n                          \"the old one from the queue, and enqueue the new one.\";\n      std::unordered_set<TaskID> task_ids{task_id};\n      local_queues_.RemoveTasks(task_ids);\n    } else {\n      RAY_LOG(WARNING) << \"Submitted task \" << task_id\n                       << \" is already queued and will not be restarted. This is most \"\n                          \"likely due to spurious reconstruction.\";\n      return;\n    }", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_actor_manager.cc"}
{"docstring_tokens": "keep keep keep add keep keep", "code_tokens": " <mask>   // filtering.\n <mask>   RAY_CHECK_OK(gcs_table_storage_->ActorTable().GetByJobId(job_id, on_done));\n <mask> }\n <mask> \n <mask> }  // namespace gcs\n <mask> }  // namespace ray\n </s> GCS server error handling for actor creation (#8899) </s> remove       // Start RPC server when all tables have finished loading initial data.\n      rpc_server_.Run();\n\n      // Store gcs rpc server address in redis.\n      StoreGcsServerAddressInRedis();\n      is_started_ = true;\n </s> add       auto actor_manager_load_initial_data_callback = [this]() {\n        // Start RPC server when all tables have finished loading initial data.\n        rpc_server_.Run();\n\n        // Store gcs rpc server address in redis.\n        StoreGcsServerAddressInRedis();\n        is_started_ = true;\n      };\n      gcs_actor_manager_->LoadInitialData(actor_manager_load_initial_data_callback); </s> remove   // The backend storage is reliable in the future, so the status must be ok.\n  RAY_CHECK_OK(gcs_actor_table_.Put(\n      actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n        RAY_CHECK_OK(status);\n        RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n                                           actor->GetActorTableData().SerializeAsString(),\n                                           nullptr));\n        // There is no promise that the node the\n        // actor tied to is still alive as the\n        // flush is asynchronously, so just\n        // invoke `Schedule` which will lease\n        // worker directly if the node is still\n        // available or select a new one if not.\n        Schedule(actor);\n      }));\n </s> add   RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                .emplace(actor->GetActorID())\n                .second);\n\n  // Lease worker directly from the node.\n  LeaseWorkerFromNode(actor, node);\n}\n\nvoid GcsActorScheduler::Reschedule(std::shared_ptr<GcsActor> actor) {\n  if (!actor->GetWorkerID().IsNil()) {\n    RAY_LOG(INFO)\n        << \"Actor \" << actor->GetActorID()\n        << \" is already tied to a leased worker. Create actor directly on worker.\";\n    auto leased_worker = std::make_shared<GcsLeasedWorker>(\n        actor->GetAddress(),\n        VectorFromProtobuf(actor->GetMutableActorTableData()->resource_mapping()),\n        actor->GetActorID());\n    auto iter_node = node_to_workers_when_creating_.find(actor->GetNodeID());\n    if (iter_node != node_to_workers_when_creating_.end()) {\n      if (0 == iter_node->second.count(leased_worker->GetWorkerID())) {\n        iter_node->second.emplace(leased_worker->GetWorkerID(), leased_worker);\n      }\n    } else {\n      node_to_workers_when_creating_[actor->GetNodeID()].emplace(\n          leased_worker->GetWorkerID(), leased_worker);\n    }\n    CreateActorOnWorker(actor, leased_worker);\n  } else {\n    Schedule(actor);\n  } </s> remove   // Invoke all callbacks for all registration requests of this actor (duplicated\n  // requests are included) and remove all of them from actor_to_register_callbacks_.\n  auto iter = actor_to_register_callbacks_.find(actor_id);\n  if (iter != actor_to_register_callbacks_.end()) {\n    for (auto &callback : iter->second) {\n      callback(actor);\n    }\n    actor_to_register_callbacks_.erase(iter);\n  }\n </s> add         // Invoke all callbacks for all registration requests of this actor (duplicated\n        // requests are included) and remove all of them from\n        // actor_to_register_callbacks_.\n        auto iter = actor_to_register_callbacks_.find(actor_id);\n        if (iter != actor_to_register_callbacks_.end()) {\n          for (auto &callback : iter->second) {\n            callback(actor);\n          }\n          actor_to_register_callbacks_.erase(iter);\n        } </s> remove   auto node_id = actor->GetNodeID();\n  if (!node_id.IsNil()) {\n    if (auto node = gcs_node_manager_.GetNode(node_id)) {\n      // If the actor is already tied to a node and the node is available, then record\n      // the relationship of the node and actor and then lease worker directly from the\n      // node.\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, node);\n      return;\n    }\n\n    // The actor is already tied to a node which is unavailable now, so we should reset\n    // the address.\n    actor->UpdateAddress(rpc::Address());\n  }\n </s> add   RAY_CHECK(actor->GetNodeID().IsNil() && actor->GetWorkerID().IsNil()); </s> add     // We will reschedule the unfinished actors, so we have to load the actor data at the\n    // end to make sure the other table data is loaded. </s> remove     actor->UpdateAddress(retry_at_raylet_address);\n    // The backend storage is reliable in the future, so the status must be ok.\n    RAY_CHECK_OK(gcs_actor_table_.Put(\n        actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n          RAY_CHECK_OK(status);\n          RAY_CHECK_OK(gcs_pub_sub_->Publish(\n              ACTOR_CHANNEL, actor->GetActorID().Hex(),\n              actor->GetActorTableData().SerializeAsString(), nullptr));\n          Schedule(actor);\n        }));\n </s> add     auto spill_back_node_id = ClientID::FromBinary(retry_at_raylet_address.raylet_id());\n    if (auto spill_back_node = gcs_node_manager_.GetNode(spill_back_node_id)) {\n      actor->UpdateAddress(retry_at_raylet_address);\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, spill_back_node);\n    } else {\n      // If the spill back node is dead, we need to schedule again.\n      actor->UpdateAddress(rpc::Address());\n      actor->GetMutableActorTableData()->clear_resource_mapping();\n      Schedule(actor);\n    }", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_actor_manager.cc"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>   ///\n <mask>   /// \\param job_id The id of finished job.\n <mask>   void OnJobFinished(const JobID &job_id);\n <mask> \n <mask>  private:\n <mask>   /// A data structure representing an actor's owner.\n <mask>   struct Owner {\n <mask>     Owner(std::shared_ptr<rpc::CoreWorkerClientInterface> client)\n <mask>         : client(std::move(client)) {}\n <mask>     /// A client that can be used to contact the owner.\n </s> GCS server error handling for actor creation (#8899) </s> add   /// Reschedule the specified actor after gcs server restarts.\n  ///\n  /// \\param actor to be scheduled.\n  virtual void Reschedule(std::shared_ptr<GcsActor> actor) = 0;\n </s> add   /// Reschedule the specified actor after gcs server restarts.\n  ///\n  /// \\param actor to be scheduled.\n  void Reschedule(std::shared_ptr<GcsActor> actor) override;\n </s> add     // We will reschedule the unfinished actors, so we have to load the actor data at the\n    // end to make sure the other table data is loaded. </s> add   void Reschedule(std::shared_ptr<gcs::GcsActor> actor) {} </s> add   // Resource mapping ids acquired by the leased worker. This field is only set when this\n  // actor already has a leased worker.\n  repeated ResourceMapEntry resource_mapping = 15; </s> add   RAY_LOG(DEBUG) << \"Actor created successfully, actor id = \" << actor_id;", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_actor_manager.h"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>   RAY_CHECK(schedule_failure_handler_ != nullptr && schedule_success_handler_ != nullptr);\n <mask> }\n <mask> \n <mask> void GcsActorScheduler::Schedule(std::shared_ptr<GcsActor> actor) {\n <mask>   auto node_id = actor->GetNodeID();\n <mask>   if (!node_id.IsNil()) {\n <mask>     if (auto node = gcs_node_manager_.GetNode(node_id)) {\n <mask>       // If the actor is already tied to a node and the node is available, then record\n <mask>       // the relationship of the node and actor and then lease worker directly from the\n <mask>       // node.\n <mask>       RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n <mask>                     .emplace(actor->GetActorID())\n <mask>                     .second);\n <mask>       LeaseWorkerFromNode(actor, node);\n <mask>       return;\n <mask>     }\n <mask> \n <mask>     // The actor is already tied to a node which is unavailable now, so we should reset\n <mask>     // the address.\n <mask>     actor->UpdateAddress(rpc::Address());\n <mask>   }\n <mask> \n <mask>   // Select a node to lease worker for the actor.\n <mask>   auto node = SelectNodeRandomly();\n <mask>   if (node == nullptr) {\n <mask>     // There are no available nodes to schedule the actor, so just trigger the failed\n </s> GCS server error handling for actor creation (#8899) </s> remove   // The backend storage is reliable in the future, so the status must be ok.\n  RAY_CHECK_OK(gcs_actor_table_.Put(\n      actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n        RAY_CHECK_OK(status);\n        RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n                                           actor->GetActorTableData().SerializeAsString(),\n                                           nullptr));\n        // There is no promise that the node the\n        // actor tied to is still alive as the\n        // flush is asynchronously, so just\n        // invoke `Schedule` which will lease\n        // worker directly if the node is still\n        // available or select a new one if not.\n        Schedule(actor);\n      }));\n </s> add   RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                .emplace(actor->GetActorID())\n                .second);\n\n  // Lease worker directly from the node.\n  LeaseWorkerFromNode(actor, node);\n}\n\nvoid GcsActorScheduler::Reschedule(std::shared_ptr<GcsActor> actor) {\n  if (!actor->GetWorkerID().IsNil()) {\n    RAY_LOG(INFO)\n        << \"Actor \" << actor->GetActorID()\n        << \" is already tied to a leased worker. Create actor directly on worker.\";\n    auto leased_worker = std::make_shared<GcsLeasedWorker>(\n        actor->GetAddress(),\n        VectorFromProtobuf(actor->GetMutableActorTableData()->resource_mapping()),\n        actor->GetActorID());\n    auto iter_node = node_to_workers_when_creating_.find(actor->GetNodeID());\n    if (iter_node != node_to_workers_when_creating_.end()) {\n      if (0 == iter_node->second.count(leased_worker->GetWorkerID())) {\n        iter_node->second.emplace(leased_worker->GetWorkerID(), leased_worker);\n      }\n    } else {\n      node_to_workers_when_creating_[actor->GetNodeID()].emplace(\n          leased_worker->GetWorkerID(), leased_worker);\n    }\n    CreateActorOnWorker(actor, leased_worker);\n  } else {\n    Schedule(actor);\n  } </s> remove     actor->UpdateAddress(retry_at_raylet_address);\n    // The backend storage is reliable in the future, so the status must be ok.\n    RAY_CHECK_OK(gcs_actor_table_.Put(\n        actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n          RAY_CHECK_OK(status);\n          RAY_CHECK_OK(gcs_pub_sub_->Publish(\n              ACTOR_CHANNEL, actor->GetActorID().Hex(),\n              actor->GetActorTableData().SerializeAsString(), nullptr));\n          Schedule(actor);\n        }));\n </s> add     auto spill_back_node_id = ClientID::FromBinary(retry_at_raylet_address.raylet_id());\n    if (auto spill_back_node = gcs_node_manager_.GetNode(spill_back_node_id)) {\n      actor->UpdateAddress(retry_at_raylet_address);\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, spill_back_node);\n    } else {\n      // If the spill back node is dead, we need to schedule again.\n      actor->UpdateAddress(rpc::Address());\n      actor->GetMutableActorTableData()->clear_resource_mapping();\n      Schedule(actor);\n    } </s> remove   // Update the address of the actor as it is tied to a new node.\n </s> add   // Update the address of the actor as it is tied to a node. </s> add   // If GCS server is restarted after sending an actor creation task to this core worker,\n  // the restarted GCS server will send the same actor creation task to the core worker\n  // again. We just need to ignore it and reply ok.\n  if (task_spec.IsActorCreationTask() &&\n      worker_context_.GetCurrentActorID() == task_spec.ActorCreationId()) {\n    send_reply_callback(Status::OK(), nullptr, nullptr);\n    RAY_LOG(INFO) << \"Ignoring duplicate actor creation task for actor \"\n                  << task_spec.ActorCreationId()\n                  << \". This is likely due to a GCS server restart.\";\n    return;\n  }\n </s> remove   // Invoke all callbacks for all registration requests of this actor (duplicated\n  // requests are included) and remove all of them from actor_to_register_callbacks_.\n  auto iter = actor_to_register_callbacks_.find(actor_id);\n  if (iter != actor_to_register_callbacks_.end()) {\n    for (auto &callback : iter->second) {\n      callback(actor);\n    }\n    actor_to_register_callbacks_.erase(iter);\n  }\n </s> add         // Invoke all callbacks for all registration requests of this actor (duplicated\n        // requests are included) and remove all of them from\n        // actor_to_register_callbacks_.\n        auto iter = actor_to_register_callbacks_.find(actor_id);\n        if (iter != actor_to_register_callbacks_.end()) {\n          for (auto &callback : iter->second) {\n            callback(actor);\n          }\n          actor_to_register_callbacks_.erase(iter);\n        } </s> remove     RAY_LOG(WARNING) << \"Submitted task \" << task_id\n                     << \" is already queued and will not be restarted. This is most \"\n                        \"likely due to spurious reconstruction.\";\n    return;\n </s> add     if (spec.IsActorCreationTask()) {\n      RAY_LOG(WARNING) << \"Submitted actor creation task \" << task_id\n                       << \" is already queued. This is most likely due to a GCS restart. \"\n                          \"We will remove \"\n                          \"the old one from the queue, and enqueue the new one.\";\n      std::unordered_set<TaskID> task_ids{task_id};\n      local_queues_.RemoveTasks(task_ids);\n    } else {\n      RAY_LOG(WARNING) << \"Submitted task \" << task_id\n                       << \" is already queued and will not be restarted. This is most \"\n                          \"likely due to spurious reconstruction.\";\n      return;\n    }", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_actor_scheduler.cc"}
{"docstring_tokens": "keep keep replace keep keep keep replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace keep keep keep keep", "code_tokens": " <mask>   }\n <mask> \n <mask>   // Update the address of the actor as it is tied to a new node.\n <mask>   rpc::Address address;\n <mask>   address.set_raylet_id(node->node_id());\n <mask>   actor->UpdateAddress(address);\n <mask>   // The backend storage is reliable in the future, so the status must be ok.\n <mask>   RAY_CHECK_OK(gcs_actor_table_.Put(\n <mask>       actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n <mask>         RAY_CHECK_OK(status);\n <mask>         RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n <mask>                                            actor->GetActorTableData().SerializeAsString(),\n <mask>                                            nullptr));\n <mask>         // There is no promise that the node the\n <mask>         // actor tied to is still alive as the\n <mask>         // flush is asynchronously, so just\n <mask>         // invoke `Schedule` which will lease\n <mask>         // worker directly if the node is still\n <mask>         // available or select a new one if not.\n <mask>         Schedule(actor);\n <mask>       }));\n <mask> }\n <mask> \n <mask> std::vector<ActorID> GcsActorScheduler::CancelOnNode(const ClientID &node_id) {\n <mask>   // Remove all the actors from the map associated with this node, and return them as they\n </s> GCS server error handling for actor creation (#8899) </s> remove     actor->UpdateAddress(retry_at_raylet_address);\n    // The backend storage is reliable in the future, so the status must be ok.\n    RAY_CHECK_OK(gcs_actor_table_.Put(\n        actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n          RAY_CHECK_OK(status);\n          RAY_CHECK_OK(gcs_pub_sub_->Publish(\n              ACTOR_CHANNEL, actor->GetActorID().Hex(),\n              actor->GetActorTableData().SerializeAsString(), nullptr));\n          Schedule(actor);\n        }));\n </s> add     auto spill_back_node_id = ClientID::FromBinary(retry_at_raylet_address.raylet_id());\n    if (auto spill_back_node = gcs_node_manager_.GetNode(spill_back_node_id)) {\n      actor->UpdateAddress(retry_at_raylet_address);\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, spill_back_node);\n    } else {\n      // If the spill back node is dead, we need to schedule again.\n      actor->UpdateAddress(rpc::Address());\n      actor->GetMutableActorTableData()->clear_resource_mapping();\n      Schedule(actor);\n    } </s> remove   auto node_id = actor->GetNodeID();\n  if (!node_id.IsNil()) {\n    if (auto node = gcs_node_manager_.GetNode(node_id)) {\n      // If the actor is already tied to a node and the node is available, then record\n      // the relationship of the node and actor and then lease worker directly from the\n      // node.\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, node);\n      return;\n    }\n\n    // The actor is already tied to a node which is unavailable now, so we should reset\n    // the address.\n    actor->UpdateAddress(rpc::Address());\n  }\n </s> add   RAY_CHECK(actor->GetNodeID().IsNil() && actor->GetWorkerID().IsNil()); </s> remove       actor_id, actor_table_data, [this, actor_id, actor_table_data](Status status) {\n </s> add       actor_id, actor_table_data,\n      [this, actor_id, actor_table_data, actor](Status status) { </s> add     mutable_actor_table_data->clear_resource_mapping(); </s> remove       }));\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_actor_scheduler.cc"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace replace replace replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>   if (worker_address.raylet_id().empty()) {\n <mask>     // The worker did not succeed in the lease, but the specified node returned a new\n <mask>     // node, and then try again on the new node.\n <mask>     RAY_CHECK(!retry_at_raylet_address.raylet_id().empty());\n <mask>     actor->UpdateAddress(retry_at_raylet_address);\n <mask>     // The backend storage is reliable in the future, so the status must be ok.\n <mask>     RAY_CHECK_OK(gcs_actor_table_.Put(\n <mask>         actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n <mask>           RAY_CHECK_OK(status);\n <mask>           RAY_CHECK_OK(gcs_pub_sub_->Publish(\n <mask>               ACTOR_CHANNEL, actor->GetActorID().Hex(),\n <mask>               actor->GetActorTableData().SerializeAsString(), nullptr));\n <mask>           Schedule(actor);\n <mask>         }));\n <mask>   } else {\n <mask>     // The worker is leased successfully from the specified node.\n <mask>     std::vector<rpc::ResourceMapEntry> resources;\n <mask>     for (auto &resource : reply.resource_mapping()) {\n <mask>       resources.emplace_back(resource);\n </s> GCS server error handling for actor creation (#8899) </s> remove   // The backend storage is reliable in the future, so the status must be ok.\n  RAY_CHECK_OK(gcs_actor_table_.Put(\n      actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n        RAY_CHECK_OK(status);\n        RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n                                           actor->GetActorTableData().SerializeAsString(),\n                                           nullptr));\n        // There is no promise that the node the\n        // actor tied to is still alive as the\n        // flush is asynchronously, so just\n        // invoke `Schedule` which will lease\n        // worker directly if the node is still\n        // available or select a new one if not.\n        Schedule(actor);\n      }));\n </s> add   RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                .emplace(actor->GetActorID())\n                .second);\n\n  // Lease worker directly from the node.\n  LeaseWorkerFromNode(actor, node);\n}\n\nvoid GcsActorScheduler::Reschedule(std::shared_ptr<GcsActor> actor) {\n  if (!actor->GetWorkerID().IsNil()) {\n    RAY_LOG(INFO)\n        << \"Actor \" << actor->GetActorID()\n        << \" is already tied to a leased worker. Create actor directly on worker.\";\n    auto leased_worker = std::make_shared<GcsLeasedWorker>(\n        actor->GetAddress(),\n        VectorFromProtobuf(actor->GetMutableActorTableData()->resource_mapping()),\n        actor->GetActorID());\n    auto iter_node = node_to_workers_when_creating_.find(actor->GetNodeID());\n    if (iter_node != node_to_workers_when_creating_.end()) {\n      if (0 == iter_node->second.count(leased_worker->GetWorkerID())) {\n        iter_node->second.emplace(leased_worker->GetWorkerID(), leased_worker);\n      }\n    } else {\n      node_to_workers_when_creating_[actor->GetNodeID()].emplace(\n          leased_worker->GetWorkerID(), leased_worker);\n    }\n    CreateActorOnWorker(actor, leased_worker);\n  } else {\n    Schedule(actor);\n  } </s> remove       actor_id, actor_table_data, [this, actor_id, actor_table_data](Status status) {\n </s> add       actor_id, actor_table_data,\n      [this, actor_id, actor_table_data, actor](Status status) { </s> add     mutable_actor_table_data->clear_resource_mapping(); </s> remove   // Update the address of the actor as it is tied to a new node.\n </s> add   // Update the address of the actor as it is tied to a node. </s> remove   auto node_id = actor->GetNodeID();\n  if (!node_id.IsNil()) {\n    if (auto node = gcs_node_manager_.GetNode(node_id)) {\n      // If the actor is already tied to a node and the node is available, then record\n      // the relationship of the node and actor and then lease worker directly from the\n      // node.\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, node);\n      return;\n    }\n\n    // The actor is already tied to a node which is unavailable now, so we should reset\n    // the address.\n    actor->UpdateAddress(rpc::Address());\n  }\n </s> add   RAY_CHECK(actor->GetNodeID().IsNil() && actor->GetWorkerID().IsNil()); </s> add   RAY_LOG(DEBUG) << \"Actor created successfully, actor id = \" << actor_id;", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_actor_scheduler.cc"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>     for (auto &resource : reply.resource_mapping()) {\n <mask>       resources.emplace_back(resource);\n <mask>     }\n <mask>     auto leased_worker = std::make_shared<GcsLeasedWorker>(\n <mask>         worker_address, std::move(resources), actor->GetActorID());\n <mask>     auto node_id = leased_worker->GetNodeID();\n <mask>     RAY_CHECK(node_to_workers_when_creating_[node_id]\n <mask>                   .emplace(leased_worker->GetWorkerID(), leased_worker)\n </s> GCS server error handling for actor creation (#8899) </s> remove     CreateActorOnWorker(actor, leased_worker);\n </s> add     RAY_CHECK_OK(gcs_actor_table_.Put(actor->GetActorID(), actor->GetActorTableData(),\n                                      [this, actor, leased_worker](Status status) {\n                                        RAY_CHECK_OK(status);\n                                        CreateActorOnWorker(actor, leased_worker);\n                                      })); </s> remove     actor->UpdateAddress(retry_at_raylet_address);\n    // The backend storage is reliable in the future, so the status must be ok.\n    RAY_CHECK_OK(gcs_actor_table_.Put(\n        actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n          RAY_CHECK_OK(status);\n          RAY_CHECK_OK(gcs_pub_sub_->Publish(\n              ACTOR_CHANNEL, actor->GetActorID().Hex(),\n              actor->GetActorTableData().SerializeAsString(), nullptr));\n          Schedule(actor);\n        }));\n </s> add     auto spill_back_node_id = ClientID::FromBinary(retry_at_raylet_address.raylet_id());\n    if (auto spill_back_node = gcs_node_manager_.GetNode(spill_back_node_id)) {\n      actor->UpdateAddress(retry_at_raylet_address);\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, spill_back_node);\n    } else {\n      // If the spill back node is dead, we need to schedule again.\n      actor->UpdateAddress(rpc::Address());\n      actor->GetMutableActorTableData()->clear_resource_mapping();\n      Schedule(actor);\n    } </s> remove   // Invoke all callbacks for all registration requests of this actor (duplicated\n  // requests are included) and remove all of them from actor_to_register_callbacks_.\n  auto iter = actor_to_register_callbacks_.find(actor_id);\n  if (iter != actor_to_register_callbacks_.end()) {\n    for (auto &callback : iter->second) {\n      callback(actor);\n    }\n    actor_to_register_callbacks_.erase(iter);\n  }\n </s> add         // Invoke all callbacks for all registration requests of this actor (duplicated\n        // requests are included) and remove all of them from\n        // actor_to_register_callbacks_.\n        auto iter = actor_to_register_callbacks_.find(actor_id);\n        if (iter != actor_to_register_callbacks_.end()) {\n          for (auto &callback : iter->second) {\n            callback(actor);\n          }\n          actor_to_register_callbacks_.erase(iter);\n        } </s> add     RAY_LOG(DEBUG) << \"The number of registered actors is \" << registered_actors_.size()\n                   << \", and the number of created actors is \" << created_actors_.size();\n    for (auto &item : registered_actors_) {\n      auto &actor = item.second;\n      if (actor->GetState() != ray::rpc::ActorTableData::ALIVE) {\n        RAY_LOG(DEBUG) << \"Rescheduling a non-alive actor, actor id = \"\n                       << actor->GetActorID() << \", state = \" << actor->GetState();\n        gcs_actor_scheduler_->Reschedule(actor);\n      }\n    }\n </s> remove   RAY_LOG(INFO) << \"Node \" << node_id << \" failed, reconstructing actors\";\n </s> add   RAY_LOG(WARNING) << \"Node \" << node_id << \" failed, reconstructing actors.\"; </s> remove   auto worker_id = actor->GetWorkerID();\n  auto node_id = actor->GetNodeID();\n  RAY_CHECK(!worker_id.IsNil());\n  RAY_CHECK(!node_id.IsNil());\n  RAY_CHECK(created_actors_[node_id].emplace(worker_id, actor_id).second);\n </s> add         auto worker_id = actor->GetWorkerID();\n        auto node_id = actor->GetNodeID();\n        RAY_CHECK(!worker_id.IsNil());\n        RAY_CHECK(!node_id.IsNil());\n        RAY_CHECK(created_actors_[node_id].emplace(worker_id, actor_id).second);\n      }));", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_actor_scheduler.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     RAY_CHECK(node_to_workers_when_creating_[node_id]\n <mask>                   .emplace(leased_worker->GetWorkerID(), leased_worker)\n <mask>                   .second);\n <mask>     actor->UpdateAddress(leased_worker->GetAddress());\n <mask>     CreateActorOnWorker(actor, leased_worker);\n <mask>   }\n <mask> }\n <mask> \n <mask> void GcsActorScheduler::CreateActorOnWorker(std::shared_ptr<GcsActor> actor,\n <mask>                                             std::shared_ptr<GcsLeasedWorker> worker) {\n </s> GCS server error handling for actor creation (#8899) </s> add       actor->GetMutableActorTableData()->add_resource_mapping()->CopyFrom(resource); </s> remove   // The backend storage is reliable in the future, so the status must be ok.\n  RAY_CHECK_OK(gcs_actor_table_.Put(\n      actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n        RAY_CHECK_OK(status);\n        RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n                                           actor->GetActorTableData().SerializeAsString(),\n                                           nullptr));\n        // There is no promise that the node the\n        // actor tied to is still alive as the\n        // flush is asynchronously, so just\n        // invoke `Schedule` which will lease\n        // worker directly if the node is still\n        // available or select a new one if not.\n        Schedule(actor);\n      }));\n </s> add   RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                .emplace(actor->GetActorID())\n                .second);\n\n  // Lease worker directly from the node.\n  LeaseWorkerFromNode(actor, node);\n}\n\nvoid GcsActorScheduler::Reschedule(std::shared_ptr<GcsActor> actor) {\n  if (!actor->GetWorkerID().IsNil()) {\n    RAY_LOG(INFO)\n        << \"Actor \" << actor->GetActorID()\n        << \" is already tied to a leased worker. Create actor directly on worker.\";\n    auto leased_worker = std::make_shared<GcsLeasedWorker>(\n        actor->GetAddress(),\n        VectorFromProtobuf(actor->GetMutableActorTableData()->resource_mapping()),\n        actor->GetActorID());\n    auto iter_node = node_to_workers_when_creating_.find(actor->GetNodeID());\n    if (iter_node != node_to_workers_when_creating_.end()) {\n      if (0 == iter_node->second.count(leased_worker->GetWorkerID())) {\n        iter_node->second.emplace(leased_worker->GetWorkerID(), leased_worker);\n      }\n    } else {\n      node_to_workers_when_creating_[actor->GetNodeID()].emplace(\n          leased_worker->GetWorkerID(), leased_worker);\n    }\n    CreateActorOnWorker(actor, leased_worker);\n  } else {\n    Schedule(actor);\n  } </s> remove   auto node_id = actor->GetNodeID();\n  if (!node_id.IsNil()) {\n    if (auto node = gcs_node_manager_.GetNode(node_id)) {\n      // If the actor is already tied to a node and the node is available, then record\n      // the relationship of the node and actor and then lease worker directly from the\n      // node.\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, node);\n      return;\n    }\n\n    // The actor is already tied to a node which is unavailable now, so we should reset\n    // the address.\n    actor->UpdateAddress(rpc::Address());\n  }\n </s> add   RAY_CHECK(actor->GetNodeID().IsNil() && actor->GetWorkerID().IsNil()); </s> remove   gcs_actor_manager_->LoadInitialData(on_done);\n </s> add  </s> add     RAY_LOG(DEBUG) << \"The number of registered actors is \" << registered_actors_.size()\n                   << \", and the number of created actors is \" << created_actors_.size();\n    for (auto &item : registered_actors_) {\n      auto &actor = item.second;\n      if (actor->GetState() != ray::rpc::ActorTableData::ALIVE) {\n        RAY_LOG(DEBUG) << \"Rescheduling a non-alive actor, actor id = \"\n                       << actor->GetActorID() << \", state = \" << actor->GetState();\n        gcs_actor_scheduler_->Reschedule(actor);\n      }\n    }\n </s> remove   auto worker_id = actor->GetWorkerID();\n  auto node_id = actor->GetNodeID();\n  RAY_CHECK(!worker_id.IsNil());\n  RAY_CHECK(!node_id.IsNil());\n  RAY_CHECK(created_actors_[node_id].emplace(worker_id, actor_id).second);\n </s> add         auto worker_id = actor->GetWorkerID();\n        auto node_id = actor->GetNodeID();\n        RAY_CHECK(!worker_id.IsNil());\n        RAY_CHECK(!node_id.IsNil());\n        RAY_CHECK(created_actors_[node_id].emplace(worker_id, actor_id).second);\n      }));", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_actor_scheduler.cc"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>   /// \\param actor to be scheduled.\n <mask>   virtual void Schedule(std::shared_ptr<GcsActor> actor) = 0;\n <mask> \n <mask>   /// Cancel all actors that are being scheduled to the specified node.\n <mask>   ///\n <mask>   /// \\param node_id ID of the node where the worker is located.\n <mask>   /// \\return ID list of actors associated with the specified node id.\n </s> GCS server error handling for actor creation (#8899) </s> add   /// Reschedule the specified actor after gcs server restarts.\n  ///\n  /// \\param actor to be scheduled.\n  void Reschedule(std::shared_ptr<GcsActor> actor) override;\n </s> add   /// Get the created actors.\n  ///\n  /// \\return The created actors.\n  const absl::flat_hash_map<ClientID, absl::flat_hash_map<WorkerID, ActorID>>\n      &GetCreatedActors() const;\n </s> remove   auto node_id = actor->GetNodeID();\n  if (!node_id.IsNil()) {\n    if (auto node = gcs_node_manager_.GetNode(node_id)) {\n      // If the actor is already tied to a node and the node is available, then record\n      // the relationship of the node and actor and then lease worker directly from the\n      // node.\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, node);\n      return;\n    }\n\n    // The actor is already tied to a node which is unavailable now, so we should reset\n    // the address.\n    actor->UpdateAddress(rpc::Address());\n  }\n </s> add   RAY_CHECK(actor->GetNodeID().IsNil() && actor->GetWorkerID().IsNil()); </s> remove   // The backend storage is reliable in the future, so the status must be ok.\n  RAY_CHECK_OK(gcs_actor_table_.Put(\n      actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n        RAY_CHECK_OK(status);\n        RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n                                           actor->GetActorTableData().SerializeAsString(),\n                                           nullptr));\n        // There is no promise that the node the\n        // actor tied to is still alive as the\n        // flush is asynchronously, so just\n        // invoke `Schedule` which will lease\n        // worker directly if the node is still\n        // available or select a new one if not.\n        Schedule(actor);\n      }));\n </s> add   RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                .emplace(actor->GetActorID())\n                .second);\n\n  // Lease worker directly from the node.\n  LeaseWorkerFromNode(actor, node);\n}\n\nvoid GcsActorScheduler::Reschedule(std::shared_ptr<GcsActor> actor) {\n  if (!actor->GetWorkerID().IsNil()) {\n    RAY_LOG(INFO)\n        << \"Actor \" << actor->GetActorID()\n        << \" is already tied to a leased worker. Create actor directly on worker.\";\n    auto leased_worker = std::make_shared<GcsLeasedWorker>(\n        actor->GetAddress(),\n        VectorFromProtobuf(actor->GetMutableActorTableData()->resource_mapping()),\n        actor->GetActorID());\n    auto iter_node = node_to_workers_when_creating_.find(actor->GetNodeID());\n    if (iter_node != node_to_workers_when_creating_.end()) {\n      if (0 == iter_node->second.count(leased_worker->GetWorkerID())) {\n        iter_node->second.emplace(leased_worker->GetWorkerID(), leased_worker);\n      }\n    } else {\n      node_to_workers_when_creating_[actor->GetNodeID()].emplace(\n          leased_worker->GetWorkerID(), leased_worker);\n    }\n    CreateActorOnWorker(actor, leased_worker);\n  } else {\n    Schedule(actor);\n  } </s> remove     actor->UpdateAddress(retry_at_raylet_address);\n    // The backend storage is reliable in the future, so the status must be ok.\n    RAY_CHECK_OK(gcs_actor_table_.Put(\n        actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n          RAY_CHECK_OK(status);\n          RAY_CHECK_OK(gcs_pub_sub_->Publish(\n              ACTOR_CHANNEL, actor->GetActorID().Hex(),\n              actor->GetActorTableData().SerializeAsString(), nullptr));\n          Schedule(actor);\n        }));\n </s> add     auto spill_back_node_id = ClientID::FromBinary(retry_at_raylet_address.raylet_id());\n    if (auto spill_back_node = gcs_node_manager_.GetNode(spill_back_node_id)) {\n      actor->UpdateAddress(retry_at_raylet_address);\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, spill_back_node);\n    } else {\n      // If the spill back node is dead, we need to schedule again.\n      actor->UpdateAddress(rpc::Address());\n      actor->GetMutableActorTableData()->clear_resource_mapping();\n      Schedule(actor);\n    } </s> remove   // Update the address of the actor as it is tied to a new node.\n </s> add   // Update the address of the actor as it is tied to a node.", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_actor_scheduler.h"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask>   void Schedule(std::shared_ptr<GcsActor> actor) override;\n <mask> \n <mask>   /// Cancel all actors that are being scheduled to the specified node.\n <mask>   ///\n <mask>   /// \\param node_id ID of the node where the worker is located.\n <mask>   /// \\return ID list of actors associated with the specified node id.\n </s> GCS server error handling for actor creation (#8899) </s> add   /// Reschedule the specified actor after gcs server restarts.\n  ///\n  /// \\param actor to be scheduled.\n  virtual void Reschedule(std::shared_ptr<GcsActor> actor) = 0;\n </s> add   /// Get the created actors.\n  ///\n  /// \\return The created actors.\n  const absl::flat_hash_map<ClientID, absl::flat_hash_map<WorkerID, ActorID>>\n      &GetCreatedActors() const;\n </s> remove   auto node_id = actor->GetNodeID();\n  if (!node_id.IsNil()) {\n    if (auto node = gcs_node_manager_.GetNode(node_id)) {\n      // If the actor is already tied to a node and the node is available, then record\n      // the relationship of the node and actor and then lease worker directly from the\n      // node.\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, node);\n      return;\n    }\n\n    // The actor is already tied to a node which is unavailable now, so we should reset\n    // the address.\n    actor->UpdateAddress(rpc::Address());\n  }\n </s> add   RAY_CHECK(actor->GetNodeID().IsNil() && actor->GetWorkerID().IsNil()); </s> remove   // The backend storage is reliable in the future, so the status must be ok.\n  RAY_CHECK_OK(gcs_actor_table_.Put(\n      actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n        RAY_CHECK_OK(status);\n        RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n                                           actor->GetActorTableData().SerializeAsString(),\n                                           nullptr));\n        // There is no promise that the node the\n        // actor tied to is still alive as the\n        // flush is asynchronously, so just\n        // invoke `Schedule` which will lease\n        // worker directly if the node is still\n        // available or select a new one if not.\n        Schedule(actor);\n      }));\n </s> add   RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                .emplace(actor->GetActorID())\n                .second);\n\n  // Lease worker directly from the node.\n  LeaseWorkerFromNode(actor, node);\n}\n\nvoid GcsActorScheduler::Reschedule(std::shared_ptr<GcsActor> actor) {\n  if (!actor->GetWorkerID().IsNil()) {\n    RAY_LOG(INFO)\n        << \"Actor \" << actor->GetActorID()\n        << \" is already tied to a leased worker. Create actor directly on worker.\";\n    auto leased_worker = std::make_shared<GcsLeasedWorker>(\n        actor->GetAddress(),\n        VectorFromProtobuf(actor->GetMutableActorTableData()->resource_mapping()),\n        actor->GetActorID());\n    auto iter_node = node_to_workers_when_creating_.find(actor->GetNodeID());\n    if (iter_node != node_to_workers_when_creating_.end()) {\n      if (0 == iter_node->second.count(leased_worker->GetWorkerID())) {\n        iter_node->second.emplace(leased_worker->GetWorkerID(), leased_worker);\n      }\n    } else {\n      node_to_workers_when_creating_[actor->GetNodeID()].emplace(\n          leased_worker->GetWorkerID(), leased_worker);\n    }\n    CreateActorOnWorker(actor, leased_worker);\n  } else {\n    Schedule(actor);\n  } </s> remove     actor->UpdateAddress(retry_at_raylet_address);\n    // The backend storage is reliable in the future, so the status must be ok.\n    RAY_CHECK_OK(gcs_actor_table_.Put(\n        actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n          RAY_CHECK_OK(status);\n          RAY_CHECK_OK(gcs_pub_sub_->Publish(\n              ACTOR_CHANNEL, actor->GetActorID().Hex(),\n              actor->GetActorTableData().SerializeAsString(), nullptr));\n          Schedule(actor);\n        }));\n </s> add     auto spill_back_node_id = ClientID::FromBinary(retry_at_raylet_address.raylet_id());\n    if (auto spill_back_node = gcs_node_manager_.GetNode(spill_back_node_id)) {\n      actor->UpdateAddress(retry_at_raylet_address);\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, spill_back_node);\n    } else {\n      // If the spill back node is dead, we need to schedule again.\n      actor->UpdateAddress(rpc::Address());\n      actor->GetMutableActorTableData()->clear_resource_mapping();\n      Schedule(actor);\n    } </s> add   // Resource mapping ids acquired by the leased worker. This field is only set when this\n  // actor already has a leased worker.\n  repeated ResourceMapEntry resource_mapping = 15;", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_actor_scheduler.h"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask> \n <mask> std::shared_ptr<rpc::GcsNodeInfo> GcsNodeManager::RemoveNode(\n <mask>     const ray::ClientID &node_id, bool is_intended /*= false*/) {\n <mask>   std::shared_ptr<rpc::GcsNodeInfo> removed_node;\n <mask>   auto iter = alive_nodes_.find(node_id);\n <mask>   if (iter != alive_nodes_.end()) {\n <mask>     removed_node = std::move(iter->second);\n </s> GCS server error handling for actor creation (#8899) </s> remove   // Invoke all callbacks for all registration requests of this actor (duplicated\n  // requests are included) and remove all of them from actor_to_register_callbacks_.\n  auto iter = actor_to_register_callbacks_.find(actor_id);\n  if (iter != actor_to_register_callbacks_.end()) {\n    for (auto &callback : iter->second) {\n      callback(actor);\n    }\n    actor_to_register_callbacks_.erase(iter);\n  }\n </s> add         // Invoke all callbacks for all registration requests of this actor (duplicated\n        // requests are included) and remove all of them from\n        // actor_to_register_callbacks_.\n        auto iter = actor_to_register_callbacks_.find(actor_id);\n        if (iter != actor_to_register_callbacks_.end()) {\n          for (auto &callback : iter->second) {\n            callback(actor);\n          }\n          actor_to_register_callbacks_.erase(iter);\n        } </s> remove       }));\n </s> add  </s> add   void Reschedule(std::shared_ptr<gcs::GcsActor> actor) {} </s> remove   RAY_LOG(INFO) << \"Node \" << node_id << \" failed, reconstructing actors\";\n </s> add   RAY_LOG(WARNING) << \"Node \" << node_id << \" failed, reconstructing actors.\"; </s> remove   const auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> add   const std::chrono::milliseconds timeout_ms_{2000};", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_node_manager.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>       new rpc::WorkerInfoGrpcService(main_service_, *worker_info_handler_));\n <mask>   rpc_server_.RegisterService(*worker_info_service_);\n <mask> \n <mask>   auto load_completed_count = std::make_shared<int>(0);\n <mask>   int load_count = 3;\n <mask>   auto on_done = [this, load_count, load_completed_count]() {\n <mask>     ++(*load_completed_count);\n <mask> \n <mask>     if (*load_completed_count == load_count) {\n <mask>       // Start RPC server when all tables have finished loading initial data.\n </s> GCS server error handling for actor creation (#8899) </s> remove       // Start RPC server when all tables have finished loading initial data.\n      rpc_server_.Run();\n\n      // Store gcs rpc server address in redis.\n      StoreGcsServerAddressInRedis();\n      is_started_ = true;\n </s> add       auto actor_manager_load_initial_data_callback = [this]() {\n        // Start RPC server when all tables have finished loading initial data.\n        rpc_server_.Run();\n\n        // Store gcs rpc server address in redis.\n        StoreGcsServerAddressInRedis();\n        is_started_ = true;\n      };\n      gcs_actor_manager_->LoadInitialData(actor_manager_load_initial_data_callback); </s> add     // We will reschedule the unfinished actors, so we have to load the actor data at the\n    // end to make sure the other table data is loaded. </s> remove       }));\n </s> add  </s> add   // If GCS server is restarted after sending an actor creation task to this core worker,\n  // the restarted GCS server will send the same actor creation task to the core worker\n  // again. We just need to ignore it and reply ok.\n  if (task_spec.IsActorCreationTask() &&\n      worker_context_.GetCurrentActorID() == task_spec.ActorCreationId()) {\n    send_reply_callback(Status::OK(), nullptr, nullptr);\n    RAY_LOG(INFO) << \"Ignoring duplicate actor creation task for actor \"\n                  << task_spec.ActorCreationId()\n                  << \". This is likely due to a GCS server restart.\";\n    return;\n  }\n </s> remove   // Invoke all callbacks for all registration requests of this actor (duplicated\n  // requests are included) and remove all of them from actor_to_register_callbacks_.\n  auto iter = actor_to_register_callbacks_.find(actor_id);\n  if (iter != actor_to_register_callbacks_.end()) {\n    for (auto &callback : iter->second) {\n      callback(actor);\n    }\n    actor_to_register_callbacks_.erase(iter);\n  }\n </s> add         // Invoke all callbacks for all registration requests of this actor (duplicated\n        // requests are included) and remove all of them from\n        // actor_to_register_callbacks_.\n        auto iter = actor_to_register_callbacks_.find(actor_id);\n        if (iter != actor_to_register_callbacks_.end()) {\n          for (auto &callback : iter->second) {\n            callback(actor);\n          }\n          actor_to_register_callbacks_.erase(iter);\n        } </s> add     RAY_LOG(DEBUG) << \"The number of registered actors is \" << registered_actors_.size()\n                   << \", and the number of created actors is \" << created_actors_.size();\n    for (auto &item : registered_actors_) {\n      auto &actor = item.second;\n      if (actor->GetState() != ray::rpc::ActorTableData::ALIVE) {\n        RAY_LOG(DEBUG) << \"Rescheduling a non-alive actor, actor id = \"\n                       << actor->GetActorID() << \", state = \" << actor->GetState();\n        gcs_actor_scheduler_->Reschedule(actor);\n      }\n    }\n", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_server.cc"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>     ++(*load_completed_count);\n <mask> \n <mask>     if (*load_completed_count == load_count) {\n <mask>       auto actor_manager_load_initial_data_callback = [this]() {\n <mask>         // Start RPC server when all tables have finished loading initial data.\n <mask>         rpc_server_.Run();\n <mask> \n <mask>         // Store gcs rpc server address in redis.\n </s> GCS server error handling for actor creation (#8899) </s> remove       // Start RPC server when all tables have finished loading initial data.\n      rpc_server_.Run();\n\n      // Store gcs rpc server address in redis.\n      StoreGcsServerAddressInRedis();\n      is_started_ = true;\n </s> add       auto actor_manager_load_initial_data_callback = [this]() {\n        // Start RPC server when all tables have finished loading initial data.\n        rpc_server_.Run();\n\n        // Store gcs rpc server address in redis.\n        StoreGcsServerAddressInRedis();\n        is_started_ = true;\n      };\n      gcs_actor_manager_->LoadInitialData(actor_manager_load_initial_data_callback); </s> remove   int load_count = 3;\n </s> add   int load_count = 2; </s> add   // If GCS server is restarted after sending an actor creation task to this core worker,\n  // the restarted GCS server will send the same actor creation task to the core worker\n  // again. We just need to ignore it and reply ok.\n  if (task_spec.IsActorCreationTask() &&\n      worker_context_.GetCurrentActorID() == task_spec.ActorCreationId()) {\n    send_reply_callback(Status::OK(), nullptr, nullptr);\n    RAY_LOG(INFO) << \"Ignoring duplicate actor creation task for actor \"\n                  << task_spec.ActorCreationId()\n                  << \". This is likely due to a GCS server restart.\";\n    return;\n  }\n </s> remove   // Invoke all callbacks for all registration requests of this actor (duplicated\n  // requests are included) and remove all of them from actor_to_register_callbacks_.\n  auto iter = actor_to_register_callbacks_.find(actor_id);\n  if (iter != actor_to_register_callbacks_.end()) {\n    for (auto &callback : iter->second) {\n      callback(actor);\n    }\n    actor_to_register_callbacks_.erase(iter);\n  }\n </s> add         // Invoke all callbacks for all registration requests of this actor (duplicated\n        // requests are included) and remove all of them from\n        // actor_to_register_callbacks_.\n        auto iter = actor_to_register_callbacks_.find(actor_id);\n        if (iter != actor_to_register_callbacks_.end()) {\n          for (auto &callback : iter->second) {\n            callback(actor);\n          }\n          actor_to_register_callbacks_.erase(iter);\n        } </s> remove   // The backend storage is reliable in the future, so the status must be ok.\n  RAY_CHECK_OK(gcs_actor_table_.Put(\n      actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n        RAY_CHECK_OK(status);\n        RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n                                           actor->GetActorTableData().SerializeAsString(),\n                                           nullptr));\n        // There is no promise that the node the\n        // actor tied to is still alive as the\n        // flush is asynchronously, so just\n        // invoke `Schedule` which will lease\n        // worker directly if the node is still\n        // available or select a new one if not.\n        Schedule(actor);\n      }));\n </s> add   RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                .emplace(actor->GetActorID())\n                .second);\n\n  // Lease worker directly from the node.\n  LeaseWorkerFromNode(actor, node);\n}\n\nvoid GcsActorScheduler::Reschedule(std::shared_ptr<GcsActor> actor) {\n  if (!actor->GetWorkerID().IsNil()) {\n    RAY_LOG(INFO)\n        << \"Actor \" << actor->GetActorID()\n        << \" is already tied to a leased worker. Create actor directly on worker.\";\n    auto leased_worker = std::make_shared<GcsLeasedWorker>(\n        actor->GetAddress(),\n        VectorFromProtobuf(actor->GetMutableActorTableData()->resource_mapping()),\n        actor->GetActorID());\n    auto iter_node = node_to_workers_when_creating_.find(actor->GetNodeID());\n    if (iter_node != node_to_workers_when_creating_.end()) {\n      if (0 == iter_node->second.count(leased_worker->GetWorkerID())) {\n        iter_node->second.emplace(leased_worker->GetWorkerID(), leased_worker);\n      }\n    } else {\n      node_to_workers_when_creating_[actor->GetNodeID()].emplace(\n          leased_worker->GetWorkerID(), leased_worker);\n    }\n    CreateActorOnWorker(actor, leased_worker);\n  } else {\n    Schedule(actor);\n  } </s> remove       }));\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_server.cc"}
{"docstring_tokens": "keep replace replace replace replace replace replace keep keep replace keep keep", "code_tokens": " <mask>     if (*load_completed_count == load_count) {\n <mask>       // Start RPC server when all tables have finished loading initial data.\n <mask>       rpc_server_.Run();\n <mask> \n <mask>       // Store gcs rpc server address in redis.\n <mask>       StoreGcsServerAddressInRedis();\n <mask>       is_started_ = true;\n <mask>     }\n <mask>   };\n <mask>   gcs_actor_manager_->LoadInitialData(on_done);\n <mask>   gcs_object_manager_->LoadInitialData(on_done);\n <mask>   gcs_node_manager_->LoadInitialData(on_done);\n </s> GCS server error handling for actor creation (#8899) </s> add     // We will reschedule the unfinished actors, so we have to load the actor data at the\n    // end to make sure the other table data is loaded. </s> remove   int load_count = 3;\n </s> add   int load_count = 2; </s> add   // If GCS server is restarted after sending an actor creation task to this core worker,\n  // the restarted GCS server will send the same actor creation task to the core worker\n  // again. We just need to ignore it and reply ok.\n  if (task_spec.IsActorCreationTask() &&\n      worker_context_.GetCurrentActorID() == task_spec.ActorCreationId()) {\n    send_reply_callback(Status::OK(), nullptr, nullptr);\n    RAY_LOG(INFO) << \"Ignoring duplicate actor creation task for actor \"\n                  << task_spec.ActorCreationId()\n                  << \". This is likely due to a GCS server restart.\";\n    return;\n  }\n </s> add     RAY_LOG(DEBUG) << \"The number of registered actors is \" << registered_actors_.size()\n                   << \", and the number of created actors is \" << created_actors_.size();\n    for (auto &item : registered_actors_) {\n      auto &actor = item.second;\n      if (actor->GetState() != ray::rpc::ActorTableData::ALIVE) {\n        RAY_LOG(DEBUG) << \"Rescheduling a non-alive actor, actor id = \"\n                       << actor->GetActorID() << \", state = \" << actor->GetState();\n        gcs_actor_scheduler_->Reschedule(actor);\n      }\n    }\n </s> remove   // Invoke all callbacks for all registration requests of this actor (duplicated\n  // requests are included) and remove all of them from actor_to_register_callbacks_.\n  auto iter = actor_to_register_callbacks_.find(actor_id);\n  if (iter != actor_to_register_callbacks_.end()) {\n    for (auto &callback : iter->second) {\n      callback(actor);\n    }\n    actor_to_register_callbacks_.erase(iter);\n  }\n </s> add         // Invoke all callbacks for all registration requests of this actor (duplicated\n        // requests are included) and remove all of them from\n        // actor_to_register_callbacks_.\n        auto iter = actor_to_register_callbacks_.find(actor_id);\n        if (iter != actor_to_register_callbacks_.end()) {\n          for (auto &callback : iter->second) {\n            callback(actor);\n          }\n          actor_to_register_callbacks_.erase(iter);\n        }", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/gcs_server.cc"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask> #include <ray/gcs/gcs_server/test/gcs_server_test_util.h>\n <mask> #include <ray/gcs/test/gcs_test_util.h>\n <mask> \n <mask> #include <memory>\n <mask> \n <mask> #include \"gtest/gtest.h\"\n <mask> \n </s> GCS server error handling for actor creation (#8899) </s> remove     RAY_LOG(WARNING) << \"Submitted task \" << task_id\n                     << \" is already queued and will not be restarted. This is most \"\n                        \"likely due to spurious reconstruction.\";\n    return;\n </s> add     if (spec.IsActorCreationTask()) {\n      RAY_LOG(WARNING) << \"Submitted actor creation task \" << task_id\n                       << \" is already queued. This is most likely due to a GCS restart. \"\n                          \"We will remove \"\n                          \"the old one from the queue, and enqueue the new one.\";\n      std::unordered_set<TaskID> task_ids{task_id};\n      local_queues_.RemoveTasks(task_ids);\n    } else {\n      RAY_LOG(WARNING) << \"Submitted task \" << task_id\n                       << \" is already queued and will not be restarted. This is most \"\n                          \"likely due to spurious reconstruction.\";\n      return;\n    } </s> remove   auto worker_id = actor->GetWorkerID();\n  auto node_id = actor->GetNodeID();\n  RAY_CHECK(!worker_id.IsNil());\n  RAY_CHECK(!node_id.IsNil());\n  RAY_CHECK(created_actors_[node_id].emplace(worker_id, actor_id).second);\n </s> add         auto worker_id = actor->GetWorkerID();\n        auto node_id = actor->GetNodeID();\n        RAY_CHECK(!worker_id.IsNil());\n        RAY_CHECK(!node_id.IsNil());\n        RAY_CHECK(created_actors_[node_id].emplace(worker_id, actor_id).second);\n      })); </s> add   RAY_LOG(INFO) << \"Removing node, node id = \" << node_id; </s> add   /// Reschedule the specified actor after gcs server restarts.\n  ///\n  /// \\param actor to be scheduled.\n  void Reschedule(std::shared_ptr<GcsActor> actor) override;\n </s> add   /// Reschedule the specified actor after gcs server restarts.\n  ///\n  /// \\param actor to be scheduled.\n  virtual void Reschedule(std::shared_ptr<GcsActor> actor) = 0;\n </s> remove     CreateActorOnWorker(actor, leased_worker);\n </s> add     RAY_CHECK_OK(gcs_actor_table_.Put(actor->GetActorID(), actor->GetActorTableData(),\n                                      [this, actor, leased_worker](Status status) {\n                                        RAY_CHECK_OK(status);\n                                        CreateActorOnWorker(actor, leased_worker);\n                                      }));", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>   MockActorScheduler() {}\n <mask> \n <mask>   void Schedule(std::shared_ptr<gcs::GcsActor> actor) { actors.push_back(actor); }\n <mask> \n <mask>   MOCK_METHOD1(CancelOnNode, std::vector<ActorID>(const ClientID &node_id));\n <mask>   MOCK_METHOD2(CancelOnWorker,\n <mask>                ActorID(const ClientID &node_id, const WorkerID &worker_id));\n <mask> \n <mask>   std::vector<std::shared_ptr<gcs::GcsActor>> actors;\n </s> GCS server error handling for actor creation (#8899) </s> remove   RAY_LOG(INFO) << \"Node \" << node_id << \" failed, reconstructing actors\";\n </s> add   RAY_LOG(WARNING) << \"Node \" << node_id << \" failed, reconstructing actors.\"; </s> add   const std::chrono::milliseconds timeout_ms_{2000}; </s> add   RAY_LOG(INFO) << \"Removing node, node id = \" << node_id; </s> remove       create_actor_request, [&finished_actors](std::shared_ptr<gcs::GcsActor> actor) {\n </s> add       create_actor_request,\n      [&finished_actors](const std::shared_ptr<gcs::GcsActor> &actor) { </s> add   /// Get the created actors.\n  ///\n  /// \\return The created actors.\n  const absl::flat_hash_map<ClientID, absl::flat_hash_map<WorkerID, ActorID>>\n      &GetCreatedActors() const;\n </s> remove   // The backend storage is reliable in the future, so the status must be ok.\n  RAY_CHECK_OK(gcs_actor_table_.Put(\n      actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n        RAY_CHECK_OK(status);\n        RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n                                           actor->GetActorTableData().SerializeAsString(),\n                                           nullptr));\n        // There is no promise that the node the\n        // actor tied to is still alive as the\n        // flush is asynchronously, so just\n        // invoke `Schedule` which will lease\n        // worker directly if the node is still\n        // available or select a new one if not.\n        Schedule(actor);\n      }));\n </s> add   RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                .emplace(actor->GetActorID())\n                .second);\n\n  // Lease worker directly from the node.\n  LeaseWorkerFromNode(actor, node);\n}\n\nvoid GcsActorScheduler::Reschedule(std::shared_ptr<GcsActor> actor) {\n  if (!actor->GetWorkerID().IsNil()) {\n    RAY_LOG(INFO)\n        << \"Actor \" << actor->GetActorID()\n        << \" is already tied to a leased worker. Create actor directly on worker.\";\n    auto leased_worker = std::make_shared<GcsLeasedWorker>(\n        actor->GetAddress(),\n        VectorFromProtobuf(actor->GetMutableActorTableData()->resource_mapping()),\n        actor->GetActorID());\n    auto iter_node = node_to_workers_when_creating_.find(actor->GetNodeID());\n    if (iter_node != node_to_workers_when_creating_.end()) {\n      if (0 == iter_node->second.count(leased_worker->GetWorkerID())) {\n        iter_node->second.emplace(leased_worker->GetWorkerID(), leased_worker);\n      }\n    } else {\n      node_to_workers_when_creating_[actor->GetNodeID()].emplace(\n          leased_worker->GetWorkerID(), leased_worker);\n    }\n    CreateActorOnWorker(actor, leased_worker);\n  } else {\n    Schedule(actor);\n  }", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>       : mock_actor_scheduler_(new MockActorScheduler()),\n <mask>         worker_client_(new MockWorkerClient()) {\n <mask>     gcs_pub_sub_ = std::make_shared<GcsServerMocker::MockGcsPubSub>(redis_client_);\n <mask>     store_client_ = std::make_shared<gcs::InMemoryStoreClient>(io_service_);\n <mask>     gcs_table_storage_ = std::make_shared<gcs::InMemoryGcsTableStorage>(io_service_);\n <mask>     gcs_actor_manager_.reset(new gcs::GcsActorManager(\n <mask>         mock_actor_scheduler_, gcs_table_storage_, gcs_pub_sub_,\n <mask>         [&](const rpc::Address &addr) { return worker_client_; }));\n </s> GCS server error handling for actor creation (#8899) </s> remove   // Invoke all callbacks for all registration requests of this actor (duplicated\n  // requests are included) and remove all of them from actor_to_register_callbacks_.\n  auto iter = actor_to_register_callbacks_.find(actor_id);\n  if (iter != actor_to_register_callbacks_.end()) {\n    for (auto &callback : iter->second) {\n      callback(actor);\n    }\n    actor_to_register_callbacks_.erase(iter);\n  }\n </s> add         // Invoke all callbacks for all registration requests of this actor (duplicated\n        // requests are included) and remove all of them from\n        // actor_to_register_callbacks_.\n        auto iter = actor_to_register_callbacks_.find(actor_id);\n        if (iter != actor_to_register_callbacks_.end()) {\n          for (auto &callback : iter->second) {\n            callback(actor);\n          }\n          actor_to_register_callbacks_.erase(iter);\n        } </s> remove   auto worker_id = actor->GetWorkerID();\n  auto node_id = actor->GetNodeID();\n  RAY_CHECK(!worker_id.IsNil());\n  RAY_CHECK(!node_id.IsNil());\n  RAY_CHECK(created_actors_[node_id].emplace(worker_id, actor_id).second);\n </s> add         auto worker_id = actor->GetWorkerID();\n        auto node_id = actor->GetNodeID();\n        RAY_CHECK(!worker_id.IsNil());\n        RAY_CHECK(!node_id.IsNil());\n        RAY_CHECK(created_actors_[node_id].emplace(worker_id, actor_id).second);\n      })); </s> add       actor->GetMutableActorTableData()->add_resource_mapping()->CopyFrom(resource); </s> remove       create_actor_request, [&finished_actors](std::shared_ptr<gcs::GcsActor> actor) {\n </s> add       create_actor_request,\n      [&finished_actors](const std::shared_ptr<gcs::GcsActor> &actor) { </s> remove     actor->UpdateAddress(retry_at_raylet_address);\n    // The backend storage is reliable in the future, so the status must be ok.\n    RAY_CHECK_OK(gcs_actor_table_.Put(\n        actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n          RAY_CHECK_OK(status);\n          RAY_CHECK_OK(gcs_pub_sub_->Publish(\n              ACTOR_CHANNEL, actor->GetActorID().Hex(),\n              actor->GetActorTableData().SerializeAsString(), nullptr));\n          Schedule(actor);\n        }));\n </s> add     auto spill_back_node_id = ClientID::FromBinary(retry_at_raylet_address.raylet_id());\n    if (auto spill_back_node = gcs_node_manager_.GetNode(spill_back_node_id)) {\n      actor->UpdateAddress(retry_at_raylet_address);\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, spill_back_node);\n    } else {\n      // If the spill back node is dead, we need to schedule again.\n      actor->UpdateAddress(rpc::Address());\n      actor->GetMutableActorTableData()->clear_resource_mapping();\n      Schedule(actor);\n    } </s> add     RAY_LOG(DEBUG) << \"The number of registered actors is \" << registered_actors_.size()\n                   << \", and the number of created actors is \" << created_actors_.size();\n    for (auto &item : registered_actors_) {\n      auto &actor = item.second;\n      if (actor->GetState() != ray::rpc::ActorTableData::ALIVE) {\n        RAY_LOG(DEBUG) << \"Rescheduling a non-alive actor, actor id = \"\n                       << actor->GetActorID() << \", state = \" << actor->GetState();\n        gcs_actor_scheduler_->Reschedule(actor);\n      }\n    }\n", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask> \n <mask>   boost::asio::io_service io_service_;\n <mask>   std::shared_ptr<gcs::StoreClient> store_client_;\n <mask>   std::shared_ptr<gcs::GcsTableStorage> gcs_table_storage_;\n <mask>   std::shared_ptr<MockActorScheduler> mock_actor_scheduler_;\n <mask>   std::shared_ptr<MockWorkerClient> worker_client_;\n <mask>   std::unique_ptr<gcs::GcsActorManager> gcs_actor_manager_;\n </s> GCS server error handling for actor creation (#8899) </s> add     std::promise<bool> promise;\n    thread_io_service_.reset(new std::thread([this, &promise] {\n      std::unique_ptr<boost::asio::io_service::work> work(\n          new boost::asio::io_service::work(io_service_));\n      promise.set_value(true);\n      io_service_.run();\n    }));\n    promise.get_future().get();\n </s> remove     RAY_LOG(WARNING) << \"Submitted task \" << task_id\n                     << \" is already queued and will not be restarted. This is most \"\n                        \"likely due to spurious reconstruction.\";\n    return;\n </s> add     if (spec.IsActorCreationTask()) {\n      RAY_LOG(WARNING) << \"Submitted actor creation task \" << task_id\n                       << \" is already queued. This is most likely due to a GCS restart. \"\n                          \"We will remove \"\n                          \"the old one from the queue, and enqueue the new one.\";\n      std::unordered_set<TaskID> task_ids{task_id};\n      local_queues_.RemoveTasks(task_ids);\n    } else {\n      RAY_LOG(WARNING) << \"Submitted task \" << task_id\n                       << \" is already queued and will not be restarted. This is most \"\n                          \"likely due to spurious reconstruction.\";\n      return;\n    } </s> remove   auto worker_id = actor->GetWorkerID();\n  auto node_id = actor->GetNodeID();\n  RAY_CHECK(!worker_id.IsNil());\n  RAY_CHECK(!node_id.IsNil());\n  RAY_CHECK(created_actors_[node_id].emplace(worker_id, actor_id).second);\n </s> add         auto worker_id = actor->GetWorkerID();\n        auto node_id = actor->GetNodeID();\n        RAY_CHECK(!worker_id.IsNil());\n        RAY_CHECK(!node_id.IsNil());\n        RAY_CHECK(created_actors_[node_id].emplace(worker_id, actor_id).second);\n      })); </s> add   /// Reschedule the specified actor after gcs server restarts.\n  ///\n  /// \\param actor to be scheduled.\n  void Reschedule(std::shared_ptr<GcsActor> actor) override;\n </s> add   /// Reschedule the specified actor after gcs server restarts.\n  ///\n  /// \\param actor to be scheduled.\n  virtual void Reschedule(std::shared_ptr<GcsActor> actor) = 0;\n </s> remove     CreateActorOnWorker(actor, leased_worker);\n </s> add     RAY_CHECK_OK(gcs_actor_table_.Put(actor->GetActorID(), actor->GetActorTableData(),\n                                      [this, actor, leased_worker](Status status) {\n                                        RAY_CHECK_OK(status);\n                                        CreateActorOnWorker(actor, leased_worker);\n                                      }));", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep add keep keep keep keep keep keep", "code_tokens": " <mask>   std::shared_ptr<GcsServerMocker::MockGcsPubSub> gcs_pub_sub_;\n <mask>   std::shared_ptr<gcs::RedisClient> redis_client_;\n <mask> };\n <mask> \n <mask> TEST_F(GcsActorManagerTest, TestBasic) {\n <mask>   auto job_id = JobID::FromInt(1);\n <mask>   auto create_actor_request = Mocker::GenCreateActorRequest(job_id);\n <mask>   std::vector<std::shared_ptr<gcs::GcsActor>> finished_actors;\n </s> GCS server error handling for actor creation (#8899) </s> remove       create_actor_request, [&finished_actors](std::shared_ptr<gcs::GcsActor> actor) {\n </s> add       create_actor_request,\n      [&finished_actors](const std::shared_ptr<gcs::GcsActor> &actor) { </s> add   WaitActorCreated(actor->GetActorID()); </s> remove   auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor1\");\n </s> add   auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor1\"); </s> remove   const auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   const auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 1, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request1 = Mocker::GenCreateActorRequest(job_id_1, 1, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   gcs_actor_manager_->LoadInitialData(on_done);\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   auto job_id = JobID::FromInt(1);\n <mask>   auto create_actor_request = Mocker::GenCreateActorRequest(job_id);\n <mask>   std::vector<std::shared_ptr<gcs::GcsActor>> finished_actors;\n <mask>   Status status = gcs_actor_manager_->RegisterActor(\n <mask>       create_actor_request, [&finished_actors](std::shared_ptr<gcs::GcsActor> actor) {\n <mask>         finished_actors.emplace_back(actor);\n <mask>       });\n <mask>   RAY_CHECK_OK(status);\n <mask> \n <mask>   ASSERT_EQ(finished_actors.size(), 0);\n </s> GCS server error handling for actor creation (#8899) </s> add   const std::chrono::milliseconds timeout_ms_{2000}; </s> add   WaitActorCreated(actor->GetActorID()); </s> remove   auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor1\");\n </s> add   auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor1\"); </s> remove   const auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   const auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 1, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request1 = Mocker::GenCreateActorRequest(job_id_1, 1, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> add   RAY_LOG(DEBUG) << \"Actor created successfully, actor id = \" << actor_id;", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>   address.set_worker_id(worker_id.Binary());\n <mask>   actor->UpdateAddress(address);\n <mask>   gcs_actor_manager_->OnActorCreationSuccess(actor);\n <mask>   ASSERT_EQ(finished_actors.size(), 1);\n <mask> \n <mask>   ASSERT_TRUE(worker_client_->Reply());\n <mask>   ASSERT_EQ(actor->GetState(), rpc::ActorTableData::DEAD);\n <mask> }\n <mask> \n </s> GCS server error handling for actor creation (#8899) </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID());", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>   address.set_raylet_id(node_id.Binary());\n <mask>   address.set_worker_id(worker_id.Binary());\n <mask>   actor->UpdateAddress(address);\n <mask>   gcs_actor_manager_->OnActorCreationSuccess(actor);\n <mask>   ASSERT_EQ(finished_actors.size(), 1);\n <mask> }\n <mask> \n <mask> TEST_F(GcsActorManagerTest, TestWorkerFailure) {\n <mask>   auto job_id = JobID::FromInt(1);\n <mask>   auto create_actor_request = Mocker::GenCreateActorRequest(job_id);\n </s> GCS server error handling for actor creation (#8899) </s> add   const std::chrono::milliseconds timeout_ms_{2000}; </s> remove       create_actor_request, [&finished_actors](std::shared_ptr<gcs::GcsActor> actor) {\n </s> add       create_actor_request,\n      [&finished_actors](const std::shared_ptr<gcs::GcsActor> &actor) { </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID());", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>   address.set_raylet_id(node_id.Binary());\n <mask>   address.set_worker_id(worker_id.Binary());\n <mask>   actor->UpdateAddress(address);\n <mask>   gcs_actor_manager_->OnActorCreationSuccess(actor);\n <mask>   ASSERT_EQ(finished_actors.size(), 1);\n <mask> \n <mask>   // Killing another worker does not affect this actor.\n <mask>   EXPECT_CALL(*mock_actor_scheduler_, CancelOnWorker(node_id, _));\n <mask>   gcs_actor_manager_->OnWorkerDead(node_id, WorkerID::FromRandom());\n </s> GCS server error handling for actor creation (#8899) </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID());", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>   address.set_raylet_id(node_id.Binary());\n <mask>   address.set_worker_id(worker_id.Binary());\n <mask>   actor->UpdateAddress(address);\n <mask>   gcs_actor_manager_->OnActorCreationSuccess(actor);\n <mask>   ASSERT_EQ(finished_actors.size(), 1);\n <mask> \n <mask>   // Killing another node does not affect this actor.\n <mask>   EXPECT_CALL(*mock_actor_scheduler_, CancelOnNode(_));\n <mask>   gcs_actor_manager_->OnNodeDead(ClientID::FromRandom());\n </s> GCS server error handling for actor creation (#8899) </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID());", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep add keep keep keep keep", "code_tokens": " <mask>   address.set_worker_id(worker_id.Binary());\n <mask>   actor->UpdateAddress(address);\n <mask>   gcs_actor_manager_->OnActorCreationSuccess(actor);\n <mask>   ASSERT_EQ(finished_actors.size(), 1);\n <mask> \n <mask>   // Remove worker and then check that the actor is being restarted.\n <mask>   EXPECT_CALL(*mock_actor_scheduler_, CancelOnNode(node_id));\n </s> GCS server error handling for actor creation (#8899) </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID());", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>   address.set_raylet_id(node_id2.Binary());\n <mask>   actor->UpdateAddress(address);\n <mask>   gcs_actor_manager_->OnActorCreationSuccess(actor);\n <mask>   ASSERT_EQ(finished_actors.size(), 1);\n <mask>   ASSERT_EQ(actor->GetState(), rpc::ActorTableData::ALIVE);\n <mask>   ASSERT_EQ(actor->GetNodeID(), node_id2);\n <mask> \n <mask>   // Killing another worker does not affect this actor.\n </s> GCS server error handling for actor creation (#8899) </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID());", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask>   address.set_raylet_id(node_id.Binary());\n <mask>   address.set_worker_id(worker_id.Binary());\n <mask>   actor->UpdateAddress(address);\n <mask>   gcs_actor_manager_->OnActorCreationSuccess(actor);\n <mask>   ASSERT_EQ(finished_actors.size(), 1);\n <mask> \n <mask>   // Remove the owner's node.\n <mask>   EXPECT_CALL(*mock_actor_scheduler_, CancelOnNode(owner_node_id));\n </s> GCS server error handling for actor creation (#8899) </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID());", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep add keep keep keep keep keep", "code_tokens": " <mask>   address.set_worker_id(worker_id.Binary());\n <mask>   actor->UpdateAddress(address);\n <mask>   gcs_actor_manager_->OnActorCreationSuccess(actor);\n <mask>   ASSERT_EQ(finished_actors.size(), 1);\n <mask> \n <mask>   // Remove the owner's node.\n <mask>   EXPECT_CALL(*mock_actor_scheduler_, CancelOnNode(owner_node_id));\n <mask>   gcs_actor_manager_->OnNodeDead(owner_node_id);\n </s> GCS server error handling for actor creation (#8899) </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID());", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> TEST_F(GcsActorManagerTest, TestNamedActors) {\n <mask>   auto job_id_1 = JobID::FromInt(1);\n <mask>   auto job_id_2 = JobID::FromInt(2);\n <mask> \n <mask>   auto request1 =\n <mask>       Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor1\");\n <mask>   Status status = gcs_actor_manager_->RegisterActor(\n <mask>       request1, [](std::shared_ptr<gcs::GcsActor> actor) {});\n <mask>   ASSERT_TRUE(status.ok());\n <mask>   ASSERT_EQ(gcs_actor_manager_->GetActorIDByName(\"actor1\").Binary(),\n <mask>             request1.task_spec().actor_creation_task_spec().actor_id());\n </s> GCS server error handling for actor creation (#8899) </s> remove   const auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   const auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 1, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request1 = Mocker::GenCreateActorRequest(job_id_1, 1, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   auto request2 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request2 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\"); </s> remove   const auto request2 =\n      Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request2 = Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   const auto request2 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request2 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   auto request3 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request3 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\");", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>   ASSERT_TRUE(status.ok());\n <mask>   ASSERT_EQ(gcs_actor_manager_->GetActorIDByName(\"actor1\").Binary(),\n <mask>             request1.task_spec().actor_creation_task_spec().actor_id());\n <mask> \n <mask>   auto request2 =\n <mask>       Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n <mask>   status = gcs_actor_manager_->RegisterActor(request2,\n <mask>                                              [](std::shared_ptr<gcs::GcsActor> actor) {});\n <mask>   ASSERT_TRUE(status.ok());\n <mask>   ASSERT_EQ(gcs_actor_manager_->GetActorIDByName(\"actor2\").Binary(),\n <mask>             request2.task_spec().actor_creation_task_spec().actor_id());\n </s> GCS server error handling for actor creation (#8899) </s> remove   const auto request2 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request2 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor1\");\n </s> add   auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor1\"); </s> remove   auto request4 =\n      Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request4 = Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\"); </s> remove   auto request3 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request3 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\"); </s> remove   const auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   const auto request2 =\n      Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request2 = Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\");", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>   // Check that looking up a non-existent name returns ActorID::Nil();\n <mask>   ASSERT_EQ(gcs_actor_manager_->GetActorIDByName(\"actor3\"), ActorID::Nil());\n <mask> \n <mask>   // Check that naming collisions return Status::Invalid.\n <mask>   auto request3 =\n <mask>       Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n <mask>   status = gcs_actor_manager_->RegisterActor(request3,\n <mask>                                              [](std::shared_ptr<gcs::GcsActor> actor) {});\n <mask>   ASSERT_TRUE(status.IsInvalid());\n <mask>   ASSERT_EQ(gcs_actor_manager_->GetActorIDByName(\"actor2\").Binary(),\n <mask>             request2.task_spec().actor_creation_task_spec().actor_id());\n </s> GCS server error handling for actor creation (#8899) </s> remove   auto request4 =\n      Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request4 = Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\"); </s> remove   auto request2 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request2 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\"); </s> remove   const auto request2 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request2 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   const auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor1\");\n </s> add   auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor1\"); </s> remove   const auto request2 =\n      Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request2 = Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\");", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>   ASSERT_EQ(gcs_actor_manager_->GetActorIDByName(\"actor2\").Binary(),\n <mask>             request2.task_spec().actor_creation_task_spec().actor_id());\n <mask> \n <mask>   // Check that naming collisions are enforced across JobIDs.\n <mask>   auto request4 =\n <mask>       Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n <mask>   status = gcs_actor_manager_->RegisterActor(request4,\n <mask>                                              [](std::shared_ptr<gcs::GcsActor> actor) {});\n <mask>   ASSERT_TRUE(status.IsInvalid());\n <mask>   ASSERT_EQ(gcs_actor_manager_->GetActorIDByName(\"actor2\").Binary(),\n <mask>             request2.task_spec().actor_creation_task_spec().actor_id());\n </s> GCS server error handling for actor creation (#8899) </s> remove   auto request3 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request3 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\"); </s> remove   auto request2 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request2 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\"); </s> remove   const auto request2 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request2 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   const auto request2 =\n      Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request2 = Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor1\");\n </s> add   auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor1\"); </s> remove   const auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\");", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask>   actor->UpdateAddress(address);\n <mask>   gcs_actor_manager_->OnActorCreationSuccess(actor);\n <mask> \n <mask>   // Remove worker and then check that the actor is dead.\n <mask>   gcs_actor_manager_->OnWorkerDead(node_id, worker_id);\n <mask>   ASSERT_EQ(actor->GetState(), rpc::ActorTableData::DEAD);\n <mask>   ASSERT_EQ(gcs_actor_manager_->GetActorIDByName(actor_name), ActorID::Nil());\n </s> GCS server error handling for actor creation (#8899) </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID());", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> TEST_F(GcsActorManagerTest, TestNamedActorDeletionNodeFailure) {\n <mask>   // Make sure named actor deletion succeeds when nodes fail.\n <mask>   const auto job_id_1 = JobID::FromInt(1);\n <mask>   const auto request1 =\n <mask>       Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n <mask>   Status status = gcs_actor_manager_->RegisterActor(\n <mask>       request1, [](std::shared_ptr<gcs::GcsActor> actor) {});\n <mask>   ASSERT_TRUE(status.ok());\n <mask>   ASSERT_EQ(gcs_actor_manager_->GetActorIDByName(\"actor\").Binary(),\n <mask>             request1.task_spec().actor_creation_task_spec().actor_id());\n </s> GCS server error handling for actor creation (#8899) </s> remove   const auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 1, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request1 = Mocker::GenCreateActorRequest(job_id_1, 1, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor1\");\n </s> add   auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor1\"); </s> remove   const auto request2 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request2 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   const auto request2 =\n      Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request2 = Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   auto request2 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request2 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\"); </s> remove   auto request3 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request3 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\");", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>   address.set_raylet_id(node_id.Binary());\n <mask>   address.set_worker_id(worker_id.Binary());\n <mask>   actor->UpdateAddress(address);\n <mask>   gcs_actor_manager_->OnActorCreationSuccess(actor);\n <mask> \n <mask>   // Remove node and then check that the actor is dead.\n <mask>   EXPECT_CALL(*mock_actor_scheduler_, CancelOnNode(node_id));\n <mask>   gcs_actor_manager_->OnNodeDead(node_id);\n <mask>   ASSERT_EQ(actor->GetState(), rpc::ActorTableData::DEAD);\n <mask> \n </s> GCS server error handling for actor creation (#8899) </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID());", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>   ASSERT_EQ(actor->GetState(), rpc::ActorTableData::DEAD);\n <mask> \n <mask>   // Create an actor with the same name. This ensures that the name has been properly\n <mask>   // deleted.\n <mask>   const auto request2 =\n <mask>       Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n <mask>   status = gcs_actor_manager_->RegisterActor(request2,\n <mask>                                              [](std::shared_ptr<gcs::GcsActor> actor) {});\n <mask>   ASSERT_TRUE(status.ok());\n <mask>   ASSERT_EQ(gcs_actor_manager_->GetActorIDByName(\"actor\").Binary(),\n <mask>             request2.task_spec().actor_creation_task_spec().actor_id());\n </s> GCS server error handling for actor creation (#8899) </s> remove   const auto request2 =\n      Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request2 = Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   auto request2 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request2 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\"); </s> remove   const auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   auto request3 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request3 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\"); </s> remove   const auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 1, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request1 = Mocker::GenCreateActorRequest(job_id_1, 1, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   auto request4 =\n      Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request4 = Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\");", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask> TEST_F(GcsActorManagerTest, TestNamedActorDeletionNotHappendWhenReconstructed) {\n <mask>   // Make sure named actor deletion succeeds when nodes fail.\n <mask>   const auto job_id_1 = JobID::FromInt(1);\n <mask>   // The dead actor will be reconstructed.\n <mask>   const auto request1 =\n <mask>       Mocker::GenCreateActorRequest(job_id_1, 1, /*is_detached=*/true, /*name=*/\"actor\");\n <mask>   Status status = gcs_actor_manager_->RegisterActor(\n <mask>       request1, [](std::shared_ptr<gcs::GcsActor> actor) {});\n <mask>   ASSERT_TRUE(status.ok());\n <mask>   ASSERT_EQ(gcs_actor_manager_->GetActorIDByName(\"actor\").Binary(),\n <mask>             request1.task_spec().actor_creation_task_spec().actor_id());\n </s> GCS server error handling for actor creation (#8899) </s> remove   const auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor1\");\n </s> add   auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor1\"); </s> remove   const auto request2 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request2 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   const auto request2 =\n      Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request2 = Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   auto request2 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request2 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\"); </s> remove   auto request3 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request3 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\");", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask>   address.set_raylet_id(node_id.Binary());\n <mask>   address.set_worker_id(worker_id.Binary());\n <mask>   actor->UpdateAddress(address);\n <mask>   gcs_actor_manager_->OnActorCreationSuccess(actor);\n <mask> \n <mask>   // Remove worker and then check that the actor is dead. The actor should be\n <mask>   // reconstructed.\n <mask>   gcs_actor_manager_->OnWorkerDead(node_id, worker_id);\n </s> GCS server error handling for actor creation (#8899) </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID()); </s> add   WaitActorCreated(actor->GetActorID());", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep keep keep replace replace keep keep keep keep keep", "code_tokens": " <mask>   // Create an actor with the same name.\n <mask>   // It should fail because actor has been reconstructed, and names shouldn't have been\n <mask>   // cleaned.\n <mask>   const auto job_id_2 = JobID::FromInt(2);\n <mask>   const auto request2 =\n <mask>       Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true, /*name=*/\"actor\");\n <mask>   status = gcs_actor_manager_->RegisterActor(request2,\n <mask>                                              [](std::shared_ptr<gcs::GcsActor> actor) {});\n <mask>   ASSERT_TRUE(status.IsInvalid());\n <mask>   ASSERT_EQ(gcs_actor_manager_->GetActorIDByName(\"actor\").Binary(),\n <mask>             request1.task_spec().actor_creation_task_spec().actor_id());\n </s> GCS server error handling for actor creation (#8899) </s> remove   const auto request2 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request2 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   const auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   auto request2 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request2 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\"); </s> remove   const auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 1, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request1 = Mocker::GenCreateActorRequest(job_id_1, 1, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   auto request4 =\n      Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true, /*name=*/\"actor2\");\n </s> add   auto request4 = Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor2\"); </s> remove   auto request1 =\n      Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true, /*name=*/\"actor1\");\n </s> add   auto request1 = Mocker::GenCreateActorRequest(job_id_1, 0, /*is_detached=*/true,\n                                                /*name=*/\"actor1\");", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_manager_test.cc"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep keep", "code_tokens": " <mask>   auto node_id_2 = ClientID::FromBinary(node2->node_id());\n <mask>   gcs_node_manager_->AddNode(node2);\n <mask>   ASSERT_EQ(2, gcs_node_manager_->GetAllAliveNodes().size());\n <mask> \n <mask>   // Grant with a spillback node(node2), and the lease request should be send to the\n <mask>   // node2.\n <mask>   ASSERT_TRUE(raylet_client_->GrantWorkerLease(node2->node_manager_address(),\n <mask>                                                node2->node_manager_port(),\n <mask>                                                WorkerID::Nil(), node_id_1, node_id_2));\n <mask>   ASSERT_EQ(3, raylet_client_->num_workers_requested);\n </s> GCS server error handling for actor creation (#8899) </s> remove   ASSERT_EQ(2, raylet_client_->num_workers_requested);\n </s> add   ASSERT_EQ(3, raylet_client_->num_workers_requested); </s> remove   auto node_id = actor->GetNodeID();\n  if (!node_id.IsNil()) {\n    if (auto node = gcs_node_manager_.GetNode(node_id)) {\n      // If the actor is already tied to a node and the node is available, then record\n      // the relationship of the node and actor and then lease worker directly from the\n      // node.\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, node);\n      return;\n    }\n\n    // The actor is already tied to a node which is unavailable now, so we should reset\n    // the address.\n    actor->UpdateAddress(rpc::Address());\n  }\n </s> add   RAY_CHECK(actor->GetNodeID().IsNil() && actor->GetWorkerID().IsNil()); </s> remove   const auto request2 =\n      Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true, /*name=*/\"actor\");\n </s> add   const auto request2 = Mocker::GenCreateActorRequest(job_id_2, 0, /*is_detached=*/true,\n                                                      /*name=*/\"actor\"); </s> remove   // The backend storage is reliable in the future, so the status must be ok.\n  RAY_CHECK_OK(gcs_actor_table_.Put(\n      actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n        RAY_CHECK_OK(status);\n        RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n                                           actor->GetActorTableData().SerializeAsString(),\n                                           nullptr));\n        // There is no promise that the node the\n        // actor tied to is still alive as the\n        // flush is asynchronously, so just\n        // invoke `Schedule` which will lease\n        // worker directly if the node is still\n        // available or select a new one if not.\n        Schedule(actor);\n      }));\n </s> add   RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                .emplace(actor->GetActorID())\n                .second);\n\n  // Lease worker directly from the node.\n  LeaseWorkerFromNode(actor, node);\n}\n\nvoid GcsActorScheduler::Reschedule(std::shared_ptr<GcsActor> actor) {\n  if (!actor->GetWorkerID().IsNil()) {\n    RAY_LOG(INFO)\n        << \"Actor \" << actor->GetActorID()\n        << \" is already tied to a leased worker. Create actor directly on worker.\";\n    auto leased_worker = std::make_shared<GcsLeasedWorker>(\n        actor->GetAddress(),\n        VectorFromProtobuf(actor->GetMutableActorTableData()->resource_mapping()),\n        actor->GetActorID());\n    auto iter_node = node_to_workers_when_creating_.find(actor->GetNodeID());\n    if (iter_node != node_to_workers_when_creating_.end()) {\n      if (0 == iter_node->second.count(leased_worker->GetWorkerID())) {\n        iter_node->second.emplace(leased_worker->GetWorkerID(), leased_worker);\n      }\n    } else {\n      node_to_workers_when_creating_[actor->GetNodeID()].emplace(\n          leased_worker->GetWorkerID(), leased_worker);\n    }\n    CreateActorOnWorker(actor, leased_worker);\n  } else {\n    Schedule(actor);\n  } </s> add   WaitActorCreated(actor->GetActorID()); </s> add   // If GCS server is restarted after sending an actor creation task to this core worker,\n  // the restarted GCS server will send the same actor creation task to the core worker\n  // again. We just need to ignore it and reply ok.\n  if (task_spec.IsActorCreationTask() &&\n      worker_context_.GetCurrentActorID() == task_spec.ActorCreationId()) {\n    send_reply_callback(Status::OK(), nullptr, nullptr);\n    RAY_LOG(INFO) << \"Ignoring duplicate actor creation task for actor \"\n                  << task_spec.ActorCreationId()\n                  << \". This is likely due to a GCS server restart.\";\n    return;\n  }\n", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_scheduler_test.cc"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>   // node2.\n <mask>   ASSERT_TRUE(raylet_client_->GrantWorkerLease(node2->node_manager_address(),\n <mask>                                                node2->node_manager_port(),\n <mask>                                                WorkerID::Nil(), node_id_1, node_id_2));\n <mask>   ASSERT_EQ(2, raylet_client_->num_workers_requested);\n <mask>   ASSERT_EQ(1, raylet_client_->callbacks.size());\n <mask>   ASSERT_EQ(0, worker_client_->callbacks.size());\n <mask> \n <mask>   // Grant a worker, then the actor creation request should be send to the worker.\n <mask>   WorkerID worker_id = WorkerID::FromRandom();\n </s> GCS server error handling for actor creation (#8899) </s> add   // Grant with an invalid spillback node, and schedule again.\n  auto invalid_node_id = ClientID::FromBinary(Mocker::GenNodeInfo()->node_id());\n  ASSERT_TRUE(raylet_client_->GrantWorkerLease(\n      node2->node_manager_address(), node2->node_manager_port(), WorkerID::Nil(),\n      node_id_1, invalid_node_id));\n  ASSERT_EQ(2, raylet_client_->num_workers_requested);\n  ASSERT_EQ(1, raylet_client_->callbacks.size());\n  ASSERT_EQ(0, worker_client_->callbacks.size());\n </s> add   // If GCS server is restarted after sending an actor creation task to this core worker,\n  // the restarted GCS server will send the same actor creation task to the core worker\n  // again. We just need to ignore it and reply ok.\n  if (task_spec.IsActorCreationTask() &&\n      worker_context_.GetCurrentActorID() == task_spec.ActorCreationId()) {\n    send_reply_callback(Status::OK(), nullptr, nullptr);\n    RAY_LOG(INFO) << \"Ignoring duplicate actor creation task for actor \"\n                  << task_spec.ActorCreationId()\n                  << \". This is likely due to a GCS server restart.\";\n    return;\n  }\n </s> add   // Resource mapping ids acquired by the leased worker. This field is only set when this\n  // actor already has a leased worker.\n  repeated ResourceMapEntry resource_mapping = 15; </s> remove   auto node_id = actor->GetNodeID();\n  if (!node_id.IsNil()) {\n    if (auto node = gcs_node_manager_.GetNode(node_id)) {\n      // If the actor is already tied to a node and the node is available, then record\n      // the relationship of the node and actor and then lease worker directly from the\n      // node.\n      RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                    .emplace(actor->GetActorID())\n                    .second);\n      LeaseWorkerFromNode(actor, node);\n      return;\n    }\n\n    // The actor is already tied to a node which is unavailable now, so we should reset\n    // the address.\n    actor->UpdateAddress(rpc::Address());\n  }\n </s> add   RAY_CHECK(actor->GetNodeID().IsNil() && actor->GetWorkerID().IsNil()); </s> add   WaitActorCreated(actor->GetActorID()); </s> remove   // Update the address of the actor as it is tied to a new node.\n </s> add   // Update the address of the actor as it is tied to a node.", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/gcs/gcs_server/test/gcs_actor_scheduler_test.cc"}
{"docstring_tokens": "keep keep keep add keep keep keep keep", "code_tokens": " <mask>   // Timestamp that the actor is created or reconstructed.\n <mask>   double timestamp = 13;\n <mask>   // The task specification of this actor's creation task.\n <mask>   TaskSpec task_spec = 14;\n <mask> }\n <mask> \n <mask> message ErrorTableData {\n <mask>   // The ID of the job that the error is for.\n </s> GCS server error handling for actor creation (#8899) </s> add   WaitActorCreated(actor->GetActorID()); </s> add   /// Get the created actors.\n  ///\n  /// \\return The created actors.\n  const absl::flat_hash_map<ClientID, absl::flat_hash_map<WorkerID, ActorID>>\n      &GetCreatedActors() const;\n </s> remove   // The backend storage is reliable in the future, so the status must be ok.\n  RAY_CHECK_OK(gcs_actor_table_.Put(\n      actor->GetActorID(), actor->GetActorTableData(), [this, actor](Status status) {\n        RAY_CHECK_OK(status);\n        RAY_CHECK_OK(gcs_pub_sub_->Publish(ACTOR_CHANNEL, actor->GetActorID().Hex(),\n                                           actor->GetActorTableData().SerializeAsString(),\n                                           nullptr));\n        // There is no promise that the node the\n        // actor tied to is still alive as the\n        // flush is asynchronously, so just\n        // invoke `Schedule` which will lease\n        // worker directly if the node is still\n        // available or select a new one if not.\n        Schedule(actor);\n      }));\n </s> add   RAY_CHECK(node_to_actors_when_leasing_[actor->GetNodeID()]\n                .emplace(actor->GetActorID())\n                .second);\n\n  // Lease worker directly from the node.\n  LeaseWorkerFromNode(actor, node);\n}\n\nvoid GcsActorScheduler::Reschedule(std::shared_ptr<GcsActor> actor) {\n  if (!actor->GetWorkerID().IsNil()) {\n    RAY_LOG(INFO)\n        << \"Actor \" << actor->GetActorID()\n        << \" is already tied to a leased worker. Create actor directly on worker.\";\n    auto leased_worker = std::make_shared<GcsLeasedWorker>(\n        actor->GetAddress(),\n        VectorFromProtobuf(actor->GetMutableActorTableData()->resource_mapping()),\n        actor->GetActorID());\n    auto iter_node = node_to_workers_when_creating_.find(actor->GetNodeID());\n    if (iter_node != node_to_workers_when_creating_.end()) {\n      if (0 == iter_node->second.count(leased_worker->GetWorkerID())) {\n        iter_node->second.emplace(leased_worker->GetWorkerID(), leased_worker);\n      }\n    } else {\n      node_to_workers_when_creating_[actor->GetNodeID()].emplace(\n          leased_worker->GetWorkerID(), leased_worker);\n    }\n    CreateActorOnWorker(actor, leased_worker);\n  } else {\n    Schedule(actor);\n  } </s> add   // If GCS server is restarted after sending an actor creation task to this core worker,\n  // the restarted GCS server will send the same actor creation task to the core worker\n  // again. We just need to ignore it and reply ok.\n  if (task_spec.IsActorCreationTask() &&\n      worker_context_.GetCurrentActorID() == task_spec.ActorCreationId()) {\n    send_reply_callback(Status::OK(), nullptr, nullptr);\n    RAY_LOG(INFO) << \"Ignoring duplicate actor creation task for actor \"\n                  << task_spec.ActorCreationId()\n                  << \". This is likely due to a GCS server restart.\";\n    return;\n  }\n </s> remove   // Invoke all callbacks for all registration requests of this actor (duplicated\n  // requests are included) and remove all of them from actor_to_register_callbacks_.\n  auto iter = actor_to_register_callbacks_.find(actor_id);\n  if (iter != actor_to_register_callbacks_.end()) {\n    for (auto &callback : iter->second) {\n      callback(actor);\n    }\n    actor_to_register_callbacks_.erase(iter);\n  }\n </s> add         // Invoke all callbacks for all registration requests of this actor (duplicated\n        // requests are included) and remove all of them from\n        // actor_to_register_callbacks_.\n        auto iter = actor_to_register_callbacks_.find(actor_id);\n        if (iter != actor_to_register_callbacks_.end()) {\n          for (auto &callback : iter->second) {\n            callback(actor);\n          }\n          actor_to_register_callbacks_.erase(iter);\n        } </s> remove   // Update the address of the actor as it is tied to a new node.\n </s> add   // Update the address of the actor as it is tied to a node.", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/protobuf/gcs.proto"}
{"docstring_tokens": "keep keep keep keep replace replace replace replace keep keep keep keep keep", "code_tokens": " <mask>   const TaskID &task_id = spec.TaskId();\n <mask>   RAY_LOG(DEBUG) << \"Submitting task: \" << task.DebugString();\n <mask> \n <mask>   if (local_queues_.HasTask(task_id)) {\n <mask>     RAY_LOG(WARNING) << \"Submitted task \" << task_id\n <mask>                      << \" is already queued and will not be restarted. This is most \"\n <mask>                         \"likely due to spurious reconstruction.\";\n <mask>     return;\n <mask>   }\n <mask> \n <mask>   if (spec.IsActorTask()) {\n <mask>     // Check whether we know the location of the actor.\n <mask>     const auto actor_entry = actor_registry_.find(spec.ActorId());\n </s> GCS server error handling for actor creation (#8899) </s> remove     RAY_LOG(INFO) << \"Worker \" << worker_id << \" on node \" << node_id\n                  << \" failed, restarting actor \" << actor_id;\n </s> add     RAY_LOG(WARNING) << \"Worker \" << worker_id << \" on node \" << node_id\n                     << \" failed, restarting actor \" << actor_id; </s> add     RAY_LOG(DEBUG) << \"The number of registered actors is \" << registered_actors_.size()\n                   << \", and the number of created actors is \" << created_actors_.size();\n    for (auto &item : registered_actors_) {\n      auto &actor = item.second;\n      if (actor->GetState() != ray::rpc::ActorTableData::ALIVE) {\n        RAY_LOG(DEBUG) << \"Rescheduling a non-alive actor, actor id = \"\n                       << actor->GetActorID() << \", state = \" << actor->GetState();\n        gcs_actor_scheduler_->Reschedule(actor);\n      }\n    }\n </s> remove   RAY_LOG(INFO) << \"Node \" << node_id << \" failed, reconstructing actors\";\n </s> add   RAY_LOG(WARNING) << \"Node \" << node_id << \" failed, reconstructing actors.\"; </s> add   // If GCS server is restarted after sending an actor creation task to this core worker,\n  // the restarted GCS server will send the same actor creation task to the core worker\n  // again. We just need to ignore it and reply ok.\n  if (task_spec.IsActorCreationTask() &&\n      worker_context_.GetCurrentActorID() == task_spec.ActorCreationId()) {\n    send_reply_callback(Status::OK(), nullptr, nullptr);\n    RAY_LOG(INFO) << \"Ignoring duplicate actor creation task for actor \"\n                  << task_spec.ActorCreationId()\n                  << \". This is likely due to a GCS server restart.\";\n    return;\n  }\n </s> add   RAY_LOG(DEBUG) << \"Executing task, task info = \" << task_spec.DebugString(); </s> add   RAY_LOG(INFO) << \"Removing node, node id = \" << node_id;", "html_url": "https://github.com/ray-project/ray/commit/8fcfcc41005c1bdf046ae6866101990db159d35a", "file_name": "src/ray/raylet/node_manager.cc"}
{"docstring_tokens": "keep add keep keep keep keep keep", "code_tokens": " <mask> )\n <mask> \n <mask> py_test(\n <mask>     name = \"test_backend_worker\",\n <mask>     size = \"small\",\n <mask>     srcs = serve_tests_srcs,\n <mask>     tags = [\"exclusive\"],\n </s> [serve] Refactor to add basic unit tests for BackendState (#14740) </s> remove     size = \"small\",\n </s> add     size = \"medium\", </s> remove         self._actor_handle = None\n        self._placement_group = None\n        self._start_time = None\n        self._prev_slow_startup_warning_time = None\n </s> add  </s> remove         self._state = ReplicaState.SHOULD_START\n </s> add         self._stopped = False </s> add         self._controller_name = controller_name </s> remove         self._controller_name = controller_name\n </s> add  </s> remove         self._version = version\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/91308b9b52960a1ffe4106e64c28d4a0284e93c7", "file_name": "python/ray/serve/BUILD"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> \n <mask> \n <mask> py_test(\n <mask>     name = \"test_handle\",\n <mask>     size = \"small\",\n <mask>     srcs = serve_tests_srcs,\n <mask>     tags = [\"exclusive\"],\n <mask>     deps = [\":serve_lib\"],\n <mask> )\n <mask> \n </s> [serve] Refactor to add basic unit tests for BackendState (#14740) </s> add py_test(\n    name = \"test_backend_state\",\n    size = \"small\",\n    srcs = serve_tests_srcs,\n    tags = [\"exclusive\"],\n    deps = [\":serve_lib\"],\n)\n </s> remove         self._actor_handle = None\n        self._placement_group = None\n        self._start_time = None\n        self._prev_slow_startup_warning_time = None\n </s> add  </s> remove         self._state = ReplicaState.SHOULD_START\n </s> add         self._stopped = False </s> add         self._controller_name = controller_name </s> remove         self._controller_name = controller_name\n </s> add  </s> remove         self._version = version\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/91308b9b52960a1ffe4106e64c28d4a0284e93c7", "file_name": "python/ray/serve/BUILD"}
{"docstring_tokens": "keep keep replace keep keep keep keep keep", "code_tokens": " <mask> import asyncio\n <mask> import time\n <mask> from typing import Dict, List, Optional\n <mask> from uuid import uuid4\n <mask> \n <mask> from ray.serve.common import GoalId\n <mask> from ray.serve.utils import logger\n <mask> \n </s> [serve] Refactor to add basic unit tests for BackendState (#14740) </s> remove from typing import Dict, List, Optional, Tuple\n </s> add from typing import Any, Dict, List, Optional, Tuple </s> remove import asyncio\n </s> add  </s> remove     assert backend_state._completed_goals() == [result_uuid]\n </s> add     # TODO(edoakes): can we remove this extra update period for completing it?\n    assert replica.cleaned_up\n    backend_state.update()\n    assert goal_manager.check_complete(delete_goal) </s> add         self._controller_name = controller_name </s> remove     size = \"small\",\n </s> add     size = \"medium\", </s> remove     def get_pending_goal_ids(self) -> List[GoalId]:\n        return list(self._pending_goals.keys())\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/91308b9b52960a1ffe4106e64c28d4a0284e93c7", "file_name": "python/ray/serve/async_goal_manager.py"}
{"docstring_tokens": "keep keep keep keep replace replace replace keep keep keep keep keep", "code_tokens": " <mask> class AsyncGoalManager:\n <mask>     def __init__(self):\n <mask>         self._pending_goals: Dict[GoalId, asyncio.Event] = dict()\n <mask> \n <mask>     def get_pending_goal_ids(self) -> List[GoalId]:\n <mask>         return list(self._pending_goals.keys())\n <mask> \n <mask>     def num_pending_goals(self) -> int:\n <mask>         return len(self._pending_goals)\n <mask> \n <mask>     def create_goal(self, goal_id: Optional[GoalId] = None) -> GoalId:\n <mask>         if goal_id is None:\n </s> [serve] Refactor to add basic unit tests for BackendState (#14740) </s> add     def check_complete(self, goal_id: GoalId) -> bool:\n        return goal_id not in self._pending_goals\n </s> remove     def __set_state__(self, d):\n </s> add     def __set_state__(self, d: Dict[Any, Any]) -> None: </s> remove     def __get_state__(self):\n </s> add         # Storing the handles is necessary to keep the actor and PG alive in\n        # the non-detached case.\n        self._actor_handle = None\n        self._placement_group = None\n\n    def __get_state__(self) -> Dict[Any, Any]: </s> remove         del clean_dict[\"_placement_group\"]\n        del clean_dict[\"_actor_handle\"]\n </s> add  </s> remove class BackendReplica:\n    def __init__(self, controller_name: str, detached: bool,\n                 replica_tag: ReplicaTag, backend_tag: BackendTag,\n                 version: str):\n        self._actor_name = format_actor_name(replica_tag, controller_name)\n </s> add class ActorReplicaWrapper:\n    \"\"\"Wraps a Ray actor for a backend replica.\n\n    This is primarily defined so that we can mock out actual Ray operations\n    for unit testing.\n\n    *All Ray API calls should be made here, not in BackendState.*\n    \"\"\"\n\n    def __init__(self, actor_name: str, detached: bool, controller_name: str,\n                 replica_tag: ReplicaTag, backend_tag: BackendTag):\n        self._actor_name = actor_name </s> remove     backend_state.backend_replicas[b1] = {i: i for i in range(1)}\n </s> add     # Once the replica is done stopping, it should be removed.\n    mock_replicas[0].set_done_stopping()\n    replica = mock_replicas[0]\n    backend_state.update()\n    assert replica_count(backend_state) == 0", "html_url": "https://github.com/ray-project/ray/commit/91308b9b52960a1ffe4106e64c28d4a0284e93c7", "file_name": "python/ray/serve/async_goal_manager.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask>             event.set()\n <mask> \n <mask>     async def wait_for_goal(self, goal_id: GoalId) -> None:\n <mask>         start = time.time()\n <mask>         if goal_id not in self._pending_goals:\n <mask>             logger.debug(f\"Goal {goal_id} not found\")\n </s> [serve] Refactor to add basic unit tests for BackendState (#14740) </s> remove     def get_pending_goal_ids(self) -> List[GoalId]:\n        return list(self._pending_goals.keys())\n\n </s> add  </s> remove     def __set_state__(self, d):\n </s> add     def __set_state__(self, d: Dict[Any, Any]) -> None: </s> remove def test_completed_goals_created_backend(mock_backend_state_inputs):\n    backend_state = mock_backend_state_inputs[0]\n    assert len(backend_state._completed_goals()) == 0\n </s> add     # force_stop shouldn't be called until after the timer.\n    assert not mock_replicas[0].force_stopped_counter\n    assert not mock_replicas[0].cleaned_up\n    assert replica_count(backend_state) == 1\n    assert replica_count(backend_state, states=[ReplicaState.STOPPING]) == 1 </s> remove     assert len(backend_state._completed_goals()) == 0\n </s> add     # Force stop should be called repeatedly until the replica stops.\n    backend_state.update()\n    assert mock_replicas[0].force_stopped_counter == 2\n    assert not mock_replicas[0].cleaned_up\n    assert replica_count(backend_state) == 1\n    assert replica_count(backend_state, states=[ReplicaState.STOPPING]) == 1\n    assert not goal_manager.check_complete(delete_goal) </s> remove     b1 = \"backend_one\"\n    backend_state.backends[b1] = generate_mock_backend_info()\n    result_uuid = uuid4()\n    backend_state.backend_goals[b1] = result_uuid\n </s> add     # Advance the timer, now the replica should be force stopped.\n    timer.advance(grace_period_s + 0.1)\n    backend_state.update()\n    assert mock_replicas[0].force_stopped_counter == 1\n    assert not mock_replicas[0].cleaned_up\n    assert replica_count(backend_state) == 1\n    assert replica_count(backend_state, states=[ReplicaState.STOPPING]) == 1\n    assert not goal_manager.check_complete(delete_goal) </s> remove     def __get_state__(self):\n </s> add         # Storing the handles is necessary to keep the actor and PG alive in\n        # the non-detached case.\n        self._actor_handle = None\n        self._placement_group = None\n\n    def __get_state__(self) -> Dict[Any, Any]:", "html_url": "https://github.com/ray-project/ray/commit/91308b9b52960a1ffe4106e64c28d4a0284e93c7", "file_name": "python/ray/serve/async_goal_manager.py"}
{"docstring_tokens": "replace keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask> import asyncio\n <mask> from collections import defaultdict\n <mask> from enum import Enum\n <mask> import time\n <mask> from typing import Dict, List, Optional, Tuple\n <mask> \n <mask> import ray\n <mask> import ray.cloudpickle as pickle\n <mask> from ray.actor import ActorHandle\n <mask> from ray.serve.async_goal_manager import AsyncGoalManager\n </s> [serve] Refactor to add basic unit tests for BackendState (#14740) </s> remove from typing import Dict, List, Optional\n </s> add from typing import Dict, Optional </s> remove     assert backend_state._completed_goals() == [result_uuid]\n </s> add     # TODO(edoakes): can we remove this extra update period for completing it?\n    assert replica.cleaned_up\n    backend_state.update()\n    assert goal_manager.check_complete(delete_goal) </s> add         self._controller_name = controller_name </s> remove     size = \"small\",\n </s> add     size = \"medium\", </s> remove     def get_pending_goal_ids(self) -> List[GoalId]:\n        return list(self._pending_goals.keys())\n\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/91308b9b52960a1ffe4106e64c28d4a0284e93c7", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep keep replace replace replace replace replace keep replace keep", "code_tokens": " <mask>     STOPPED = 6\n <mask> \n <mask> \n <mask> class BackendReplica:\n <mask>     def __init__(self, controller_name: str, detached: bool,\n <mask>                  replica_tag: ReplicaTag, backend_tag: BackendTag,\n <mask>                  version: str):\n <mask>         self._actor_name = format_actor_name(replica_tag, controller_name)\n <mask>         self._placement_group_name = self._actor_name + \"_placement_group\"\n <mask>         self._controller_name = controller_name\n <mask>         self._detached = detached\n </s> [serve] Refactor to add basic unit tests for BackendState (#14740) </s> add         self._controller_name = controller_name </s> remove         self._actor_handle = None\n        self._placement_group = None\n        self._start_time = None\n        self._prev_slow_startup_warning_time = None\n </s> add  </s> remove     def get_pending_goal_ids(self) -> List[GoalId]:\n        return list(self._pending_goals.keys())\n\n </s> add  </s> remove         self._state = ReplicaState.SHOULD_START\n </s> add         self._stopped = False </s> remove     b1 = \"backend_one\"\n    backend_state.backends[b1] = generate_mock_backend_info()\n    result_uuid = uuid4()\n    backend_state.backend_goals[b1] = result_uuid\n </s> add     # Advance the timer, now the replica should be force stopped.\n    timer.advance(grace_period_s + 0.1)\n    backend_state.update()\n    assert mock_replicas[0].force_stopped_counter == 1\n    assert not mock_replicas[0].cleaned_up\n    assert replica_count(backend_state) == 1\n    assert replica_count(backend_state, states=[ReplicaState.STOPPING]) == 1\n    assert not goal_manager.check_complete(delete_goal)", "html_url": "https://github.com/ray-project/ray/commit/91308b9b52960a1ffe4106e64c28d4a0284e93c7", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep keep add keep keep keep keep keep", "code_tokens": " <mask>                  replica_tag: ReplicaTag, backend_tag: BackendTag):\n <mask>         self._actor_name = actor_name\n <mask>         self._placement_group_name = self._actor_name + \"_placement_group\"\n <mask>         self._detached = detached\n <mask>         self._replica_tag = replica_tag\n <mask>         self._backend_tag = backend_tag\n <mask> \n <mask>         self._startup_obj_ref = None\n <mask>         self._drain_obj_ref = None\n </s> [serve] Refactor to add basic unit tests for BackendState (#14740) </s> remove         self._controller_name = controller_name\n </s> add  </s> remove class BackendReplica:\n    def __init__(self, controller_name: str, detached: bool,\n                 replica_tag: ReplicaTag, backend_tag: BackendTag,\n                 version: str):\n        self._actor_name = format_actor_name(replica_tag, controller_name)\n </s> add class ActorReplicaWrapper:\n    \"\"\"Wraps a Ray actor for a backend replica.\n\n    This is primarily defined so that we can mock out actual Ray operations\n    for unit testing.\n\n    *All Ray API calls should be made here, not in BackendState.*\n    \"\"\"\n\n    def __init__(self, actor_name: str, detached: bool, controller_name: str,\n                 replica_tag: ReplicaTag, backend_tag: BackendTag):\n        self._actor_name = actor_name </s> remove         self._actor_handle = None\n        self._placement_group = None\n        self._start_time = None\n        self._prev_slow_startup_warning_time = None\n </s> add  </s> remove         self._state = ReplicaState.SHOULD_START\n </s> add         self._stopped = False </s> remove         self._version = version\n </s> add  </s> remove     def __set_state__(self, d):\n </s> add     def __set_state__(self, d: Dict[Any, Any]) -> None:", "html_url": "https://github.com/ray-project/ray/commit/91308b9b52960a1ffe4106e64c28d4a0284e93c7", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep replace replace replace replace keep keep replace keep keep", "code_tokens": " <mask>         self._backend_tag = backend_tag\n <mask>         self._actor_handle = None\n <mask>         self._placement_group = None\n <mask>         self._start_time = None\n <mask>         self._prev_slow_startup_warning_time = None\n <mask>         self._startup_obj_ref = None\n <mask>         self._drain_obj_ref = None\n <mask>         self._state = ReplicaState.SHOULD_START\n <mask>         self._actor_resources = None\n <mask>         self._version = version\n </s> [serve] Refactor to add basic unit tests for BackendState (#14740) </s> remove         self._version = version\n </s> add  </s> remove     def __get_state__(self):\n </s> add         # Storing the handles is necessary to keep the actor and PG alive in\n        # the non-detached case.\n        self._actor_handle = None\n        self._placement_group = None\n\n    def __get_state__(self) -> Dict[Any, Any]: </s> remove         self._controller_name = controller_name\n </s> add  </s> add         self._controller_name = controller_name </s> remove     def __set_state__(self, d):\n </s> add     def __set_state__(self, d: Dict[Any, Any]) -> None:", "html_url": "https://github.com/ray-project/ray/commit/91308b9b52960a1ffe4106e64c28d4a0284e93c7", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep keep keep replace keep replace keep keep keep", "code_tokens": " <mask>         self._startup_obj_ref = None\n <mask>         self._drain_obj_ref = None\n <mask>         self._state = ReplicaState.SHOULD_START\n <mask>         self._actor_resources = None\n <mask>         self._version = version\n <mask> \n <mask>     def __get_state__(self):\n <mask>         clean_dict = self.__dict__.copy()\n <mask>         del clean_dict[\"_placement_group\"]\n <mask>         del clean_dict[\"_actor_handle\"]\n </s> [serve] Refactor to add basic unit tests for BackendState (#14740) </s> remove         self._state = ReplicaState.SHOULD_START\n </s> add         self._stopped = False </s> remove         del clean_dict[\"_placement_group\"]\n        del clean_dict[\"_actor_handle\"]\n </s> add  </s> remove         self._actor_handle = None\n        self._placement_group = None\n        self._start_time = None\n        self._prev_slow_startup_warning_time = None\n </s> add  </s> remove     def __set_state__(self, d):\n </s> add     def __set_state__(self, d: Dict[Any, Any]) -> None: </s> add         self._controller_name = controller_name", "html_url": "https://github.com/ray-project/ray/commit/91308b9b52960a1ffe4106e64c28d4a0284e93c7", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep replace replace keep keep keep keep replace keep keep keep keep", "code_tokens": " <mask>     def __get_state__(self):\n <mask>         clean_dict = self.__dict__.copy()\n <mask>         del clean_dict[\"_placement_group\"]\n <mask>         del clean_dict[\"_actor_handle\"]\n <mask>         del clean_dict[\"_startup_obj_ref\"]\n <mask>         del clean_dict[\"_drain_obj_ref\"]\n <mask>         return clean_dict\n <mask> \n <mask>     def __set_state__(self, d):\n <mask>         self.__dict__ = d\n <mask>         self._actor_handle = None\n <mask>         self._startup_obj_ref = None\n <mask>         self._drain_obj_ref = None\n </s> [serve] Refactor to add basic unit tests for BackendState (#14740) </s> remove         self._version = version\n </s> add  </s> remove     def __get_state__(self):\n </s> add         # Storing the handles is necessary to keep the actor and PG alive in\n        # the non-detached case.\n        self._actor_handle = None\n        self._placement_group = None\n\n    def __get_state__(self) -> Dict[Any, Any]: </s> remove         self._state = ReplicaState.SHOULD_START\n </s> add         self._stopped = False </s> remove         self._actor_handle = None\n        self._placement_group = None\n        self._start_time = None\n        self._prev_slow_startup_warning_time = None\n </s> add  </s> add         self._controller_name = controller_name", "html_url": "https://github.com/ray-project/ray/commit/91308b9b52960a1ffe4106e64c28d4a0284e93c7", "file_name": "python/ray/serve/backend_state.py"}
{"docstring_tokens": "keep keep replace replace replace keep replace replace replace replace keep keep keep keep", "code_tokens": " <mask> \n <mask> \n <mask> def test_completed_goals_created_backend(mock_backend_state_inputs):\n <mask>     backend_state = mock_backend_state_inputs[0]\n <mask>     assert len(backend_state._completed_goals()) == 0\n <mask> \n <mask>     b1 = \"backend_one\"\n <mask>     backend_state.backends[b1] = generate_mock_backend_info()\n <mask>     result_uuid = uuid4()\n <mask>     backend_state.backend_goals[b1] = result_uuid\n <mask> \n <mask>     assert len(backend_state._completed_goals()) == 0\n <mask> \n <mask>     backend_state.backend_replicas[b1] = {i: i for i in range(1)}\n </s> [serve] Refactor to add basic unit tests for BackendState (#14740) </s> remove     assert len(backend_state._completed_goals()) == 0\n </s> add     # Force stop should be called repeatedly until the replica stops.\n    backend_state.update()\n    assert mock_replicas[0].force_stopped_counter == 2\n    assert not mock_replicas[0].cleaned_up\n    assert replica_count(backend_state) == 1\n    assert replica_count(backend_state, states=[ReplicaState.STOPPING]) == 1\n    assert not goal_manager.check_complete(delete_goal) </s> remove     backend_state.backend_replicas[b1] = {i: i for i in range(1)}\n </s> add     # Once the replica is done stopping, it should be removed.\n    mock_replicas[0].set_done_stopping()\n    replica = mock_replicas[0]\n    backend_state.update()\n    assert replica_count(backend_state) == 0 </s> remove     assert backend_state._completed_goals() == [result_uuid]\n </s> add     # TODO(edoakes): can we remove this extra update period for completing it?\n    assert replica.cleaned_up\n    backend_state.update()\n    assert goal_manager.check_complete(delete_goal) </s> remove class BackendReplica:\n    def __init__(self, controller_name: str, detached: bool,\n                 replica_tag: ReplicaTag, backend_tag: BackendTag,\n                 version: str):\n        self._actor_name = format_actor_name(replica_tag, controller_name)\n </s> add class ActorReplicaWrapper:\n    \"\"\"Wraps a Ray actor for a backend replica.\n\n    This is primarily defined so that we can mock out actual Ray operations\n    for unit testing.\n\n    *All Ray API calls should be made here, not in BackendState.*\n    \"\"\"\n\n    def __init__(self, actor_name: str, detached: bool, controller_name: str,\n                 replica_tag: ReplicaTag, backend_tag: BackendTag):\n        self._actor_name = actor_name </s> remove         self._actor_handle = None\n        self._placement_group = None\n        self._start_time = None\n        self._prev_slow_startup_warning_time = None\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/91308b9b52960a1ffe4106e64c28d4a0284e93c7", "file_name": "python/ray/serve/tests/test_backend_state.py"}
{"docstring_tokens": "keep keep keep keep replace keep replace keep keep keep", "code_tokens": " <mask>     backend_state.backends[b1] = generate_mock_backend_info()\n <mask>     result_uuid = uuid4()\n <mask>     backend_state.backend_goals[b1] = result_uuid\n <mask> \n <mask>     assert len(backend_state._completed_goals()) == 0\n <mask> \n <mask>     backend_state.backend_replicas[b1] = {i: i for i in range(1)}\n <mask> \n <mask>     assert backend_state._completed_goals() == [result_uuid]\n <mask> \n </s> [serve] Refactor to add basic unit tests for BackendState (#14740) </s> remove def test_completed_goals_created_backend(mock_backend_state_inputs):\n    backend_state = mock_backend_state_inputs[0]\n    assert len(backend_state._completed_goals()) == 0\n </s> add     # force_stop shouldn't be called until after the timer.\n    assert not mock_replicas[0].force_stopped_counter\n    assert not mock_replicas[0].cleaned_up\n    assert replica_count(backend_state) == 1\n    assert replica_count(backend_state, states=[ReplicaState.STOPPING]) == 1 </s> remove     b1 = \"backend_one\"\n    backend_state.backends[b1] = generate_mock_backend_info()\n    result_uuid = uuid4()\n    backend_state.backend_goals[b1] = result_uuid\n </s> add     # Advance the timer, now the replica should be force stopped.\n    timer.advance(grace_period_s + 0.1)\n    backend_state.update()\n    assert mock_replicas[0].force_stopped_counter == 1\n    assert not mock_replicas[0].cleaned_up\n    assert replica_count(backend_state) == 1\n    assert replica_count(backend_state, states=[ReplicaState.STOPPING]) == 1\n    assert not goal_manager.check_complete(delete_goal) </s> remove     assert backend_state._completed_goals() == [result_uuid]\n </s> add     # TODO(edoakes): can we remove this extra update period for completing it?\n    assert replica.cleaned_up\n    backend_state.update()\n    assert goal_manager.check_complete(delete_goal) </s> remove class BackendReplica:\n    def __init__(self, controller_name: str, detached: bool,\n                 replica_tag: ReplicaTag, backend_tag: BackendTag,\n                 version: str):\n        self._actor_name = format_actor_name(replica_tag, controller_name)\n </s> add class ActorReplicaWrapper:\n    \"\"\"Wraps a Ray actor for a backend replica.\n\n    This is primarily defined so that we can mock out actual Ray operations\n    for unit testing.\n\n    *All Ray API calls should be made here, not in BackendState.*\n    \"\"\"\n\n    def __init__(self, actor_name: str, detached: bool, controller_name: str,\n                 replica_tag: ReplicaTag, backend_tag: BackendTag):\n        self._actor_name = actor_name </s> remove         self._actor_handle = None\n        self._placement_group = None\n        self._start_time = None\n        self._prev_slow_startup_warning_time = None\n </s> add ", "html_url": "https://github.com/ray-project/ray/commit/91308b9b52960a1ffe4106e64c28d4a0284e93c7", "file_name": "python/ray/serve/tests/test_backend_state.py"}
{"docstring_tokens": "keep keep keep keep replace keep keep keep keep keep", "code_tokens": " <mask>     assert len(backend_state._completed_goals()) == 0\n <mask> \n <mask>     backend_state.backend_replicas[b1] = {i: i for i in range(1)}\n <mask> \n <mask>     assert backend_state._completed_goals() == [result_uuid]\n <mask> \n <mask> \n <mask> if __name__ == \"__main__\":\n <mask>     import sys\n <mask>     sys.exit(pytest.main([\"-v\", \"-s\", __file__]))\n </s> [serve] Refactor to add basic unit tests for BackendState (#14740) </s> remove     backend_state.backend_replicas[b1] = {i: i for i in range(1)}\n </s> add     # Once the replica is done stopping, it should be removed.\n    mock_replicas[0].set_done_stopping()\n    replica = mock_replicas[0]\n    backend_state.update()\n    assert replica_count(backend_state) == 0 </s> remove     assert len(backend_state._completed_goals()) == 0\n </s> add     # Force stop should be called repeatedly until the replica stops.\n    backend_state.update()\n    assert mock_replicas[0].force_stopped_counter == 2\n    assert not mock_replicas[0].cleaned_up\n    assert replica_count(backend_state) == 1\n    assert replica_count(backend_state, states=[ReplicaState.STOPPING]) == 1\n    assert not goal_manager.check_complete(delete_goal) </s> remove def test_completed_goals_created_backend(mock_backend_state_inputs):\n    backend_state = mock_backend_state_inputs[0]\n    assert len(backend_state._completed_goals()) == 0\n </s> add     # force_stop shouldn't be called until after the timer.\n    assert not mock_replicas[0].force_stopped_counter\n    assert not mock_replicas[0].cleaned_up\n    assert replica_count(backend_state) == 1\n    assert replica_count(backend_state, states=[ReplicaState.STOPPING]) == 1 </s> remove     b1 = \"backend_one\"\n    backend_state.backends[b1] = generate_mock_backend_info()\n    result_uuid = uuid4()\n    backend_state.backend_goals[b1] = result_uuid\n </s> add     # Advance the timer, now the replica should be force stopped.\n    timer.advance(grace_period_s + 0.1)\n    backend_state.update()\n    assert mock_replicas[0].force_stopped_counter == 1\n    assert not mock_replicas[0].cleaned_up\n    assert replica_count(backend_state) == 1\n    assert replica_count(backend_state, states=[ReplicaState.STOPPING]) == 1\n    assert not goal_manager.check_complete(delete_goal) </s> remove from typing import Dict, List, Optional, Tuple\n </s> add from typing import Any, Dict, List, Optional, Tuple </s> remove from typing import Dict, List, Optional\n </s> add from typing import Dict, Optional", "html_url": "https://github.com/ray-project/ray/commit/91308b9b52960a1ffe4106e64c28d4a0284e93c7", "file_name": "python/ray/serve/tests/test_backend_state.py"}
{"docstring_tokens": "keep add keep keep keep keep", "code_tokens": " <mask>             redis_password,\n <mask>             session_dir,\n <mask>         )\n <mask>     else:\n <mask>         java_worker_command = []\n <mask> \n </s> [core] Java worker should respect the user provided node_ip_address (#13732) </s> add     if node_ip_address is not None:\n        pairs.append((\"ray.node-ip\", node_ip_address))\n </s> add         node_ip_address (str): The ip address for this node. </s> remove                               raylet_name, redis_password, session_dir):\n </s> add                               raylet_name, redis_password, session_dir,\n                              node_ip_address):", "html_url": "https://github.com/ray-project/ray/commit/918ad84f08ee3e019e79d049442fc509777abac0", "file_name": "python/ray/_private/services.py"}
